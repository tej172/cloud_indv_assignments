{"Hello, how are you? What is the exact model are you using?": "Hello! I'm doing well, thank you for asking. I'm functioning as expected and ready to help.\n\nI am a large language model, trained by Google.", "\n        For the project `NER-and-Question-Classification-with-Word2Vec`:\n\n        Codebase Context:\n        \n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        \n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```": "```yaml\n- name: Word Embeddings (Word2Vec)\n  description: |\n    This represents words as dense numerical vectors. Instead of just IDs, these vectors capture semantic meaning, so words used in similar contexts (like 'king' and 'queen') have similar vectors. Word2Vec is the specific algorithm used here to create these vectors from a large text corpus.\n    Think of it like giving each word a unique coordinate on a 'meaning map'. Words with similar meanings are neighbours on this map, helping the models understand context and relationships.\n  file_indices:\n    - 0 # src/embeddings/word2vec_utils.py # Likely loads/trains Word2Vec\n    - 1 # config/embeddings.yaml # Configuration for embeddings path/parameters\n    - 2 # notebooks/Explore_Embeddings.ipynb # Visualization or testing of embeddings\n\n- name: Data Preprocessing & Loading\n  description: |\n    This involves preparing the raw text data (sentences, questions) for the models. Tasks include cleaning text (removing noise), tokenization (splitting text into words or sub-words), converting tokens to IDs, potentially padding sequences to equal lengths, and loading data in batches.\n    It's like a kitchen prep station: washing, chopping, and measuring ingredients (text data) before the chef (the model) can start cooking (training).\n  file_indices:\n    - 3 # src/data/loader.py # Loads datasets (NER, QC)\n    - 4 # src/data/preprocess.py # Contains cleaning and tokenization logic\n    - 5 # src/data/utils.py # Helper functions for data handling\n\n- name: Named Entity Recognition (NER) Model\n  description: |\n    The core component responsible for identifying and classifying named entities (like persons, locations, organizations, dates) within unstructured text. It typically takes word embeddings as input and predicts a tag (e.g., B-PER, I-LOC, O) for each word.\n    It's like an automated highlighter that scans text and marks specific categories of important nouns (entities) with different colors based on their type.\n  file_indices:\n    - 6 # src/ner/model.py # Defines the neural network architecture for NER\n    - 7 # src/ner/train_ner.py # Script for training the NER model\n    - 8 # src/ner/predictor.py # Logic to use a trained NER model for predictions\n\n- name: Question Classification Model\n  description: |\n    This model analyzes an input question and assigns it to one of several predefined categories (e.g., 'HUM' for human, 'LOC' for location, 'NUM' for numeric, 'DESC' for description). This helps understand the *intent* behind the question.\n    Imagine it as a librarian sorting incoming query slips into different bins based on the *type* of information requested (person, place, definition, etc.).\n  file_indices:\n    - 9 # src/qc/model.py # Defines the neural network architecture for QC\n    - 10 # src/qc/train_qc.py # Script for training the Question Classification model\n    - 11 # src/qc/classifier.py # Logic to use a trained QC model for predictions\n\n- name: Model Training Pipeline\n  description: |\n    Orchestrates the process of training the NER and/or Question Classification models. This usually involves loading data, feeding it to the model in batches, calculating loss, performing backpropagation to update model weights, and periodically evaluating performance on a validation set.\n    It's like a structured workout plan for the models, guiding them through exercises (data batches) and tracking their progress (loss/accuracy) to help them learn effectively.\n  file_indices:\n    - 7 # src/ner/train_ner.py\n    - 10 # src/qc/train_qc.py\n    - 12 # src/training/trainer_base.py # A potential base class for training loops\n    - 13 # src/utils/optimization.py # Optimizers like Adam, learning rate schedulers\n\n- name: Inference / Prediction Service\n  description: |\n    This component takes new, unseen text (for NER) or questions (for QC) as input and uses the already trained models to generate outputs (entity tags or question categories). It encapsulates the loading of trained models and the prediction logic.\n    Think of it as the operational front desk where the fully trained staff (models) handle new customer requests (input text/questions) based on their learned skills.\n  file_indices:\n    - 8 # src/ner/predictor.py\n    - 11 # src/qc/classifier.py\n    - 14 # api/app.py # Potential API endpoint using the predictors\n    - 15 # predict.py # A command-line script for making predictions\n\n- name: Configuration Management\n  description: |\n    Handles loading and providing access to project settings and hyperparameters. This includes file paths (datasets, embeddings, saved models), model architecture choices, training parameters (learning rate, batch size), etc., often stored in YAML or JSON files.\n    It's like the central control panel or settings menu for the entire application, allowing easy adjustment of parameters without changing the core code logic.\n  file_indices:\n    - 1 # config/embeddings.yaml\n    - 16 # config/ner_params.yaml\n    - 17 # config/qc_params.yaml\n    - 18 # src/utils/config_loader.py # Utility to load and parse config files\n\n- name: Evaluation Framework\n  description: |\n    Provides tools and scripts to measure the performance of the trained NER and QC models on test data. It calculates relevant metrics like accuracy, precision, recall, F1-score for NER (often per-entity type) and accuracy or confusion matrix for QC.\n    This is the quality assurance department, rigorously testing the trained models on unseen data (like a final exam) to objectively measure how well they perform their tasks.\n  file_indices:\n    - 19 # src/evaluation/metrics.py # Functions to calculate performance metrics\n    - 20 # scripts/evaluate_ner.py # Script to run NER evaluation\n    - 21 # scripts/evaluate_qc.py # Script to run QC evaluation\n```", "\n        For the project `python-sdk`:\n\n        Codebase Context:\n        --- File Index 0: examples/clients/simple-chatbot/mcp_simple_chatbot/main.py ---\nimport asyncio\nimport json\nimport logging\nimport os\nimport shutil\nfrom contextlib import AsyncExitStack\nfrom typing import Any\n\nimport httpx\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\n\nclass Configuration:\n    \"\"\"Manages configuration and environment variables for the MCP client.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize configuration with environment variables.\"\"\"\n        self.load_env()\n        self.api_key = os.getenv(\"LLM_API_KEY\")\n\n    @staticmethod\n    def load_env() -> None:\n        \"\"\"Load environment variables from .env file.\"\"\"\n        load_dotenv()\n\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        \"\"\"Load server configuration from JSON file.\n\n        Args:\n            file_path: Path to the JSON configuration file.\n\n        Returns:\n            Dict containing server configuration.\n\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n\n    @property\n    def llm_api_key(self) -> str:\n        \"\"\"Get the LLM API key.\n\n        Returns:\n            The API key as a string.\n\n        Raises:\n            ValueError: If the API key is not found in environment variables.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"LLM_API_KEY not found in environment variables\")\n        return self.api_key\n\n\nclass Server:\n    \"\"\"Manages MCP server connections and tool execution.\"\"\"\n\n    def __init__(self, name: str, config: dict[str, Any]) -> None:\n        self.name: str = name\n        self.config: dict[str, Any] = config\n        self.stdio_context: Any | None = None\n        self.session: ClientSession | None = None\n        self._cleanup_lock: asyncio.Lock = asyncio.Lock()\n        self.exit_stack: AsyncExitStack = AsyncExitStack()\n\n    async def initialize(self) -> None:\n        \"\"\"Initialize the server connection.\"\"\"\n        command = (\n            shutil.which(\"npx\")\n            if self.config[\"command\"] == \"npx\"\n            else self.config[\"command\"]\n        )\n        if command is None:\n            raise ValueError(\"The command must be a valid string and cannot be None.\")\n\n        server_params = StdioServerParameters(\n            command=command,\n            args=self.config[\"args\"],\n            env={**os.environ, **self.config[\"env\"]}\n            if self.config.get(\"env\")\n            else None,\n        )\n        try:\n            stdio_transport = await self.exit_stack.enter_async_context(\n                stdio_client(server_params)\n            )\n            read, write = stdio_transport\n            session = await self.exit_stack.enter_async_context(\n                ClientSession(read, write)\n            )\n            await session.initialize()\n            self.session = session\n        except Exception as e:\n            logging.error(f\"Error initializing server {self.name}: {e}\")\n            await self.cleanup()\n            raise\n\n    async def list_tools(self) -> list[Any]:\n        \"\"\"List available tools from the server.\n\n        Returns:\n            A list of available tools.\n\n        Raises:\n            RuntimeError: If the server is not initialized.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        tools_response = await self.session.list_tools()\n        tools = []\n\n        for item in tools_response:\n            if isinstance(item, tuple) and item[0] == \"tools\":\n                tools.extend(\n                    Tool(tool.name, tool.description, tool.inputSchema)\n                    for tool in item[1]\n                )\n\n        return tools\n\n    async def execute_tool(\n        self,\n        tool_name: str,\n        arguments: dict[str, Any],\n        retries: int = 2,\n        delay: float = 1.0,\n    ) -> Any:\n        \"\"\"Execute a tool with retry mechanism.\n\n        Args:\n            tool_name: Name of the tool to execute.\n            arguments: Tool arguments.\n            retries: Number of retry attempts.\n            delay: Delay between retries in seconds.\n\n        Returns:\n            Tool execution result.\n\n        Raises:\n            RuntimeError: If server is not initialized.\n            Exception: If tool execution fails after all retries.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        attempt = 0\n        while attempt < retries:\n            try:\n                logging.info(f\"Executing {tool_name}...\")\n                result = await self.session.call_tool(tool_name, arguments)\n\n                return result\n\n            except Exception as e:\n                attempt += 1\n                logging.warning(\n                    f\"Error executing tool: {e}. Attempt {attempt} of {retries}.\"\n                )\n                if attempt < retries:\n                    logging.info(f\"Retrying in {delay} seconds...\")\n                    await asyncio.sleep(delay)\n                else:\n                    logging.error(\"Max retries reached. Failing.\")\n                    raise\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up server resources.\"\"\"\n        async with self._cleanup_lock:\n            try:\n                await self.exit_stack.aclose()\n                self.session = None\n                self.stdio_context = None\n            except Exception as e:\n                logging.error(f\"Error during cleanup of server {self.name}: {e}\")\n\n\nclass Tool:\n    \"\"\"Represents a tool with its properties and formatting.\"\"\"\n\n    def __init__(\n        self, name: str, description: str, input_schema: dict[str, Any]\n    ) -> None:\n        self.name: str = name\n        self.description: str = description\n        self.input_schema: dict[str, Any] = input_schema\n\n    def format_for_llm(self) -> str:\n        \"\"\"Format tool information for LLM.\n\n        Returns:\n            A formatted string describing the tool.\n        \"\"\"\n        args_desc = []\n        if \"properties\" in self.input_schema:\n            for param_name, param_info in self.input_schema[\"properties\"].items():\n                arg_desc = (\n                    f\"- {param_name}: {param_info.get('description', 'No description')}\"\n                )\n                if param_name in self.input_schema.get(\"required\", []):\n                    arg_desc += \" (required)\"\n                args_desc.append(arg_desc)\n\n        return f\"\"\"\nTool: {self.name}\nDescription: {self.description}\nArguments:\n{chr(10).join(args_desc)}\n\"\"\"\n\n\nclass LLMClient:\n    \"\"\"Manages communication with the LLM provider.\"\"\"\n\n    def __init__(self, api_key: str) -> None:\n        self.api_key: str = api_key\n\n    def get_response(self, messages: list[dict[str, str]]) -> str:\n        \"\"\"Get a response from the LLM.\n\n        Args:\n            messages: A list of message dictionaries.\n\n        Returns:\n            The LLM's response as a string.\n\n        Raises:\n            httpx.RequestError: If the request to the LLM fails.\n        \"\"\"\n        url = \"https://api.groq.com/openai/v1/chat/completions\"\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n        payload = {\n            \"messages\": messages,\n            \"model\": \"llama-3.2-90b-vision-preview\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096,\n            \"top_p\": 1,\n            \"stream\": False,\n            \"stop\": None,\n        }\n\n        try:\n            with httpx.Client() as client:\n                response = client.post(url, headers=headers, json=payload)\n                response.raise_for_status()\n                data = response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n\n        except httpx.RequestError as e:\n            error_message = f\"Error getting LLM response: {str(e)}\"\n            logging.error(error_message)\n\n            if isinstance(e, httpx.HTTPStatusError):\n                status_code = e.response.status_code\n                logging.error(f\"Status code: {status_code}\")\n                logging.error(f\"Response details: {e.response.text}\")\n\n            return (\n                f\"I encountered an error: {error_message}. \"\n                \"Please try again or rephrase your request.\"\n            )\n\n\nclass ChatSession:\n    \"\"\"Orchestrates the interaction between user, LLM, and tools.\"\"\"\n\n    def __init__(self, servers: list[Server], llm_client: LLMClient) -> None:\n        self.servers: list[Server] = servers\n        self.llm_client: LLMClient = llm_client\n\n    async def cleanup_servers(self) -> None:\n        \"\"\"Clean up all servers properly.\"\"\"\n        cleanup_tasks = [\n            asyncio.create_task(server.cleanup()) for server in self.servers\n        ]\n        if cleanup_tasks:\n            try:\n                await asyncio.gather(*cleanup_tasks, return_exceptions=True)\n            except Exception as e:\n                logging.warning(f\"Warning during final cleanup: {e}\")\n\n    async def process_llm_response(self, llm_response: str) -> str:\n        \"\"\"Process the LLM response and execute tools if needed.\n\n        Args:\n            llm_response: The response from the LLM.\n\n        Returns:\n            The result of tool execution or the original response.\n        \"\"\"\n        import json\n\n        try:\n            tool_call = json.loads(llm_response)\n            if \"tool\" in tool_call and \"arguments\" in tool_call:\n                logging.info(f\"Executing tool: {tool_call['tool']}\")\n                logging.info(f\"With arguments: {tool_call['arguments']}\")\n\n                for server in self.servers:\n                    tools = await server.list_tools()\n                    if any(tool.name == tool_call[\"tool\"] for tool in tools):\n                        try:\n                            result = await server.execute_tool(\n                                tool_call[\"tool\"], tool_call[\"arguments\"]\n                            )\n\n                            if isinstance(result, dict) and \"progress\" in result:\n                                progress = result[\"progress\"]\n                                total = result[\"total\"]\n                                percentage = (progress / total) * 100\n                                logging.info(\n                                    f\"Progress: {progress}/{total} \"\n                                    f\"({percentage:.1f}%)\"\n                                )\n\n                            return f\"Tool execution result: {result}\"\n                        except Exception as e:\n                            error_msg = f\"Error executing tool: {str(e)}\"\n                            logging.error(error_msg)\n                            return error_msg\n\n                return f\"No server found with tool: {tool_call['tool']}\"\n            return llm_response\n        except json.JSONDecodeError:\n            return llm_response\n\n    async def start(self) -> None:\n        \"\"\"Main chat session handler.\"\"\"\n        try:\n            for server in self.servers:\n                try:\n                    await server.initialize()\n                except Exception as e:\n                    logging.error(f\"Failed to initialize server: {e}\")\n                    await self.cleanup_servers()\n                    return\n\n            all_tools = []\n            for server in self.servers:\n                tools = await server.list_tools()\n                all_tools.extend(tools)\n\n            tools_description = \"\\n\".join([tool.format_for_llm() for tool in all_tools])\n\n            system_message = (\n                \"You are a helpful assistant with access to these tools:\\n\\n\"\n                f\"{tools_description}\\n\"\n                \"Choose the appropriate tool based on the user's question. \"\n                \"If no tool is needed, reply directly.\\n\\n\"\n                \"IMPORTANT: When you need to use a tool, you must ONLY respond with \"\n                \"the exact JSON object format below, nothing else:\\n\"\n                \"{\\n\"\n                '    \"tool\": \"tool-name\",\\n'\n                '    \"arguments\": {\\n'\n                '        \"argument-name\": \"value\"\\n'\n                \"    }\\n\"\n                \"}\\n\\n\"\n                \"After receiving a tool's response:\\n\"\n                \"1. Transform the raw data into a natural, conversational response\\n\"\n                \"2. Keep responses concise but informative\\n\"\n                \"3. Focus on the most relevant information\\n\"\n                \"4. Use appropriate context from the user's question\\n\"\n                \"5. Avoid simply repeating the raw data\\n\\n\"\n                \"Please use only the tools that are explicitly defined above.\"\n            )\n\n            messages = [{\"role\": \"system\", \"content\": system_message}]\n\n            while True:\n                try:\n                    user_input = input(\"You: \").strip().lower()\n                    if user_input in [\"quit\", \"exit\"]:\n                        logging.info(\"\\nExiting...\")\n                        break\n\n                    messages.append({\"role\": \"user\", \"content\": user_input})\n\n                    llm_response = self.llm_client.get_response(messages)\n                    logging.info(\"\\nAssistant: %s\", llm_response)\n\n                    result = await self.process_llm_response(llm_response)\n\n                    if result != llm_response:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n                        messages.append({\"role\": \"system\", \"content\": result})\n\n                        final_response = self.llm_client.get_response(messages)\n                        logging.info(\"\\nFinal response: %s\", final_response)\n                        messages.append(\n                            {\"role\": \"assistant\", \"content\": final_response}\n                        )\n                    else:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n\n                except KeyboardInterrupt:\n                    logging.info(\"\\nExiting...\")\n                    break\n\n        finally:\n            await self.cleanup_servers()\n\n\nasync def main() -> None:\n    \"\"\"Initialize and run the chat session.\"\"\"\n    config = Configuration()\n    server_config = config.load_config(\"servers_config.json\")\n    servers = [\n        Server(name, srv_config)\n        for name, srv_config in server_config[\"mcpServers\"].items()\n    ]\n    llm_client = LLMClient(config.llm_api_key)\n    chat_session = ChatSession(servers, llm_client)\n    await chat_session.start()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n--- File Index 1: examples/fastmcp/complex_inputs.py ---\n\"\"\"\nFastMCP Complex inputs Example\n\nDemonstrates validation via pydantic with complex models.\n\"\"\"\n\nfrom typing import Annotated\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Shrimp Tank\")\n\n\nclass ShrimpTank(BaseModel):\n    class Shrimp(BaseModel):\n        name: Annotated[str, Field(max_length=10)]\n\n    shrimp: list[Shrimp]\n\n\n@mcp.tool()\ndef name_shrimp(\n    tank: ShrimpTank,\n    # You can use pydantic Field in function signatures for validation.\n    extra_names: Annotated[list[str], Field(max_length=10)],\n) -> list[str]:\n    \"\"\"List all shrimp names in the tank\"\"\"\n    return [shrimp.name for shrimp in tank.shrimp] + extra_names\n\n\n--- File Index 2: examples/fastmcp/desktop.py ---\n\"\"\"\nFastMCP Desktop Example\n\nA simple example that exposes the desktop directory as a resource.\n\"\"\"\n\nfrom pathlib import Path\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Demo\")\n\n\n@mcp.resource(\"dir://desktop\")\ndef desktop() -> list[str]:\n    \"\"\"List the files in the user's desktop\"\"\"\n    desktop = Path.home() / \"Desktop\"\n    return [str(f) for f in desktop.iterdir()]\n\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n--- File Index 3: examples/fastmcp/echo.py ---\n\"\"\"\nFastMCP Echo Server\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Echo Server\")\n\n\n@mcp.tool()\ndef echo_tool(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return text\n\n\n@mcp.resource(\"echo://static\")\ndef echo_resource() -> str:\n    return \"Echo!\"\n\n\n@mcp.resource(\"echo://{text}\")\ndef echo_template(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return f\"Echo: {text}\"\n\n\n@mcp.prompt(\"echo\")\ndef echo_prompt(text: str) -> str:\n    return text\n\n\n--- File Index 4: examples/fastmcp/memory.py ---\n# /// script\n# dependencies = [\"pydantic-ai-slim[openai]\", \"asyncpg\", \"numpy\", \"pgvector\"]\n# ///\n\n# uv pip install 'pydantic-ai-slim[openai]' asyncpg numpy pgvector\n\n\"\"\"\nRecursive memory system inspired by the human brain's clustering of memories.\nUses OpenAI's 'text-embedding-3-small' model and pgvector for efficient\nsimilarity search.\n\"\"\"\n\nimport asyncio\nimport math\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Annotated, Self\n\nimport asyncpg\nimport numpy as np\nfrom openai import AsyncOpenAI\nfrom pgvector.asyncpg import register_vector  # Import register_vector\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nfrom mcp.server.fastmcp import FastMCP\n\nMAX_DEPTH = 5\nSIMILARITY_THRESHOLD = 0.7\nDECAY_FACTOR = 0.99\nREINFORCEMENT_FACTOR = 1.1\n\nDEFAULT_LLM_MODEL = \"openai:gpt-4o\"\nDEFAULT_EMBEDDING_MODEL = \"text-embedding-3-small\"\n\nmcp = FastMCP(\n    \"memory\",\n    dependencies=[\n        \"pydantic-ai-slim[openai]\",\n        \"asyncpg\",\n        \"numpy\",\n        \"pgvector\",\n    ],\n)\n\nDB_DSN = \"postgresql://postgres:postgres@localhost:54320/memory_db\"\n# reset memory with rm ~/.fastmcp/{USER}/memory/*\nPROFILE_DIR = (\n    Path.home() / \".fastmcp\" / os.environ.get(\"USER\", \"anon\") / \"memory\"\n).resolve()\nPROFILE_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    a_array = np.array(a, dtype=np.float64)\n    b_array = np.array(b, dtype=np.float64)\n    return np.dot(a_array, b_array) / (\n        np.linalg.norm(a_array) * np.linalg.norm(b_array)\n    )\n\n\nasync def do_ai[T](\n    user_prompt: str,\n    system_prompt: str,\n    result_type: type[T] | Annotated,\n    deps=None,\n) -> T:\n    agent = Agent(\n        DEFAULT_LLM_MODEL,\n        system_prompt=system_prompt,\n        result_type=result_type,\n    )\n    result = await agent.run(user_prompt, deps=deps)\n    return result.data\n\n\n@dataclass\nclass Deps:\n    openai: AsyncOpenAI\n    pool: asyncpg.Pool\n\n\nasync def get_db_pool() -> asyncpg.Pool:\n    async def init(conn):\n        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n        await register_vector(conn)\n\n    pool = await asyncpg.create_pool(DB_DSN, init=init)\n    return pool\n\n\nclass MemoryNode(BaseModel):\n    id: int | None = None\n    content: str\n    summary: str = \"\"\n    importance: float = 1.0\n    access_count: int = 0\n    timestamp: float = Field(\n        default_factory=lambda: datetime.now(timezone.utc).timestamp()\n    )\n    embedding: list[float]\n\n    @classmethod\n    async def from_content(cls, content: str, deps: Deps):\n        embedding = await get_embedding(content, deps)\n        return cls(content=content, embedding=embedding)\n\n    async def save(self, deps: Deps):\n        async with deps.pool.acquire() as conn:\n            if self.id is None:\n                result = await conn.fetchrow(\n                    \"\"\"\n                    INSERT INTO memories (content, summary, importance, access_count,\n                        timestamp, embedding)\n                    VALUES ($1, $2, $3, $4, $5, $6)\n                    RETURNING id\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                )\n                self.id = result[\"id\"]\n            else:\n                await conn.execute(\n                    \"\"\"\n                    UPDATE memories\n                    SET content = $1, summary = $2, importance = $3,\n                        access_count = $4, timestamp = $5, embedding = $6\n                    WHERE id = $7\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                    self.id,\n                )\n\n    async def merge_with(self, other: Self, deps: Deps):\n        self.content = await do_ai(\n            f\"{self.content}\\n\\n{other.content}\",\n            \"Combine the following two texts into a single, coherent text.\",\n            str,\n            deps,\n        )\n        self.importance += other.importance\n        self.access_count += other.access_count\n        self.embedding = [(a + b) / 2 for a, b in zip(self.embedding, other.embedding)]\n        self.summary = await do_ai(\n            self.content, \"Summarize the following text concisely.\", str, deps\n        )\n        await self.save(deps)\n        # Delete the merged node from the database\n        if other.id is not None:\n            await delete_memory(other.id, deps)\n\n    def get_effective_importance(self):\n        return self.importance * (1 + math.log(self.access_count + 1))\n\n\nasync def get_embedding(text: str, deps: Deps) -> list[float]:\n    embedding_response = await deps.openai.embeddings.create(\n        input=text,\n        model=DEFAULT_EMBEDDING_MODEL,\n    )\n    return embedding_response.data[0].embedding\n\n\nasync def delete_memory(memory_id: int, deps: Deps):\n    async with deps.pool.acquire() as conn:\n        await conn.execute(\"DELETE FROM memories WHERE id = $1\", memory_id)\n\n\nasync def add_memory(content: str, deps: Deps):\n    new_memory = await MemoryNode.from_content(content, deps)\n    await new_memory.save(deps)\n\n    similar_memories = await find_similar_memories(new_memory.embedding, deps)\n    for memory in similar_memories:\n        if memory.id != new_memory.id:\n            await new_memory.merge_with(memory, deps)\n\n    await update_importance(new_memory.embedding, deps)\n\n    await prune_memories(deps)\n\n    return f\"Remembered: {content}\"\n\n\nasync def find_similar_memories(embedding: list[float], deps: Deps) -> list[MemoryNode]:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, content, summary, importance, access_count, timestamp, embedding\n            FROM memories\n            ORDER BY embedding <-> $1\n            LIMIT 5\n            \"\"\",\n            embedding,\n        )\n    memories = [\n        MemoryNode(\n            id=row[\"id\"],\n            content=row[\"content\"],\n            summary=row[\"summary\"],\n            importance=row[\"importance\"],\n            access_count=row[\"access_count\"],\n            timestamp=row[\"timestamp\"],\n            embedding=row[\"embedding\"],\n        )\n        for row in rows\n    ]\n    return memories\n\n\nasync def update_importance(user_embedding: list[float], deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"SELECT id, importance, access_count, embedding FROM memories\"\n        )\n        for row in rows:\n            memory_embedding = row[\"embedding\"]\n            similarity = cosine_similarity(user_embedding, memory_embedding)\n            if similarity > SIMILARITY_THRESHOLD:\n                new_importance = row[\"importance\"] * REINFORCEMENT_FACTOR\n                new_access_count = row[\"access_count\"] + 1\n            else:\n                new_importance = row[\"importance\"] * DECAY_FACTOR\n                new_access_count = row[\"access_count\"]\n            await conn.execute(\n                \"\"\"\n                UPDATE memories\n                SET importance = $1, access_count = $2\n                WHERE id = $3\n                \"\"\",\n                new_importance,\n                new_access_count,\n                row[\"id\"],\n            )\n\n\nasync def prune_memories(deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            OFFSET $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n        for row in rows:\n            await conn.execute(\"DELETE FROM memories WHERE id = $1\", row[\"id\"])\n\n\nasync def display_memory_tree(deps: Deps) -> str:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT content, summary, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            LIMIT $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n    result = \"\"\n    for row in rows:\n        effective_importance = row[\"importance\"] * (\n            1 + math.log(row[\"access_count\"] + 1)\n        )\n        summary = row[\"summary\"] or row[\"content\"]\n        result += f\"- {summary} (Importance: {effective_importance:.2f})\\n\"\n    return result\n\n\n@mcp.tool()\nasync def remember(\n    contents: list[str] = Field(\n        description=\"List of observations or memories to store\"\n    ),\n):\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    try:\n        return \"\\n\".join(\n            await asyncio.gather(*[add_memory(content, deps) for content in contents])\n        )\n    finally:\n        await deps.pool.close()\n\n\n@mcp.tool()\nasync def read_profile() -> str:\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    profile = await display_memory_tree(deps)\n    await deps.pool.close()\n    return profile\n\n\nasync def initialize_database():\n    pool = await asyncpg.create_pool(\n        \"postgresql://postgres:postgres@localhost:54320/postgres\"\n    )\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                SELECT pg_terminate_backend(pg_stat_activity.pid)\n                FROM pg_stat_activity\n                WHERE pg_stat_activity.datname = 'memory_db'\n                AND pid <> pg_backend_pid();\n            \"\"\")\n            await conn.execute(\"DROP DATABASE IF EXISTS memory_db;\")\n            await conn.execute(\"CREATE DATABASE memory_db;\")\n    finally:\n        await pool.close()\n\n    pool = await asyncpg.create_pool(DB_DSN)\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n\n            await register_vector(conn)\n\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS memories (\n                    id SERIAL PRIMARY KEY,\n                    content TEXT NOT NULL,\n                    summary TEXT,\n                    importance REAL NOT NULL,\n                    access_count INT NOT NULL,\n                    timestamp DOUBLE PRECISION NOT NULL,\n                    embedding vector(1536) NOT NULL\n                );\n                CREATE INDEX IF NOT EXISTS idx_memories_embedding ON memories\n                    USING hnsw (embedding vector_l2_ops);\n            \"\"\")\n    finally:\n        await pool.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(initialize_database())\n\n\n--- File Index 5: examples/fastmcp/parameter_descriptions.py ---\n\"\"\"\nFastMCP Example showing parameter descriptions\n\"\"\"\n\nfrom pydantic import Field\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Parameter Descriptions Server\")\n\n\n@mcp.tool()\ndef greet_user(\n    name: str = Field(description=\"The name of the person to greet\"),\n    title: str = Field(description=\"Optional title like Mr/Ms/Dr\", default=\"\"),\n    times: int = Field(description=\"Number of times to repeat the greeting\", default=1),\n) -> str:\n    \"\"\"Greet a user with optional title and repetition\"\"\"\n    greeting = f\"Hello {title + ' ' if title else ''}{name}!\"\n    return \"\\n\".join([greeting] * times)\n\n\n--- File Index 6: examples/fastmcp/readme-quickstart.py ---\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n--- File Index 7: examples/fastmcp/screenshot.py ---\n\"\"\"\nFastMCP Screenshot Example\n\nGive Claude a tool to capture and view screenshots.\n\"\"\"\n\nimport io\n\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.utilities.types import Image\n\n# Create server\nmcp = FastMCP(\"Screenshot Demo\", dependencies=[\"pyautogui\", \"Pillow\"])\n\n\n@mcp.tool()\ndef take_screenshot() -> Image:\n    \"\"\"\n    Take a screenshot of the user's screen and return it as an image. Use\n    this tool anytime the user wants you to look at something they're doing.\n    \"\"\"\n    import pyautogui\n\n    buffer = io.BytesIO()\n\n    # if the file exceeds ~1MB, it will be rejected by Claude\n    screenshot = pyautogui.screenshot()\n    screenshot.convert(\"RGB\").save(buffer, format=\"JPEG\", quality=60, optimize=True)\n    return Image(data=buffer.getvalue(), format=\"jpeg\")\n\n\n--- File Index 8: examples/fastmcp/simple_echo.py ---\n\"\"\"\nFastMCP Echo Server\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Echo Server\")\n\n\n@mcp.tool()\ndef echo(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return text\n\n\n--- File Index 9: examples/fastmcp/text_me.py ---\n# /// script\n# dependencies = []\n# ///\n\n\"\"\"\nFastMCP Text Me Server\n--------------------------------\nThis defines a simple FastMCP server that sends a text message to a phone number via https://surgemsg.com/.\n\nTo run this example, create a `.env` file with the following values:\n\nSURGE_API_KEY=...\nSURGE_ACCOUNT_ID=...\nSURGE_MY_PHONE_NUMBER=...\nSURGE_MY_FIRST_NAME=...\nSURGE_MY_LAST_NAME=...\n\nVisit https://surgemsg.com/ and click \"Get Started\" to obtain these values.\n\"\"\"\n\nfrom typing import Annotated\n\nimport httpx\nfrom pydantic import BeforeValidator\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\n\nfrom mcp.server.fastmcp import FastMCP\n\n\nclass SurgeSettings(BaseSettings):\n    model_config: SettingsConfigDict = SettingsConfigDict(\n        env_prefix=\"SURGE_\", env_file=\".env\"\n    )\n\n    api_key: str\n    account_id: str\n    my_phone_number: Annotated[\n        str, BeforeValidator(lambda v: \"+\" + v if not v.startswith(\"+\") else v)\n    ]\n    my_first_name: str\n    my_last_name: str\n\n\n# Create server\nmcp = FastMCP(\"Text me\")\nsurge_settings = SurgeSettings()  # type: ignore\n\n\n@mcp.tool(name=\"textme\", description=\"Send a text message to me\")\ndef text_me(text_content: str) -> str:\n    \"\"\"Send a text message to a phone number via https://surgemsg.com/\"\"\"\n    with httpx.Client() as client:\n        response = client.post(\n            \"https://api.surgemsg.com/messages\",\n            headers={\n                \"Authorization\": f\"Bearer {surge_settings.api_key}\",\n                \"Surge-Account\": surge_settings.account_id,\n                \"Content-Type\": \"application/json\",\n            },\n            json={\n                \"body\": text_content,\n                \"conversation\": {\n                    \"contact\": {\n                        \"first_name\": surge_settings.my_first_name,\n                        \"last_name\": surge_settings.my_last_name,\n                        \"phone_number\": surge_settings.my_phone_number,\n                    }\n                },\n            },\n        )\n        response.raise_for_status()\n        return f\"Message sent: {text_content}\"\n\n\n--- File Index 10: examples/fastmcp/unicode_example.py ---\n\"\"\"\nExample FastMCP server that uses Unicode characters in various places to help test\nUnicode handling in tools and inspectors.\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP()\n\n\n@mcp.tool(\n    description=\"\ud83c\udf1f A tool that uses various Unicode characters in its description: \"\n    \"\u00e1 \u00e9 \u00ed \u00f3 \u00fa \u00f1 \u6f22\u5b57 \ud83c\udf89\"\n)\ndef hello_unicode(name: str = \"\u4e16\u754c\", greeting: str = \"\u00a1Hola\") -> str:\n    \"\"\"\n    A simple tool that demonstrates Unicode handling in:\n    - Tool description (emojis, accents, CJK characters)\n    - Parameter defaults (CJK characters)\n    - Return values (Spanish punctuation, emojis)\n    \"\"\"\n    return f\"{greeting}, {name}! \ud83d\udc4b\"\n\n\n@mcp.tool(description=\"\ud83c\udfa8 Tool that returns a list of emoji categories\")\ndef list_emoji_categories() -> list[str]:\n    \"\"\"Returns a list of emoji categories with emoji examples.\"\"\"\n    return [\n        \"\ud83d\ude00 Smileys & Emotion\",\n        \"\ud83d\udc4b People & Body\",\n        \"\ud83d\udc36 Animals & Nature\",\n        \"\ud83c\udf4e Food & Drink\",\n        \"\u26bd Activities\",\n        \"\ud83c\udf0d Travel & Places\",\n        \"\ud83d\udca1 Objects\",\n        \"\u2764\ufe0f Symbols\",\n        \"\ud83d\udea9 Flags\",\n    ]\n\n\n@mcp.tool(description=\"\ud83d\udd24 Tool that returns text in different scripts\")\ndef multilingual_hello() -> str:\n    \"\"\"Returns hello in different scripts and writing systems.\"\"\"\n    return \"\\n\".join(\n        [\n            \"English: Hello!\",\n            \"Spanish: \u00a1Hola!\",\n            \"French: Bonjour!\",\n            \"German: Gr\u00fc\u00df Gott!\",\n            \"Russian: \u041f\u0440\u0438\u0432\u0435\u0442!\",\n            \"Greek: \u0393\u03b5\u03b9\u03b1 \u03c3\u03b1\u03c2!\",\n            \"Hebrew: !\u05e9\u05b8\u05c1\u05dc\u05d5\u05b9\u05dd\",\n            \"Arabic: !\u0645\u0631\u062d\u0628\u0627\",\n            \"Hindi: \u0928\u092e\u0938\u094d\u0924\u0947!\",\n            \"Chinese: \u4f60\u597d!\",\n            \"Japanese: \u3053\u3093\u306b\u3061\u306f!\",\n            \"Korean: \uc548\ub155\ud558\uc138\uc694!\",\n            \"Thai: \u0e2a\u0e27\u0e31\u0e2a\u0e14\u0e35!\",\n        ]\n    )\n\n\nif __name__ == \"__main__\":\n    mcp.run()\n\n\n--- File Index 11: examples/servers/simple-prompt/mcp_simple_prompt/__init__.py ---\n\n\n\n--- File Index 12: examples/servers/simple-prompt/mcp_simple_prompt/__main__.py ---\nimport sys\n\nfrom .server import main\n\nsys.exit(main())\n\n\n--- File Index 13: examples/servers/simple-prompt/mcp_simple_prompt/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\ndef create_messages(\n    context: str | None = None, topic: str | None = None\n) -> list[types.PromptMessage]:\n    \"\"\"Create the messages for the prompt.\"\"\"\n    messages = []\n\n    # Add context if provided\n    if context:\n        messages.append(\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(\n                    type=\"text\", text=f\"Here is some relevant context: {context}\"\n                ),\n            )\n        )\n\n    # Add the main prompt\n    prompt = \"Please help me with \"\n    if topic:\n        prompt += f\"the following topic: {topic}\"\n    else:\n        prompt += \"whatever questions I may have.\"\n\n    messages.append(\n        types.PromptMessage(\n            role=\"user\", content=types.TextContent(type=\"text\", text=prompt)\n        )\n    )\n\n    return messages\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-prompt\")\n\n    @app.list_prompts()\n    async def list_prompts() -> list[types.Prompt]:\n        return [\n            types.Prompt(\n                name=\"simple\",\n                description=\"A simple prompt that can take optional context and topic \"\n                \"arguments\",\n                arguments=[\n                    types.PromptArgument(\n                        name=\"context\",\n                        description=\"Additional context to consider\",\n                        required=False,\n                    ),\n                    types.PromptArgument(\n                        name=\"topic\",\n                        description=\"Specific topic to focus on\",\n                        required=False,\n                    ),\n                ],\n            )\n        ]\n\n    @app.get_prompt()\n    async def get_prompt(\n        name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        if name != \"simple\":\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        if arguments is None:\n            arguments = {}\n\n        return types.GetPromptResult(\n            messages=create_messages(\n                context=arguments.get(\"context\"), topic=arguments.get(\"topic\")\n            ),\n            description=\"A simple prompt with optional context and topic arguments\",\n        )\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File Index 14: examples/servers/simple-resource/mcp_simple_resource/__init__.py ---\n\n\n\n--- File Index 15: examples/servers/simple-resource/mcp_simple_resource/__main__.py ---\nimport sys\n\nfrom .server import main\n\nsys.exit(main())\n\n\n--- File Index 16: examples/servers/simple-resource/mcp_simple_resource/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\nfrom pydantic import FileUrl\n\nSAMPLE_RESOURCES = {\n    \"greeting\": \"Hello! This is a sample text resource.\",\n    \"help\": \"This server provides a few sample text resources for testing.\",\n    \"about\": \"This is the simple-resource MCP server implementation.\",\n}\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-resource\")\n\n    @app.list_resources()\n    async def list_resources() -> list[types.Resource]:\n        return [\n            types.Resource(\n                uri=FileUrl(f\"file:///{name}.txt\"),\n                name=name,\n                description=f\"A sample text resource named {name}\",\n                mimeType=\"text/plain\",\n            )\n            for name in SAMPLE_RESOURCES.keys()\n        ]\n\n    @app.read_resource()\n    async def read_resource(uri: FileUrl) -> str | bytes:\n        name = uri.path.replace(\".txt\", \"\").lstrip(\"/\")\n\n        if name not in SAMPLE_RESOURCES:\n            raise ValueError(f\"Unknown resource: {uri}\")\n\n        return SAMPLE_RESOURCES[name]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File Index 17: examples/servers/simple-tool/mcp_simple_tool/__init__.py ---\n\n\n\n--- File Index 18: examples/servers/simple-tool/mcp_simple_tool/__main__.py ---\nimport sys\n\nfrom .server import main\n\nsys.exit(main())\n\n\n--- File Index 19: examples/servers/simple-tool/mcp_simple_tool/server.py ---\nimport anyio\nimport click\nimport httpx\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\nasync def fetch_website(\n    url: str,\n) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    headers = {\n        \"User-Agent\": \"MCP Test Server (github.com/modelcontextprotocol/python-sdk)\"\n    }\n    async with httpx.AsyncClient(follow_redirects=True, headers=headers) as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        return [types.TextContent(type=\"text\", text=response.text)]\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-website-fetcher\")\n\n    @app.call_tool()\n    async def fetch_tool(\n        name: str, arguments: dict\n    ) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n        if name != \"fetch\":\n            raise ValueError(f\"Unknown tool: {name}\")\n        if \"url\" not in arguments:\n            raise ValueError(\"Missing required argument 'url'\")\n        return await fetch_website(arguments[\"url\"])\n\n    @app.list_tools()\n    async def list_tools() -> list[types.Tool]:\n        return [\n            types.Tool(\n                name=\"fetch\",\n                description=\"Fetches a website and returns its content\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"required\": [\"url\"],\n                    \"properties\": {\n                        \"url\": {\n                            \"type\": \"string\",\n                            \"description\": \"URL to fetch\",\n                        }\n                    },\n                },\n            )\n        ]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File Index 20: src/mcp/__init__.py ---\nfrom .client.session import ClientSession\nfrom .client.stdio import StdioServerParameters, stdio_client\nfrom .server.session import ServerSession\nfrom .server.stdio import stdio_server\nfrom .shared.exceptions import McpError\nfrom .types import (\n    CallToolRequest,\n    ClientCapabilities,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    CompleteRequest,\n    CreateMessageRequest,\n    CreateMessageResult,\n    ErrorData,\n    GetPromptRequest,\n    GetPromptResult,\n    Implementation,\n    IncludeContext,\n    InitializedNotification,\n    InitializeRequest,\n    InitializeResult,\n    JSONRPCError,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    ListPromptsRequest,\n    ListPromptsResult,\n    ListResourcesRequest,\n    ListResourcesResult,\n    ListToolsResult,\n    LoggingLevel,\n    LoggingMessageNotification,\n    Notification,\n    PingRequest,\n    ProgressNotification,\n    PromptsCapability,\n    ReadResourceRequest,\n    ReadResourceResult,\n    Resource,\n    ResourcesCapability,\n    ResourceUpdatedNotification,\n    RootsCapability,\n    SamplingMessage,\n    ServerCapabilities,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n    SetLevelRequest,\n    StopReason,\n    SubscribeRequest,\n    Tool,\n    ToolsCapability,\n    UnsubscribeRequest,\n)\nfrom .types import (\n    Role as SamplingRole,\n)\n\n__all__ = [\n    \"CallToolRequest\",\n    \"ClientCapabilities\",\n    \"ClientNotification\",\n    \"ClientRequest\",\n    \"ClientResult\",\n    \"ClientSession\",\n    \"CreateMessageRequest\",\n    \"CreateMessageResult\",\n    \"ErrorData\",\n    \"GetPromptRequest\",\n    \"GetPromptResult\",\n    \"Implementation\",\n    \"IncludeContext\",\n    \"InitializeRequest\",\n    \"InitializeResult\",\n    \"InitializedNotification\",\n    \"JSONRPCError\",\n    \"JSONRPCRequest\",\n    \"ListPromptsRequest\",\n    \"ListPromptsResult\",\n    \"ListResourcesRequest\",\n    \"ListResourcesResult\",\n    \"ListToolsResult\",\n    \"LoggingLevel\",\n    \"LoggingMessageNotification\",\n    \"McpError\",\n    \"Notification\",\n    \"PingRequest\",\n    \"ProgressNotification\",\n    \"PromptsCapability\",\n    \"ReadResourceRequest\",\n    \"ReadResourceResult\",\n    \"ResourcesCapability\",\n    \"ResourceUpdatedNotification\",\n    \"Resource\",\n    \"RootsCapability\",\n    \"SamplingMessage\",\n    \"SamplingRole\",\n    \"ServerCapabilities\",\n    \"ServerNotification\",\n    \"ServerRequest\",\n    \"ServerResult\",\n    \"ServerSession\",\n    \"SetLevelRequest\",\n    \"StdioServerParameters\",\n    \"StopReason\",\n    \"SubscribeRequest\",\n    \"Tool\",\n    \"ToolsCapability\",\n    \"UnsubscribeRequest\",\n    \"stdio_client\",\n    \"stdio_server\",\n    \"CompleteRequest\",\n    \"JSONRPCResponse\",\n]\n\n\n--- File Index 21: src/mcp/cli/__init__.py ---\n\"\"\"FastMCP CLI package.\"\"\"\n\nfrom .cli import app\n\nif __name__ == \"__main__\":\n    app()\n\n\n--- File Index 22: src/mcp/cli/claude.py ---\n\"\"\"Claude app integration utilities.\"\"\"\n\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Any\n\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\nMCP_PACKAGE = \"mcp[cli]\"\n\n\ndef get_claude_config_path() -> Path | None:\n    \"\"\"Get the Claude config directory based on platform.\"\"\"\n    if sys.platform == \"win32\":\n        path = Path(Path.home(), \"AppData\", \"Roaming\", \"Claude\")\n    elif sys.platform == \"darwin\":\n        path = Path(Path.home(), \"Library\", \"Application Support\", \"Claude\")\n    elif sys.platform.startswith(\"linux\"):\n        path = Path(\n            os.environ.get(\"XDG_CONFIG_HOME\", Path.home() / \".config\"), \"Claude\"\n        )\n    else:\n        return None\n\n    if path.exists():\n        return path\n    return None\n\n\ndef update_claude_config(\n    file_spec: str,\n    server_name: str,\n    *,\n    with_editable: Path | None = None,\n    with_packages: list[str] | None = None,\n    env_vars: dict[str, str] | None = None,\n) -> bool:\n    \"\"\"Add or update a FastMCP server in Claude's configuration.\n\n    Args:\n        file_spec: Path to the server file, optionally with :object suffix\n        server_name: Name for the server in Claude's config\n        with_editable: Optional directory to install in editable mode\n        with_packages: Optional list of additional packages to install\n        env_vars: Optional dictionary of environment variables. These are merged with\n            any existing variables, with new values taking precedence.\n\n    Raises:\n        RuntimeError: If Claude Desktop's config directory is not found, indicating\n            Claude Desktop may not be installed or properly set up.\n    \"\"\"\n    config_dir = get_claude_config_path()\n    if not config_dir:\n        raise RuntimeError(\n            \"Claude Desktop config directory not found. Please ensure Claude Desktop\"\n            \" is installed and has been run at least once to initialize its config.\"\n        )\n\n    config_file = config_dir / \"claude_desktop_config.json\"\n    if not config_file.exists():\n        try:\n            config_file.write_text(\"{}\")\n        except Exception as e:\n            logger.error(\n                \"Failed to create Claude config file\",\n                extra={\n                    \"error\": str(e),\n                    \"config_file\": str(config_file),\n                },\n            )\n            return False\n\n    try:\n        config = json.loads(config_file.read_text())\n        if \"mcpServers\" not in config:\n            config[\"mcpServers\"] = {}\n\n        # Always preserve existing env vars and merge with new ones\n        if (\n            server_name in config[\"mcpServers\"]\n            and \"env\" in config[\"mcpServers\"][server_name]\n        ):\n            existing_env = config[\"mcpServers\"][server_name][\"env\"]\n            if env_vars:\n                # New vars take precedence over existing ones\n                env_vars = {**existing_env, **env_vars}\n            else:\n                env_vars = existing_env\n\n        # Build uv run command\n        args = [\"run\"]\n\n        # Collect all packages in a set to deduplicate\n        packages = {MCP_PACKAGE}\n        if with_packages:\n            packages.update(pkg for pkg in with_packages if pkg)\n\n        # Add all packages with --with\n        for pkg in sorted(packages):\n            args.extend([\"--with\", pkg])\n\n        if with_editable:\n            args.extend([\"--with-editable\", str(with_editable)])\n\n        # Convert file path to absolute before adding to command\n        # Split off any :object suffix first\n        if \":\" in file_spec:\n            file_path, server_object = file_spec.rsplit(\":\", 1)\n            file_spec = f\"{Path(file_path).resolve()}:{server_object}\"\n        else:\n            file_spec = str(Path(file_spec).resolve())\n\n        # Add fastmcp run command\n        args.extend([\"mcp\", \"run\", file_spec])\n\n        server_config: dict[str, Any] = {\"command\": \"uv\", \"args\": args}\n\n        # Add environment variables if specified\n        if env_vars:\n            server_config[\"env\"] = env_vars\n\n        config[\"mcpServers\"][server_name] = server_config\n\n        config_file.write_text(json.dumps(config, indent=2))\n        logger.info(\n            f\"Added server '{server_name}' to Claude config\",\n            extra={\"config_file\": str(config_file)},\n        )\n        return True\n    except Exception as e:\n        logger.error(\n            \"Failed to update Claude config\",\n            extra={\n                \"error\": str(e),\n                \"config_file\": str(config_file),\n            },\n        )\n        return False\n\n\n--- File Index 23: src/mcp/cli/cli.py ---\n\"\"\"MCP CLI tools.\"\"\"\n\nimport importlib.metadata\nimport importlib.util\nimport os\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import Annotated\n\ntry:\n    import typer\nexcept ImportError:\n    print(\"Error: typer is required. Install with 'pip install mcp[cli]'\")\n    sys.exit(1)\n\ntry:\n    from mcp.cli import claude\n    from mcp.server.fastmcp.utilities.logging import get_logger\nexcept ImportError:\n    print(\"Error: mcp.server.fastmcp is not installed or not in PYTHONPATH\")\n    sys.exit(1)\n\ntry:\n    import dotenv\nexcept ImportError:\n    dotenv = None\n\nlogger = get_logger(\"cli\")\n\napp = typer.Typer(\n    name=\"mcp\",\n    help=\"MCP development tools\",\n    add_completion=False,\n    no_args_is_help=True,  # Show help if no args provided\n)\n\n\ndef _get_npx_command():\n    \"\"\"Get the correct npx command for the current platform.\"\"\"\n    if sys.platform == \"win32\":\n        # Try both npx.cmd and npx.exe on Windows\n        for cmd in [\"npx.cmd\", \"npx.exe\", \"npx\"]:\n            try:\n                subprocess.run(\n                    [cmd, \"--version\"], check=True, capture_output=True, shell=True\n                )\n                return cmd\n            except subprocess.CalledProcessError:\n                continue\n        return None\n    return \"npx\"  # On Unix-like systems, just use npx\n\n\ndef _parse_env_var(env_var: str) -> tuple[str, str]:\n    \"\"\"Parse environment variable string in format KEY=VALUE.\"\"\"\n    if \"=\" not in env_var:\n        logger.error(\n            f\"Invalid environment variable format: {env_var}. Must be KEY=VALUE\"\n        )\n        sys.exit(1)\n    key, value = env_var.split(\"=\", 1)\n    return key.strip(), value.strip()\n\n\ndef _build_uv_command(\n    file_spec: str,\n    with_editable: Path | None = None,\n    with_packages: list[str] | None = None,\n) -> list[str]:\n    \"\"\"Build the uv run command that runs a MCP server through mcp run.\"\"\"\n    cmd = [\"uv\"]\n\n    cmd.extend([\"run\", \"--with\", \"mcp\"])\n\n    if with_editable:\n        cmd.extend([\"--with-editable\", str(with_editable)])\n\n    if with_packages:\n        for pkg in with_packages:\n            if pkg:\n                cmd.extend([\"--with\", pkg])\n\n    # Add mcp run command\n    cmd.extend([\"mcp\", \"run\", file_spec])\n    return cmd\n\n\ndef _parse_file_path(file_spec: str) -> tuple[Path, str | None]:\n    \"\"\"Parse a file path that may include a server object specification.\n\n    Args:\n        file_spec: Path to file, optionally with :object suffix\n\n    Returns:\n        Tuple of (file_path, server_object)\n    \"\"\"\n    # First check if we have a Windows path (e.g., C:\\...)\n    has_windows_drive = len(file_spec) > 1 and file_spec[1] == \":\"\n\n    # Split on the last colon, but only if it's not part of the Windows drive letter\n    # and there's actually another colon in the string after the drive letter\n    if \":\" in (file_spec[2:] if has_windows_drive else file_spec):\n        file_str, server_object = file_spec.rsplit(\":\", 1)\n    else:\n        file_str, server_object = file_spec, None\n\n    # Resolve the file path\n    file_path = Path(file_str).expanduser().resolve()\n    if not file_path.exists():\n        logger.error(f\"File not found: {file_path}\")\n        sys.exit(1)\n    if not file_path.is_file():\n        logger.error(f\"Not a file: {file_path}\")\n        sys.exit(1)\n\n    return file_path, server_object\n\n\ndef _import_server(file: Path, server_object: str | None = None):\n    \"\"\"Import a MCP server from a file.\n\n    Args:\n        file: Path to the file\n        server_object: Optional object name in format \"module:object\" or just \"object\"\n\n    Returns:\n        The server object\n    \"\"\"\n    # Add parent directory to Python path so imports can be resolved\n    file_dir = str(file.parent)\n    if file_dir not in sys.path:\n        sys.path.insert(0, file_dir)\n\n    # Import the module\n    spec = importlib.util.spec_from_file_location(\"server_module\", file)\n    if not spec or not spec.loader:\n        logger.error(\"Could not load module\", extra={\"file\": str(file)})\n        sys.exit(1)\n\n    module = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(module)\n\n    # If no object specified, try common server names\n    if not server_object:\n        # Look for the most common server object names\n        for name in [\"mcp\", \"server\", \"app\"]:\n            if hasattr(module, name):\n                return getattr(module, name)\n\n        logger.error(\n            f\"No server object found in {file}. Please either:\\n\"\n            \"1. Use a standard variable name (mcp, server, or app)\\n\"\n            \"2. Specify the object name with file:object syntax\",\n            extra={\"file\": str(file)},\n        )\n        sys.exit(1)\n\n    # Handle module:object syntax\n    if \":\" in server_object:\n        module_name, object_name = server_object.split(\":\", 1)\n        try:\n            server_module = importlib.import_module(module_name)\n            server = getattr(server_module, object_name, None)\n        except ImportError:\n            logger.error(\n                f\"Could not import module '{module_name}'\",\n                extra={\"file\": str(file)},\n            )\n            sys.exit(1)\n    else:\n        # Just object name\n        server = getattr(module, server_object, None)\n\n    if server is None:\n        logger.error(\n            f\"Server object '{server_object}' not found\",\n            extra={\"file\": str(file)},\n        )\n        sys.exit(1)\n\n    return server\n\n\n@app.command()\ndef version() -> None:\n    \"\"\"Show the MCP version.\"\"\"\n    try:\n        version = importlib.metadata.version(\"mcp\")\n        print(f\"MCP version {version}\")\n    except importlib.metadata.PackageNotFoundError:\n        print(\"MCP version unknown (package not installed)\")\n        sys.exit(1)\n\n\n@app.command()\ndef dev(\n    file_spec: str = typer.Argument(\n        ...,\n        help=\"Python file to run, optionally with :object suffix\",\n    ),\n    with_editable: Annotated[\n        Path | None,\n        typer.Option(\n            \"--with-editable\",\n            \"-e\",\n            help=\"Directory containing pyproject.toml to install in editable mode\",\n            exists=True,\n            file_okay=False,\n            resolve_path=True,\n        ),\n    ] = None,\n    with_packages: Annotated[\n        list[str],\n        typer.Option(\n            \"--with\",\n            help=\"Additional packages to install\",\n        ),\n    ] = [],\n) -> None:\n    \"\"\"Run a MCP server with the MCP Inspector.\"\"\"\n    file, server_object = _parse_file_path(file_spec)\n\n    logger.debug(\n        \"Starting dev server\",\n        extra={\n            \"file\": str(file),\n            \"server_object\": server_object,\n            \"with_editable\": str(with_editable) if with_editable else None,\n            \"with_packages\": with_packages,\n        },\n    )\n\n    try:\n        # Import server to get dependencies\n        server = _import_server(file, server_object)\n        if hasattr(server, \"dependencies\"):\n            with_packages = list(set(with_packages + server.dependencies))\n\n        uv_cmd = _build_uv_command(file_spec, with_editable, with_packages)\n\n        # Get the correct npx command\n        npx_cmd = _get_npx_command()\n        if not npx_cmd:\n            logger.error(\n                \"npx not found. Please ensure Node.js and npm are properly installed \"\n                \"and added to your system PATH.\"\n            )\n            sys.exit(1)\n\n        # Run the MCP Inspector command with shell=True on Windows\n        shell = sys.platform == \"win32\"\n        process = subprocess.run(\n            [npx_cmd, \"@modelcontextprotocol/inspector\"] + uv_cmd,\n            check=True,\n            shell=shell,\n            env=dict(os.environ.items()),  # Convert to list of tuples for env update\n        )\n        sys.exit(process.returncode)\n    except subprocess.CalledProcessError as e:\n        logger.error(\n            \"Dev server failed\",\n            extra={\n                \"file\": str(file),\n                \"error\": str(e),\n                \"returncode\": e.returncode,\n            },\n        )\n        sys.exit(e.returncode)\n    except FileNotFoundError:\n        logger.error(\n            \"npx not found. Please ensure Node.js and npm are properly installed \"\n            \"and added to your system PATH. You may need to restart your terminal \"\n            \"after installation.\",\n            extra={\"file\": str(file)},\n        )\n        sys.exit(1)\n\n\n@app.command()\ndef run(\n    file_spec: str = typer.Argument(\n        ...,\n        help=\"Python file to run, optionally with :object suffix\",\n    ),\n    transport: Annotated[\n        str | None,\n        typer.Option(\n            \"--transport\",\n            \"-t\",\n            help=\"Transport protocol to use (stdio or sse)\",\n        ),\n    ] = None,\n) -> None:\n    \"\"\"Run a MCP server.\n\n    The server can be specified in two ways:\\n\n    1. Module approach: server.py - runs the module directly, expecting a server.run() call.\\n\n    2. Import approach: server.py:app - imports and runs the specified server object.\\n\\n\n\n    Note: This command runs the server directly. You are responsible for ensuring\n    all dependencies are available.\\n\n    For dependency management, use `mcp install` or `mcp dev` instead.\n    \"\"\"  # noqa: E501\n    file, server_object = _parse_file_path(file_spec)\n\n    logger.debug(\n        \"Running server\",\n        extra={\n            \"file\": str(file),\n            \"server_object\": server_object,\n            \"transport\": transport,\n        },\n    )\n\n    try:\n        # Import and get server object\n        server = _import_server(file, server_object)\n\n        # Run the server\n        kwargs = {}\n        if transport:\n            kwargs[\"transport\"] = transport\n\n        server.run(**kwargs)\n\n    except Exception as e:\n        logger.error(\n            f\"Failed to run server: {e}\",\n            extra={\n                \"file\": str(file),\n                \"error\": str(e),\n            },\n        )\n        sys.exit(1)\n\n\n@app.command()\ndef install(\n    file_spec: str = typer.Argument(\n        ...,\n        help=\"Python file to run, optionally with :object suffix\",\n    ),\n    server_name: Annotated[\n        str | None,\n        typer.Option(\n            \"--name\",\n            \"-n\",\n            help=\"Custom name for the server (defaults to server's name attribute or\"\n            \" file name)\",\n        ),\n    ] = None,\n    with_editable: Annotated[\n        Path | None,\n        typer.Option(\n            \"--with-editable\",\n            \"-e\",\n            help=\"Directory containing pyproject.toml to install in editable mode\",\n            exists=True,\n            file_okay=False,\n            resolve_path=True,\n        ),\n    ] = None,\n    with_packages: Annotated[\n        list[str],\n        typer.Option(\n            \"--with\",\n            help=\"Additional packages to install\",\n        ),\n    ] = [],\n    env_vars: Annotated[\n        list[str],\n        typer.Option(\n            \"--env-var\",\n            \"-v\",\n            help=\"Environment variables in KEY=VALUE format\",\n        ),\n    ] = [],\n    env_file: Annotated[\n        Path | None,\n        typer.Option(\n            \"--env-file\",\n            \"-f\",\n            help=\"Load environment variables from a .env file\",\n            exists=True,\n            file_okay=True,\n            dir_okay=False,\n            resolve_path=True,\n        ),\n    ] = None,\n) -> None:\n    \"\"\"Install a MCP server in the Claude desktop app.\n\n    Environment variables are preserved once added and only updated if new values\n    are explicitly provided.\n    \"\"\"\n    file, server_object = _parse_file_path(file_spec)\n\n    logger.debug(\n        \"Installing server\",\n        extra={\n            \"file\": str(file),\n            \"server_name\": server_name,\n            \"server_object\": server_object,\n            \"with_editable\": str(with_editable) if with_editable else None,\n            \"with_packages\": with_packages,\n        },\n    )\n\n    if not claude.get_claude_config_path():\n        logger.error(\"Claude app not found\")\n        sys.exit(1)\n\n    # Try to import server to get its name, but fall back to file name if dependencies\n    # missing\n    name = server_name\n    server = None\n    if not name:\n        try:\n            server = _import_server(file, server_object)\n            name = server.name\n        except (ImportError, ModuleNotFoundError) as e:\n            logger.debug(\n                \"Could not import server (likely missing dependencies), using file\"\n                \" name\",\n                extra={\"error\": str(e)},\n            )\n            name = file.stem\n\n    # Get server dependencies if available\n    server_dependencies = getattr(server, \"dependencies\", []) if server else []\n    if server_dependencies:\n        with_packages = list(set(with_packages + server_dependencies))\n\n    # Process environment variables if provided\n    env_dict: dict[str, str] | None = None\n    if env_file or env_vars:\n        env_dict = {}\n        # Load from .env file if specified\n        if env_file:\n            if dotenv:\n                try:\n                    env_dict |= {\n                        k: v\n                        for k, v in dotenv.dotenv_values(env_file).items()\n                        if v is not None\n                    }\n                except Exception as e:\n                    logger.error(f\"Failed to load .env file: {e}\")\n                    sys.exit(1)\n            else:\n                logger.error(\"python-dotenv is not installed. Cannot load .env file.\")\n                sys.exit(1)\n\n        # Add command line environment variables\n        for env_var in env_vars:\n            key, value = _parse_env_var(env_var)\n            env_dict[key] = value\n\n    if claude.update_claude_config(\n        file_spec,\n        name,\n        with_editable=with_editable,\n        with_packages=with_packages,\n        env_vars=env_dict,\n    ):\n        logger.info(f\"Successfully installed {name} in Claude app\")\n    else:\n        logger.error(f\"Failed to install {name} in Claude app\")\n        sys.exit(1)\n\n\n--- File Index 24: src/mcp/client/__init__.py ---\n\n\n--- File Index 25: src/mcp/client/__main__.py ---\nimport argparse\nimport logging\nimport sys\nfrom functools import partial\nfrom urllib.parse import urlparse\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import ClientSession\nfrom mcp.client.sse import sse_client\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\nfrom mcp.shared.session import RequestResponder\nfrom mcp.types import JSONRPCMessage\n\nif not sys.warnoptions:\n    import warnings\n\n    warnings.simplefilter(\"ignore\")\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"client\")\n\n\nasync def message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    if isinstance(message, Exception):\n        logger.error(\"Error: %s\", message)\n        return\n\n    logger.info(\"Received message from server: %s\", message)\n\n\nasync def run_session(\n    read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    write_stream: MemoryObjectSendStream[JSONRPCMessage],\n    client_info: types.Implementation | None = None,\n):\n    async with ClientSession(\n        read_stream,\n        write_stream,\n        message_handler=message_handler,\n        client_info=client_info,\n    ) as session:\n        logger.info(\"Initializing session\")\n        await session.initialize()\n        logger.info(\"Initialized\")\n\n\nasync def main(command_or_url: str, args: list[str], env: list[tuple[str, str]]):\n    env_dict = dict(env)\n\n    if urlparse(command_or_url).scheme in (\"http\", \"https\"):\n        # Use SSE client for HTTP(S) URLs\n        async with sse_client(command_or_url) as streams:\n            await run_session(*streams)\n    else:\n        # Use stdio client for commands\n        server_parameters = StdioServerParameters(\n            command=command_or_url, args=args, env=env_dict\n        )\n        async with stdio_client(server_parameters) as streams:\n            await run_session(*streams)\n\n\ndef cli():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command_or_url\", help=\"Command or URL to connect to\")\n    parser.add_argument(\"args\", nargs=\"*\", help=\"Additional arguments\")\n    parser.add_argument(\n        \"-e\",\n        \"--env\",\n        nargs=2,\n        action=\"append\",\n        metavar=(\"KEY\", \"VALUE\"),\n        help=\"Environment variables to set. Can be used multiple times.\",\n        default=[],\n    )\n\n    args = parser.parse_args()\n    anyio.run(partial(main, args.command_or_url, args.args, args.env), backend=\"trio\")\n\n\nif __name__ == \"__main__\":\n    cli()\n\n\n--- File Index 26: src/mcp/client/session.py ---\nfrom datetime import timedelta\nfrom typing import Any, Protocol\n\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl, TypeAdapter\n\nimport mcp.types as types\nfrom mcp.shared.context import RequestContext\nfrom mcp.shared.session import BaseSession, RequestResponder\nfrom mcp.shared.version import SUPPORTED_PROTOCOL_VERSIONS\n\nDEFAULT_CLIENT_INFO = types.Implementation(name=\"mcp\", version=\"0.1.0\")\n\n\nclass SamplingFnT(Protocol):\n    async def __call__(\n        self,\n        context: RequestContext[\"ClientSession\", Any],\n        params: types.CreateMessageRequestParams,\n    ) -> types.CreateMessageResult | types.ErrorData: ...\n\n\nclass ListRootsFnT(Protocol):\n    async def __call__(\n        self, context: RequestContext[\"ClientSession\", Any]\n    ) -> types.ListRootsResult | types.ErrorData: ...\n\n\nclass LoggingFnT(Protocol):\n    async def __call__(\n        self,\n        params: types.LoggingMessageNotificationParams,\n    ) -> None: ...\n\n\nclass MessageHandlerFnT(Protocol):\n    async def __call__(\n        self,\n        message: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None: ...\n\n\nasync def _default_message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    await anyio.lowlevel.checkpoint()\n\n\nasync def _default_sampling_callback(\n    context: RequestContext[\"ClientSession\", Any],\n    params: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"Sampling not supported\",\n    )\n\n\nasync def _default_list_roots_callback(\n    context: RequestContext[\"ClientSession\", Any],\n) -> types.ListRootsResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"List roots not supported\",\n    )\n\n\nasync def _default_logging_callback(\n    params: types.LoggingMessageNotificationParams,\n) -> None:\n    pass\n\n\nClientResponse: TypeAdapter[types.ClientResult | types.ErrorData] = TypeAdapter(\n    types.ClientResult | types.ErrorData\n)\n\n\nclass ClientSession(\n    BaseSession[\n        types.ClientRequest,\n        types.ClientNotification,\n        types.ClientResult,\n        types.ServerRequest,\n        types.ServerNotification,\n    ]\n):\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        read_timeout_seconds: timedelta | None = None,\n        sampling_callback: SamplingFnT | None = None,\n        list_roots_callback: ListRootsFnT | None = None,\n        logging_callback: LoggingFnT | None = None,\n        message_handler: MessageHandlerFnT | None = None,\n        client_info: types.Implementation | None = None,\n    ) -> None:\n        super().__init__(\n            read_stream,\n            write_stream,\n            types.ServerRequest,\n            types.ServerNotification,\n            read_timeout_seconds=read_timeout_seconds,\n        )\n        self._client_info = client_info or DEFAULT_CLIENT_INFO\n        self._sampling_callback = sampling_callback or _default_sampling_callback\n        self._list_roots_callback = list_roots_callback or _default_list_roots_callback\n        self._logging_callback = logging_callback or _default_logging_callback\n        self._message_handler = message_handler or _default_message_handler\n\n    async def initialize(self) -> types.InitializeResult:\n        sampling = types.SamplingCapability()\n        roots = types.RootsCapability(\n            # TODO: Should this be based on whether we\n            # _will_ send notifications, or only whether\n            # they're supported?\n            listChanged=True,\n        )\n\n        result = await self.send_request(\n            types.ClientRequest(\n                types.InitializeRequest(\n                    method=\"initialize\",\n                    params=types.InitializeRequestParams(\n                        protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                        capabilities=types.ClientCapabilities(\n                            sampling=sampling,\n                            experimental=None,\n                            roots=roots,\n                        ),\n                        clientInfo=self._client_info,\n                    ),\n                )\n            ),\n            types.InitializeResult,\n        )\n\n        if result.protocolVersion not in SUPPORTED_PROTOCOL_VERSIONS:\n            raise RuntimeError(\n                \"Unsupported protocol version from the server: \"\n                f\"{result.protocolVersion}\"\n            )\n\n        await self.send_notification(\n            types.ClientNotification(\n                types.InitializedNotification(method=\"notifications/initialized\")\n            )\n        )\n\n        return result\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                ),\n            )\n        )\n\n    async def set_logging_level(self, level: types.LoggingLevel) -> types.EmptyResult:\n        \"\"\"Send a logging/setLevel request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SetLevelRequest(\n                    method=\"logging/setLevel\",\n                    params=types.SetLevelRequestParams(level=level),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def list_resources(self) -> types.ListResourcesResult:\n        \"\"\"Send a resources/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourcesRequest(\n                    method=\"resources/list\",\n                )\n            ),\n            types.ListResourcesResult,\n        )\n\n    async def list_resource_templates(self) -> types.ListResourceTemplatesResult:\n        \"\"\"Send a resources/templates/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourceTemplatesRequest(\n                    method=\"resources/templates/list\",\n                )\n            ),\n            types.ListResourceTemplatesResult,\n        )\n\n    async def read_resource(self, uri: AnyUrl) -> types.ReadResourceResult:\n        \"\"\"Send a resources/read request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ReadResourceRequest(\n                    method=\"resources/read\",\n                    params=types.ReadResourceRequestParams(uri=uri),\n                )\n            ),\n            types.ReadResourceResult,\n        )\n\n    async def subscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/subscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SubscribeRequest(\n                    method=\"resources/subscribe\",\n                    params=types.SubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def unsubscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/unsubscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.UnsubscribeRequest(\n                    method=\"resources/unsubscribe\",\n                    params=types.UnsubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CallToolRequest(\n                    method=\"tools/call\",\n                    params=types.CallToolRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.CallToolResult,\n        )\n\n    async def list_prompts(self) -> types.ListPromptsResult:\n        \"\"\"Send a prompts/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListPromptsRequest(\n                    method=\"prompts/list\",\n                )\n            ),\n            types.ListPromptsResult,\n        )\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.GetPromptRequest(\n                    method=\"prompts/get\",\n                    params=types.GetPromptRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.GetPromptResult,\n        )\n\n    async def complete(\n        self,\n        ref: types.ResourceReference | types.PromptReference,\n        argument: dict[str, str],\n    ) -> types.CompleteResult:\n        \"\"\"Send a completion/complete request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CompleteRequest(\n                    method=\"completion/complete\",\n                    params=types.CompleteRequestParams(\n                        ref=ref,\n                        argument=types.CompletionArgument(**argument),\n                    ),\n                )\n            ),\n            types.CompleteResult,\n        )\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListToolsRequest(\n                    method=\"tools/list\",\n                )\n            ),\n            types.ListToolsResult,\n        )\n\n    async def send_roots_list_changed(self) -> None:\n        \"\"\"Send a roots/list_changed notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.RootsListChangedNotification(\n                    method=\"notifications/roots/list_changed\",\n                )\n            )\n        )\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ServerRequest, types.ClientResult]\n    ) -> None:\n        ctx = RequestContext[ClientSession, Any](\n            request_id=responder.request_id,\n            meta=responder.request_meta,\n            session=self,\n            lifespan_context=None,\n        )\n\n        match responder.request.root:\n            case types.CreateMessageRequest(params=params):\n                with responder:\n                    response = await self._sampling_callback(ctx, params)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.ListRootsRequest():\n                with responder:\n                    response = await self._list_roots_callback(ctx)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.PingRequest():\n                with responder:\n                    return await responder.respond(\n                        types.ClientResult(root=types.EmptyResult())\n                    )\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None:\n        \"\"\"Handle incoming messages by forwarding to the message handler.\"\"\"\n        await self._message_handler(req)\n\n    async def _received_notification(\n        self, notification: types.ServerNotification\n    ) -> None:\n        \"\"\"Handle notifications from the server.\"\"\"\n        # Process specific notification types\n        match notification.root:\n            case types.LoggingMessageNotification(params=params):\n                await self._logging_callback(params)\n            case _:\n                pass\n\n\n--- File Index 27: src/mcp/client/sse.py ---\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import urljoin, urlparse\n\nimport anyio\nimport httpx\nfrom anyio.abc import TaskStatus\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom httpx_sse import aconnect_sse\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\ndef remove_request_params(url: str) -> str:\n    return urljoin(url, urlparse(url).path)\n\n\n@asynccontextmanager\nasync def sse_client(\n    url: str,\n    headers: dict[str, Any] | None = None,\n    timeout: float = 5,\n    sse_read_timeout: float = 60 * 5,\n):\n    \"\"\"\n    Client transport for SSE.\n\n    `sse_read_timeout` determines how long (in seconds) the client will wait for a new\n    event before disconnecting. All other HTTP operations are controlled by `timeout`.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async with anyio.create_task_group() as tg:\n        try:\n            logger.info(f\"Connecting to SSE endpoint: {remove_request_params(url)}\")\n            async with httpx.AsyncClient(headers=headers) as client:\n                async with aconnect_sse(\n                    client,\n                    \"GET\",\n                    url,\n                    timeout=httpx.Timeout(timeout, read=sse_read_timeout),\n                ) as event_source:\n                    event_source.response.raise_for_status()\n                    logger.debug(\"SSE connection established\")\n\n                    async def sse_reader(\n                        task_status: TaskStatus[str] = anyio.TASK_STATUS_IGNORED,\n                    ):\n                        try:\n                            async for sse in event_source.aiter_sse():\n                                logger.debug(f\"Received SSE event: {sse.event}\")\n                                match sse.event:\n                                    case \"endpoint\":\n                                        endpoint_url = urljoin(url, sse.data)\n                                        logger.info(\n                                            f\"Received endpoint URL: {endpoint_url}\"\n                                        )\n\n                                        url_parsed = urlparse(url)\n                                        endpoint_parsed = urlparse(endpoint_url)\n                                        if (\n                                            url_parsed.netloc != endpoint_parsed.netloc\n                                            or url_parsed.scheme\n                                            != endpoint_parsed.scheme\n                                        ):\n                                            error_msg = (\n                                                \"Endpoint origin does not match \"\n                                                f\"connection origin: {endpoint_url}\"\n                                            )\n                                            logger.error(error_msg)\n                                            raise ValueError(error_msg)\n\n                                        task_status.started(endpoint_url)\n\n                                    case \"message\":\n                                        try:\n                                            message = types.JSONRPCMessage.model_validate_json(  # noqa: E501\n                                                sse.data\n                                            )\n                                            logger.debug(\n                                                f\"Received server message: {message}\"\n                                            )\n                                        except Exception as exc:\n                                            logger.error(\n                                                f\"Error parsing server message: {exc}\"\n                                            )\n                                            await read_stream_writer.send(exc)\n                                            continue\n\n                                        await read_stream_writer.send(message)\n                                    case _:\n                                        logger.warning(\n                                            f\"Unknown SSE event: {sse.event}\"\n                                        )\n                        except Exception as exc:\n                            logger.error(f\"Error in sse_reader: {exc}\")\n                            await read_stream_writer.send(exc)\n                        finally:\n                            await read_stream_writer.aclose()\n\n                    async def post_writer(endpoint_url: str):\n                        try:\n                            async with write_stream_reader:\n                                async for message in write_stream_reader:\n                                    logger.debug(f\"Sending client message: {message}\")\n                                    response = await client.post(\n                                        endpoint_url,\n                                        json=message.model_dump(\n                                            by_alias=True,\n                                            mode=\"json\",\n                                            exclude_none=True,\n                                        ),\n                                    )\n                                    response.raise_for_status()\n                                    logger.debug(\n                                        \"Client message sent successfully: \"\n                                        f\"{response.status_code}\"\n                                    )\n                        except Exception as exc:\n                            logger.error(f\"Error in post_writer: {exc}\")\n                        finally:\n                            await write_stream.aclose()\n\n                    endpoint_url = await tg.start(sse_reader)\n                    logger.info(\n                        f\"Starting post writer with endpoint URL: {endpoint_url}\"\n                    )\n                    tg.start_soon(post_writer, endpoint_url)\n\n                    try:\n                        yield read_stream, write_stream\n                    finally:\n                        tg.cancel_scope.cancel()\n        finally:\n            await read_stream_writer.aclose()\n            await write_stream.aclose()\n\n\n--- File Index 28: src/mcp/client/stdio/__init__.py ---\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\nfrom typing import Literal, TextIO\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom anyio.streams.text import TextReceiveStream\nfrom pydantic import BaseModel, Field\n\nimport mcp.types as types\n\nfrom .win32 import (\n    create_windows_process,\n    get_windows_executable_command,\n    terminate_windows_process,\n)\n\n# Environment variables to inherit by default\nDEFAULT_INHERITED_ENV_VARS = (\n    [\n        \"APPDATA\",\n        \"HOMEDRIVE\",\n        \"HOMEPATH\",\n        \"LOCALAPPDATA\",\n        \"PATH\",\n        \"PROCESSOR_ARCHITECTURE\",\n        \"SYSTEMDRIVE\",\n        \"SYSTEMROOT\",\n        \"TEMP\",\n        \"USERNAME\",\n        \"USERPROFILE\",\n    ]\n    if sys.platform == \"win32\"\n    else [\"HOME\", \"LOGNAME\", \"PATH\", \"SHELL\", \"TERM\", \"USER\"]\n)\n\n\ndef get_default_environment() -> dict[str, str]:\n    \"\"\"\n    Returns a default environment object including only environment variables deemed\n    safe to inherit.\n    \"\"\"\n    env: dict[str, str] = {}\n\n    for key in DEFAULT_INHERITED_ENV_VARS:\n        value = os.environ.get(key)\n        if value is None:\n            continue\n\n        if value.startswith(\"()\"):\n            # Skip functions, which are a security risk\n            continue\n\n        env[key] = value\n\n    return env\n\n\nclass StdioServerParameters(BaseModel):\n    command: str\n    \"\"\"The executable to run to start the server.\"\"\"\n\n    args: list[str] = Field(default_factory=list)\n    \"\"\"Command line arguments to pass to the executable.\"\"\"\n\n    env: dict[str, str] | None = None\n    \"\"\"\n    The environment to use when spawning the process.\n\n    If not specified, the result of get_default_environment() will be used.\n    \"\"\"\n\n    cwd: str | Path | None = None\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    encoding: str = \"utf-8\"\n    \"\"\"\n    The text encoding used when sending/receiving messages to the server\n\n    defaults to utf-8\n    \"\"\"\n\n    encoding_error_handler: Literal[\"strict\", \"ignore\", \"replace\"] = \"strict\"\n    \"\"\"\n    The text encoding error handler.\n\n    See https://docs.python.org/3/library/codecs.html#codec-base-classes for\n    explanations of possible values\n    \"\"\"\n\n\n@asynccontextmanager\nasync def stdio_client(server: StdioServerParameters, errlog: TextIO = sys.stderr):\n    \"\"\"\n    Client transport for stdio: this will connect to a server by spawning a\n    process and communicating with it over stdin/stdout.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    command = _get_executable_command(server.command)\n\n    # Open process with stderr piped for capture\n    process = await _create_platform_compatible_process(\n        command=command,\n        args=server.args,\n        env=(\n            {**get_default_environment(), **server.env}\n            if server.env is not None\n            else get_default_environment()\n        ),\n        errlog=errlog,\n        cwd=server.cwd,\n    )\n\n    async def stdout_reader():\n        assert process.stdout, \"Opened process is missing stdout\"\n\n        try:\n            async with read_stream_writer:\n                buffer = \"\"\n                async for chunk in TextReceiveStream(\n                    process.stdout,\n                    encoding=server.encoding,\n                    errors=server.encoding_error_handler,\n                ):\n                    lines = (buffer + chunk).split(\"\\n\")\n                    buffer = lines.pop()\n\n                    for line in lines:\n                        try:\n                            message = types.JSONRPCMessage.model_validate_json(line)\n                        except Exception as exc:\n                            await read_stream_writer.send(exc)\n                            continue\n\n                        await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdin_writer():\n        assert process.stdin, \"Opened process is missing stdin\"\n\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await process.stdin.send(\n                        (json + \"\\n\").encode(\n                            encoding=server.encoding,\n                            errors=server.encoding_error_handler,\n                        )\n                    )\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with (\n        anyio.create_task_group() as tg,\n        process,\n    ):\n        tg.start_soon(stdout_reader)\n        tg.start_soon(stdin_writer)\n        try:\n            yield read_stream, write_stream\n        finally:\n            # Clean up process to prevent any dangling orphaned processes\n            if sys.platform == \"win32\":\n                await terminate_windows_process(process)\n            else:\n                process.terminate()\n\n\ndef _get_executable_command(command: str) -> str:\n    \"\"\"\n    Get the correct executable command normalized for the current platform.\n\n    Args:\n        command: Base command (e.g., 'uvx', 'npx')\n\n    Returns:\n        str: Platform-appropriate command\n    \"\"\"\n    if sys.platform == \"win32\":\n        return get_windows_executable_command(command)\n    else:\n        return command\n\n\nasync def _create_platform_compatible_process(\n    command: str,\n    args: list[str],\n    env: dict[str, str] | None = None,\n    errlog: TextIO = sys.stderr,\n    cwd: Path | str | None = None,\n):\n    \"\"\"\n    Creates a subprocess in a platform-compatible way.\n    Returns a process handle.\n    \"\"\"\n    if sys.platform == \"win32\":\n        process = await create_windows_process(command, args, env, errlog, cwd)\n    else:\n        process = await anyio.open_process(\n            [command, *args], env=env, stderr=errlog, cwd=cwd\n        )\n\n    return process\n\n\n--- File Index 29: src/mcp/client/stdio/win32.py ---\n\"\"\"\nWindows-specific functionality for stdio client operations.\n\"\"\"\n\nimport shutil\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom typing import TextIO\n\nimport anyio\nfrom anyio.abc import Process\n\n\ndef get_windows_executable_command(command: str) -> str:\n    \"\"\"\n    Get the correct executable command normalized for Windows.\n\n    On Windows, commands might exist with specific extensions (.exe, .cmd, etc.)\n    that need to be located for proper execution.\n\n    Args:\n        command: Base command (e.g., 'uvx', 'npx')\n\n    Returns:\n        str: Windows-appropriate command path\n    \"\"\"\n    try:\n        # First check if command exists in PATH as-is\n        if command_path := shutil.which(command):\n            return command_path\n\n        # Check for Windows-specific extensions\n        for ext in [\".cmd\", \".bat\", \".exe\", \".ps1\"]:\n            ext_version = f\"{command}{ext}\"\n            if ext_path := shutil.which(ext_version):\n                return ext_path\n\n        # For regular commands or if we couldn't find special versions\n        return command\n    except OSError:\n        # Handle file system errors during path resolution\n        # (permissions, broken symlinks, etc.)\n        return command\n\n\nasync def create_windows_process(\n    command: str,\n    args: list[str],\n    env: dict[str, str] | None = None,\n    errlog: TextIO = sys.stderr,\n    cwd: Path | str | None = None,\n):\n    \"\"\"\n    Creates a subprocess in a Windows-compatible way.\n\n    Windows processes need special handling for console windows and\n    process creation flags.\n\n    Args:\n        command: The command to execute\n        args: Command line arguments\n        env: Environment variables\n        errlog: Where to send stderr output\n        cwd: Working directory for the process\n\n    Returns:\n        A process handle\n    \"\"\"\n    try:\n        # Try with Windows-specific flags to hide console window\n        process = await anyio.open_process(\n            [command, *args],\n            env=env,\n            # Ensure we don't create console windows for each process\n            creationflags=subprocess.CREATE_NO_WINDOW  # type: ignore\n            if hasattr(subprocess, \"CREATE_NO_WINDOW\")\n            else 0,\n            stderr=errlog,\n            cwd=cwd,\n        )\n        return process\n    except Exception:\n        # Don't raise, let's try to create the process without creation flags\n        process = await anyio.open_process(\n            [command, *args], env=env, stderr=errlog, cwd=cwd\n        )\n        return process\n\n\nasync def terminate_windows_process(process: Process):\n    \"\"\"\n    Terminate a Windows process.\n\n    Note: On Windows, terminating a process with process.terminate() doesn't\n    always guarantee immediate process termination.\n    So we give it 2s to exit, or we call process.kill()\n    which sends a SIGKILL equivalent signal.\n\n    Args:\n        process: The process to terminate\n    \"\"\"\n    try:\n        process.terminate()\n        with anyio.fail_after(2.0):\n            await process.wait()\n    except TimeoutError:\n        # Force kill if it doesn't terminate\n        process.kill()\n\n\n--- File Index 30: src/mcp/client/websocket.py ---\nimport json\nimport logging\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom websockets.asyncio.client import connect as ws_connect\nfrom websockets.typing import Subprotocol\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_client(\n    url: str,\n) -> AsyncGenerator[\n    tuple[\n        MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        MemoryObjectSendStream[types.JSONRPCMessage],\n    ],\n    None,\n]:\n    \"\"\"\n    WebSocket client transport for MCP, symmetrical to the server version.\n\n    Connects to 'url' using the 'mcp' subprotocol, then yields:\n        (read_stream, write_stream)\n\n    - read_stream: As you read from this stream, you'll receive either valid\n      JSONRPCMessage objects or Exception objects (when validation fails).\n    - write_stream: Write JSONRPCMessage objects to this stream to send them\n      over the WebSocket to the server.\n    \"\"\"\n\n    # Create two in-memory streams:\n    # - One for incoming messages (read_stream, written by ws_reader)\n    # - One for outgoing messages (write_stream, read by ws_writer)\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    # Connect using websockets, requesting the \"mcp\" subprotocol\n    async with ws_connect(url, subprotocols=[Subprotocol(\"mcp\")]) as ws:\n\n        async def ws_reader():\n            \"\"\"\n            Reads text messages from the WebSocket, parses them as JSON-RPC messages,\n            and sends them into read_stream_writer.\n            \"\"\"\n            async with read_stream_writer:\n                async for raw_text in ws:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(raw_text)\n                        await read_stream_writer.send(message)\n                    except ValidationError as exc:\n                        # If JSON parse or model validation fails, send the exception\n                        await read_stream_writer.send(exc)\n\n        async def ws_writer():\n            \"\"\"\n            Reads JSON-RPC messages from write_stream_reader and\n            sends them to the server.\n            \"\"\"\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    # Convert to a dict, then to JSON\n                    msg_dict = message.model_dump(\n                        by_alias=True, mode=\"json\", exclude_none=True\n                    )\n                    await ws.send(json.dumps(msg_dict))\n\n        async with anyio.create_task_group() as tg:\n            # Start reader and writer tasks\n            tg.start_soon(ws_reader)\n            tg.start_soon(ws_writer)\n\n            # Yield the receive/send streams\n            yield (read_stream, write_stream)\n\n            # Once the caller's 'async with' block exits, we shut down\n            tg.cancel_scope.cancel()\n\n\n--- File Index 31: src/mcp/server/__init__.py ---\nfrom .fastmcp import FastMCP\nfrom .lowlevel import NotificationOptions, Server\nfrom .models import InitializationOptions\n\n__all__ = [\"Server\", \"FastMCP\", \"NotificationOptions\", \"InitializationOptions\"]\n\n\n--- File Index 32: src/mcp/server/__main__.py ---\nimport importlib.metadata\nimport logging\nimport sys\n\nimport anyio\n\nfrom mcp.server.models import InitializationOptions\nfrom mcp.server.session import ServerSession\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import ServerCapabilities\n\nif not sys.warnoptions:\n    import warnings\n\n    warnings.simplefilter(\"ignore\")\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"server\")\n\n\nasync def receive_loop(session: ServerSession):\n    logger.info(\"Starting receive loop\")\n    async for message in session.incoming_messages:\n        if isinstance(message, Exception):\n            logger.error(\"Error: %s\", message)\n            continue\n\n        logger.info(\"Received message from client: %s\", message)\n\n\nasync def main():\n    version = importlib.metadata.version(\"mcp\")\n    async with stdio_server() as (read_stream, write_stream):\n        async with (\n            ServerSession(\n                read_stream,\n                write_stream,\n                InitializationOptions(\n                    server_name=\"mcp\",\n                    server_version=version,\n                    capabilities=ServerCapabilities(),\n                ),\n            ) as session,\n            write_stream,\n        ):\n            await receive_loop(session)\n\n\nif __name__ == \"__main__\":\n    anyio.run(main, backend=\"trio\")\n\n\n--- File Index 33: src/mcp/server/fastmcp/__init__.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom importlib.metadata import version\n\nfrom .server import Context, FastMCP\nfrom .utilities.types import Image\n\n__version__ = version(\"mcp\")\n__all__ = [\"FastMCP\", \"Context\", \"Image\"]\n\n\n--- File Index 34: src/mcp/server/fastmcp/exceptions.py ---\n\"\"\"Custom exceptions for FastMCP.\"\"\"\n\n\nclass FastMCPError(Exception):\n    \"\"\"Base error for FastMCP.\"\"\"\n\n\nclass ValidationError(FastMCPError):\n    \"\"\"Error in validating parameters or return values.\"\"\"\n\n\nclass ResourceError(FastMCPError):\n    \"\"\"Error in resource operations.\"\"\"\n\n\nclass ToolError(FastMCPError):\n    \"\"\"Error in tool operations.\"\"\"\n\n\nclass InvalidSignature(Exception):\n    \"\"\"Invalid signature for use with FastMCP.\"\"\"\n\n\n--- File Index 35: src/mcp/server/fastmcp/prompts/__init__.py ---\nfrom .base import Prompt\nfrom .manager import PromptManager\n\n__all__ = [\"Prompt\", \"PromptManager\"]\n\n\n--- File Index 36: src/mcp/server/fastmcp/prompts/base.py ---\n\"\"\"Base classes for FastMCP prompts.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom typing import Any, Literal\n\nimport pydantic_core\nfrom pydantic import BaseModel, Field, TypeAdapter, validate_call\n\nfrom mcp.types import EmbeddedResource, ImageContent, TextContent\n\nCONTENT_TYPES = TextContent | ImageContent | EmbeddedResource\n\n\nclass Message(BaseModel):\n    \"\"\"Base class for all prompt messages.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"]\n    content: CONTENT_TYPES\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        if isinstance(content, str):\n            content = TextContent(type=\"text\", text=content)\n        super().__init__(content=content, **kwargs)\n\n\nclass UserMessage(Message):\n    \"\"\"A message from the user.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"user\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nclass AssistantMessage(Message):\n    \"\"\"A message from the assistant.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"assistant\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nmessage_validator = TypeAdapter[UserMessage | AssistantMessage](\n    UserMessage | AssistantMessage\n)\n\nSyncPromptResult = (\n    str | Message | dict[str, Any] | Sequence[str | Message | dict[str, Any]]\n)\nPromptResult = SyncPromptResult | Awaitable[SyncPromptResult]\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument that can be passed to a prompt.\"\"\"\n\n    name: str = Field(description=\"Name of the argument\")\n    description: str | None = Field(\n        None, description=\"Description of what the argument does\"\n    )\n    required: bool = Field(\n        default=False, description=\"Whether the argument is required\"\n    )\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt template that can be rendered with parameters.\"\"\"\n\n    name: str = Field(description=\"Name of the prompt\")\n    description: str | None = Field(\n        None, description=\"Description of what the prompt does\"\n    )\n    arguments: list[PromptArgument] | None = Field(\n        None, description=\"Arguments that can be passed to the prompt\"\n    )\n    fn: Callable[..., PromptResult | Awaitable[PromptResult]] = Field(exclude=True)\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., PromptResult | Awaitable[PromptResult]],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> \"Prompt\":\n        \"\"\"Create a Prompt from a function.\n\n        The function can return:\n        - A string (converted to a message)\n        - A Message object\n        - A dict (converted to a message)\n        - A sequence of any of the above\n        \"\"\"\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        # Get schema from TypeAdapter - will fail if function isn't properly typed\n        parameters = TypeAdapter(fn).json_schema()\n\n        # Convert parameters to PromptArguments\n        arguments: list[PromptArgument] = []\n        if \"properties\" in parameters:\n            for param_name, param in parameters[\"properties\"].items():\n                required = param_name in parameters.get(\"required\", [])\n                arguments.append(\n                    PromptArgument(\n                        name=param_name,\n                        description=param.get(\"description\"),\n                        required=required,\n                    )\n                )\n\n        # ensure the arguments are properly cast\n        fn = validate_call(fn)\n\n        return cls(\n            name=func_name,\n            description=description or fn.__doc__ or \"\",\n            arguments=arguments,\n            fn=fn,\n        )\n\n    async def render(self, arguments: dict[str, Any] | None = None) -> list[Message]:\n        \"\"\"Render the prompt with arguments.\"\"\"\n        # Validate required arguments\n        if self.arguments:\n            required = {arg.name for arg in self.arguments if arg.required}\n            provided = set(arguments or {})\n            missing = required - provided\n            if missing:\n                raise ValueError(f\"Missing required arguments: {missing}\")\n\n        try:\n            # Call function and check if result is a coroutine\n            result = self.fn(**(arguments or {}))\n            if inspect.iscoroutine(result):\n                result = await result\n\n            # Validate messages\n            if not isinstance(result, list | tuple):\n                result = [result]\n\n            # Convert result to messages\n            messages: list[Message] = []\n            for msg in result:  # type: ignore[reportUnknownVariableType]\n                try:\n                    if isinstance(msg, Message):\n                        messages.append(msg)\n                    elif isinstance(msg, dict):\n                        messages.append(message_validator.validate_python(msg))\n                    elif isinstance(msg, str):\n                        content = TextContent(type=\"text\", text=msg)\n                        messages.append(UserMessage(content=content))\n                    else:\n                        content = json.dumps(pydantic_core.to_jsonable_python(msg))\n                        messages.append(Message(role=\"user\", content=content))\n                except Exception:\n                    raise ValueError(\n                        f\"Could not convert prompt result to message: {msg}\"\n                    )\n\n            return messages\n        except Exception as e:\n            raise ValueError(f\"Error rendering prompt {self.name}: {e}\")\n\n\n--- File Index 37: src/mcp/server/fastmcp/prompts/manager.py ---\n\"\"\"Prompt management functionality.\"\"\"\n\nfrom typing import Any\n\nfrom mcp.server.fastmcp.prompts.base import Message, Prompt\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass PromptManager:\n    \"\"\"Manages FastMCP prompts.\"\"\"\n\n    def __init__(self, warn_on_duplicate_prompts: bool = True):\n        self._prompts: dict[str, Prompt] = {}\n        self.warn_on_duplicate_prompts = warn_on_duplicate_prompts\n\n    def get_prompt(self, name: str) -> Prompt | None:\n        \"\"\"Get prompt by name.\"\"\"\n        return self._prompts.get(name)\n\n    def list_prompts(self) -> list[Prompt]:\n        \"\"\"List all registered prompts.\"\"\"\n        return list(self._prompts.values())\n\n    def add_prompt(\n        self,\n        prompt: Prompt,\n    ) -> Prompt:\n        \"\"\"Add a prompt to the manager.\"\"\"\n\n        # Check for duplicates\n        existing = self._prompts.get(prompt.name)\n        if existing:\n            if self.warn_on_duplicate_prompts:\n                logger.warning(f\"Prompt already exists: {prompt.name}\")\n            return existing\n\n        self._prompts[prompt.name] = prompt\n        return prompt\n\n    async def render_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> list[Message]:\n        \"\"\"Render a prompt by name with arguments.\"\"\"\n        prompt = self.get_prompt(name)\n        if not prompt:\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        return await prompt.render(arguments)\n\n\n--- File Index 38: src/mcp/server/fastmcp/prompts/prompt_manager.py ---\n\"\"\"Prompt management functionality.\"\"\"\n\nfrom mcp.server.fastmcp.prompts.base import Prompt\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass PromptManager:\n    \"\"\"Manages FastMCP prompts.\"\"\"\n\n    def __init__(self, warn_on_duplicate_prompts: bool = True):\n        self._prompts: dict[str, Prompt] = {}\n        self.warn_on_duplicate_prompts = warn_on_duplicate_prompts\n\n    def add_prompt(self, prompt: Prompt) -> Prompt:\n        \"\"\"Add a prompt to the manager.\"\"\"\n        logger.debug(f\"Adding prompt: {prompt.name}\")\n        existing = self._prompts.get(prompt.name)\n        if existing:\n            if self.warn_on_duplicate_prompts:\n                logger.warning(f\"Prompt already exists: {prompt.name}\")\n            return existing\n        self._prompts[prompt.name] = prompt\n        return prompt\n\n    def get_prompt(self, name: str) -> Prompt | None:\n        \"\"\"Get prompt by name.\"\"\"\n        return self._prompts.get(name)\n\n    def list_prompts(self) -> list[Prompt]:\n        \"\"\"List all registered prompts.\"\"\"\n        return list(self._prompts.values())\n\n\n--- File Index 39: src/mcp/server/fastmcp/resources/__init__.py ---\nfrom .base import Resource\nfrom .resource_manager import ResourceManager\nfrom .templates import ResourceTemplate\nfrom .types import (\n    BinaryResource,\n    DirectoryResource,\n    FileResource,\n    FunctionResource,\n    HttpResource,\n    TextResource,\n)\n\n__all__ = [\n    \"Resource\",\n    \"TextResource\",\n    \"BinaryResource\",\n    \"FunctionResource\",\n    \"FileResource\",\n    \"HttpResource\",\n    \"DirectoryResource\",\n    \"ResourceTemplate\",\n    \"ResourceManager\",\n]\n\n\n--- File Index 40: src/mcp/server/fastmcp/resources/base.py ---\n\"\"\"Base classes and interfaces for FastMCP resources.\"\"\"\n\nimport abc\nfrom typing import Annotated\n\nfrom pydantic import (\n    AnyUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    UrlConstraints,\n    ValidationInfo,\n    field_validator,\n)\n\n\nclass Resource(BaseModel, abc.ABC):\n    \"\"\"Base class for all resources.\"\"\"\n\n    model_config = ConfigDict(validate_default=True)\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)] = Field(\n        default=..., description=\"URI of the resource\"\n    )\n    name: str | None = Field(description=\"Name of the resource\", default=None)\n    description: str | None = Field(\n        description=\"Description of the resource\", default=None\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n        pattern=r\"^[a-zA-Z0-9]+/[a-zA-Z0-9\\-+.]+$\",\n    )\n\n    @field_validator(\"name\", mode=\"before\")\n    @classmethod\n    def set_default_name(cls, name: str | None, info: ValidationInfo) -> str:\n        \"\"\"Set default name from URI if not provided.\"\"\"\n        if name:\n            return name\n        if uri := info.data.get(\"uri\"):\n            return str(uri)\n        raise ValueError(\"Either name or uri must be provided\")\n\n    @abc.abstractmethod\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource content.\"\"\"\n        pass\n\n\n--- File Index 41: src/mcp/server/fastmcp/resources/resource_manager.py ---\n\"\"\"Resource manager functionality.\"\"\"\n\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom pydantic import AnyUrl\n\nfrom mcp.server.fastmcp.resources.base import Resource\nfrom mcp.server.fastmcp.resources.templates import ResourceTemplate\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass ResourceManager:\n    \"\"\"Manages FastMCP resources.\"\"\"\n\n    def __init__(self, warn_on_duplicate_resources: bool = True):\n        self._resources: dict[str, Resource] = {}\n        self._templates: dict[str, ResourceTemplate] = {}\n        self.warn_on_duplicate_resources = warn_on_duplicate_resources\n\n    def add_resource(self, resource: Resource) -> Resource:\n        \"\"\"Add a resource to the manager.\n\n        Args:\n            resource: A Resource instance to add\n\n        Returns:\n            The added resource. If a resource with the same URI already exists,\n            returns the existing resource.\n        \"\"\"\n        logger.debug(\n            \"Adding resource\",\n            extra={\n                \"uri\": resource.uri,\n                \"type\": type(resource).__name__,\n                \"resource_name\": resource.name,\n            },\n        )\n        existing = self._resources.get(str(resource.uri))\n        if existing:\n            if self.warn_on_duplicate_resources:\n                logger.warning(f\"Resource already exists: {resource.uri}\")\n            return existing\n        self._resources[str(resource.uri)] = resource\n        return resource\n\n    def add_template(\n        self,\n        fn: Callable[..., Any],\n        uri_template: str,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> ResourceTemplate:\n        \"\"\"Add a template from a function.\"\"\"\n        template = ResourceTemplate.from_function(\n            fn,\n            uri_template=uri_template,\n            name=name,\n            description=description,\n            mime_type=mime_type,\n        )\n        self._templates[template.uri_template] = template\n        return template\n\n    async def get_resource(self, uri: AnyUrl | str) -> Resource | None:\n        \"\"\"Get resource by URI, checking concrete resources first, then templates.\"\"\"\n        uri_str = str(uri)\n        logger.debug(\"Getting resource\", extra={\"uri\": uri_str})\n\n        # First check concrete resources\n        if resource := self._resources.get(uri_str):\n            return resource\n\n        # Then check templates\n        for template in self._templates.values():\n            if params := template.matches(uri_str):\n                try:\n                    return await template.create_resource(uri_str, params)\n                except Exception as e:\n                    raise ValueError(f\"Error creating resource from template: {e}\")\n\n        raise ValueError(f\"Unknown resource: {uri}\")\n\n    def list_resources(self) -> list[Resource]:\n        \"\"\"List all registered resources.\"\"\"\n        logger.debug(\"Listing resources\", extra={\"count\": len(self._resources)})\n        return list(self._resources.values())\n\n    def list_templates(self) -> list[ResourceTemplate]:\n        \"\"\"List all registered templates.\"\"\"\n        logger.debug(\"Listing templates\", extra={\"count\": len(self._templates)})\n        return list(self._templates.values())\n\n\n--- File Index 42: src/mcp/server/fastmcp/resources/templates.py ---\n\"\"\"Resource template functionality.\"\"\"\n\nfrom __future__ import annotations\n\nimport inspect\nimport re\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom pydantic import BaseModel, Field, TypeAdapter, validate_call\n\nfrom mcp.server.fastmcp.resources.types import FunctionResource, Resource\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template for dynamically creating resources.\"\"\"\n\n    uri_template: str = Field(\n        description=\"URI template with parameters (e.g. weather://{city}/current)\"\n    )\n    name: str = Field(description=\"Name of the resource\")\n    description: str | None = Field(description=\"Description of what the resource does\")\n    mime_type: str = Field(\n        default=\"text/plain\", description=\"MIME type of the resource content\"\n    )\n    fn: Callable[..., Any] = Field(exclude=True)\n    parameters: dict[str, Any] = Field(\n        description=\"JSON schema for function parameters\"\n    )\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., Any],\n        uri_template: str,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> ResourceTemplate:\n        \"\"\"Create a template from a function.\"\"\"\n        func_name = name or fn.__name__\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        # Get schema from TypeAdapter - will fail if function isn't properly typed\n        parameters = TypeAdapter(fn).json_schema()\n\n        # ensure the arguments are properly cast\n        fn = validate_call(fn)\n\n        return cls(\n            uri_template=uri_template,\n            name=func_name,\n            description=description or fn.__doc__ or \"\",\n            mime_type=mime_type or \"text/plain\",\n            fn=fn,\n            parameters=parameters,\n        )\n\n    def matches(self, uri: str) -> dict[str, Any] | None:\n        \"\"\"Check if URI matches template and extract parameters.\"\"\"\n        # Convert template to regex pattern\n        pattern = self.uri_template.replace(\"{\", \"(?P<\").replace(\"}\", \">[^/]+)\")\n        match = re.match(f\"^{pattern}$\", uri)\n        if match:\n            return match.groupdict()\n        return None\n\n    async def create_resource(self, uri: str, params: dict[str, Any]) -> Resource:\n        \"\"\"Create a resource from the template with the given parameters.\"\"\"\n        try:\n            # Call function and check if result is a coroutine\n            result = self.fn(**params)\n            if inspect.iscoroutine(result):\n                result = await result\n\n            return FunctionResource(\n                uri=uri,  # type: ignore\n                name=self.name,\n                description=self.description,\n                mime_type=self.mime_type,\n                fn=lambda: result,  # Capture result in closure\n            )\n        except Exception as e:\n            raise ValueError(f\"Error creating resource from template: {e}\")\n\n\n--- File Index 43: src/mcp/server/fastmcp/resources/types.py ---\n\"\"\"Concrete resource implementations.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\n\nimport anyio\nimport anyio.to_thread\nimport httpx\nimport pydantic.json\nimport pydantic_core\nfrom pydantic import Field, ValidationInfo\n\nfrom mcp.server.fastmcp.resources.base import Resource\n\n\nclass TextResource(Resource):\n    \"\"\"A resource that reads from a string.\"\"\"\n\n    text: str = Field(description=\"Text content of the resource\")\n\n    async def read(self) -> str:\n        \"\"\"Read the text content.\"\"\"\n        return self.text\n\n\nclass BinaryResource(Resource):\n    \"\"\"A resource that reads from bytes.\"\"\"\n\n    data: bytes = Field(description=\"Binary content of the resource\")\n\n    async def read(self) -> bytes:\n        \"\"\"Read the binary content.\"\"\"\n        return self.data\n\n\nclass FunctionResource(Resource):\n    \"\"\"A resource that defers data loading by wrapping a function.\n\n    The function is only called when the resource is read, allowing for lazy loading\n    of potentially expensive data. This is particularly useful when listing resources,\n    as the function won't be called until the resource is actually accessed.\n\n    The function can return:\n    - str for text content (default)\n    - bytes for binary content\n    - other types will be converted to JSON\n    \"\"\"\n\n    fn: Callable[[], Any] = Field(exclude=True)\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource by calling the wrapped function.\"\"\"\n        try:\n            result = (\n                await self.fn() if inspect.iscoroutinefunction(self.fn) else self.fn()\n            )\n            if isinstance(result, Resource):\n                return await result.read()\n            if isinstance(result, bytes):\n                return result\n            if isinstance(result, str):\n                return result\n            try:\n                return json.dumps(pydantic_core.to_jsonable_python(result))\n            except (TypeError, pydantic_core.PydanticSerializationError):\n                # If JSON serialization fails, try str()\n                return str(result)\n        except Exception as e:\n            raise ValueError(f\"Error reading resource {self.uri}: {e}\")\n\n\nclass FileResource(Resource):\n    \"\"\"A resource that reads from a file.\n\n    Set is_binary=True to read file as binary data instead of text.\n    \"\"\"\n\n    path: Path = Field(description=\"Path to the file\")\n    is_binary: bool = Field(\n        default=False,\n        description=\"Whether to read the file as binary data\",\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    @pydantic.field_validator(\"is_binary\")\n    @classmethod\n    def set_binary_from_mime_type(cls, is_binary: bool, info: ValidationInfo) -> bool:\n        \"\"\"Set is_binary based on mime_type if not explicitly set.\"\"\"\n        if is_binary:\n            return True\n        mime_type = info.data.get(\"mime_type\", \"text/plain\")\n        return not mime_type.startswith(\"text/\")\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the file content.\"\"\"\n        try:\n            if self.is_binary:\n                return await anyio.to_thread.run_sync(self.path.read_bytes)\n            return await anyio.to_thread.run_sync(self.path.read_text)\n        except Exception as e:\n            raise ValueError(f\"Error reading file {self.path}: {e}\")\n\n\nclass HttpResource(Resource):\n    \"\"\"A resource that reads from an HTTP endpoint.\"\"\"\n\n    url: str = Field(description=\"URL to fetch content from\")\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the HTTP content.\"\"\"\n        async with httpx.AsyncClient() as client:\n            response = await client.get(self.url)\n            response.raise_for_status()\n            return response.text\n\n\nclass DirectoryResource(Resource):\n    \"\"\"A resource that lists files in a directory.\"\"\"\n\n    path: Path = Field(description=\"Path to the directory\")\n    recursive: bool = Field(\n        default=False, description=\"Whether to list files recursively\"\n    )\n    pattern: str | None = Field(\n        default=None, description=\"Optional glob pattern to filter files\"\n    )\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    def list_files(self) -> list[Path]:\n        \"\"\"List files in the directory.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Directory not found: {self.path}\")\n        if not self.path.is_dir():\n            raise NotADirectoryError(f\"Not a directory: {self.path}\")\n\n        try:\n            if self.pattern:\n                return (\n                    list(self.path.glob(self.pattern))\n                    if not self.recursive\n                    else list(self.path.rglob(self.pattern))\n                )\n            return (\n                list(self.path.glob(\"*\"))\n                if not self.recursive\n                else list(self.path.rglob(\"*\"))\n            )\n        except Exception as e:\n            raise ValueError(f\"Error listing directory {self.path}: {e}\")\n\n    async def read(self) -> str:  # Always returns JSON string\n        \"\"\"Read the directory listing.\"\"\"\n        try:\n            files = await anyio.to_thread.run_sync(self.list_files)\n            file_list = [str(f.relative_to(self.path)) for f in files if f.is_file()]\n            return json.dumps({\"files\": file_list}, indent=2)\n        except Exception as e:\n            raise ValueError(f\"Error reading directory {self.path}: {e}\")\n\n\n--- File Index 44: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File Index 45: src/mcp/server/fastmcp/tools/__init__.py ---\nfrom .base import Tool\nfrom .tool_manager import ToolManager\n\n__all__ = [\"Tool\", \"ToolManager\"]\n\n\n--- File Index 46: src/mcp/server/fastmcp/tools/base.py ---\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, get_origin\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.utilities.func_metadata import FuncMetadata, func_metadata\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n    from mcp.shared.context import LifespanContextT\n\n\nclass Tool(BaseModel):\n    \"\"\"Internal tool registration info.\"\"\"\n\n    fn: Callable[..., Any] = Field(exclude=True)\n    name: str = Field(description=\"Name of the tool\")\n    description: str = Field(description=\"Description of what the tool does\")\n    parameters: dict[str, Any] = Field(description=\"JSON schema for tool parameters\")\n    fn_metadata: FuncMetadata = Field(\n        description=\"Metadata about the function including a pydantic model for tool\"\n        \" arguments\"\n    )\n    is_async: bool = Field(description=\"Whether the tool is async\")\n    context_kwarg: str | None = Field(\n        None, description=\"Name of the kwarg that should receive context\"\n    )\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n        context_kwarg: str | None = None,\n    ) -> Tool:\n        \"\"\"Create a Tool from a function.\"\"\"\n        from mcp.server.fastmcp import Context\n\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        func_doc = description or fn.__doc__ or \"\"\n        is_async = inspect.iscoroutinefunction(fn)\n\n        if context_kwarg is None:\n            sig = inspect.signature(fn)\n            for param_name, param in sig.parameters.items():\n                if get_origin(param.annotation) is not None:\n                    continue\n                if issubclass(param.annotation, Context):\n                    context_kwarg = param_name\n                    break\n\n        func_arg_metadata = func_metadata(\n            fn,\n            skip_names=[context_kwarg] if context_kwarg is not None else [],\n        )\n        parameters = func_arg_metadata.arg_model.model_json_schema()\n\n        return cls(\n            fn=fn,\n            name=func_name,\n            description=func_doc,\n            parameters=parameters,\n            fn_metadata=func_arg_metadata,\n            is_async=is_async,\n            context_kwarg=context_kwarg,\n        )\n\n    async def run(\n        self,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Run the tool with arguments.\"\"\"\n        try:\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn,\n                self.is_async,\n                arguments,\n                {self.context_kwarg: context}\n                if self.context_kwarg is not None\n                else None,\n            )\n        except Exception as e:\n            raise ToolError(f\"Error executing tool {self.name}: {e}\") from e\n\n\n--- File Index 47: src/mcp/server/fastmcp/tools/tool_manager.py ---\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.tools.base import Tool\nfrom mcp.server.fastmcp.utilities.logging import get_logger\nfrom mcp.shared.context import LifespanContextT\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n\nlogger = get_logger(__name__)\n\n\nclass ToolManager:\n    \"\"\"Manages FastMCP tools.\"\"\"\n\n    def __init__(self, warn_on_duplicate_tools: bool = True):\n        self._tools: dict[str, Tool] = {}\n        self.warn_on_duplicate_tools = warn_on_duplicate_tools\n\n    def get_tool(self, name: str) -> Tool | None:\n        \"\"\"Get tool by name.\"\"\"\n        return self._tools.get(name)\n\n    def list_tools(self) -> list[Tool]:\n        \"\"\"List all registered tools.\"\"\"\n        return list(self._tools.values())\n\n    def add_tool(\n        self,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> Tool:\n        \"\"\"Add a tool to the server.\"\"\"\n        tool = Tool.from_function(fn, name=name, description=description)\n        existing = self._tools.get(tool.name)\n        if existing:\n            if self.warn_on_duplicate_tools:\n                logger.warning(f\"Tool already exists: {tool.name}\")\n            return existing\n        self._tools[tool.name] = tool\n        return tool\n\n    async def call_tool(\n        self,\n        name: str,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        tool = self.get_tool(name)\n        if not tool:\n            raise ToolError(f\"Unknown tool: {name}\")\n\n        return await tool.run(arguments, context=context)\n\n\n--- File Index 48: src/mcp/server/fastmcp/utilities/__init__.py ---\n\"\"\"FastMCP utility modules.\"\"\"\n\n\n--- File Index 49: src/mcp/server/fastmcp/utilities/func_metadata.py ---\nimport inspect\nimport json\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom typing import (\n    Annotated,\n    Any,\n    ForwardRef,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, WithJsonSchema, create_model\nfrom pydantic._internal._typing_extra import eval_type_backport\nfrom pydantic.fields import FieldInfo\nfrom pydantic_core import PydanticUndefined\n\nfrom mcp.server.fastmcp.exceptions import InvalidSignature\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass ArgModelBase(BaseModel):\n    \"\"\"A model representing the arguments to a function.\"\"\"\n\n    def model_dump_one_level(self) -> dict[str, Any]:\n        \"\"\"Return a dict of the model's fields, one level deep.\n\n        That is, sub-models etc are not dumped - they are kept as pydantic models.\n        \"\"\"\n        kwargs: dict[str, Any] = {}\n        for field_name in self.__class__.model_fields.keys():\n            kwargs[field_name] = getattr(self, field_name)\n        return kwargs\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n\nclass FuncMetadata(BaseModel):\n    arg_model: Annotated[type[ArgModelBase], WithJsonSchema(None)]\n    # We can add things in the future like\n    #  - Maybe some args are excluded from attempting to parse from JSON\n    #  - Maybe some args are special (like context) for dependency injection\n\n    async def call_fn_with_arg_validation(\n        self,\n        fn: Callable[..., Any] | Awaitable[Any],\n        fn_is_async: bool,\n        arguments_to_validate: dict[str, Any],\n        arguments_to_pass_directly: dict[str, Any] | None,\n    ) -> Any:\n        \"\"\"Call the given function with arguments validated and injected.\n\n        Arguments are first attempted to be parsed from JSON, then validated against\n        the argument model, before being passed to the function.\n        \"\"\"\n        arguments_pre_parsed = self.pre_parse_json(arguments_to_validate)\n        arguments_parsed_model = self.arg_model.model_validate(arguments_pre_parsed)\n        arguments_parsed_dict = arguments_parsed_model.model_dump_one_level()\n\n        arguments_parsed_dict |= arguments_to_pass_directly or {}\n\n        if fn_is_async:\n            if isinstance(fn, Awaitable):\n                return await fn\n            return await fn(**arguments_parsed_dict)\n        if isinstance(fn, Callable):\n            return fn(**arguments_parsed_dict)\n        raise TypeError(\"fn must be either Callable or Awaitable\")\n\n    def pre_parse_json(self, data: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Pre-parse data from JSON.\n\n        Return a dict with same keys as input but with values parsed from JSON\n        if appropriate.\n\n        This is to handle cases like `[\"a\", \"b\", \"c\"]` being passed in as JSON inside\n        a string rather than an actual list. Claude desktop is prone to this - in fact\n        it seems incapable of NOT doing this. For sub-models, it tends to pass\n        dicts (JSON objects) as JSON strings, which can be pre-parsed here.\n        \"\"\"\n        new_data = data.copy()  # Shallow copy\n        for field_name in self.arg_model.model_fields.keys():\n            if field_name not in data.keys():\n                continue\n            if isinstance(data[field_name], str):\n                try:\n                    pre_parsed = json.loads(data[field_name])\n                except json.JSONDecodeError:\n                    continue  # Not JSON - skip\n                if isinstance(pre_parsed, str | int | float):\n                    # This is likely that the raw value is e.g. `\"hello\"` which we\n                    # Should really be parsed as '\"hello\"' in Python - but if we parse\n                    # it as JSON it'll turn into just 'hello'. So we skip it.\n                    continue\n                new_data[field_name] = pre_parsed\n        assert new_data.keys() == data.keys()\n        return new_data\n\n    model_config = ConfigDict(\n        arbitrary_types_allowed=True,\n    )\n\n\ndef func_metadata(\n    func: Callable[..., Any], skip_names: Sequence[str] = ()\n) -> FuncMetadata:\n    \"\"\"Given a function, return metadata including a pydantic model representing its\n    signature.\n\n    The use case for this is\n    ```\n    meta = func_to_pyd(func)\n    validated_args = meta.arg_model.model_validate(some_raw_data_dict)\n    return func(**validated_args.model_dump_one_level())\n    ```\n\n    **critically** it also provides pre-parse helper to attempt to parse things from\n    JSON.\n\n    Args:\n        func: The function to convert to a pydantic model\n        skip_names: A list of parameter names to skip. These will not be included in\n            the model.\n    Returns:\n        A pydantic model representing the function's signature.\n    \"\"\"\n    sig = _get_typed_signature(func)\n    params = sig.parameters\n    dynamic_pydantic_model_params: dict[str, Any] = {}\n    globalns = getattr(func, \"__globals__\", {})\n    for param in params.values():\n        if param.name.startswith(\"_\"):\n            raise InvalidSignature(\n                f\"Parameter {param.name} of {func.__name__} cannot start with '_'\"\n            )\n        if param.name in skip_names:\n            continue\n        annotation = param.annotation\n\n        # `x: None` / `x: None = None`\n        if annotation is None:\n            annotation = Annotated[\n                None,\n                Field(\n                    default=param.default\n                    if param.default is not inspect.Parameter.empty\n                    else PydanticUndefined\n                ),\n            ]\n\n        # Untyped field\n        if annotation is inspect.Parameter.empty:\n            annotation = Annotated[\n                Any,\n                Field(),\n                # \ud83e\udd37\n                WithJsonSchema({\"title\": param.name, \"type\": \"string\"}),\n            ]\n\n        field_info = FieldInfo.from_annotated_attribute(\n            _get_typed_annotation(annotation, globalns),\n            param.default\n            if param.default is not inspect.Parameter.empty\n            else PydanticUndefined,\n        )\n        dynamic_pydantic_model_params[param.name] = (field_info.annotation, field_info)\n        continue\n\n    arguments_model = create_model(\n        f\"{func.__name__}Arguments\",\n        **dynamic_pydantic_model_params,\n        __base__=ArgModelBase,\n    )\n    resp = FuncMetadata(arg_model=arguments_model)\n    return resp\n\n\ndef _get_typed_annotation(annotation: Any, globalns: dict[str, Any]) -> Any:\n    def try_eval_type(\n        value: Any, globalns: dict[str, Any], localns: dict[str, Any]\n    ) -> tuple[Any, bool]:\n        try:\n            return eval_type_backport(value, globalns, localns), True\n        except NameError:\n            return value, False\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n        annotation, status = try_eval_type(annotation, globalns, globalns)\n\n        # This check and raise could perhaps be skipped, and we (FastMCP) just call\n        # model_rebuild right before using it \ud83e\udd37\n        if status is False:\n            raise InvalidSignature(f\"Unable to evaluate type annotation {annotation}\")\n\n    return annotation\n\n\ndef _get_typed_signature(call: Callable[..., Any]) -> inspect.Signature:\n    \"\"\"Get function signature while evaluating forward references\"\"\"\n    signature = inspect.signature(call)\n    globalns = getattr(call, \"__globals__\", {})\n    typed_params = [\n        inspect.Parameter(\n            name=param.name,\n            kind=param.kind,\n            default=param.default,\n            annotation=_get_typed_annotation(param.annotation, globalns),\n        )\n        for param in signature.parameters.values()\n    ]\n    typed_signature = inspect.Signature(typed_params)\n    return typed_signature\n\n\n--- File Index 50: src/mcp/server/fastmcp/utilities/logging.py ---\n\"\"\"Logging utilities for FastMCP.\"\"\"\n\nimport logging\nfrom typing import Literal\n\n\ndef get_logger(name: str) -> logging.Logger:\n    \"\"\"Get a logger nested under MCPnamespace.\n\n    Args:\n        name: the name of the logger, which will be prefixed with 'FastMCP.'\n\n    Returns:\n        a configured logger instance\n    \"\"\"\n    return logging.getLogger(name)\n\n\ndef configure_logging(\n    level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\",\n) -> None:\n    \"\"\"Configure logging for MCP.\n\n    Args:\n        level: the log level to use\n    \"\"\"\n    handlers: list[logging.Handler] = []\n    try:\n        from rich.console import Console\n        from rich.logging import RichHandler\n\n        handlers.append(RichHandler(console=Console(stderr=True), rich_tracebacks=True))\n    except ImportError:\n        pass\n\n    if not handlers:\n        handlers.append(logging.StreamHandler())\n\n    logging.basicConfig(\n        level=level,\n        format=\"%(message)s\",\n        handlers=handlers,\n    )\n\n\n--- File Index 51: src/mcp/server/fastmcp/utilities/types.py ---\n\"\"\"Common types used across FastMCP.\"\"\"\n\nimport base64\nfrom pathlib import Path\n\nfrom mcp.types import ImageContent\n\n\nclass Image:\n    \"\"\"Helper class for returning images from tools.\"\"\"\n\n    def __init__(\n        self,\n        path: str | Path | None = None,\n        data: bytes | None = None,\n        format: str | None = None,\n    ):\n        if path is None and data is None:\n            raise ValueError(\"Either path or data must be provided\")\n        if path is not None and data is not None:\n            raise ValueError(\"Only one of path or data can be provided\")\n\n        self.path = Path(path) if path else None\n        self.data = data\n        self._format = format\n        self._mime_type = self._get_mime_type()\n\n    def _get_mime_type(self) -> str:\n        \"\"\"Get MIME type from format or guess from file extension.\"\"\"\n        if self._format:\n            return f\"image/{self._format.lower()}\"\n\n        if self.path:\n            suffix = self.path.suffix.lower()\n            return {\n                \".png\": \"image/png\",\n                \".jpg\": \"image/jpeg\",\n                \".jpeg\": \"image/jpeg\",\n                \".gif\": \"image/gif\",\n                \".webp\": \"image/webp\",\n            }.get(suffix, \"application/octet-stream\")\n        return \"image/png\"  # default for raw binary data\n\n    def to_image_content(self) -> ImageContent:\n        \"\"\"Convert to MCP ImageContent.\"\"\"\n        if self.path:\n            with open(self.path, \"rb\") as f:\n                data = base64.b64encode(f.read()).decode()\n        elif self.data is not None:\n            data = base64.b64encode(self.data).decode()\n        else:\n            raise ValueError(\"No image data available\")\n\n        return ImageContent(type=\"image\", data=data, mimeType=self._mime_type)\n\n\n--- File Index 52: src/mcp/server/lowlevel/__init__.py ---\nfrom .server import NotificationOptions, Server\n\n__all__ = [\"Server\", \"NotificationOptions\"]\n\n\n--- File Index 53: src/mcp/server/lowlevel/helper_types.py ---\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass ReadResourceContents:\n    \"\"\"Contents returned from a read_resource call.\"\"\"\n\n    content: str | bytes\n    mime_type: str | None = None\n\n\n--- File Index 54: src/mcp/server/lowlevel/server.py ---\n\"\"\"\nMCP Server Module\n\nThis module provides a framework for creating an MCP (Model Context Protocol) server.\nIt allows you to easily define and handle various types of requests and notifications\nin an asynchronous manner.\n\nUsage:\n1. Create a Server instance:\n   server = Server(\"your_server_name\")\n\n2. Define request handlers using decorators:\n   @server.list_prompts()\n   async def handle_list_prompts() -> list[types.Prompt]:\n       # Implementation\n\n   @server.get_prompt()\n   async def handle_get_prompt(\n       name: str, arguments: dict[str, str] | None\n   ) -> types.GetPromptResult:\n       # Implementation\n\n   @server.list_tools()\n   async def handle_list_tools() -> list[types.Tool]:\n       # Implementation\n\n   @server.call_tool()\n   async def handle_call_tool(\n       name: str, arguments: dict | None\n   ) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n       # Implementation\n\n   @server.list_resource_templates()\n   async def handle_list_resource_templates() -> list[types.ResourceTemplate]:\n       # Implementation\n\n3. Define notification handlers if needed:\n   @server.progress_notification()\n   async def handle_progress(\n       progress_token: str | int, progress: float, total: float | None\n   ) -> None:\n       # Implementation\n\n4. Run the server:\n   async def main():\n       async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n           await server.run(\n               read_stream,\n               write_stream,\n               InitializationOptions(\n                   server_name=\"your_server_name\",\n                   server_version=\"your_version\",\n                   capabilities=server.get_capabilities(\n                       notification_options=NotificationOptions(),\n                       experimental_capabilities={},\n                   ),\n               ),\n           )\n\n   asyncio.run(main())\n\nThe Server class provides methods to register handlers for various MCP requests and\nnotifications. It automatically manages the request context and handles incoming\nmessages from the client.\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport contextvars\nimport logging\nimport warnings\nfrom collections.abc import AsyncIterator, Awaitable, Callable, Iterable\nfrom contextlib import AbstractAsyncContextManager, AsyncExitStack, asynccontextmanager\nfrom typing import Any, Generic, TypeVar\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl\n\nimport mcp.types as types\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.models import InitializationOptions\nfrom mcp.server.session import ServerSession\nfrom mcp.server.stdio import stdio_server as stdio_server\nfrom mcp.shared.context import RequestContext\nfrom mcp.shared.exceptions import McpError\nfrom mcp.shared.session import RequestResponder\n\nlogger = logging.getLogger(__name__)\n\nLifespanResultT = TypeVar(\"LifespanResultT\")\n\n# This will be properly typed in each Server instance's context\nrequest_ctx: contextvars.ContextVar[RequestContext[ServerSession, Any]] = (\n    contextvars.ContextVar(\"request_ctx\")\n)\n\n\nclass NotificationOptions:\n    def __init__(\n        self,\n        prompts_changed: bool = False,\n        resources_changed: bool = False,\n        tools_changed: bool = False,\n    ):\n        self.prompts_changed = prompts_changed\n        self.resources_changed = resources_changed\n        self.tools_changed = tools_changed\n\n\n@asynccontextmanager\nasync def lifespan(server: Server[LifespanResultT]) -> AsyncIterator[object]:\n    \"\"\"Default lifespan context manager that does nothing.\n\n    Args:\n        server: The server instance this lifespan is managing\n\n    Returns:\n        An empty context object\n    \"\"\"\n    yield {}\n\n\nclass Server(Generic[LifespanResultT]):\n    def __init__(\n        self,\n        name: str,\n        version: str | None = None,\n        instructions: str | None = None,\n        lifespan: Callable[\n            [Server[LifespanResultT]], AbstractAsyncContextManager[LifespanResultT]\n        ] = lifespan,\n    ):\n        self.name = name\n        self.version = version\n        self.instructions = instructions\n        self.lifespan = lifespan\n        self.request_handlers: dict[\n            type, Callable[..., Awaitable[types.ServerResult]]\n        ] = {\n            types.PingRequest: _ping_handler,\n        }\n        self.notification_handlers: dict[type, Callable[..., Awaitable[None]]] = {}\n        self.notification_options = NotificationOptions()\n        logger.debug(f\"Initializing server '{name}'\")\n\n    def create_initialization_options(\n        self,\n        notification_options: NotificationOptions | None = None,\n        experimental_capabilities: dict[str, dict[str, Any]] | None = None,\n    ) -> InitializationOptions:\n        \"\"\"Create initialization options from this server instance.\"\"\"\n\n        def pkg_version(package: str) -> str:\n            try:\n                from importlib.metadata import version\n\n                return version(package)\n            except Exception:\n                pass\n\n            return \"unknown\"\n\n        return InitializationOptions(\n            server_name=self.name,\n            server_version=self.version if self.version else pkg_version(\"mcp\"),\n            capabilities=self.get_capabilities(\n                notification_options or NotificationOptions(),\n                experimental_capabilities or {},\n            ),\n            instructions=self.instructions,\n        )\n\n    def get_capabilities(\n        self,\n        notification_options: NotificationOptions,\n        experimental_capabilities: dict[str, dict[str, Any]],\n    ) -> types.ServerCapabilities:\n        \"\"\"Convert existing handlers to a ServerCapabilities object.\"\"\"\n        prompts_capability = None\n        resources_capability = None\n        tools_capability = None\n        logging_capability = None\n\n        # Set prompt capabilities if handler exists\n        if types.ListPromptsRequest in self.request_handlers:\n            prompts_capability = types.PromptsCapability(\n                listChanged=notification_options.prompts_changed\n            )\n\n        # Set resource capabilities if handler exists\n        if types.ListResourcesRequest in self.request_handlers:\n            resources_capability = types.ResourcesCapability(\n                subscribe=False, listChanged=notification_options.resources_changed\n            )\n\n        # Set tool capabilities if handler exists\n        if types.ListToolsRequest in self.request_handlers:\n            tools_capability = types.ToolsCapability(\n                listChanged=notification_options.tools_changed\n            )\n\n        # Set logging capabilities if handler exists\n        if types.SetLevelRequest in self.request_handlers:\n            logging_capability = types.LoggingCapability()\n\n        return types.ServerCapabilities(\n            prompts=prompts_capability,\n            resources=resources_capability,\n            tools=tools_capability,\n            logging=logging_capability,\n            experimental=experimental_capabilities,\n        )\n\n    @property\n    def request_context(self) -> RequestContext[ServerSession, LifespanResultT]:\n        \"\"\"If called outside of a request context, this will raise a LookupError.\"\"\"\n        return request_ctx.get()\n\n    def list_prompts(self):\n        def decorator(func: Callable[[], Awaitable[list[types.Prompt]]]):\n            logger.debug(\"Registering handler for PromptListRequest\")\n\n            async def handler(_: Any):\n                prompts = await func()\n                return types.ServerResult(types.ListPromptsResult(prompts=prompts))\n\n            self.request_handlers[types.ListPromptsRequest] = handler\n            return func\n\n        return decorator\n\n    def get_prompt(self):\n        def decorator(\n            func: Callable[\n                [str, dict[str, str] | None], Awaitable[types.GetPromptResult]\n            ],\n        ):\n            logger.debug(\"Registering handler for GetPromptRequest\")\n\n            async def handler(req: types.GetPromptRequest):\n                prompt_get = await func(req.params.name, req.params.arguments)\n                return types.ServerResult(prompt_get)\n\n            self.request_handlers[types.GetPromptRequest] = handler\n            return func\n\n        return decorator\n\n    def list_resources(self):\n        def decorator(func: Callable[[], Awaitable[list[types.Resource]]]):\n            logger.debug(\"Registering handler for ListResourcesRequest\")\n\n            async def handler(_: Any):\n                resources = await func()\n                return types.ServerResult(\n                    types.ListResourcesResult(resources=resources)\n                )\n\n            self.request_handlers[types.ListResourcesRequest] = handler\n            return func\n\n        return decorator\n\n    def list_resource_templates(self):\n        def decorator(func: Callable[[], Awaitable[list[types.ResourceTemplate]]]):\n            logger.debug(\"Registering handler for ListResourceTemplatesRequest\")\n\n            async def handler(_: Any):\n                templates = await func()\n                return types.ServerResult(\n                    types.ListResourceTemplatesResult(resourceTemplates=templates)\n                )\n\n            self.request_handlers[types.ListResourceTemplatesRequest] = handler\n            return func\n\n        return decorator\n\n    def read_resource(self):\n        def decorator(\n            func: Callable[\n                [AnyUrl], Awaitable[str | bytes | Iterable[ReadResourceContents]]\n            ],\n        ):\n            logger.debug(\"Registering handler for ReadResourceRequest\")\n\n            async def handler(req: types.ReadResourceRequest):\n                result = await func(req.params.uri)\n\n                def create_content(data: str | bytes, mime_type: str | None):\n                    match data:\n                        case str() as data:\n                            return types.TextResourceContents(\n                                uri=req.params.uri,\n                                text=data,\n                                mimeType=mime_type or \"text/plain\",\n                            )\n                        case bytes() as data:\n                            import base64\n\n                            return types.BlobResourceContents(\n                                uri=req.params.uri,\n                                blob=base64.b64encode(data).decode(),\n                                mimeType=mime_type or \"application/octet-stream\",\n                            )\n\n                match result:\n                    case str() | bytes() as data:\n                        warnings.warn(\n                            \"Returning str or bytes from read_resource is deprecated. \"\n                            \"Use Iterable[ReadResourceContents] instead.\",\n                            DeprecationWarning,\n                            stacklevel=2,\n                        )\n                        content = create_content(data, None)\n                    case Iterable() as contents:\n                        contents_list = [\n                            create_content(content_item.content, content_item.mime_type)\n                            for content_item in contents\n                        ]\n                        return types.ServerResult(\n                            types.ReadResourceResult(\n                                contents=contents_list,\n                            )\n                        )\n                    case _:\n                        raise ValueError(\n                            f\"Unexpected return type from read_resource: {type(result)}\"\n                        )\n\n                return types.ServerResult(\n                    types.ReadResourceResult(\n                        contents=[content],\n                    )\n                )\n\n            self.request_handlers[types.ReadResourceRequest] = handler\n            return func\n\n        return decorator\n\n    def set_logging_level(self):\n        def decorator(func: Callable[[types.LoggingLevel], Awaitable[None]]):\n            logger.debug(\"Registering handler for SetLevelRequest\")\n\n            async def handler(req: types.SetLevelRequest):\n                await func(req.params.level)\n                return types.ServerResult(types.EmptyResult())\n\n            self.request_handlers[types.SetLevelRequest] = handler\n            return func\n\n        return decorator\n\n    def subscribe_resource(self):\n        def decorator(func: Callable[[AnyUrl], Awaitable[None]]):\n            logger.debug(\"Registering handler for SubscribeRequest\")\n\n            async def handler(req: types.SubscribeRequest):\n                await func(req.params.uri)\n                return types.ServerResult(types.EmptyResult())\n\n            self.request_handlers[types.SubscribeRequest] = handler\n            return func\n\n        return decorator\n\n    def unsubscribe_resource(self):\n        def decorator(func: Callable[[AnyUrl], Awaitable[None]]):\n            logger.debug(\"Registering handler for UnsubscribeRequest\")\n\n            async def handler(req: types.UnsubscribeRequest):\n                await func(req.params.uri)\n                return types.ServerResult(types.EmptyResult())\n\n            self.request_handlers[types.UnsubscribeRequest] = handler\n            return func\n\n        return decorator\n\n    def list_tools(self):\n        def decorator(func: Callable[[], Awaitable[list[types.Tool]]]):\n            logger.debug(\"Registering handler for ListToolsRequest\")\n\n            async def handler(_: Any):\n                tools = await func()\n                return types.ServerResult(types.ListToolsResult(tools=tools))\n\n            self.request_handlers[types.ListToolsRequest] = handler\n            return func\n\n        return decorator\n\n    def call_tool(self):\n        def decorator(\n            func: Callable[\n                ...,\n                Awaitable[\n                    Iterable[\n                        types.TextContent | types.ImageContent | types.EmbeddedResource\n                    ]\n                ],\n            ],\n        ):\n            logger.debug(\"Registering handler for CallToolRequest\")\n\n            async def handler(req: types.CallToolRequest):\n                try:\n                    results = await func(req.params.name, (req.params.arguments or {}))\n                    return types.ServerResult(\n                        types.CallToolResult(content=list(results), isError=False)\n                    )\n                except Exception as e:\n                    return types.ServerResult(\n                        types.CallToolResult(\n                            content=[types.TextContent(type=\"text\", text=str(e))],\n                            isError=True,\n                        )\n                    )\n\n            self.request_handlers[types.CallToolRequest] = handler\n            return func\n\n        return decorator\n\n    def progress_notification(self):\n        def decorator(\n            func: Callable[[str | int, float, float | None], Awaitable[None]],\n        ):\n            logger.debug(\"Registering handler for ProgressNotification\")\n\n            async def handler(req: types.ProgressNotification):\n                await func(\n                    req.params.progressToken, req.params.progress, req.params.total\n                )\n\n            self.notification_handlers[types.ProgressNotification] = handler\n            return func\n\n        return decorator\n\n    def completion(self):\n        \"\"\"Provides completions for prompts and resource templates\"\"\"\n\n        def decorator(\n            func: Callable[\n                [\n                    types.PromptReference | types.ResourceReference,\n                    types.CompletionArgument,\n                ],\n                Awaitable[types.Completion | None],\n            ],\n        ):\n            logger.debug(\"Registering handler for CompleteRequest\")\n\n            async def handler(req: types.CompleteRequest):\n                completion = await func(req.params.ref, req.params.argument)\n                return types.ServerResult(\n                    types.CompleteResult(\n                        completion=completion\n                        if completion is not None\n                        else types.Completion(values=[], total=None, hasMore=None),\n                    )\n                )\n\n            self.request_handlers[types.CompleteRequest] = handler\n            return func\n\n        return decorator\n\n    async def run(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        initialization_options: InitializationOptions,\n        # When False, exceptions are returned as messages to the client.\n        # When True, exceptions are raised, which will cause the server to shut down\n        # but also make tracing exceptions much easier during testing and when using\n        # in-process servers.\n        raise_exceptions: bool = False,\n    ):\n        async with AsyncExitStack() as stack:\n            lifespan_context = await stack.enter_async_context(self.lifespan(self))\n            session = await stack.enter_async_context(\n                ServerSession(read_stream, write_stream, initialization_options)\n            )\n\n            async with anyio.create_task_group() as tg:\n                async for message in session.incoming_messages:\n                    logger.debug(f\"Received message: {message}\")\n\n                    tg.start_soon(\n                        self._handle_message,\n                        message,\n                        session,\n                        lifespan_context,\n                        raise_exceptions,\n                    )\n\n    async def _handle_message(\n        self,\n        message: RequestResponder[types.ClientRequest, types.ServerResult]\n        | types.ClientNotification\n        | Exception,\n        session: ServerSession,\n        lifespan_context: LifespanResultT,\n        raise_exceptions: bool = False,\n    ):\n        with warnings.catch_warnings(record=True) as w:\n            # TODO(Marcelo): We should be checking if message is Exception here.\n            match message:  # type: ignore[reportMatchNotExhaustive]\n                case (\n                    RequestResponder(request=types.ClientRequest(root=req)) as responder\n                ):\n                    with responder:\n                        await self._handle_request(\n                            message, req, session, lifespan_context, raise_exceptions\n                        )\n                case types.ClientNotification(root=notify):\n                    await self._handle_notification(notify)\n\n            for warning in w:\n                logger.info(f\"Warning: {warning.category.__name__}: {warning.message}\")\n\n    async def _handle_request(\n        self,\n        message: RequestResponder[types.ClientRequest, types.ServerResult],\n        req: Any,\n        session: ServerSession,\n        lifespan_context: LifespanResultT,\n        raise_exceptions: bool,\n    ):\n        logger.info(f\"Processing request of type {type(req).__name__}\")\n        if type(req) in self.request_handlers:\n            handler = self.request_handlers[type(req)]\n            logger.debug(f\"Dispatching request of type {type(req).__name__}\")\n\n            token = None\n            try:\n                # Set our global state that can be retrieved via\n                # app.get_request_context()\n                token = request_ctx.set(\n                    RequestContext(\n                        message.request_id,\n                        message.request_meta,\n                        session,\n                        lifespan_context,\n                    )\n                )\n                response = await handler(req)\n            except McpError as err:\n                response = err.error\n            except Exception as err:\n                if raise_exceptions:\n                    raise err\n                response = types.ErrorData(code=0, message=str(err), data=None)\n            finally:\n                # Reset the global state after we are done\n                if token is not None:\n                    request_ctx.reset(token)\n\n            await message.respond(response)\n        else:\n            await message.respond(\n                types.ErrorData(\n                    code=types.METHOD_NOT_FOUND,\n                    message=\"Method not found\",\n                )\n            )\n\n        logger.debug(\"Response sent\")\n\n    async def _handle_notification(self, notify: Any):\n        if type(notify) in self.notification_handlers:\n            assert type(notify) in self.notification_handlers\n\n            handler = self.notification_handlers[type(notify)]\n            logger.debug(\n                f\"Dispatching notification of type \" f\"{type(notify).__name__}\"\n            )\n\n            try:\n                await handler(notify)\n            except Exception as err:\n                logger.error(f\"Uncaught exception in notification handler: \" f\"{err}\")\n\n\nasync def _ping_handler(request: types.PingRequest) -> types.ServerResult:\n    return types.ServerResult(types.EmptyResult())\n\n\n--- File Index 55: src/mcp/server/models.py ---\n\"\"\"\nThis module provides simpler types to use with the server for managing prompts\nand tools.\n\"\"\"\n\nfrom pydantic import BaseModel\n\nfrom mcp.types import (\n    ServerCapabilities,\n)\n\n\nclass InitializationOptions(BaseModel):\n    server_name: str\n    server_version: str\n    capabilities: ServerCapabilities\n    instructions: str | None = None\n\n\n--- File Index 56: src/mcp/server/session.py ---\n\"\"\"\nServerSession Module\n\nThis module provides the ServerSession class, which manages communication between the\nserver and client in the MCP (Model Context Protocol) framework. It is most commonly\nused in MCP servers to interact with the client.\n\nCommon usage pattern:\n```\n    server = Server(name)\n\n    @server.call_tool()\n    async def handle_tool_call(ctx: RequestContext, arguments: dict[str, Any]) -> Any:\n        # Check client capabilities before proceeding\n        if ctx.session.check_client_capability(\n            types.ClientCapabilities(experimental={\"advanced_tools\": dict()})\n        ):\n            # Perform advanced tool operations\n            result = await perform_advanced_tool_operation(arguments)\n        else:\n            # Fall back to basic tool operations\n            result = await perform_basic_tool_operation(arguments)\n\n        return result\n\n    @server.list_prompts()\n    async def handle_list_prompts(ctx: RequestContext) -> list[types.Prompt]:\n        # Access session for any necessary checks or operations\n        if ctx.session.client_params:\n            # Customize prompts based on client initialization parameters\n            return generate_custom_prompts(ctx.session.client_params)\n        else:\n            return default_prompts\n```\n\nThe ServerSession class is typically used internally by the Server class and should not\nbe instantiated directly by users of the MCP framework.\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl\n\nimport mcp.types as types\nfrom mcp.server.models import InitializationOptions\nfrom mcp.shared.session import (\n    BaseSession,\n    RequestResponder,\n)\n\n\nclass InitializationState(Enum):\n    NotInitialized = 1\n    Initializing = 2\n    Initialized = 3\n\n\nServerSessionT = TypeVar(\"ServerSessionT\", bound=\"ServerSession\")\n\nServerRequestResponder = (\n    RequestResponder[types.ClientRequest, types.ServerResult]\n    | types.ClientNotification\n    | Exception\n)\n\n\nclass ServerSession(\n    BaseSession[\n        types.ServerRequest,\n        types.ServerNotification,\n        types.ServerResult,\n        types.ClientRequest,\n        types.ClientNotification,\n    ]\n):\n    _initialized: InitializationState = InitializationState.NotInitialized\n    _client_params: types.InitializeRequestParams | None = None\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        init_options: InitializationOptions,\n    ) -> None:\n        super().__init__(\n            read_stream, write_stream, types.ClientRequest, types.ClientNotification\n        )\n        self._initialization_state = InitializationState.NotInitialized\n        self._init_options = init_options\n        self._incoming_message_stream_writer, self._incoming_message_stream_reader = (\n            anyio.create_memory_object_stream[ServerRequestResponder](0)\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_reader.aclose()\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_writer.aclose()\n        )\n\n    @property\n    def client_params(self) -> types.InitializeRequestParams | None:\n        return self._client_params\n\n    def check_client_capability(self, capability: types.ClientCapabilities) -> bool:\n        \"\"\"Check if the client supports a specific capability.\"\"\"\n        if self._client_params is None:\n            return False\n\n        # Get client capabilities from initialization params\n        client_caps = self._client_params.capabilities\n\n        # Check each specified capability in the passed in capability object\n        if capability.roots is not None:\n            if client_caps.roots is None:\n                return False\n            if capability.roots.listChanged and not client_caps.roots.listChanged:\n                return False\n\n        if capability.sampling is not None:\n            if client_caps.sampling is None:\n                return False\n\n        if capability.experimental is not None:\n            if client_caps.experimental is None:\n                return False\n            # Check each experimental capability\n            for exp_key, exp_value in capability.experimental.items():\n                if (\n                    exp_key not in client_caps.experimental\n                    or client_caps.experimental[exp_key] != exp_value\n                ):\n                    return False\n\n        return True\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ClientRequest, types.ServerResult]\n    ):\n        match responder.request.root:\n            case types.InitializeRequest(params=params):\n                self._initialization_state = InitializationState.Initializing\n                self._client_params = params\n                with responder:\n                    await responder.respond(\n                        types.ServerResult(\n                            types.InitializeResult(\n                                protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                                capabilities=self._init_options.capabilities,\n                                serverInfo=types.Implementation(\n                                    name=self._init_options.server_name,\n                                    version=self._init_options.server_version,\n                                ),\n                                instructions=self._init_options.instructions,\n                            )\n                        )\n                    )\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received request before initialization was complete\"\n                    )\n\n    async def _received_notification(\n        self, notification: types.ClientNotification\n    ) -> None:\n        # Need this to avoid ASYNC910\n        await anyio.lowlevel.checkpoint()\n        match notification.root:\n            case types.InitializedNotification():\n                self._initialization_state = InitializationState.Initialized\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received notification before initialization was complete\"\n                    )\n\n    async def send_log_message(\n        self, level: types.LoggingLevel, data: Any, logger: str | None = None\n    ) -> None:\n        \"\"\"Send a log message notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.LoggingMessageNotification(\n                    method=\"notifications/message\",\n                    params=types.LoggingMessageNotificationParams(\n                        level=level,\n                        data=data,\n                        logger=logger,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_updated(self, uri: AnyUrl) -> None:\n        \"\"\"Send a resource updated notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceUpdatedNotification(\n                    method=\"notifications/resources/updated\",\n                    params=types.ResourceUpdatedNotificationParams(uri=uri),\n                )\n            )\n        )\n\n    async def create_message(\n        self,\n        messages: list[types.SamplingMessage],\n        *,\n        max_tokens: int,\n        system_prompt: str | None = None,\n        include_context: types.IncludeContext | None = None,\n        temperature: float | None = None,\n        stop_sequences: list[str] | None = None,\n        metadata: dict[str, Any] | None = None,\n        model_preferences: types.ModelPreferences | None = None,\n    ) -> types.CreateMessageResult:\n        \"\"\"Send a sampling/create_message request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.CreateMessageRequest(\n                    method=\"sampling/createMessage\",\n                    params=types.CreateMessageRequestParams(\n                        messages=messages,\n                        systemPrompt=system_prompt,\n                        includeContext=include_context,\n                        temperature=temperature,\n                        maxTokens=max_tokens,\n                        stopSequences=stop_sequences,\n                        metadata=metadata,\n                        modelPreferences=model_preferences,\n                    ),\n                )\n            ),\n            types.CreateMessageResult,\n        )\n\n    async def list_roots(self) -> types.ListRootsResult:\n        \"\"\"Send a roots/list request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.ListRootsRequest(\n                    method=\"roots/list\",\n                )\n            ),\n            types.ListRootsResult,\n        )\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_list_changed(self) -> None:\n        \"\"\"Send a resource list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceListChangedNotification(\n                    method=\"notifications/resources/list_changed\",\n                )\n            )\n        )\n\n    async def send_tool_list_changed(self) -> None:\n        \"\"\"Send a tool list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ToolListChangedNotification(\n                    method=\"notifications/tools/list_changed\",\n                )\n            )\n        )\n\n    async def send_prompt_list_changed(self) -> None:\n        \"\"\"Send a prompt list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.PromptListChangedNotification(\n                    method=\"notifications/prompts/list_changed\",\n                )\n            )\n        )\n\n    async def _handle_incoming(self, req: ServerRequestResponder) -> None:\n        await self._incoming_message_stream_writer.send(req)\n\n    @property\n    def incoming_messages(\n        self,\n    ) -> MemoryObjectReceiveStream[ServerRequestResponder]:\n        return self._incoming_message_stream_reader\n\n\n--- File Index 57: src/mcp/server/sse.py ---\n\"\"\"\nSSE Server Transport Module\n\nThis module implements a Server-Sent Events (SSE) transport layer for MCP servers.\n\nExample usage:\n```\n    # Create an SSE transport at an endpoint\n    sse = SseServerTransport(\"/messages/\")\n\n    # Create Starlette routes for SSE and message handling\n    routes = [\n        Route(\"/sse\", endpoint=handle_sse),\n        Mount(\"/messages/\", app=sse.handle_post_message),\n    ]\n\n    # Define handler functions\n    async def handle_sse(request):\n        async with sse.connect_sse(\n            request.scope, request.receive, request._send\n        ) as streams:\n            await app.run(\n                streams[0], streams[1], app.create_initialization_options()\n            )\n\n    # Create and run Starlette app\n    starlette_app = Starlette(routes=routes)\n    uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n```\n\nSee SseServerTransport class documentation for more details.\n\"\"\"\n\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import quote\nfrom uuid import UUID, uuid4\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom sse_starlette import EventSourceResponse\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.types import Receive, Scope, Send\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\nclass SseServerTransport:\n    \"\"\"\n    SSE server transport for MCP. This class provides _two_ ASGI applications,\n    suitable to be used with a framework like Starlette and a server like Hypercorn:\n\n        1. connect_sse() is an ASGI application which receives incoming GET requests,\n           and sets up a new SSE stream to send server messages to the client.\n        2. handle_post_message() is an ASGI application which receives incoming POST\n           requests, which should contain client messages that link to a\n           previously-established SSE session.\n    \"\"\"\n\n    _endpoint: str\n    _read_stream_writers: dict[\n        UUID, MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    ]\n\n    def __init__(self, endpoint: str) -> None:\n        \"\"\"\n        Creates a new SSE server transport, which will direct the client to POST\n        messages to the relative or absolute URL given.\n        \"\"\"\n\n        super().__init__()\n        self._endpoint = endpoint\n        self._read_stream_writers = {}\n        logger.debug(f\"SseServerTransport initialized with endpoint: {endpoint}\")\n\n    @asynccontextmanager\n    async def connect_sse(self, scope: Scope, receive: Receive, send: Send):\n        if scope[\"type\"] != \"http\":\n            logger.error(\"connect_sse received non-HTTP request\")\n            raise ValueError(\"connect_sse can only handle HTTP requests\")\n\n        logger.debug(\"Setting up SSE connection\")\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n        read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n        write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n        read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n        write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n        session_id = uuid4()\n        session_uri = f\"{quote(self._endpoint)}?session_id={session_id.hex}\"\n        self._read_stream_writers[session_id] = read_stream_writer\n        logger.debug(f\"Created new session with ID: {session_id}\")\n\n        sse_stream_writer, sse_stream_reader = anyio.create_memory_object_stream[\n            dict[str, Any]\n        ](0)\n\n        async def sse_writer():\n            logger.debug(\"Starting SSE writer\")\n            async with sse_stream_writer, write_stream_reader:\n                await sse_stream_writer.send({\"event\": \"endpoint\", \"data\": session_uri})\n                logger.debug(f\"Sent endpoint event: {session_uri}\")\n\n                async for message in write_stream_reader:\n                    logger.debug(f\"Sending message via SSE: {message}\")\n                    await sse_stream_writer.send(\n                        {\n                            \"event\": \"message\",\n                            \"data\": message.model_dump_json(\n                                by_alias=True, exclude_none=True\n                            ),\n                        }\n                    )\n\n        async with anyio.create_task_group() as tg:\n            response = EventSourceResponse(\n                content=sse_stream_reader, data_sender_callable=sse_writer\n            )\n            logger.debug(\"Starting SSE response task\")\n            tg.start_soon(response, scope, receive, send)\n\n            logger.debug(\"Yielding read and write streams\")\n            yield (read_stream, write_stream)\n\n    async def handle_post_message(\n        self, scope: Scope, receive: Receive, send: Send\n    ) -> None:\n        logger.debug(\"Handling POST message\")\n        request = Request(scope, receive)\n\n        session_id_param = request.query_params.get(\"session_id\")\n        if session_id_param is None:\n            logger.warning(\"Received request without session_id\")\n            response = Response(\"session_id is required\", status_code=400)\n            return await response(scope, receive, send)\n\n        try:\n            session_id = UUID(hex=session_id_param)\n            logger.debug(f\"Parsed session ID: {session_id}\")\n        except ValueError:\n            logger.warning(f\"Received invalid session ID: {session_id_param}\")\n            response = Response(\"Invalid session ID\", status_code=400)\n            return await response(scope, receive, send)\n\n        writer = self._read_stream_writers.get(session_id)\n        if not writer:\n            logger.warning(f\"Could not find session for ID: {session_id}\")\n            response = Response(\"Could not find session\", status_code=404)\n            return await response(scope, receive, send)\n\n        body = await request.body()\n        logger.debug(f\"Received JSON: {body}\")\n\n        try:\n            message = types.JSONRPCMessage.model_validate_json(body)\n            logger.debug(f\"Validated client message: {message}\")\n        except ValidationError as err:\n            logger.error(f\"Failed to parse message: {err}\")\n            response = Response(\"Could not parse message\", status_code=400)\n            await response(scope, receive, send)\n            await writer.send(err)\n            return\n\n        logger.debug(f\"Sending message to writer: {message}\")\n        response = Response(\"Accepted\", status_code=202)\n        await response(scope, receive, send)\n        await writer.send(message)\n\n\n--- File Index 58: src/mcp/server/stdio.py ---\n\"\"\"\nStdio Server Transport Module\n\nThis module provides functionality for creating an stdio-based transport layer\nthat can be used to communicate with an MCP client through standard input/output\nstreams.\n\nExample usage:\n```\n    async def run_server():\n        async with stdio_server() as (read_stream, write_stream):\n            # read_stream contains incoming JSONRPCMessages from stdin\n            # write_stream allows sending JSONRPCMessages to stdout\n            server = await create_my_server()\n            await server.run(read_stream, write_stream, init_options)\n\n    anyio.run(run_server)\n```\n\"\"\"\n\nimport sys\nfrom contextlib import asynccontextmanager\nfrom io import TextIOWrapper\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\n\n\n@asynccontextmanager\nasync def stdio_server(\n    stdin: anyio.AsyncFile[str] | None = None,\n    stdout: anyio.AsyncFile[str] | None = None,\n):\n    \"\"\"\n    Server transport for stdio: this communicates with an MCP client by reading\n    from the current process' stdin and writing to stdout.\n    \"\"\"\n    # Purposely not using context managers for these, as we don't want to close\n    # standard process handles. Encoding of stdin/stdout as text streams on\n    # python is platform-dependent (Windows is particularly problematic), so we\n    # re-wrap the underlying binary stream to ensure UTF-8.\n    if not stdin:\n        stdin = anyio.wrap_file(TextIOWrapper(sys.stdin.buffer, encoding=\"utf-8\"))\n    if not stdout:\n        stdout = anyio.wrap_file(TextIOWrapper(sys.stdout.buffer, encoding=\"utf-8\"))\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def stdin_reader():\n        try:\n            async with read_stream_writer:\n                async for line in stdin:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(line)\n                    except Exception as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdout_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await stdout.write(json + \"\\n\")\n                    await stdout.flush()\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(stdin_reader)\n        tg.start_soon(stdout_writer)\n        yield read_stream, write_stream\n\n\n--- File Index 59: src/mcp/server/websocket.py ---\nimport logging\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic_core import ValidationError\nfrom starlette.types import Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_server(scope: Scope, receive: Receive, send: Send):\n    \"\"\"\n    WebSocket server transport for MCP. This is an ASGI application, suitable to be\n    used with a framework like Starlette and a server like Hypercorn.\n    \"\"\"\n\n    websocket = WebSocket(scope, receive, send)\n    await websocket.accept(subprotocol=\"mcp\")\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def ws_reader():\n        try:\n            async with read_stream_writer:\n                async for msg in websocket.iter_text():\n                    try:\n                        client_message = types.JSONRPCMessage.model_validate_json(msg)\n                    except ValidationError as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(client_message)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async def ws_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    obj = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await websocket.send_text(obj)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(ws_reader)\n        tg.start_soon(ws_writer)\n        yield (read_stream, write_stream)\n\n\n--- File Index 60: src/mcp/shared/__init__.py ---\n\n\n--- File Index 61: src/mcp/shared/context.py ---\nfrom dataclasses import dataclass\nfrom typing import Any, Generic\n\nfrom typing_extensions import TypeVar\n\nfrom mcp.shared.session import BaseSession\nfrom mcp.types import RequestId, RequestParams\n\nSessionT = TypeVar(\"SessionT\", bound=BaseSession[Any, Any, Any, Any, Any])\nLifespanContextT = TypeVar(\"LifespanContextT\")\n\n\n@dataclass\nclass RequestContext(Generic[SessionT, LifespanContextT]):\n    request_id: RequestId\n    meta: RequestParams.Meta | None\n    session: SessionT\n    lifespan_context: LifespanContextT\n\n\n--- File Index 62: src/mcp/shared/exceptions.py ---\nfrom mcp.types import ErrorData\n\n\nclass McpError(Exception):\n    \"\"\"\n    Exception type raised when an error arrives over an MCP connection.\n    \"\"\"\n\n    error: ErrorData\n\n    def __init__(self, error: ErrorData):\n        \"\"\"Initialize McpError.\"\"\"\n        super().__init__(error.message)\n        self.error = error\n\n\n--- File Index 63: src/mcp/shared/memory.py ---\n\"\"\"\nIn-memory transports\n\"\"\"\n\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom datetime import timedelta\nfrom typing import Any\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import (\n    ClientSession,\n    ListRootsFnT,\n    LoggingFnT,\n    MessageHandlerFnT,\n    SamplingFnT,\n)\nfrom mcp.server import Server\nfrom mcp.types import JSONRPCMessage\n\nMessageStream = tuple[\n    MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    MemoryObjectSendStream[JSONRPCMessage],\n]\n\n\n@asynccontextmanager\nasync def create_client_server_memory_streams() -> (\n    AsyncGenerator[tuple[MessageStream, MessageStream], None]\n):\n    \"\"\"\n    Creates a pair of bidirectional memory streams for client-server communication.\n\n    Returns:\n        A tuple of (client_streams, server_streams) where each is a tuple of\n        (read_stream, write_stream)\n    \"\"\"\n    # Create streams for both directions\n    server_to_client_send, server_to_client_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n    client_to_server_send, client_to_server_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n\n    client_streams = (server_to_client_receive, client_to_server_send)\n    server_streams = (client_to_server_receive, server_to_client_send)\n\n    async with (\n        server_to_client_receive,\n        client_to_server_send,\n        client_to_server_receive,\n        server_to_client_send,\n    ):\n        yield client_streams, server_streams\n\n\n@asynccontextmanager\nasync def create_connected_server_and_client_session(\n    server: Server[Any],\n    read_timeout_seconds: timedelta | None = None,\n    sampling_callback: SamplingFnT | None = None,\n    list_roots_callback: ListRootsFnT | None = None,\n    logging_callback: LoggingFnT | None = None,\n    message_handler: MessageHandlerFnT | None = None,\n    client_info: types.Implementation | None = None,\n    raise_exceptions: bool = False,\n) -> AsyncGenerator[ClientSession, None]:\n    \"\"\"Creates a ClientSession that is connected to a running MCP server.\"\"\"\n    async with create_client_server_memory_streams() as (\n        client_streams,\n        server_streams,\n    ):\n        client_read, client_write = client_streams\n        server_read, server_write = server_streams\n\n        # Create a cancel scope for the server task\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(\n                lambda: server.run(\n                    server_read,\n                    server_write,\n                    server.create_initialization_options(),\n                    raise_exceptions=raise_exceptions,\n                )\n            )\n\n            try:\n                async with ClientSession(\n                    read_stream=client_read,\n                    write_stream=client_write,\n                    read_timeout_seconds=read_timeout_seconds,\n                    sampling_callback=sampling_callback,\n                    list_roots_callback=list_roots_callback,\n                    logging_callback=logging_callback,\n                    message_handler=message_handler,\n                    client_info=client_info,\n                ) as client_session:\n                    await client_session.initialize()\n                    yield client_session\n            finally:\n                tg.cancel_scope.cancel()\n\n\n--- File Index 64: src/mcp/shared/progress.py ---\nfrom collections.abc import Generator\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom typing import Generic\n\nfrom pydantic import BaseModel\n\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.shared.session import (\n    BaseSession,\n    ReceiveNotificationT,\n    ReceiveRequestT,\n    SendNotificationT,\n    SendRequestT,\n    SendResultT,\n)\nfrom mcp.types import ProgressToken\n\n\nclass Progress(BaseModel):\n    progress: float\n    total: float | None\n\n\n@dataclass\nclass ProgressContext(\n    Generic[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ]\n):\n    session: BaseSession[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ]\n    progress_token: ProgressToken\n    total: float | None\n    current: float = field(default=0.0, init=False)\n\n    async def progress(self, amount: float) -> None:\n        self.current += amount\n\n        await self.session.send_progress_notification(\n            self.progress_token, self.current, total=self.total\n        )\n\n\n@contextmanager\ndef progress(\n    ctx: RequestContext[\n        BaseSession[\n            SendRequestT,\n            SendNotificationT,\n            SendResultT,\n            ReceiveRequestT,\n            ReceiveNotificationT,\n        ],\n        LifespanContextT,\n    ],\n    total: float | None = None,\n) -> Generator[\n    ProgressContext[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ],\n    None,\n]:\n    if ctx.meta is None or ctx.meta.progressToken is None:\n        raise ValueError(\"No progress token provided\")\n\n    progress_ctx = ProgressContext(ctx.session, ctx.meta.progressToken, total)\n    try:\n        yield progress_ctx\n    finally:\n        pass\n\n\n--- File Index 65: src/mcp/shared/session.py ---\nimport logging\nfrom collections.abc import Callable\nfrom contextlib import AsyncExitStack\nfrom datetime import timedelta\nfrom types import TracebackType\nfrom typing import Any, Generic, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nimport httpx\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\nfrom mcp.shared.exceptions import McpError\nfrom mcp.types import (\n    CancelledNotification,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    ErrorData,\n    JSONRPCError,\n    JSONRPCMessage,\n    JSONRPCNotification,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    RequestParams,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n)\n\nSendRequestT = TypeVar(\"SendRequestT\", ClientRequest, ServerRequest)\nSendResultT = TypeVar(\"SendResultT\", ClientResult, ServerResult)\nSendNotificationT = TypeVar(\"SendNotificationT\", ClientNotification, ServerNotification)\nReceiveRequestT = TypeVar(\"ReceiveRequestT\", ClientRequest, ServerRequest)\nReceiveResultT = TypeVar(\"ReceiveResultT\", bound=BaseModel)\nReceiveNotificationT = TypeVar(\n    \"ReceiveNotificationT\", ClientNotification, ServerNotification\n)\n\nRequestId = str | int\n\n\nclass RequestResponder(Generic[ReceiveRequestT, SendResultT]):\n    \"\"\"Handles responding to MCP requests and manages request lifecycle.\n\n    This class MUST be used as a context manager to ensure proper cleanup and\n    cancellation handling:\n\n    Example:\n        with request_responder as resp:\n            await resp.respond(result)\n\n    The context manager ensures:\n    1. Proper cancellation scope setup and cleanup\n    2. Request completion tracking\n    3. Cleanup of in-flight requests\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: RequestId,\n        request_meta: RequestParams.Meta | None,\n        request: ReceiveRequestT,\n        session: \"\"\"BaseSession[\n            SendRequestT,\n            SendNotificationT,\n            SendResultT,\n            ReceiveRequestT,\n            ReceiveNotificationT\n        ]\"\"\",\n        on_complete: Callable[[\"RequestResponder[ReceiveRequestT, SendResultT]\"], Any],\n    ) -> None:\n        self.request_id = request_id\n        self.request_meta = request_meta\n        self.request = request\n        self._session = session\n        self._completed = False\n        self._cancel_scope = anyio.CancelScope()\n        self._on_complete = on_complete\n        self._entered = False  # Track if we're in a context manager\n\n    def __enter__(self) -> \"RequestResponder[ReceiveRequestT, SendResultT]\":\n        \"\"\"Enter the context manager, enabling request cancellation tracking.\"\"\"\n        self._entered = True\n        self._cancel_scope = anyio.CancelScope()\n        self._cancel_scope.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit the context manager, performing cleanup and notifying completion.\"\"\"\n        try:\n            if self._completed:\n                self._on_complete(self)\n        finally:\n            self._entered = False\n            if not self._cancel_scope:\n                raise RuntimeError(\"No active cancel scope\")\n            self._cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n\n    async def respond(self, response: SendResultT | ErrorData) -> None:\n        \"\"\"Send a response for this request.\n\n        Must be called within a context manager block.\n        Raises:\n            RuntimeError: If not used within a context manager\n            AssertionError: If request was already responded to\n        \"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        assert not self._completed, \"Request already responded to\"\n\n        if not self.cancelled:\n            self._completed = True\n\n            await self._session._send_response(  # type: ignore[reportPrivateUsage]\n                request_id=self.request_id, response=response\n            )\n\n    async def cancel(self) -> None:\n        \"\"\"Cancel this request and mark it as completed.\"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        if not self._cancel_scope:\n            raise RuntimeError(\"No active cancel scope\")\n\n        self._cancel_scope.cancel()\n        self._completed = True  # Mark as completed so it's removed from in_flight\n        # Send an error response to indicate cancellation\n        await self._session._send_response(  # type: ignore[reportPrivateUsage]\n            request_id=self.request_id,\n            response=ErrorData(code=0, message=\"Request cancelled\", data=None),\n        )\n\n    @property\n    def in_flight(self) -> bool:\n        return not self._completed and not self.cancelled\n\n    @property\n    def cancelled(self) -> bool:\n        return self._cancel_scope.cancel_called\n\n\nclass BaseSession(\n    Generic[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ],\n):\n    \"\"\"\n    Implements an MCP \"session\" on top of read/write streams, including features\n    like request/response linking, notifications, and progress.\n\n    This class is an async context manager that automatically starts processing\n    messages when entered.\n    \"\"\"\n\n    _response_streams: dict[\n        RequestId, MemoryObjectSendStream[JSONRPCResponse | JSONRPCError]\n    ]\n    _request_id: int\n    _in_flight: dict[RequestId, RequestResponder[ReceiveRequestT, SendResultT]]\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[JSONRPCMessage],\n        receive_request_type: type[ReceiveRequestT],\n        receive_notification_type: type[ReceiveNotificationT],\n        # If none, reading will never time out\n        read_timeout_seconds: timedelta | None = None,\n    ) -> None:\n        self._read_stream = read_stream\n        self._write_stream = write_stream\n        self._response_streams = {}\n        self._request_id = 0\n        self._receive_request_type = receive_request_type\n        self._receive_notification_type = receive_notification_type\n        self._read_timeout_seconds = read_timeout_seconds\n        self._in_flight = {}\n\n        self._exit_stack = AsyncExitStack()\n\n    async def __aenter__(self) -> Self:\n        self._task_group = anyio.create_task_group()\n        await self._task_group.__aenter__()\n        self._task_group.start_soon(self._receive_loop)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self._exit_stack.aclose()\n        # Using BaseSession as a context manager should not block on exit (this\n        # would be very surprising behavior), so make sure to cancel the tasks\n        # in the task group.\n        self._task_group.cancel_scope.cancel()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    async def send_request(\n        self,\n        request: SendRequestT,\n        result_type: type[ReceiveResultT],\n    ) -> ReceiveResultT:\n        \"\"\"\n        Sends a request and wait for a response. Raises an McpError if the\n        response contains an error.\n\n        Do not use this method to emit notifications! Use send_notification()\n        instead.\n        \"\"\"\n\n        request_id = self._request_id\n        self._request_id = request_id + 1\n\n        response_stream, response_stream_reader = anyio.create_memory_object_stream[\n            JSONRPCResponse | JSONRPCError\n        ](1)\n        self._response_streams[request_id] = response_stream\n\n        self._exit_stack.push_async_callback(lambda: response_stream.aclose())\n        self._exit_stack.push_async_callback(lambda: response_stream_reader.aclose())\n\n        jsonrpc_request = JSONRPCRequest(\n            jsonrpc=\"2.0\",\n            id=request_id,\n            **request.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        # TODO: Support progress callbacks\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_request))\n\n        try:\n            with anyio.fail_after(\n                None\n                if self._read_timeout_seconds is None\n                else self._read_timeout_seconds.total_seconds()\n            ):\n                response_or_error = await response_stream_reader.receive()\n        except TimeoutError:\n            raise McpError(\n                ErrorData(\n                    code=httpx.codes.REQUEST_TIMEOUT,\n                    message=(\n                        f\"Timed out while waiting for response to \"\n                        f\"{request.__class__.__name__}. Waited \"\n                        f\"{self._read_timeout_seconds} seconds.\"\n                    ),\n                )\n            )\n\n        if isinstance(response_or_error, JSONRPCError):\n            raise McpError(response_or_error.error)\n        else:\n            return result_type.model_validate(response_or_error.result)\n\n    async def send_notification(self, notification: SendNotificationT) -> None:\n        \"\"\"\n        Emits a notification, which is a one-way message that does not expect\n        a response.\n        \"\"\"\n        jsonrpc_notification = JSONRPCNotification(\n            jsonrpc=\"2.0\",\n            **notification.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_notification))\n\n    async def _send_response(\n        self, request_id: RequestId, response: SendResultT | ErrorData\n    ) -> None:\n        if isinstance(response, ErrorData):\n            jsonrpc_error = JSONRPCError(jsonrpc=\"2.0\", id=request_id, error=response)\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_error))\n        else:\n            jsonrpc_response = JSONRPCResponse(\n                jsonrpc=\"2.0\",\n                id=request_id,\n                result=response.model_dump(\n                    by_alias=True, mode=\"json\", exclude_none=True\n                ),\n            )\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_response))\n\n    async def _receive_loop(self) -> None:\n        async with (\n            self._read_stream,\n            self._write_stream,\n        ):\n            async for message in self._read_stream:\n                if isinstance(message, Exception):\n                    await self._handle_incoming(message)\n                elif isinstance(message.root, JSONRPCRequest):\n                    validated_request = self._receive_request_type.model_validate(\n                        message.root.model_dump(\n                            by_alias=True, mode=\"json\", exclude_none=True\n                        )\n                    )\n\n                    responder = RequestResponder(\n                        request_id=message.root.id,\n                        request_meta=validated_request.root.params.meta\n                        if validated_request.root.params\n                        else None,\n                        request=validated_request,\n                        session=self,\n                        on_complete=lambda r: self._in_flight.pop(r.request_id, None),\n                    )\n\n                    self._in_flight[responder.request_id] = responder\n                    await self._received_request(responder)\n\n                    if not responder._completed:  # type: ignore[reportPrivateUsage]\n                        await self._handle_incoming(responder)\n\n                elif isinstance(message.root, JSONRPCNotification):\n                    try:\n                        notification = self._receive_notification_type.model_validate(\n                            message.root.model_dump(\n                                by_alias=True, mode=\"json\", exclude_none=True\n                            )\n                        )\n                        # Handle cancellation notifications\n                        if isinstance(notification.root, CancelledNotification):\n                            cancelled_id = notification.root.params.requestId\n                            if cancelled_id in self._in_flight:\n                                await self._in_flight[cancelled_id].cancel()\n                        else:\n                            await self._received_notification(notification)\n                            await self._handle_incoming(notification)\n                    except Exception as e:\n                        # For other validation errors, log and continue\n                        logging.warning(\n                            f\"Failed to validate notification: {e}. \"\n                            f\"Message was: {message.root}\"\n                        )\n                else:  # Response or error\n                    stream = self._response_streams.pop(message.root.id, None)\n                    if stream:\n                        await stream.send(message.root)\n                    else:\n                        await self._handle_incoming(\n                            RuntimeError(\n                                \"Received response with an unknown \"\n                                f\"request ID: {message}\"\n                            )\n                        )\n\n    async def _received_request(\n        self, responder: RequestResponder[ReceiveRequestT, SendResultT]\n    ) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a request without needing to\n        listen on the message stream.\n\n        If the request is responded to within this method, it will not be\n        forwarded on to the message stream.\n        \"\"\"\n\n    async def _received_notification(self, notification: ReceiveNotificationT) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a notification without needing\n        to listen on the message stream.\n        \"\"\"\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"\n        Sends a progress notification for a request that is currently being\n        processed.\n        \"\"\"\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[ReceiveRequestT, SendResultT]\n        | ReceiveNotificationT\n        | Exception,\n    ) -> None:\n        \"\"\"A generic handler for incoming messages. Overwritten by subclasses.\"\"\"\n        pass\n\n\n--- File Index 66: src/mcp/shared/version.py ---\nfrom mcp.types import LATEST_PROTOCOL_VERSION\n\nSUPPORTED_PROTOCOL_VERSIONS: tuple[int, str] = (1, LATEST_PROTOCOL_VERSION)\n\n\n--- File Index 67: src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\n\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        - 0 # examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\n- 1 # examples/fastmcp/complex_inputs.py\n- 2 # examples/fastmcp/desktop.py\n- 3 # examples/fastmcp/echo.py\n- 4 # examples/fastmcp/memory.py\n- 5 # examples/fastmcp/parameter_descriptions.py\n- 6 # examples/fastmcp/readme-quickstart.py\n- 7 # examples/fastmcp/screenshot.py\n- 8 # examples/fastmcp/simple_echo.py\n- 9 # examples/fastmcp/text_me.py\n- 10 # examples/fastmcp/unicode_example.py\n- 11 # examples/servers/simple-prompt/mcp_simple_prompt/__init__.py\n- 12 # examples/servers/simple-prompt/mcp_simple_prompt/__main__.py\n- 13 # examples/servers/simple-prompt/mcp_simple_prompt/server.py\n- 14 # examples/servers/simple-resource/mcp_simple_resource/__init__.py\n- 15 # examples/servers/simple-resource/mcp_simple_resource/__main__.py\n- 16 # examples/servers/simple-resource/mcp_simple_resource/server.py\n- 17 # examples/servers/simple-tool/mcp_simple_tool/__init__.py\n- 18 # examples/servers/simple-tool/mcp_simple_tool/__main__.py\n- 19 # examples/servers/simple-tool/mcp_simple_tool/server.py\n- 20 # src/mcp/__init__.py\n- 21 # src/mcp/cli/__init__.py\n- 22 # src/mcp/cli/claude.py\n- 23 # src/mcp/cli/cli.py\n- 24 # src/mcp/client/__init__.py\n- 25 # src/mcp/client/__main__.py\n- 26 # src/mcp/client/session.py\n- 27 # src/mcp/client/sse.py\n- 28 # src/mcp/client/stdio/__init__.py\n- 29 # src/mcp/client/stdio/win32.py\n- 30 # src/mcp/client/websocket.py\n- 31 # src/mcp/server/__init__.py\n- 32 # src/mcp/server/__main__.py\n- 33 # src/mcp/server/fastmcp/__init__.py\n- 34 # src/mcp/server/fastmcp/exceptions.py\n- 35 # src/mcp/server/fastmcp/prompts/__init__.py\n- 36 # src/mcp/server/fastmcp/prompts/base.py\n- 37 # src/mcp/server/fastmcp/prompts/manager.py\n- 38 # src/mcp/server/fastmcp/prompts/prompt_manager.py\n- 39 # src/mcp/server/fastmcp/resources/__init__.py\n- 40 # src/mcp/server/fastmcp/resources/base.py\n- 41 # src/mcp/server/fastmcp/resources/resource_manager.py\n- 42 # src/mcp/server/fastmcp/resources/templates.py\n- 43 # src/mcp/server/fastmcp/resources/types.py\n- 44 # src/mcp/server/fastmcp/server.py\n- 45 # src/mcp/server/fastmcp/tools/__init__.py\n- 46 # src/mcp/server/fastmcp/tools/base.py\n- 47 # src/mcp/server/fastmcp/tools/tool_manager.py\n- 48 # src/mcp/server/fastmcp/utilities/__init__.py\n- 49 # src/mcp/server/fastmcp/utilities/func_metadata.py\n- 50 # src/mcp/server/fastmcp/utilities/logging.py\n- 51 # src/mcp/server/fastmcp/utilities/types.py\n- 52 # src/mcp/server/lowlevel/__init__.py\n- 53 # src/mcp/server/lowlevel/helper_types.py\n- 54 # src/mcp/server/lowlevel/server.py\n- 55 # src/mcp/server/models.py\n- 56 # src/mcp/server/session.py\n- 57 # src/mcp/server/sse.py\n- 58 # src/mcp/server/stdio.py\n- 59 # src/mcp/server/websocket.py\n- 60 # src/mcp/shared/__init__.py\n- 61 # src/mcp/shared/context.py\n- 62 # src/mcp/shared/exceptions.py\n- 63 # src/mcp/shared/memory.py\n- 64 # src/mcp/shared/progress.py\n- 65 # src/mcp/shared/session.py\n- 66 # src/mcp/shared/version.py\n- 67 # src/mcp/types.py\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```": "```yaml\n- name: MCP Protocol Types\n  description: |\n    These are the fundamental building blocks defining the communication \"language\" between MCP clients and servers. They specify the exact structure of requests (what one side asks for), responses (the answer), and notifications (one-way messages). Think of it like the shared grammar and vocabulary rules that both the client and server must follow to understand each other, ensuring messages like asking for tools or getting results are formatted correctly. All interactions rely on these shared Pydantic models.\n  file_indices:\n    - 20 # src/mcp/__init__.py (exports many types)\n    - 67 # src/mcp/types.py (defines all core protocol types)\n    - 26 # src/mcp/client/session.py (uses client types)\n    - 56 # src/mcp/server/session.py (uses server types)\n    - 65 # src/mcp/shared/session.py (uses request/response types)\n\n- name: FastMCP Server\n  description: |\n    A high-level, user-friendly framework for creating MCP servers. It simplifies server development by letting you define tools, resources, and prompts using Python decorators (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`) on regular functions. It handles the underlying MCP protocol details, JSON schema generation, and request routing automatically. Think of it as a helpful assistant that lets you focus on writing the *logic* for your server's capabilities, rather than the complex communication setup.\n  file_indices:\n    - 33 # src/mcp/server/fastmcp/__init__.py\n    - 44 # src/mcp/server/fastmcp/server.py (main FastMCP class definition)\n    - 6 # examples/fastmcp/readme-quickstart.py (simple FastMCP usage)\n    - 2 # examples/fastmcp/desktop.py\n    - 3 # examples/fastmcp/echo.py\n    - 7 # examples/fastmcp/screenshot.py\n\n- name: Client Session\n  description: |\n    Manages the client-side connection to an MCP server. It handles sending requests (like asking to list tools or call a specific tool) and receiving responses and notifications from the server. It abstracts away the low-level communication details, providing Python methods like `list_tools()` or `call_tool()`. Imagine it as the control panel you use to interact with a remote service (the MCP server), translating your Python calls into the correct MCP protocol messages.\n  file_indices:\n    - 20 # src/mcp/__init__.py (exports ClientSession)\n    - 26 # src/mcp/client/session.py (main ClientSession class definition)\n    - 0 # examples/clients/simple-chatbot/mcp_simple_chatbot/main.py (example client using ClientSession)\n    - 25 # src/mcp/client/__main__.py (basic client example)\n    - 63 # src/mcp/shared/memory.py (used in testing client/server)\n\n- name: Tools\n  description: |\n    Functions exposed by an MCP server that a client can execute remotely. Tools take specific inputs (defined by a JSON schema) and return results. They allow a client (or an AI model via the client) to perform actions or computations using the server's capabilities. Think of them as specialized commands or functions the server makes available, like a calculator offering an `add` tool or a system offering a `take_screenshot` tool.\n  file_indices:\n    - 67 # src/mcp/types.py (defines Tool type)\n    - 45 # src/mcp/server/fastmcp/tools/__init__.py\n    - 46 # src/mcp/server/fastmcp/tools/base.py (FastMCP Tool base)\n    - 47 # src/mcp/server/fastmcp/tools/tool_manager.py (FastMCP Tool management)\n    - 44 # src/mcp/server/fastmcp/server.py (FastMCP @tool decorator)\n    - 19 # examples/servers/simple-tool/mcp_simple_tool/server.py (low-level tool example)\n    - 6 # examples/fastmcp/readme-quickstart.py (FastMCP tool example)\n    - 7 # examples/fastmcp/screenshot.py\n\n- name: Resources\n  description: |\n    Data sources exposed by an MCP server that a client can read. Resources are identified by URIs (like `file:///path/to/file.txt` or `http://example.com/data`) and can represent text, binary data, files, directories, or even dynamically generated content from functions. Think of resources as the specific files, web pages, or data streams that the server can provide access to upon request, like asking a library server for a specific book (resource) via its catalog number (URI).\n  file_indices:\n    - 67 # src/mcp/types.py (defines Resource type)\n    - 39 # src/mcp/server/fastmcp/resources/__init__.py\n    - 40 # src/mcp/server/fastmcp/resources/base.py (Resource base class)\n    - 43 # src/mcp/server/fastmcp/resources/types.py (Concrete resource types)\n    - 41 # src/mcp/server/fastmcp/resources/resource_manager.py (Management)\n    - 44 # src/mcp/server/fastmcp/server.py (FastMCP @resource decorator)\n    - 16 # examples/servers/simple-resource/mcp_simple_resource/server.py (low-level resource example)\n    - 2 # examples/fastmcp/desktop.py (FastMCP resource example)\n\n- name: Transports\n  description: |\n    The underlying communication mechanism used to connect a client and a server. MCP supports different transports like standard input/output (stdio), Server-Sent Events (SSE over HTTP), and WebSockets. The transport handles the actual sending and receiving of the formatted MCP messages. Think of it as the delivery method for messages between client and server \u2013 like choosing between sending letters via postal mail (stdio), a continuous news ticker feed (SSE), or a direct phone line (WebSocket).\n  file_indices:\n    - 28 # src/mcp/client/stdio/__init__.py\n    - 58 # src/mcp/server/stdio.py\n    - 27 # src/mcp/client/sse.py\n    - 57 # src/mcp/server/sse.py\n    - 30 # src/mcp/client/websocket.py\n    - 59 # src/mcp/server/websocket.py\n    - 0 # examples/clients/simple-chatbot/mcp_simple_chatbot/main.py (uses stdio client)\n    - 13 # examples/servers/simple-prompt/mcp_simple_prompt/server.py (shows stdio/sse server selection)\n\n- name: Prompts\n  description: |\n    Templates provided by the server for generating sequences of messages, often used to instruct Large Language Models (LLMs). Prompts can be static or dynamic, accepting arguments to customize the generated messages. They allow a server to provide pre-defined interaction structures or instructions to the client/LLM. Think of them as pre-written recipes or instruction manuals that the server provides, which can sometimes be filled in with specific ingredients (arguments).\n  file_indices:\n    - 67 # src/mcp/types.py (defines Prompt type)\n    - 35 # src/mcp/server/fastmcp/prompts/__init__.py\n    - 36 # src/mcp/server/fastmcp/prompts/base.py (FastMCP Prompt base)\n    - 37 # src/mcp/server/fastmcp/prompts/manager.py (FastMCP Prompt management)\n    - 44 # src/mcp/server/fastmcp/server.py (FastMCP @prompt decorator)\n    - 13 # examples/servers/simple-prompt/mcp_simple_prompt/server.py (low-level prompt example)\n    - 3 # examples/fastmcp/echo.py (FastMCP prompt example)\n\n- name: FastMCP Context\n  description: |\n    An object provided within FastMCP tool and resource functions that gives access to server capabilities during request handling. It allows the function logic to interact with the MCP session, for example, to send log messages back to the client, report progress on long-running tasks, or even read other resources provided by the same server. Think of it as a special toolkit automatically handed to your tool/resource function when it's called, containing utilities relevant to the current request.\n  file_indices:\n    - 44 # src/mcp/server/fastmcp/server.py (defines Context class and its injection)\n    - 46 # src/mcp/server/fastmcp/tools/base.py (handles context injection for tools)\n    - 47 # src/mcp/server/fastmcp/tools/tool_manager.py (passes context during tool call)\n    - 61 # src/mcp/shared/context.py (underlying RequestContext used by FastMCP Context)\n    - 4 # examples/fastmcp/memory.py (shows usage in tools)\n```", "\nBased on the following abstractions and relevant code snippets from the project `python-sdk`:\n\nList of Abstraction Indices and Names:\n0 # MCP Protocol Types\n1 # FastMCP Server\n2 # Client Session\n3 # Tools\n4 # Resources\n5 # Transports\n6 # Prompts\n7 # FastMCP Context\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: MCP Protocol Types (Relevant file indices: [20, 26, 56, 65, 67])\n  Description: These are the fundamental building blocks defining the communication \"language\" between MCP clients and servers. They specify the exact structure of requests (what one side asks for), responses (the answer), and notifications (one-way messages). Think of it like the shared grammar and vocabulary rules that both the client and server must follow to understand each other, ensuring messages like asking for tools or getting results are formatted correctly. All interactions rely on these shared Pydantic models.\n\n- Index 1: FastMCP Server (Relevant file indices: [2, 3, 6, 7, 33, 44])\n  Description: A high-level, user-friendly framework for creating MCP servers. It simplifies server development by letting you define tools, resources, and prompts using Python decorators (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`) on regular functions. It handles the underlying MCP protocol details, JSON schema generation, and request routing automatically. Think of it as a helpful assistant that lets you focus on writing the *logic* for your server's capabilities, rather than the complex communication setup.\n\n- Index 2: Client Session (Relevant file indices: [0, 20, 25, 26, 63])\n  Description: Manages the client-side connection to an MCP server. It handles sending requests (like asking to list tools or call a specific tool) and receiving responses and notifications from the server. It abstracts away the low-level communication details, providing Python methods like `list_tools()` or `call_tool()`. Imagine it as the control panel you use to interact with a remote service (the MCP server), translating your Python calls into the correct MCP protocol messages.\n\n- Index 3: Tools (Relevant file indices: [6, 7, 19, 44, 45, 46, 47, 67])\n  Description: Functions exposed by an MCP server that a client can execute remotely. Tools take specific inputs (defined by a JSON schema) and return results. They allow a client (or an AI model via the client) to perform actions or computations using the server's capabilities. Think of them as specialized commands or functions the server makes available, like a calculator offering an `add` tool or a system offering a `take_screenshot` tool.\n\n- Index 4: Resources (Relevant file indices: [2, 16, 39, 40, 41, 43, 44, 67])\n  Description: Data sources exposed by an MCP server that a client can read. Resources are identified by URIs (like `file:///path/to/file.txt` or `http://example.com/data`) and can represent text, binary data, files, directories, or even dynamically generated content from functions. Think of resources as the specific files, web pages, or data streams that the server can provide access to upon request, like asking a library server for a specific book (resource) via its catalog number (URI).\n\n- Index 5: Transports (Relevant file indices: [0, 13, 27, 28, 30, 57, 58, 59])\n  Description: The underlying communication mechanism used to connect a client and a server. MCP supports different transports like standard input/output (stdio), Server-Sent Events (SSE over HTTP), and WebSockets. The transport handles the actual sending and receiving of the formatted MCP messages. Think of it as the delivery method for messages between client and server \u2013 like choosing between sending letters via postal mail (stdio), a continuous news ticker feed (SSE), or a direct phone line (WebSocket).\n\n- Index 6: Prompts (Relevant file indices: [3, 13, 35, 36, 37, 44, 67])\n  Description: Templates provided by the server for generating sequences of messages, often used to instruct Large Language Models (LLMs). Prompts can be static or dynamic, accepting arguments to customize the generated messages. They allow a server to provide pre-defined interaction structures or instructions to the client/LLM. Think of them as pre-written recipes or instruction manuals that the server provides, which can sometimes be filled in with specific ingredients (arguments).\n\n- Index 7: FastMCP Context (Relevant file indices: [4, 44, 46, 47, 61])\n  Description: An object provided within FastMCP tool and resource functions that gives access to server capabilities during request handling. It allows the function logic to interact with the MCP session, for example, to send log messages back to the client, report progress on long-running tasks, or even read other resources provided by the same server. Think of it as a special toolkit automatically handed to your tool/resource function when it's called, containing utilities relevant to the current request.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 0 # examples/clients/simple-chatbot/mcp_simple_chatbot/main.py ---\nimport asyncio\nimport json\nimport logging\nimport os\nimport shutil\nfrom contextlib import AsyncExitStack\nfrom typing import Any\n\nimport httpx\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\n\nclass Configuration:\n    \"\"\"Manages configuration and environment variables for the MCP client.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize configuration with environment variables.\"\"\"\n        self.load_env()\n        self.api_key = os.getenv(\"LLM_API_KEY\")\n\n    @staticmethod\n    def load_env() -> None:\n        \"\"\"Load environment variables from .env file.\"\"\"\n        load_dotenv()\n\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        \"\"\"Load server configuration from JSON file.\n\n        Args:\n            file_path: Path to the JSON configuration file.\n\n        Returns:\n            Dict containing server configuration.\n\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n\n    @property\n    def llm_api_key(self) -> str:\n        \"\"\"Get the LLM API key.\n\n        Returns:\n            The API key as a string.\n\n        Raises:\n            ValueError: If the API key is not found in environment variables.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"LLM_API_KEY not found in environment variables\")\n        return self.api_key\n\n\nclass Server:\n    \"\"\"Manages MCP server connections and tool execution.\"\"\"\n\n    def __init__(self, name: str, config: dict[str, Any]) -> None:\n        self.name: str = name\n        self.config: dict[str, Any] = config\n        self.stdio_context: Any | None = None\n        self.session: ClientSession | None = None\n        self._cleanup_lock: asyncio.Lock = asyncio.Lock()\n        self.exit_stack: AsyncExitStack = AsyncExitStack()\n\n    async def initialize(self) -> None:\n        \"\"\"Initialize the server connection.\"\"\"\n        command = (\n            shutil.which(\"npx\")\n            if self.config[\"command\"] == \"npx\"\n            else self.config[\"command\"]\n        )\n        if command is None:\n            raise ValueError(\"The command must be a valid string and cannot be None.\")\n\n        server_params = StdioServerParameters(\n            command=command,\n            args=self.config[\"args\"],\n            env={**os.environ, **self.config[\"env\"]}\n            if self.config.get(\"env\")\n            else None,\n        )\n        try:\n            stdio_transport = await self.exit_stack.enter_async_context(\n                stdio_client(server_params)\n            )\n            read, write = stdio_transport\n            session = await self.exit_stack.enter_async_context(\n                ClientSession(read, write)\n            )\n            await session.initialize()\n            self.session = session\n        except Exception as e:\n            logging.error(f\"Error initializing server {self.name}: {e}\")\n            await self.cleanup()\n            raise\n\n    async def list_tools(self) -> list[Any]:\n        \"\"\"List available tools from the server.\n\n        Returns:\n            A list of available tools.\n\n        Raises:\n            RuntimeError: If the server is not initialized.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        tools_response = await self.session.list_tools()\n        tools = []\n\n        for item in tools_response:\n            if isinstance(item, tuple) and item[0] == \"tools\":\n                tools.extend(\n                    Tool(tool.name, tool.description, tool.inputSchema)\n                    for tool in item[1]\n                )\n\n        return tools\n\n    async def execute_tool(\n        self,\n        tool_name: str,\n        arguments: dict[str, Any],\n        retries: int = 2,\n        delay: float = 1.0,\n    ) -> Any:\n        \"\"\"Execute a tool with retry mechanism.\n\n        Args:\n            tool_name: Name of the tool to execute.\n            arguments: Tool arguments.\n            retries: Number of retry attempts.\n            delay: Delay between retries in seconds.\n\n        Returns:\n            Tool execution result.\n\n        Raises:\n            RuntimeError: If server is not initialized.\n            Exception: If tool execution fails after all retries.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        attempt = 0\n        while attempt < retries:\n            try:\n                logging.info(f\"Executing {tool_name}...\")\n                result = await self.session.call_tool(tool_name, arguments)\n\n                return result\n\n            except Exception as e:\n                attempt += 1\n                logging.warning(\n                    f\"Error executing tool: {e}. Attempt {attempt} of {retries}.\"\n                )\n                if attempt < retries:\n                    logging.info(f\"Retrying in {delay} seconds...\")\n                    await asyncio.sleep(delay)\n                else:\n                    logging.error(\"Max retries reached. Failing.\")\n                    raise\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up server resources.\"\"\"\n        async with self._cleanup_lock:\n            try:\n                await self.exit_stack.aclose()\n                self.session = None\n                self.stdio_context = None\n            except Exception as e:\n                logging.error(f\"Error during cleanup of server {self.name}: {e}\")\n\n\nclass Tool:\n    \"\"\"Represents a tool with its properties and formatting.\"\"\"\n\n    def __init__(\n        self, name: str, description: str, input_schema: dict[str, Any]\n    ) -> None:\n        self.name: str = name\n        self.description: str = description\n        self.input_schema: dict[str, Any] = input_schema\n\n    def format_for_llm(self) -> str:\n        \"\"\"Format tool information for LLM.\n\n        Returns:\n            A formatted string describing the tool.\n        \"\"\"\n        args_desc = []\n        if \"properties\" in self.input_schema:\n            for param_name, param_info in self.input_schema[\"properties\"].items():\n                arg_desc = (\n                    f\"- {param_name}: {param_info.get('description', 'No description')}\"\n                )\n                if param_name in self.input_schema.get(\"required\", []):\n                    arg_desc += \" (required)\"\n                args_desc.append(arg_desc)\n\n        return f\"\"\"\nTool: {self.name}\nDescription: {self.description}\nArguments:\n{chr(10).join(args_desc)}\n\"\"\"\n\n\nclass LLMClient:\n    \"\"\"Manages communication with the LLM provider.\"\"\"\n\n    def __init__(self, api_key: str) -> None:\n        self.api_key: str = api_key\n\n    def get_response(self, messages: list[dict[str, str]]) -> str:\n        \"\"\"Get a response from the LLM.\n\n        Args:\n            messages: A list of message dictionaries.\n\n        Returns:\n            The LLM's response as a string.\n\n        Raises:\n            httpx.RequestError: If the request to the LLM fails.\n        \"\"\"\n        url = \"https://api.groq.com/openai/v1/chat/completions\"\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n        payload = {\n            \"messages\": messages,\n            \"model\": \"llama-3.2-90b-vision-preview\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096,\n            \"top_p\": 1,\n            \"stream\": False,\n            \"stop\": None,\n        }\n\n        try:\n            with httpx.Client() as client:\n                response = client.post(url, headers=headers, json=payload)\n                response.raise_for_status()\n                data = response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n\n        except httpx.RequestError as e:\n            error_message = f\"Error getting LLM response: {str(e)}\"\n            logging.error(error_message)\n\n            if isinstance(e, httpx.HTTPStatusError):\n                status_code = e.response.status_code\n                logging.error(f\"Status code: {status_code}\")\n                logging.error(f\"Response details: {e.response.text}\")\n\n            return (\n                f\"I encountered an error: {error_message}. \"\n                \"Please try again or rephrase your request.\"\n            )\n\n\nclass ChatSession:\n    \"\"\"Orchestrates the interaction between user, LLM, and tools.\"\"\"\n\n    def __init__(self, servers: list[Server], llm_client: LLMClient) -> None:\n        self.servers: list[Server] = servers\n        self.llm_client: LLMClient = llm_client\n\n    async def cleanup_servers(self) -> None:\n        \"\"\"Clean up all servers properly.\"\"\"\n        cleanup_tasks = [\n            asyncio.create_task(server.cleanup()) for server in self.servers\n        ]\n        if cleanup_tasks:\n            try:\n                await asyncio.gather(*cleanup_tasks, return_exceptions=True)\n            except Exception as e:\n                logging.warning(f\"Warning during final cleanup: {e}\")\n\n    async def process_llm_response(self, llm_response: str) -> str:\n        \"\"\"Process the LLM response and execute tools if needed.\n\n        Args:\n            llm_response: The response from the LLM.\n\n        Returns:\n            The result of tool execution or the original response.\n        \"\"\"\n        import json\n\n        try:\n            tool_call = json.loads(llm_response)\n            if \"tool\" in tool_call and \"arguments\" in tool_call:\n                logging.info(f\"Executing tool: {tool_call['tool']}\")\n                logging.info(f\"With arguments: {tool_call['arguments']}\")\n\n                for server in self.servers:\n                    tools = await server.list_tools()\n                    if any(tool.name == tool_call[\"tool\"] for tool in tools):\n                        try:\n                            result = await server.execute_tool(\n                                tool_call[\"tool\"], tool_call[\"arguments\"]\n                            )\n\n                            if isinstance(result, dict) and \"progress\" in result:\n                                progress = result[\"progress\"]\n                                total = result[\"total\"]\n                                percentage = (progress / total) * 100\n                                logging.info(\n                                    f\"Progress: {progress}/{total} \"\n                                    f\"({percentage:.1f}%)\"\n                                )\n\n                            return f\"Tool execution result: {result}\"\n                        except Exception as e:\n                            error_msg = f\"Error executing tool: {str(e)}\"\n                            logging.error(error_msg)\n                            return error_msg\n\n                return f\"No server found with tool: {tool_call['tool']}\"\n            return llm_response\n        except json.JSONDecodeError:\n            return llm_response\n\n    async def start(self) -> None:\n        \"\"\"Main chat session handler.\"\"\"\n        try:\n            for server in self.servers:\n                try:\n                    await server.initialize()\n                except Exception as e:\n                    logging.error(f\"Failed to initialize server: {e}\")\n                    await self.cleanup_servers()\n                    return\n\n            all_tools = []\n            for server in self.servers:\n                tools = await server.list_tools()\n                all_tools.extend(tools)\n\n            tools_description = \"\\n\".join([tool.format_for_llm() for tool in all_tools])\n\n            system_message = (\n                \"You are a helpful assistant with access to these tools:\\n\\n\"\n                f\"{tools_description}\\n\"\n                \"Choose the appropriate tool based on the user's question. \"\n                \"If no tool is needed, reply directly.\\n\\n\"\n                \"IMPORTANT: When you need to use a tool, you must ONLY respond with \"\n                \"the exact JSON object format below, nothing else:\\n\"\n                \"{\\n\"\n                '    \"tool\": \"tool-name\",\\n'\n                '    \"arguments\": {\\n'\n                '        \"argument-name\": \"value\"\\n'\n                \"    }\\n\"\n                \"}\\n\\n\"\n                \"After receiving a tool's response:\\n\"\n                \"1. Transform the raw data into a natural, conversational response\\n\"\n                \"2. Keep responses concise but informative\\n\"\n                \"3. Focus on the most relevant information\\n\"\n                \"4. Use appropriate context from the user's question\\n\"\n                \"5. Avoid simply repeating the raw data\\n\\n\"\n                \"Please use only the tools that are explicitly defined above.\"\n            )\n\n            messages = [{\"role\": \"system\", \"content\": system_message}]\n\n            while True:\n                try:\n                    user_input = input(\"You: \").strip().lower()\n                    if user_input in [\"quit\", \"exit\"]:\n                        logging.info(\"\\nExiting...\")\n                        break\n\n                    messages.append({\"role\": \"user\", \"content\": user_input})\n\n                    llm_response = self.llm_client.get_response(messages)\n                    logging.info(\"\\nAssistant: %s\", llm_response)\n\n                    result = await self.process_llm_response(llm_response)\n\n                    if result != llm_response:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n                        messages.append({\"role\": \"system\", \"content\": result})\n\n                        final_response = self.llm_client.get_response(messages)\n                        logging.info(\"\\nFinal response: %s\", final_response)\n                        messages.append(\n                            {\"role\": \"assistant\", \"content\": final_response}\n                        )\n                    else:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n\n                except KeyboardInterrupt:\n                    logging.info(\"\\nExiting...\")\n                    break\n\n        finally:\n            await self.cleanup_servers()\n\n\nasync def main() -> None:\n    \"\"\"Initialize and run the chat session.\"\"\"\n    config = Configuration()\n    server_config = config.load_config(\"servers_config.json\")\n    servers = [\n        Server(name, srv_config)\n        for name, srv_config in server_config[\"mcpServers\"].items()\n    ]\n    llm_client = LLMClient(config.llm_api_key)\n    chat_session = ChatSession(servers, llm_client)\n    await chat_session.start()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n--- File: 2 # examples/fastmcp/desktop.py ---\n\"\"\"\nFastMCP Desktop Example\n\nA simple example that exposes the desktop directory as a resource.\n\"\"\"\n\nfrom pathlib import Path\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Demo\")\n\n\n@mcp.resource(\"dir://desktop\")\ndef desktop() -> list[str]:\n    \"\"\"List the files in the user's desktop\"\"\"\n    desktop = Path.home() / \"Desktop\"\n    return [str(f) for f in desktop.iterdir()]\n\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n--- File: 3 # examples/fastmcp/echo.py ---\n\"\"\"\nFastMCP Echo Server\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Echo Server\")\n\n\n@mcp.tool()\ndef echo_tool(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return text\n\n\n@mcp.resource(\"echo://static\")\ndef echo_resource() -> str:\n    return \"Echo!\"\n\n\n@mcp.resource(\"echo://{text}\")\ndef echo_template(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return f\"Echo: {text}\"\n\n\n@mcp.prompt(\"echo\")\ndef echo_prompt(text: str) -> str:\n    return text\n\n\n--- File: 4 # examples/fastmcp/memory.py ---\n# /// script\n# dependencies = [\"pydantic-ai-slim[openai]\", \"asyncpg\", \"numpy\", \"pgvector\"]\n# ///\n\n# uv pip install 'pydantic-ai-slim[openai]' asyncpg numpy pgvector\n\n\"\"\"\nRecursive memory system inspired by the human brain's clustering of memories.\nUses OpenAI's 'text-embedding-3-small' model and pgvector for efficient\nsimilarity search.\n\"\"\"\n\nimport asyncio\nimport math\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Annotated, Self\n\nimport asyncpg\nimport numpy as np\nfrom openai import AsyncOpenAI\nfrom pgvector.asyncpg import register_vector  # Import register_vector\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nfrom mcp.server.fastmcp import FastMCP\n\nMAX_DEPTH = 5\nSIMILARITY_THRESHOLD = 0.7\nDECAY_FACTOR = 0.99\nREINFORCEMENT_FACTOR = 1.1\n\nDEFAULT_LLM_MODEL = \"openai:gpt-4o\"\nDEFAULT_EMBEDDING_MODEL = \"text-embedding-3-small\"\n\nmcp = FastMCP(\n    \"memory\",\n    dependencies=[\n        \"pydantic-ai-slim[openai]\",\n        \"asyncpg\",\n        \"numpy\",\n        \"pgvector\",\n    ],\n)\n\nDB_DSN = \"postgresql://postgres:postgres@localhost:54320/memory_db\"\n# reset memory with rm ~/.fastmcp/{USER}/memory/*\nPROFILE_DIR = (\n    Path.home() / \".fastmcp\" / os.environ.get(\"USER\", \"anon\") / \"memory\"\n).resolve()\nPROFILE_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    a_array = np.array(a, dtype=np.float64)\n    b_array = np.array(b, dtype=np.float64)\n    return np.dot(a_array, b_array) / (\n        np.linalg.norm(a_array) * np.linalg.norm(b_array)\n    )\n\n\nasync def do_ai[T](\n    user_prompt: str,\n    system_prompt: str,\n    result_type: type[T] | Annotated,\n    deps=None,\n) -> T:\n    agent = Agent(\n        DEFAULT_LLM_MODEL,\n        system_prompt=system_prompt,\n        result_type=result_type,\n    )\n    result = await agent.run(user_prompt, deps=deps)\n    return result.data\n\n\n@dataclass\nclass Deps:\n    openai: AsyncOpenAI\n    pool: asyncpg.Pool\n\n\nasync def get_db_pool() -> asyncpg.Pool:\n    async def init(conn):\n        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n        await register_vector(conn)\n\n    pool = await asyncpg.create_pool(DB_DSN, init=init)\n    return pool\n\n\nclass MemoryNode(BaseModel):\n    id: int | None = None\n    content: str\n    summary: str = \"\"\n    importance: float = 1.0\n    access_count: int = 0\n    timestamp: float = Field(\n        default_factory=lambda: datetime.now(timezone.utc).timestamp()\n    )\n    embedding: list[float]\n\n    @classmethod\n    async def from_content(cls, content: str, deps: Deps):\n        embedding = await get_embedding(content, deps)\n        return cls(content=content, embedding=embedding)\n\n    async def save(self, deps: Deps):\n        async with deps.pool.acquire() as conn:\n            if self.id is None:\n                result = await conn.fetchrow(\n                    \"\"\"\n                    INSERT INTO memories (content, summary, importance, access_count,\n                        timestamp, embedding)\n                    VALUES ($1, $2, $3, $4, $5, $6)\n                    RETURNING id\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                )\n                self.id = result[\"id\"]\n            else:\n                await conn.execute(\n                    \"\"\"\n                    UPDATE memories\n                    SET content = $1, summary = $2, importance = $3,\n                        access_count = $4, timestamp = $5, embedding = $6\n                    WHERE id = $7\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                    self.id,\n                )\n\n    async def merge_with(self, other: Self, deps: Deps):\n        self.content = await do_ai(\n            f\"{self.content}\\n\\n{other.content}\",\n            \"Combine the following two texts into a single, coherent text.\",\n            str,\n            deps,\n        )\n        self.importance += other.importance\n        self.access_count += other.access_count\n        self.embedding = [(a + b) / 2 for a, b in zip(self.embedding, other.embedding)]\n        self.summary = await do_ai(\n            self.content, \"Summarize the following text concisely.\", str, deps\n        )\n        await self.save(deps)\n        # Delete the merged node from the database\n        if other.id is not None:\n            await delete_memory(other.id, deps)\n\n    def get_effective_importance(self):\n        return self.importance * (1 + math.log(self.access_count + 1))\n\n\nasync def get_embedding(text: str, deps: Deps) -> list[float]:\n    embedding_response = await deps.openai.embeddings.create(\n        input=text,\n        model=DEFAULT_EMBEDDING_MODEL,\n    )\n    return embedding_response.data[0].embedding\n\n\nasync def delete_memory(memory_id: int, deps: Deps):\n    async with deps.pool.acquire() as conn:\n        await conn.execute(\"DELETE FROM memories WHERE id = $1\", memory_id)\n\n\nasync def add_memory(content: str, deps: Deps):\n    new_memory = await MemoryNode.from_content(content, deps)\n    await new_memory.save(deps)\n\n    similar_memories = await find_similar_memories(new_memory.embedding, deps)\n    for memory in similar_memories:\n        if memory.id != new_memory.id:\n            await new_memory.merge_with(memory, deps)\n\n    await update_importance(new_memory.embedding, deps)\n\n    await prune_memories(deps)\n\n    return f\"Remembered: {content}\"\n\n\nasync def find_similar_memories(embedding: list[float], deps: Deps) -> list[MemoryNode]:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, content, summary, importance, access_count, timestamp, embedding\n            FROM memories\n            ORDER BY embedding <-> $1\n            LIMIT 5\n            \"\"\",\n            embedding,\n        )\n    memories = [\n        MemoryNode(\n            id=row[\"id\"],\n            content=row[\"content\"],\n            summary=row[\"summary\"],\n            importance=row[\"importance\"],\n            access_count=row[\"access_count\"],\n            timestamp=row[\"timestamp\"],\n            embedding=row[\"embedding\"],\n        )\n        for row in rows\n    ]\n    return memories\n\n\nasync def update_importance(user_embedding: list[float], deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"SELECT id, importance, access_count, embedding FROM memories\"\n        )\n        for row in rows:\n            memory_embedding = row[\"embedding\"]\n            similarity = cosine_similarity(user_embedding, memory_embedding)\n            if similarity > SIMILARITY_THRESHOLD:\n                new_importance = row[\"importance\"] * REINFORCEMENT_FACTOR\n                new_access_count = row[\"access_count\"] + 1\n            else:\n                new_importance = row[\"importance\"] * DECAY_FACTOR\n                new_access_count = row[\"access_count\"]\n            await conn.execute(\n                \"\"\"\n                UPDATE memories\n                SET importance = $1, access_count = $2\n                WHERE id = $3\n                \"\"\",\n                new_importance,\n                new_access_count,\n                row[\"id\"],\n            )\n\n\nasync def prune_memories(deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            OFFSET $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n        for row in rows:\n            await conn.execute(\"DELETE FROM memories WHERE id = $1\", row[\"id\"])\n\n\nasync def display_memory_tree(deps: Deps) -> str:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT content, summary, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            LIMIT $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n    result = \"\"\n    for row in rows:\n        effective_importance = row[\"importance\"] * (\n            1 + math.log(row[\"access_count\"] + 1)\n        )\n        summary = row[\"summary\"] or row[\"content\"]\n        result += f\"- {summary} (Importance: {effective_importance:.2f})\\n\"\n    return result\n\n\n@mcp.tool()\nasync def remember(\n    contents: list[str] = Field(\n        description=\"List of observations or memories to store\"\n    ),\n):\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    try:\n        return \"\\n\".join(\n            await asyncio.gather(*[add_memory(content, deps) for content in contents])\n        )\n    finally:\n        await deps.pool.close()\n\n\n@mcp.tool()\nasync def read_profile() -> str:\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    profile = await display_memory_tree(deps)\n    await deps.pool.close()\n    return profile\n\n\nasync def initialize_database():\n    pool = await asyncpg.create_pool(\n        \"postgresql://postgres:postgres@localhost:54320/postgres\"\n    )\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                SELECT pg_terminate_backend(pg_stat_activity.pid)\n                FROM pg_stat_activity\n                WHERE pg_stat_activity.datname = 'memory_db'\n                AND pid <> pg_backend_pid();\n            \"\"\")\n            await conn.execute(\"DROP DATABASE IF EXISTS memory_db;\")\n            await conn.execute(\"CREATE DATABASE memory_db;\")\n    finally:\n        await pool.close()\n\n    pool = await asyncpg.create_pool(DB_DSN)\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n\n            await register_vector(conn)\n\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS memories (\n                    id SERIAL PRIMARY KEY,\n                    content TEXT NOT NULL,\n                    summary TEXT,\n                    importance REAL NOT NULL,\n                    access_count INT NOT NULL,\n                    timestamp DOUBLE PRECISION NOT NULL,\n                    embedding vector(1536) NOT NULL\n                );\n                CREATE INDEX IF NOT EXISTS idx_memories_embedding ON memories\n                    USING hnsw (embedding vector_l2_ops);\n            \"\"\")\n    finally:\n        await pool.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(initialize_database())\n\n\n--- File: 6 # examples/fastmcp/readme-quickstart.py ---\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n--- File: 7 # examples/fastmcp/screenshot.py ---\n\"\"\"\nFastMCP Screenshot Example\n\nGive Claude a tool to capture and view screenshots.\n\"\"\"\n\nimport io\n\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.utilities.types import Image\n\n# Create server\nmcp = FastMCP(\"Screenshot Demo\", dependencies=[\"pyautogui\", \"Pillow\"])\n\n\n@mcp.tool()\ndef take_screenshot() -> Image:\n    \"\"\"\n    Take a screenshot of the user's screen and return it as an image. Use\n    this tool anytime the user wants you to look at something they're doing.\n    \"\"\"\n    import pyautogui\n\n    buffer = io.BytesIO()\n\n    # if the file exceeds ~1MB, it will be rejected by Claude\n    screenshot = pyautogui.screenshot()\n    screenshot.convert(\"RGB\").save(buffer, format=\"JPEG\", quality=60, optimize=True)\n    return Image(data=buffer.getvalue(), format=\"jpeg\")\n\n\n--- File: 13 # examples/servers/simple-prompt/mcp_simple_prompt/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\ndef create_messages(\n    context: str | None = None, topic: str | None = None\n) -> list[types.PromptMessage]:\n    \"\"\"Create the messages for the prompt.\"\"\"\n    messages = []\n\n    # Add context if provided\n    if context:\n        messages.append(\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(\n                    type=\"text\", text=f\"Here is some relevant context: {context}\"\n                ),\n            )\n        )\n\n    # Add the main prompt\n    prompt = \"Please help me with \"\n    if topic:\n        prompt += f\"the following topic: {topic}\"\n    else:\n        prompt += \"whatever questions I may have.\"\n\n    messages.append(\n        types.PromptMessage(\n            role=\"user\", content=types.TextContent(type=\"text\", text=prompt)\n        )\n    )\n\n    return messages\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-prompt\")\n\n    @app.list_prompts()\n    async def list_prompts() -> list[types.Prompt]:\n        return [\n            types.Prompt(\n                name=\"simple\",\n                description=\"A simple prompt that can take optional context and topic \"\n                \"arguments\",\n                arguments=[\n                    types.PromptArgument(\n                        name=\"context\",\n                        description=\"Additional context to consider\",\n                        required=False,\n                    ),\n                    types.PromptArgument(\n                        name=\"topic\",\n                        description=\"Specific topic to focus on\",\n                        required=False,\n                    ),\n                ],\n            )\n        ]\n\n    @app.get_prompt()\n    async def get_prompt(\n        name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        if name != \"simple\":\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        if arguments is None:\n            arguments = {}\n\n        return types.GetPromptResult(\n            messages=create_messages(\n                context=arguments.get(\"context\"), topic=arguments.get(\"topic\")\n            ),\n            description=\"A simple prompt with optional context and topic arguments\",\n        )\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: 16 # examples/servers/simple-resource/mcp_simple_resource/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\nfrom pydantic import FileUrl\n\nSAMPLE_RESOURCES = {\n    \"greeting\": \"Hello! This is a sample text resource.\",\n    \"help\": \"This server provides a few sample text resources for testing.\",\n    \"about\": \"This is the simple-resource MCP server implementation.\",\n}\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-resource\")\n\n    @app.list_resources()\n    async def list_resources() -> list[types.Resource]:\n        return [\n            types.Resource(\n                uri=FileUrl(f\"file:///{name}.txt\"),\n                name=name,\n                description=f\"A sample text resource named {name}\",\n                mimeType=\"text/plain\",\n            )\n            for name in SAMPLE_RESOURCES.keys()\n        ]\n\n    @app.read_resource()\n    async def read_resource(uri: FileUrl) -> str | bytes:\n        name = uri.path.replace(\".txt\", \"\").lstrip(\"/\")\n\n        if name not in SAMPLE_RESOURCES:\n            raise ValueError(f\"Unknown resource: {uri}\")\n\n        return SAMPLE_RESOURCES[name]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: 19 # examples/servers/simple-tool/mcp_simple_tool/server.py ---\nimport anyio\nimport click\nimport httpx\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\nasync def fetch_website(\n    url: str,\n) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    headers = {\n        \"User-Agent\": \"MCP Test Server (github.com/modelcontextprotocol/python-sdk)\"\n    }\n    async with httpx.AsyncClient(follow_redirects=True, headers=headers) as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        return [types.TextContent(type=\"text\", text=response.text)]\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-website-fetcher\")\n\n    @app.call_tool()\n    async def fetch_tool(\n        name: str, arguments: dict\n    ) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n        if name != \"fetch\":\n            raise ValueError(f\"Unknown tool: {name}\")\n        if \"url\" not in arguments:\n            raise ValueError(\"Missing required argument 'url'\")\n        return await fetch_website(arguments[\"url\"])\n\n    @app.list_tools()\n    async def list_tools() -> list[types.Tool]:\n        return [\n            types.Tool(\n                name=\"fetch\",\n                description=\"Fetches a website and returns its content\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"required\": [\"url\"],\n                    \"properties\": {\n                        \"url\": {\n                            \"type\": \"string\",\n                            \"description\": \"URL to fetch\",\n                        }\n                    },\n                },\n            )\n        ]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: 20 # src/mcp/__init__.py ---\nfrom .client.session import ClientSession\nfrom .client.stdio import StdioServerParameters, stdio_client\nfrom .server.session import ServerSession\nfrom .server.stdio import stdio_server\nfrom .shared.exceptions import McpError\nfrom .types import (\n    CallToolRequest,\n    ClientCapabilities,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    CompleteRequest,\n    CreateMessageRequest,\n    CreateMessageResult,\n    ErrorData,\n    GetPromptRequest,\n    GetPromptResult,\n    Implementation,\n    IncludeContext,\n    InitializedNotification,\n    InitializeRequest,\n    InitializeResult,\n    JSONRPCError,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    ListPromptsRequest,\n    ListPromptsResult,\n    ListResourcesRequest,\n    ListResourcesResult,\n    ListToolsResult,\n    LoggingLevel,\n    LoggingMessageNotification,\n    Notification,\n    PingRequest,\n    ProgressNotification,\n    PromptsCapability,\n    ReadResourceRequest,\n    ReadResourceResult,\n    Resource,\n    ResourcesCapability,\n    ResourceUpdatedNotification,\n    RootsCapability,\n    SamplingMessage,\n    ServerCapabilities,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n    SetLevelRequest,\n    StopReason,\n    SubscribeRequest,\n    Tool,\n    ToolsCapability,\n    UnsubscribeRequest,\n)\nfrom .types import (\n    Role as SamplingRole,\n)\n\n__all__ = [\n    \"CallToolRequest\",\n    \"ClientCapabilities\",\n    \"ClientNotification\",\n    \"ClientRequest\",\n    \"ClientResult\",\n    \"ClientSession\",\n    \"CreateMessageRequest\",\n    \"CreateMessageResult\",\n    \"ErrorData\",\n    \"GetPromptRequest\",\n    \"GetPromptResult\",\n    \"Implementation\",\n    \"IncludeContext\",\n    \"InitializeRequest\",\n    \"InitializeResult\",\n    \"InitializedNotification\",\n    \"JSONRPCError\",\n    \"JSONRPCRequest\",\n    \"ListPromptsRequest\",\n    \"ListPromptsResult\",\n    \"ListResourcesRequest\",\n    \"ListResourcesResult\",\n    \"ListToolsResult\",\n    \"LoggingLevel\",\n    \"LoggingMessageNotification\",\n    \"McpError\",\n    \"Notification\",\n    \"PingRequest\",\n    \"ProgressNotification\",\n    \"PromptsCapability\",\n    \"ReadResourceRequest\",\n    \"ReadResourceResult\",\n    \"ResourcesCapability\",\n    \"ResourceUpdatedNotification\",\n    \"Resource\",\n    \"RootsCapability\",\n    \"SamplingMessage\",\n    \"SamplingRole\",\n    \"ServerCapabilities\",\n    \"ServerNotification\",\n    \"ServerRequest\",\n    \"ServerResult\",\n    \"ServerSession\",\n    \"SetLevelRequest\",\n    \"StdioServerParameters\",\n    \"StopReason\",\n    \"SubscribeRequest\",\n    \"Tool\",\n    \"ToolsCapability\",\n    \"UnsubscribeRequest\",\n    \"stdio_client\",\n    \"stdio_server\",\n    \"CompleteRequest\",\n    \"JSONRPCResponse\",\n]\n\n\n--- File: 25 # src/mcp/client/__main__.py ---\nimport argparse\nimport logging\nimport sys\nfrom functools import partial\nfrom urllib.parse import urlparse\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import ClientSession\nfrom mcp.client.sse import sse_client\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\nfrom mcp.shared.session import RequestResponder\nfrom mcp.types import JSONRPCMessage\n\nif not sys.warnoptions:\n    import warnings\n\n    warnings.simplefilter(\"ignore\")\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"client\")\n\n\nasync def message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    if isinstance(message, Exception):\n        logger.error(\"Error: %s\", message)\n        return\n\n    logger.info(\"Received message from server: %s\", message)\n\n\nasync def run_session(\n    read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    write_stream: MemoryObjectSendStream[JSONRPCMessage],\n    client_info: types.Implementation | None = None,\n):\n    async with ClientSession(\n        read_stream,\n        write_stream,\n        message_handler=message_handler,\n        client_info=client_info,\n    ) as session:\n        logger.info(\"Initializing session\")\n        await session.initialize()\n        logger.info(\"Initialized\")\n\n\nasync def main(command_or_url: str, args: list[str], env: list[tuple[str, str]]):\n    env_dict = dict(env)\n\n    if urlparse(command_or_url).scheme in (\"http\", \"https\"):\n        # Use SSE client for HTTP(S) URLs\n        async with sse_client(command_or_url) as streams:\n            await run_session(*streams)\n    else:\n        # Use stdio client for commands\n        server_parameters = StdioServerParameters(\n            command=command_or_url, args=args, env=env_dict\n        )\n        async with stdio_client(server_parameters) as streams:\n            await run_session(*streams)\n\n\ndef cli():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command_or_url\", help=\"Command or URL to connect to\")\n    parser.add_argument(\"args\", nargs=\"*\", help=\"Additional arguments\")\n    parser.add_argument(\n        \"-e\",\n        \"--env\",\n        nargs=2,\n        action=\"append\",\n        metavar=(\"KEY\", \"VALUE\"),\n        help=\"Environment variables to set. Can be used multiple times.\",\n        default=[],\n    )\n\n    args = parser.parse_args()\n    anyio.run(partial(main, args.command_or_url, args.args, args.env), backend=\"trio\")\n\n\nif __name__ == \"__main__\":\n    cli()\n\n\n--- File: 26 # src/mcp/client/session.py ---\nfrom datetime import timedelta\nfrom typing import Any, Protocol\n\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl, TypeAdapter\n\nimport mcp.types as types\nfrom mcp.shared.context import RequestContext\nfrom mcp.shared.session import BaseSession, RequestResponder\nfrom mcp.shared.version import SUPPORTED_PROTOCOL_VERSIONS\n\nDEFAULT_CLIENT_INFO = types.Implementation(name=\"mcp\", version=\"0.1.0\")\n\n\nclass SamplingFnT(Protocol):\n    async def __call__(\n        self,\n        context: RequestContext[\"ClientSession\", Any],\n        params: types.CreateMessageRequestParams,\n    ) -> types.CreateMessageResult | types.ErrorData: ...\n\n\nclass ListRootsFnT(Protocol):\n    async def __call__(\n        self, context: RequestContext[\"ClientSession\", Any]\n    ) -> types.ListRootsResult | types.ErrorData: ...\n\n\nclass LoggingFnT(Protocol):\n    async def __call__(\n        self,\n        params: types.LoggingMessageNotificationParams,\n    ) -> None: ...\n\n\nclass MessageHandlerFnT(Protocol):\n    async def __call__(\n        self,\n        message: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None: ...\n\n\nasync def _default_message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    await anyio.lowlevel.checkpoint()\n\n\nasync def _default_sampling_callback(\n    context: RequestContext[\"ClientSession\", Any],\n    params: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"Sampling not supported\",\n    )\n\n\nasync def _default_list_roots_callback(\n    context: RequestContext[\"ClientSession\", Any],\n) -> types.ListRootsResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"List roots not supported\",\n    )\n\n\nasync def _default_logging_callback(\n    params: types.LoggingMessageNotificationParams,\n) -> None:\n    pass\n\n\nClientResponse: TypeAdapter[types.ClientResult | types.ErrorData] = TypeAdapter(\n    types.ClientResult | types.ErrorData\n)\n\n\nclass ClientSession(\n    BaseSession[\n        types.ClientRequest,\n        types.ClientNotification,\n        types.ClientResult,\n        types.ServerRequest,\n        types.ServerNotification,\n    ]\n):\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        read_timeout_seconds: timedelta | None = None,\n        sampling_callback: SamplingFnT | None = None,\n        list_roots_callback: ListRootsFnT | None = None,\n        logging_callback: LoggingFnT | None = None,\n        message_handler: MessageHandlerFnT | None = None,\n        client_info: types.Implementation | None = None,\n    ) -> None:\n        super().__init__(\n            read_stream,\n            write_stream,\n            types.ServerRequest,\n            types.ServerNotification,\n            read_timeout_seconds=read_timeout_seconds,\n        )\n        self._client_info = client_info or DEFAULT_CLIENT_INFO\n        self._sampling_callback = sampling_callback or _default_sampling_callback\n        self._list_roots_callback = list_roots_callback or _default_list_roots_callback\n        self._logging_callback = logging_callback or _default_logging_callback\n        self._message_handler = message_handler or _default_message_handler\n\n    async def initialize(self) -> types.InitializeResult:\n        sampling = types.SamplingCapability()\n        roots = types.RootsCapability(\n            # TODO: Should this be based on whether we\n            # _will_ send notifications, or only whether\n            # they're supported?\n            listChanged=True,\n        )\n\n        result = await self.send_request(\n            types.ClientRequest(\n                types.InitializeRequest(\n                    method=\"initialize\",\n                    params=types.InitializeRequestParams(\n                        protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                        capabilities=types.ClientCapabilities(\n                            sampling=sampling,\n                            experimental=None,\n                            roots=roots,\n                        ),\n                        clientInfo=self._client_info,\n                    ),\n                )\n            ),\n            types.InitializeResult,\n        )\n\n        if result.protocolVersion not in SUPPORTED_PROTOCOL_VERSIONS:\n            raise RuntimeError(\n                \"Unsupported protocol version from the server: \"\n                f\"{result.protocolVersion}\"\n            )\n\n        await self.send_notification(\n            types.ClientNotification(\n                types.InitializedNotification(method=\"notifications/initialized\")\n            )\n        )\n\n        return result\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                ),\n            )\n        )\n\n    async def set_logging_level(self, level: types.LoggingLevel) -> types.EmptyResult:\n        \"\"\"Send a logging/setLevel request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SetLevelRequest(\n                    method=\"logging/setLevel\",\n                    params=types.SetLevelRequestParams(level=level),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def list_resources(self) -> types.ListResourcesResult:\n        \"\"\"Send a resources/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourcesRequest(\n                    method=\"resources/list\",\n                )\n            ),\n            types.ListResourcesResult,\n        )\n\n    async def list_resource_templates(self) -> types.ListResourceTemplatesResult:\n        \"\"\"Send a resources/templates/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourceTemplatesRequest(\n                    method=\"resources/templates/list\",\n                )\n            ),\n            types.ListResourceTemplatesResult,\n        )\n\n    async def read_resource(self, uri: AnyUrl) -> types.ReadResourceResult:\n        \"\"\"Send a resources/read request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ReadResourceRequest(\n                    method=\"resources/read\",\n                    params=types.ReadResourceRequestParams(uri=uri),\n                )\n            ),\n            types.ReadResourceResult,\n        )\n\n    async def subscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/subscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SubscribeRequest(\n                    method=\"resources/subscribe\",\n                    params=types.SubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def unsubscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/unsubscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.UnsubscribeRequest(\n                    method=\"resources/unsubscribe\",\n                    params=types.UnsubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CallToolRequest(\n                    method=\"tools/call\",\n                    params=types.CallToolRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.CallToolResult,\n        )\n\n    async def list_prompts(self) -> types.ListPromptsResult:\n        \"\"\"Send a prompts/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListPromptsRequest(\n                    method=\"prompts/list\",\n                )\n            ),\n            types.ListPromptsResult,\n        )\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.GetPromptRequest(\n                    method=\"prompts/get\",\n                    params=types.GetPromptRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.GetPromptResult,\n        )\n\n    async def complete(\n        self,\n        ref: types.ResourceReference | types.PromptReference,\n        argument: dict[str, str],\n    ) -> types.CompleteResult:\n        \"\"\"Send a completion/complete request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CompleteRequest(\n                    method=\"completion/complete\",\n                    params=types.CompleteRequestParams(\n                        ref=ref,\n                        argument=types.CompletionArgument(**argument),\n                    ),\n                )\n            ),\n            types.CompleteResult,\n        )\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListToolsRequest(\n                    method=\"tools/list\",\n                )\n            ),\n            types.ListToolsResult,\n        )\n\n    async def send_roots_list_changed(self) -> None:\n        \"\"\"Send a roots/list_changed notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.RootsListChangedNotification(\n                    method=\"notifications/roots/list_changed\",\n                )\n            )\n        )\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ServerRequest, types.ClientResult]\n    ) -> None:\n        ctx = RequestContext[ClientSession, Any](\n            request_id=responder.request_id,\n            meta=responder.request_meta,\n            session=self,\n            lifespan_context=None,\n        )\n\n        match responder.request.root:\n            case types.CreateMessageRequest(params=params):\n                with responder:\n                    response = await self._sampling_callback(ctx, params)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.ListRootsRequest():\n                with responder:\n                    response = await self._list_roots_callback(ctx)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.PingRequest():\n                with responder:\n                    return await responder.respond(\n                        types.ClientResult(root=types.EmptyResult())\n                    )\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None:\n        \"\"\"Handle incoming messages by forwarding to the message handler.\"\"\"\n        await self._message_handler(req)\n\n    async def _received_notification(\n        self, notification: types.ServerNotification\n    ) -> None:\n        \"\"\"Handle notifications from the server.\"\"\"\n        # Process specific notification types\n        match notification.root:\n            case types.LoggingMessageNotification(params=params):\n                await self._logging_callback(params)\n            case _:\n                pass\n\n\n--- File: 27 # src/mcp/client/sse.py ---\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import urljoin, urlparse\n\nimport anyio\nimport httpx\nfrom anyio.abc import TaskStatus\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom httpx_sse import aconnect_sse\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\ndef remove_request_params(url: str) -> str:\n    return urljoin(url, urlparse(url).path)\n\n\n@asynccontextmanager\nasync def sse_client(\n    url: str,\n    headers: dict[str, Any] | None = None,\n    timeout: float = 5,\n    sse_read_timeout: float = 60 * 5,\n):\n    \"\"\"\n    Client transport for SSE.\n\n    `sse_read_timeout` determines how long (in seconds) the client will wait for a new\n    event before disconnecting. All other HTTP operations are controlled by `timeout`.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async with anyio.create_task_group() as tg:\n        try:\n            logger.info(f\"Connecting to SSE endpoint: {remove_request_params(url)}\")\n            async with httpx.AsyncClient(headers=headers) as client:\n                async with aconnect_sse(\n                    client,\n                    \"GET\",\n                    url,\n                    timeout=httpx.Timeout(timeout, read=sse_read_timeout),\n                ) as event_source:\n                    event_source.response.raise_for_status()\n                    logger.debug(\"SSE connection established\")\n\n                    async def sse_reader(\n                        task_status: TaskStatus[str] = anyio.TASK_STATUS_IGNORED,\n                    ):\n                        try:\n                            async for sse in event_source.aiter_sse():\n                                logger.debug(f\"Received SSE event: {sse.event}\")\n                                match sse.event:\n                                    case \"endpoint\":\n                                        endpoint_url = urljoin(url, sse.data)\n                                        logger.info(\n                                            f\"Received endpoint URL: {endpoint_url}\"\n                                        )\n\n                                        url_parsed = urlparse(url)\n                                        endpoint_parsed = urlparse(endpoint_url)\n                                        if (\n                                            url_parsed.netloc != endpoint_parsed.netloc\n                                            or url_parsed.scheme\n                                            != endpoint_parsed.scheme\n                                        ):\n                                            error_msg = (\n                                                \"Endpoint origin does not match \"\n                                                f\"connection origin: {endpoint_url}\"\n                                            )\n                                            logger.error(error_msg)\n                                            raise ValueError(error_msg)\n\n                                        task_status.started(endpoint_url)\n\n                                    case \"message\":\n                                        try:\n                                            message = types.JSONRPCMessage.model_validate_json(  # noqa: E501\n                                                sse.data\n                                            )\n                                            logger.debug(\n                                                f\"Received server message: {message}\"\n                                            )\n                                        except Exception as exc:\n                                            logger.error(\n                                                f\"Error parsing server message: {exc}\"\n                                            )\n                                            await read_stream_writer.send(exc)\n                                            continue\n\n                                        await read_stream_writer.send(message)\n                                    case _:\n                                        logger.warning(\n                                            f\"Unknown SSE event: {sse.event}\"\n                                        )\n                        except Exception as exc:\n                            logger.error(f\"Error in sse_reader: {exc}\")\n                            await read_stream_writer.send(exc)\n                        finally:\n                            await read_stream_writer.aclose()\n\n                    async def post_writer(endpoint_url: str):\n                        try:\n                            async with write_stream_reader:\n                                async for message in write_stream_reader:\n                                    logger.debug(f\"Sending client message: {message}\")\n                                    response = await client.post(\n                                        endpoint_url,\n                                        json=message.model_dump(\n                                            by_alias=True,\n                                            mode=\"json\",\n                                            exclude_none=True,\n                                        ),\n                                    )\n                                    response.raise_for_status()\n                                    logger.debug(\n                                        \"Client message sent successfully: \"\n                                        f\"{response.status_code}\"\n                                    )\n                        except Exception as exc:\n                            logger.error(f\"Error in post_writer: {exc}\")\n                        finally:\n                            await write_stream.aclose()\n\n                    endpoint_url = await tg.start(sse_reader)\n                    logger.info(\n                        f\"Starting post writer with endpoint URL: {endpoint_url}\"\n                    )\n                    tg.start_soon(post_writer, endpoint_url)\n\n                    try:\n                        yield read_stream, write_stream\n                    finally:\n                        tg.cancel_scope.cancel()\n        finally:\n            await read_stream_writer.aclose()\n            await write_stream.aclose()\n\n\n--- File: 28 # src/mcp/client/stdio/__init__.py ---\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\nfrom typing import Literal, TextIO\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom anyio.streams.text import TextReceiveStream\nfrom pydantic import BaseModel, Field\n\nimport mcp.types as types\n\nfrom .win32 import (\n    create_windows_process,\n    get_windows_executable_command,\n    terminate_windows_process,\n)\n\n# Environment variables to inherit by default\nDEFAULT_INHERITED_ENV_VARS = (\n    [\n        \"APPDATA\",\n        \"HOMEDRIVE\",\n        \"HOMEPATH\",\n        \"LOCALAPPDATA\",\n        \"PATH\",\n        \"PROCESSOR_ARCHITECTURE\",\n        \"SYSTEMDRIVE\",\n        \"SYSTEMROOT\",\n        \"TEMP\",\n        \"USERNAME\",\n        \"USERPROFILE\",\n    ]\n    if sys.platform == \"win32\"\n    else [\"HOME\", \"LOGNAME\", \"PATH\", \"SHELL\", \"TERM\", \"USER\"]\n)\n\n\ndef get_default_environment() -> dict[str, str]:\n    \"\"\"\n    Returns a default environment object including only environment variables deemed\n    safe to inherit.\n    \"\"\"\n    env: dict[str, str] = {}\n\n    for key in DEFAULT_INHERITED_ENV_VARS:\n        value = os.environ.get(key)\n        if value is None:\n            continue\n\n        if value.startswith(\"()\"):\n            # Skip functions, which are a security risk\n            continue\n\n        env[key] = value\n\n    return env\n\n\nclass StdioServerParameters(BaseModel):\n    command: str\n    \"\"\"The executable to run to start the server.\"\"\"\n\n    args: list[str] = Field(default_factory=list)\n    \"\"\"Command line arguments to pass to the executable.\"\"\"\n\n    env: dict[str, str] | None = None\n    \"\"\"\n    The environment to use when spawning the process.\n\n    If not specified, the result of get_default_environment() will be used.\n    \"\"\"\n\n    cwd: str | Path | None = None\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    encoding: str = \"utf-8\"\n    \"\"\"\n    The text encoding used when sending/receiving messages to the server\n\n    defaults to utf-8\n    \"\"\"\n\n    encoding_error_handler: Literal[\"strict\", \"ignore\", \"replace\"] = \"strict\"\n    \"\"\"\n    The text encoding error handler.\n\n    See https://docs.python.org/3/library/codecs.html#codec-base-classes for\n    explanations of possible values\n    \"\"\"\n\n\n@asynccontextmanager\nasync def stdio_client(server: StdioServerParameters, errlog: TextIO = sys.stderr):\n    \"\"\"\n    Client transport for stdio: this will connect to a server by spawning a\n    process and communicating with it over stdin/stdout.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    command = _get_executable_command(server.command)\n\n    # Open process with stderr piped for capture\n    process = await _create_platform_compatible_process(\n        command=command,\n        args=server.args,\n        env=(\n            {**get_default_environment(), **server.env}\n            if server.env is not None\n            else get_default_environment()\n        ),\n        errlog=errlog,\n        cwd=server.cwd,\n    )\n\n    async def stdout_reader():\n        assert process.stdout, \"Opened process is missing stdout\"\n\n        try:\n            async with read_stream_writer:\n                buffer = \"\"\n                async for chunk in TextReceiveStream(\n                    process.stdout,\n                    encoding=server.encoding,\n                    errors=server.encoding_error_handler,\n                ):\n                    lines = (buffer + chunk).split(\"\\n\")\n                    buffer = lines.pop()\n\n                    for line in lines:\n                        try:\n                            message = types.JSONRPCMessage.model_validate_json(line)\n                        except Exception as exc:\n                            await read_stream_writer.send(exc)\n                            continue\n\n                        await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdin_writer():\n        assert process.stdin, \"Opened process is missing stdin\"\n\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await process.stdin.send(\n                        (json + \"\\n\").encode(\n                            encoding=server.encoding,\n                            errors=server.encoding_error_handler,\n                        )\n                    )\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with (\n        anyio.create_task_group() as tg,\n        process,\n    ):\n        tg.start_soon(stdout_reader)\n        tg.start_soon(stdin_writer)\n        try:\n            yield read_stream, write_stream\n        finally:\n            # Clean up process to prevent any dangling orphaned processes\n            if sys.platform == \"win32\":\n                await terminate_windows_process(process)\n            else:\n                process.terminate()\n\n\ndef _get_executable_command(command: str) -> str:\n    \"\"\"\n    Get the correct executable command normalized for the current platform.\n\n    Args:\n        command: Base command (e.g., 'uvx', 'npx')\n\n    Returns:\n        str: Platform-appropriate command\n    \"\"\"\n    if sys.platform == \"win32\":\n        return get_windows_executable_command(command)\n    else:\n        return command\n\n\nasync def _create_platform_compatible_process(\n    command: str,\n    args: list[str],\n    env: dict[str, str] | None = None,\n    errlog: TextIO = sys.stderr,\n    cwd: Path | str | None = None,\n):\n    \"\"\"\n    Creates a subprocess in a platform-compatible way.\n    Returns a process handle.\n    \"\"\"\n    if sys.platform == \"win32\":\n        process = await create_windows_process(command, args, env, errlog, cwd)\n    else:\n        process = await anyio.open_process(\n            [command, *args], env=env, stderr=errlog, cwd=cwd\n        )\n\n    return process\n\n\n--- File: 30 # src/mcp/client/websocket.py ---\nimport json\nimport logging\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom websockets.asyncio.client import connect as ws_connect\nfrom websockets.typing import Subprotocol\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_client(\n    url: str,\n) -> AsyncGenerator[\n    tuple[\n        MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        MemoryObjectSendStream[types.JSONRPCMessage],\n    ],\n    None,\n]:\n    \"\"\"\n    WebSocket client transport for MCP, symmetrical to the server version.\n\n    Connects to 'url' using the 'mcp' subprotocol, then yields:\n        (read_stream, write_stream)\n\n    - read_stream: As you read from this stream, you'll receive either valid\n      JSONRPCMessage objects or Exception objects (when validation fails).\n    - write_stream: Write JSONRPCMessage objects to this stream to send them\n      over the WebSocket to the server.\n    \"\"\"\n\n    # Create two in-memory streams:\n    # - One for incoming messages (read_stream, written by ws_reader)\n    # - One for outgoing messages (write_stream, read by ws_writer)\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    # Connect using websockets, requesting the \"mcp\" subprotocol\n    async with ws_connect(url, subprotocols=[Subprotocol(\"mcp\")]) as ws:\n\n        async def ws_reader():\n            \"\"\"\n            Reads text messages from the WebSocket, parses them as JSON-RPC messages,\n            and sends them into read_stream_writer.\n            \"\"\"\n            async with read_stream_writer:\n                async for raw_text in ws:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(raw_text)\n                        await read_stream_writer.send(message)\n                    except ValidationError as exc:\n                        # If JSON parse or model validation fails, send the exception\n                        await read_stream_writer.send(exc)\n\n        async def ws_writer():\n            \"\"\"\n            Reads JSON-RPC messages from write_stream_reader and\n            sends them to the server.\n            \"\"\"\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    # Convert to a dict, then to JSON\n                    msg_dict = message.model_dump(\n                        by_alias=True, mode=\"json\", exclude_none=True\n                    )\n                    await ws.send(json.dumps(msg_dict))\n\n        async with anyio.create_task_group() as tg:\n            # Start reader and writer tasks\n            tg.start_soon(ws_reader)\n            tg.start_soon(ws_writer)\n\n            # Yield the receive/send streams\n            yield (read_stream, write_stream)\n\n            # Once the caller's 'async with' block exits, we shut down\n            tg.cancel_scope.cancel()\n\n\n--- File: 33 # src/mcp/server/fastmcp/__init__.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom importlib.metadata import version\n\nfrom .server import Context, FastMCP\nfrom .utilities.types import Image\n\n__version__ = version(\"mcp\")\n__all__ = [\"FastMCP\", \"Context\", \"Image\"]\n\n\n--- File: 35 # src/mcp/server/fastmcp/prompts/__init__.py ---\nfrom .base import Prompt\nfrom .manager import PromptManager\n\n__all__ = [\"Prompt\", \"PromptManager\"]\n\n\n--- File: 36 # src/mcp/server/fastmcp/prompts/base.py ---\n\"\"\"Base classes for FastMCP prompts.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom typing import Any, Literal\n\nimport pydantic_core\nfrom pydantic import BaseModel, Field, TypeAdapter, validate_call\n\nfrom mcp.types import EmbeddedResource, ImageContent, TextContent\n\nCONTENT_TYPES = TextContent | ImageContent | EmbeddedResource\n\n\nclass Message(BaseModel):\n    \"\"\"Base class for all prompt messages.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"]\n    content: CONTENT_TYPES\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        if isinstance(content, str):\n            content = TextContent(type=\"text\", text=content)\n        super().__init__(content=content, **kwargs)\n\n\nclass UserMessage(Message):\n    \"\"\"A message from the user.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"user\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nclass AssistantMessage(Message):\n    \"\"\"A message from the assistant.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"assistant\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nmessage_validator = TypeAdapter[UserMessage | AssistantMessage](\n    UserMessage | AssistantMessage\n)\n\nSyncPromptResult = (\n    str | Message | dict[str, Any] | Sequence[str | Message | dict[str, Any]]\n)\nPromptResult = SyncPromptResult | Awaitable[SyncPromptResult]\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument that can be passed to a prompt.\"\"\"\n\n    name: str = Field(description=\"Name of the argument\")\n    description: str | None = Field(\n        None, description=\"Description of what the argument does\"\n    )\n    required: bool = Field(\n        default=False, description=\"Whether the argument is required\"\n    )\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt template that can be rendered with parameters.\"\"\"\n\n    name: str = Field(description=\"Name of the prompt\")\n    description: str | None = Field(\n        None, description=\"Description of what the prompt does\"\n    )\n    arguments: list[PromptArgument] | None = Field(\n        None, description=\"Arguments that can be passed to the prompt\"\n    )\n    fn: Callable[..., PromptResult | Awaitable[PromptResult]] = Field(exclude=True)\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., PromptResult | Awaitable[PromptResult]],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> \"Prompt\":\n        \"\"\"Create a Prompt from a function.\n\n        The function can return:\n        - A string (converted to a message)\n        - A Message object\n        - A dict (converted to a message)\n        - A sequence of any of the above\n        \"\"\"\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        # Get schema from TypeAdapter - will fail if function isn't properly typed\n        parameters = TypeAdapter(fn).json_schema()\n\n        # Convert parameters to PromptArguments\n        arguments: list[PromptArgument] = []\n        if \"properties\" in parameters:\n            for param_name, param in parameters[\"properties\"].items():\n                required = param_name in parameters.get(\"required\", [])\n                arguments.append(\n                    PromptArgument(\n                        name=param_name,\n                        description=param.get(\"description\"),\n                        required=required,\n                    )\n                )\n\n        # ensure the arguments are properly cast\n        fn = validate_call(fn)\n\n        return cls(\n            name=func_name,\n            description=description or fn.__doc__ or \"\",\n            arguments=arguments,\n            fn=fn,\n        )\n\n    async def render(self, arguments: dict[str, Any] | None = None) -> list[Message]:\n        \"\"\"Render the prompt with arguments.\"\"\"\n        # Validate required arguments\n        if self.arguments:\n            required = {arg.name for arg in self.arguments if arg.required}\n            provided = set(arguments or {})\n            missing = required - provided\n            if missing:\n                raise ValueError(f\"Missing required arguments: {missing}\")\n\n        try:\n            # Call function and check if result is a coroutine\n            result = self.fn(**(arguments or {}))\n            if inspect.iscoroutine(result):\n                result = await result\n\n            # Validate messages\n            if not isinstance(result, list | tuple):\n                result = [result]\n\n            # Convert result to messages\n            messages: list[Message] = []\n            for msg in result:  # type: ignore[reportUnknownVariableType]\n                try:\n                    if isinstance(msg, Message):\n                        messages.append(msg)\n                    elif isinstance(msg, dict):\n                        messages.append(message_validator.validate_python(msg))\n                    elif isinstance(msg, str):\n                        content = TextContent(type=\"text\", text=msg)\n                        messages.append(UserMessage(content=content))\n                    else:\n                        content = json.dumps(pydantic_core.to_jsonable_python(msg))\n                        messages.append(Message(role=\"user\", content=content))\n                except Exception:\n                    raise ValueError(\n                        f\"Could not convert prompt result to message: {msg}\"\n                    )\n\n            return messages\n        except Exception as e:\n            raise ValueError(f\"Error rendering prompt {self.name}: {e}\")\n\n\n--- File: 37 # src/mcp/server/fastmcp/prompts/manager.py ---\n\"\"\"Prompt management functionality.\"\"\"\n\nfrom typing import Any\n\nfrom mcp.server.fastmcp.prompts.base import Message, Prompt\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass PromptManager:\n    \"\"\"Manages FastMCP prompts.\"\"\"\n\n    def __init__(self, warn_on_duplicate_prompts: bool = True):\n        self._prompts: dict[str, Prompt] = {}\n        self.warn_on_duplicate_prompts = warn_on_duplicate_prompts\n\n    def get_prompt(self, name: str) -> Prompt | None:\n        \"\"\"Get prompt by name.\"\"\"\n        return self._prompts.get(name)\n\n    def list_prompts(self) -> list[Prompt]:\n        \"\"\"List all registered prompts.\"\"\"\n        return list(self._prompts.values())\n\n    def add_prompt(\n        self,\n        prompt: Prompt,\n    ) -> Prompt:\n        \"\"\"Add a prompt to the manager.\"\"\"\n\n        # Check for duplicates\n        existing = self._prompts.get(prompt.name)\n        if existing:\n            if self.warn_on_duplicate_prompts:\n                logger.warning(f\"Prompt already exists: {prompt.name}\")\n            return existing\n\n        self._prompts[prompt.name] = prompt\n        return prompt\n\n    async def render_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> list[Message]:\n        \"\"\"Render a prompt by name with arguments.\"\"\"\n        prompt = self.get_prompt(name)\n        if not prompt:\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        return await prompt.render(arguments)\n\n\n--- File: 39 # src/mcp/server/fastmcp/resources/__init__.py ---\nfrom .base import Resource\nfrom .resource_manager import ResourceManager\nfrom .templates import ResourceTemplate\nfrom .types import (\n    BinaryResource,\n    DirectoryResource,\n    FileResource,\n    FunctionResource,\n    HttpResource,\n    TextResource,\n)\n\n__all__ = [\n    \"Resource\",\n    \"TextResource\",\n    \"BinaryResource\",\n    \"FunctionResource\",\n    \"FileResource\",\n    \"HttpResource\",\n    \"DirectoryResource\",\n    \"ResourceTemplate\",\n    \"ResourceManager\",\n]\n\n\n--- File: 40 # src/mcp/server/fastmcp/resources/base.py ---\n\"\"\"Base classes and interfaces for FastMCP resources.\"\"\"\n\nimport abc\nfrom typing import Annotated\n\nfrom pydantic import (\n    AnyUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    UrlConstraints,\n    ValidationInfo,\n    field_validator,\n)\n\n\nclass Resource(BaseModel, abc.ABC):\n    \"\"\"Base class for all resources.\"\"\"\n\n    model_config = ConfigDict(validate_default=True)\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)] = Field(\n        default=..., description=\"URI of the resource\"\n    )\n    name: str | None = Field(description=\"Name of the resource\", default=None)\n    description: str | None = Field(\n        description=\"Description of the resource\", default=None\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n        pattern=r\"^[a-zA-Z0-9]+/[a-zA-Z0-9\\-+.]+$\",\n    )\n\n    @field_validator(\"name\", mode=\"before\")\n    @classmethod\n    def set_default_name(cls, name: str | None, info: ValidationInfo) -> str:\n        \"\"\"Set default name from URI if not provided.\"\"\"\n        if name:\n            return name\n        if uri := info.data.get(\"uri\"):\n            return str(uri)\n        raise ValueError(\"Either name or uri must be provided\")\n\n    @abc.abstractmethod\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource content.\"\"\"\n        pass\n\n\n--- File: 41 # src/mcp/server/fastmcp/resources/resource_manager.py ---\n\"\"\"Resource manager functionality.\"\"\"\n\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom pydantic import AnyUrl\n\nfrom mcp.server.fastmcp.resources.base import Resource\nfrom mcp.server.fastmcp.resources.templates import ResourceTemplate\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass ResourceManager:\n    \"\"\"Manages FastMCP resources.\"\"\"\n\n    def __init__(self, warn_on_duplicate_resources: bool = True):\n        self._resources: dict[str, Resource] = {}\n        self._templates: dict[str, ResourceTemplate] = {}\n        self.warn_on_duplicate_resources = warn_on_duplicate_resources\n\n    def add_resource(self, resource: Resource) -> Resource:\n        \"\"\"Add a resource to the manager.\n\n        Args:\n            resource: A Resource instance to add\n\n        Returns:\n            The added resource. If a resource with the same URI already exists,\n            returns the existing resource.\n        \"\"\"\n        logger.debug(\n            \"Adding resource\",\n            extra={\n                \"uri\": resource.uri,\n                \"type\": type(resource).__name__,\n                \"resource_name\": resource.name,\n            },\n        )\n        existing = self._resources.get(str(resource.uri))\n        if existing:\n            if self.warn_on_duplicate_resources:\n                logger.warning(f\"Resource already exists: {resource.uri}\")\n            return existing\n        self._resources[str(resource.uri)] = resource\n        return resource\n\n    def add_template(\n        self,\n        fn: Callable[..., Any],\n        uri_template: str,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> ResourceTemplate:\n        \"\"\"Add a template from a function.\"\"\"\n        template = ResourceTemplate.from_function(\n            fn,\n            uri_template=uri_template,\n            name=name,\n            description=description,\n            mime_type=mime_type,\n        )\n        self._templates[template.uri_template] = template\n        return template\n\n    async def get_resource(self, uri: AnyUrl | str) -> Resource | None:\n        \"\"\"Get resource by URI, checking concrete resources first, then templates.\"\"\"\n        uri_str = str(uri)\n        logger.debug(\"Getting resource\", extra={\"uri\": uri_str})\n\n        # First check concrete resources\n        if resource := self._resources.get(uri_str):\n            return resource\n\n        # Then check templates\n        for template in self._templates.values():\n            if params := template.matches(uri_str):\n                try:\n                    return await template.create_resource(uri_str, params)\n                except Exception as e:\n                    raise ValueError(f\"Error creating resource from template: {e}\")\n\n        raise ValueError(f\"Unknown resource: {uri}\")\n\n    def list_resources(self) -> list[Resource]:\n        \"\"\"List all registered resources.\"\"\"\n        logger.debug(\"Listing resources\", extra={\"count\": len(self._resources)})\n        return list(self._resources.values())\n\n    def list_templates(self) -> list[ResourceTemplate]:\n        \"\"\"List all registered templates.\"\"\"\n        logger.debug(\"Listing templates\", extra={\"count\": len(self._templates)})\n        return list(self._templates.values())\n\n\n--- File: 43 # src/mcp/server/fastmcp/resources/types.py ---\n\"\"\"Concrete resource implementations.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\n\nimport anyio\nimport anyio.to_thread\nimport httpx\nimport pydantic.json\nimport pydantic_core\nfrom pydantic import Field, ValidationInfo\n\nfrom mcp.server.fastmcp.resources.base import Resource\n\n\nclass TextResource(Resource):\n    \"\"\"A resource that reads from a string.\"\"\"\n\n    text: str = Field(description=\"Text content of the resource\")\n\n    async def read(self) -> str:\n        \"\"\"Read the text content.\"\"\"\n        return self.text\n\n\nclass BinaryResource(Resource):\n    \"\"\"A resource that reads from bytes.\"\"\"\n\n    data: bytes = Field(description=\"Binary content of the resource\")\n\n    async def read(self) -> bytes:\n        \"\"\"Read the binary content.\"\"\"\n        return self.data\n\n\nclass FunctionResource(Resource):\n    \"\"\"A resource that defers data loading by wrapping a function.\n\n    The function is only called when the resource is read, allowing for lazy loading\n    of potentially expensive data. This is particularly useful when listing resources,\n    as the function won't be called until the resource is actually accessed.\n\n    The function can return:\n    - str for text content (default)\n    - bytes for binary content\n    - other types will be converted to JSON\n    \"\"\"\n\n    fn: Callable[[], Any] = Field(exclude=True)\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource by calling the wrapped function.\"\"\"\n        try:\n            result = (\n                await self.fn() if inspect.iscoroutinefunction(self.fn) else self.fn()\n            )\n            if isinstance(result, Resource):\n                return await result.read()\n            if isinstance(result, bytes):\n                return result\n            if isinstance(result, str):\n                return result\n            try:\n                return json.dumps(pydantic_core.to_jsonable_python(result))\n            except (TypeError, pydantic_core.PydanticSerializationError):\n                # If JSON serialization fails, try str()\n                return str(result)\n        except Exception as e:\n            raise ValueError(f\"Error reading resource {self.uri}: {e}\")\n\n\nclass FileResource(Resource):\n    \"\"\"A resource that reads from a file.\n\n    Set is_binary=True to read file as binary data instead of text.\n    \"\"\"\n\n    path: Path = Field(description=\"Path to the file\")\n    is_binary: bool = Field(\n        default=False,\n        description=\"Whether to read the file as binary data\",\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    @pydantic.field_validator(\"is_binary\")\n    @classmethod\n    def set_binary_from_mime_type(cls, is_binary: bool, info: ValidationInfo) -> bool:\n        \"\"\"Set is_binary based on mime_type if not explicitly set.\"\"\"\n        if is_binary:\n            return True\n        mime_type = info.data.get(\"mime_type\", \"text/plain\")\n        return not mime_type.startswith(\"text/\")\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the file content.\"\"\"\n        try:\n            if self.is_binary:\n                return await anyio.to_thread.run_sync(self.path.read_bytes)\n            return await anyio.to_thread.run_sync(self.path.read_text)\n        except Exception as e:\n            raise ValueError(f\"Error reading file {self.path}: {e}\")\n\n\nclass HttpResource(Resource):\n    \"\"\"A resource that reads from an HTTP endpoint.\"\"\"\n\n    url: str = Field(description=\"URL to fetch content from\")\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the HTTP content.\"\"\"\n        async with httpx.AsyncClient() as client:\n            response = await client.get(self.url)\n            response.raise_for_status()\n            return response.text\n\n\nclass DirectoryResource(Resource):\n    \"\"\"A resource that lists files in a directory.\"\"\"\n\n    path: Path = Field(description=\"Path to the directory\")\n    recursive: bool = Field(\n        default=False, description=\"Whether to list files recursively\"\n    )\n    pattern: str | None = Field(\n        default=None, description=\"Optional glob pattern to filter files\"\n    )\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    def list_files(self) -> list[Path]:\n        \"\"\"List files in the directory.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Directory not found: {self.path}\")\n        if not self.path.is_dir():\n            raise NotADirectoryError(f\"Not a directory: {self.path}\")\n\n        try:\n            if self.pattern:\n                return (\n                    list(self.path.glob(self.pattern))\n                    if not self.recursive\n                    else list(self.path.rglob(self.pattern))\n                )\n            return (\n                list(self.path.glob(\"*\"))\n                if not self.recursive\n                else list(self.path.rglob(\"*\"))\n            )\n        except Exception as e:\n            raise ValueError(f\"Error listing directory {self.path}: {e}\")\n\n    async def read(self) -> str:  # Always returns JSON string\n        \"\"\"Read the directory listing.\"\"\"\n        try:\n            files = await anyio.to_thread.run_sync(self.list_files)\n            file_list = [str(f.relative_to(self.path)) for f in files if f.is_file()]\n            return json.dumps({\"files\": file_list}, indent=2)\n        except Exception as e:\n            raise ValueError(f\"Error reading directory {self.path}: {e}\")\n\n\n--- File: 44 # src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File: 45 # src/mcp/server/fastmcp/tools/__init__.py ---\nfrom .base import Tool\nfrom .tool_manager import ToolManager\n\n__all__ = [\"Tool\", \"ToolManager\"]\n\n\n--- File: 46 # src/mcp/server/fastmcp/tools/base.py ---\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, get_origin\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.utilities.func_metadata import FuncMetadata, func_metadata\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n    from mcp.shared.context import LifespanContextT\n\n\nclass Tool(BaseModel):\n    \"\"\"Internal tool registration info.\"\"\"\n\n    fn: Callable[..., Any] = Field(exclude=True)\n    name: str = Field(description=\"Name of the tool\")\n    description: str = Field(description=\"Description of what the tool does\")\n    parameters: dict[str, Any] = Field(description=\"JSON schema for tool parameters\")\n    fn_metadata: FuncMetadata = Field(\n        description=\"Metadata about the function including a pydantic model for tool\"\n        \" arguments\"\n    )\n    is_async: bool = Field(description=\"Whether the tool is async\")\n    context_kwarg: str | None = Field(\n        None, description=\"Name of the kwarg that should receive context\"\n    )\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n        context_kwarg: str | None = None,\n    ) -> Tool:\n        \"\"\"Create a Tool from a function.\"\"\"\n        from mcp.server.fastmcp import Context\n\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        func_doc = description or fn.__doc__ or \"\"\n        is_async = inspect.iscoroutinefunction(fn)\n\n        if context_kwarg is None:\n            sig = inspect.signature(fn)\n            for param_name, param in sig.parameters.items():\n                if get_origin(param.annotation) is not None:\n                    continue\n                if issubclass(param.annotation, Context):\n                    context_kwarg = param_name\n                    break\n\n        func_arg_metadata = func_metadata(\n            fn,\n            skip_names=[context_kwarg] if context_kwarg is not None else [],\n        )\n        parameters = func_arg_metadata.arg_model.model_json_schema()\n\n        return cls(\n            fn=fn,\n            name=func_name,\n            description=func_doc,\n            parameters=parameters,\n            fn_metadata=func_arg_metadata,\n            is_async=is_async,\n            context_kwarg=context_kwarg,\n        )\n\n    async def run(\n        self,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Run the tool with arguments.\"\"\"\n        try:\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn,\n                self.is_async,\n                arguments,\n                {self.context_kwarg: context}\n                if self.context_kwarg is not None\n                else None,\n            )\n        except Exception as e:\n            raise ToolError(f\"Error executing tool {self.name}: {e}\") from e\n\n\n--- File: 47 # src/mcp/server/fastmcp/tools/tool_manager.py ---\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.tools.base import Tool\nfrom mcp.server.fastmcp.utilities.logging import get_logger\nfrom mcp.shared.context import LifespanContextT\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n\nlogger = get_logger(__name__)\n\n\nclass ToolManager:\n    \"\"\"Manages FastMCP tools.\"\"\"\n\n    def __init__(self, warn_on_duplicate_tools: bool = True):\n        self._tools: dict[str, Tool] = {}\n        self.warn_on_duplicate_tools = warn_on_duplicate_tools\n\n    def get_tool(self, name: str) -> Tool | None:\n        \"\"\"Get tool by name.\"\"\"\n        return self._tools.get(name)\n\n    def list_tools(self) -> list[Tool]:\n        \"\"\"List all registered tools.\"\"\"\n        return list(self._tools.values())\n\n    def add_tool(\n        self,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> Tool:\n        \"\"\"Add a tool to the server.\"\"\"\n        tool = Tool.from_function(fn, name=name, description=description)\n        existing = self._tools.get(tool.name)\n        if existing:\n            if self.warn_on_duplicate_tools:\n                logger.warning(f\"Tool already exists: {tool.name}\")\n            return existing\n        self._tools[tool.name] = tool\n        return tool\n\n    async def call_tool(\n        self,\n        name: str,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        tool = self.get_tool(name)\n        if not tool:\n            raise ToolError(f\"Unknown tool: {name}\")\n\n        return await tool.run(arguments, context=context)\n\n\n--- File: 56 # src/mcp/server/session.py ---\n\"\"\"\nServerSession Module\n\nThis module provides the ServerSession class, which manages communication between the\nserver and client in the MCP (Model Context Protocol) framework. It is most commonly\nused in MCP servers to interact with the client.\n\nCommon usage pattern:\n```\n    server = Server(name)\n\n    @server.call_tool()\n    async def handle_tool_call(ctx: RequestContext, arguments: dict[str, Any]) -> Any:\n        # Check client capabilities before proceeding\n        if ctx.session.check_client_capability(\n            types.ClientCapabilities(experimental={\"advanced_tools\": dict()})\n        ):\n            # Perform advanced tool operations\n            result = await perform_advanced_tool_operation(arguments)\n        else:\n            # Fall back to basic tool operations\n            result = await perform_basic_tool_operation(arguments)\n\n        return result\n\n    @server.list_prompts()\n    async def handle_list_prompts(ctx: RequestContext) -> list[types.Prompt]:\n        # Access session for any necessary checks or operations\n        if ctx.session.client_params:\n            # Customize prompts based on client initialization parameters\n            return generate_custom_prompts(ctx.session.client_params)\n        else:\n            return default_prompts\n```\n\nThe ServerSession class is typically used internally by the Server class and should not\nbe instantiated directly by users of the MCP framework.\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl\n\nimport mcp.types as types\nfrom mcp.server.models import InitializationOptions\nfrom mcp.shared.session import (\n    BaseSession,\n    RequestResponder,\n)\n\n\nclass InitializationState(Enum):\n    NotInitialized = 1\n    Initializing = 2\n    Initialized = 3\n\n\nServerSessionT = TypeVar(\"ServerSessionT\", bound=\"ServerSession\")\n\nServerRequestResponder = (\n    RequestResponder[types.ClientRequest, types.ServerResult]\n    | types.ClientNotification\n    | Exception\n)\n\n\nclass ServerSession(\n    BaseSession[\n        types.ServerRequest,\n        types.ServerNotification,\n        types.ServerResult,\n        types.ClientRequest,\n        types.ClientNotification,\n    ]\n):\n    _initialized: InitializationState = InitializationState.NotInitialized\n    _client_params: types.InitializeRequestParams | None = None\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        init_options: InitializationOptions,\n    ) -> None:\n        super().__init__(\n            read_stream, write_stream, types.ClientRequest, types.ClientNotification\n        )\n        self._initialization_state = InitializationState.NotInitialized\n        self._init_options = init_options\n        self._incoming_message_stream_writer, self._incoming_message_stream_reader = (\n            anyio.create_memory_object_stream[ServerRequestResponder](0)\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_reader.aclose()\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_writer.aclose()\n        )\n\n    @property\n    def client_params(self) -> types.InitializeRequestParams | None:\n        return self._client_params\n\n    def check_client_capability(self, capability: types.ClientCapabilities) -> bool:\n        \"\"\"Check if the client supports a specific capability.\"\"\"\n        if self._client_params is None:\n            return False\n\n        # Get client capabilities from initialization params\n        client_caps = self._client_params.capabilities\n\n        # Check each specified capability in the passed in capability object\n        if capability.roots is not None:\n            if client_caps.roots is None:\n                return False\n            if capability.roots.listChanged and not client_caps.roots.listChanged:\n                return False\n\n        if capability.sampling is not None:\n            if client_caps.sampling is None:\n                return False\n\n        if capability.experimental is not None:\n            if client_caps.experimental is None:\n                return False\n            # Check each experimental capability\n            for exp_key, exp_value in capability.experimental.items():\n                if (\n                    exp_key not in client_caps.experimental\n                    or client_caps.experimental[exp_key] != exp_value\n                ):\n                    return False\n\n        return True\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ClientRequest, types.ServerResult]\n    ):\n        match responder.request.root:\n            case types.InitializeRequest(params=params):\n                self._initialization_state = InitializationState.Initializing\n                self._client_params = params\n                with responder:\n                    await responder.respond(\n                        types.ServerResult(\n                            types.InitializeResult(\n                                protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                                capabilities=self._init_options.capabilities,\n                                serverInfo=types.Implementation(\n                                    name=self._init_options.server_name,\n                                    version=self._init_options.server_version,\n                                ),\n                                instructions=self._init_options.instructions,\n                            )\n                        )\n                    )\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received request before initialization was complete\"\n                    )\n\n    async def _received_notification(\n        self, notification: types.ClientNotification\n    ) -> None:\n        # Need this to avoid ASYNC910\n        await anyio.lowlevel.checkpoint()\n        match notification.root:\n            case types.InitializedNotification():\n                self._initialization_state = InitializationState.Initialized\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received notification before initialization was complete\"\n                    )\n\n    async def send_log_message(\n        self, level: types.LoggingLevel, data: Any, logger: str | None = None\n    ) -> None:\n        \"\"\"Send a log message notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.LoggingMessageNotification(\n                    method=\"notifications/message\",\n                    params=types.LoggingMessageNotificationParams(\n                        level=level,\n                        data=data,\n                        logger=logger,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_updated(self, uri: AnyUrl) -> None:\n        \"\"\"Send a resource updated notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceUpdatedNotification(\n                    method=\"notifications/resources/updated\",\n                    params=types.ResourceUpdatedNotificationParams(uri=uri),\n                )\n            )\n        )\n\n    async def create_message(\n        self,\n        messages: list[types.SamplingMessage],\n        *,\n        max_tokens: int,\n        system_prompt: str | None = None,\n        include_context: types.IncludeContext | None = None,\n        temperature: float | None = None,\n        stop_sequences: list[str] | None = None,\n        metadata: dict[str, Any] | None = None,\n        model_preferences: types.ModelPreferences | None = None,\n    ) -> types.CreateMessageResult:\n        \"\"\"Send a sampling/create_message request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.CreateMessageRequest(\n                    method=\"sampling/createMessage\",\n                    params=types.CreateMessageRequestParams(\n                        messages=messages,\n                        systemPrompt=system_prompt,\n                        includeContext=include_context,\n                        temperature=temperature,\n                        maxTokens=max_tokens,\n                        stopSequences=stop_sequences,\n                        metadata=metadata,\n                        modelPreferences=model_preferences,\n                    ),\n                )\n            ),\n            types.CreateMessageResult,\n        )\n\n    async def list_roots(self) -> types.ListRootsResult:\n        \"\"\"Send a roots/list request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.ListRootsRequest(\n                    method=\"roots/list\",\n                )\n            ),\n            types.ListRootsResult,\n        )\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_list_changed(self) -> None:\n        \"\"\"Send a resource list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceListChangedNotification(\n                    method=\"notifications/resources/list_changed\",\n                )\n            )\n        )\n\n    async def send_tool_list_changed(self) -> None:\n        \"\"\"Send a tool list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ToolListChangedNotification(\n                    method=\"notifications/tools/list_changed\",\n                )\n            )\n        )\n\n    async def send_prompt_list_changed(self) -> None:\n        \"\"\"Send a prompt list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.PromptListChangedNotification(\n                    method=\"notifications/prompts/list_changed\",\n                )\n            )\n        )\n\n    async def _handle_incoming(self, req: ServerRequestResponder) -> None:\n        await self._incoming_message_stream_writer.send(req)\n\n    @property\n    def incoming_messages(\n        self,\n    ) -> MemoryObjectReceiveStream[ServerRequestResponder]:\n        return self._incoming_message_stream_reader\n\n\n--- File: 57 # src/mcp/server/sse.py ---\n\"\"\"\nSSE Server Transport Module\n\nThis module implements a Server-Sent Events (SSE) transport layer for MCP servers.\n\nExample usage:\n```\n    # Create an SSE transport at an endpoint\n    sse = SseServerTransport(\"/messages/\")\n\n    # Create Starlette routes for SSE and message handling\n    routes = [\n        Route(\"/sse\", endpoint=handle_sse),\n        Mount(\"/messages/\", app=sse.handle_post_message),\n    ]\n\n    # Define handler functions\n    async def handle_sse(request):\n        async with sse.connect_sse(\n            request.scope, request.receive, request._send\n        ) as streams:\n            await app.run(\n                streams[0], streams[1], app.create_initialization_options()\n            )\n\n    # Create and run Starlette app\n    starlette_app = Starlette(routes=routes)\n    uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n```\n\nSee SseServerTransport class documentation for more details.\n\"\"\"\n\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import quote\nfrom uuid import UUID, uuid4\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom sse_starlette import EventSourceResponse\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.types import Receive, Scope, Send\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\nclass SseServerTransport:\n    \"\"\"\n    SSE server transport for MCP. This class provides _two_ ASGI applications,\n    suitable to be used with a framework like Starlette and a server like Hypercorn:\n\n        1. connect_sse() is an ASGI application which receives incoming GET requests,\n           and sets up a new SSE stream to send server messages to the client.\n        2. handle_post_message() is an ASGI application which receives incoming POST\n           requests, which should contain client messages that link to a\n           previously-established SSE session.\n    \"\"\"\n\n    _endpoint: str\n    _read_stream_writers: dict[\n        UUID, MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    ]\n\n    def __init__(self, endpoint: str) -> None:\n        \"\"\"\n        Creates a new SSE server transport, which will direct the client to POST\n        messages to the relative or absolute URL given.\n        \"\"\"\n\n        super().__init__()\n        self._endpoint = endpoint\n        self._read_stream_writers = {}\n        logger.debug(f\"SseServerTransport initialized with endpoint: {endpoint}\")\n\n    @asynccontextmanager\n    async def connect_sse(self, scope: Scope, receive: Receive, send: Send):\n        if scope[\"type\"] != \"http\":\n            logger.error(\"connect_sse received non-HTTP request\")\n            raise ValueError(\"connect_sse can only handle HTTP requests\")\n\n        logger.debug(\"Setting up SSE connection\")\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n        read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n        write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n        read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n        write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n        session_id = uuid4()\n        session_uri = f\"{quote(self._endpoint)}?session_id={session_id.hex}\"\n        self._read_stream_writers[session_id] = read_stream_writer\n        logger.debug(f\"Created new session with ID: {session_id}\")\n\n        sse_stream_writer, sse_stream_reader = anyio.create_memory_object_stream[\n            dict[str, Any]\n        ](0)\n\n        async def sse_writer():\n            logger.debug(\"Starting SSE writer\")\n            async with sse_stream_writer, write_stream_reader:\n                await sse_stream_writer.send({\"event\": \"endpoint\", \"data\": session_uri})\n                logger.debug(f\"Sent endpoint event: {session_uri}\")\n\n                async for message in write_stream_reader:\n                    logger.debug(f\"Sending message via SSE: {message}\")\n                    await sse_stream_writer.send(\n                        {\n                            \"event\": \"message\",\n                            \"data\": message.model_dump_json(\n                                by_alias=True, exclude_none=True\n                            ),\n                        }\n                    )\n\n        async with anyio.create_task_group() as tg:\n            response = EventSourceResponse(\n                content=sse_stream_reader, data_sender_callable=sse_writer\n            )\n            logger.debug(\"Starting SSE response task\")\n            tg.start_soon(response, scope, receive, send)\n\n            logger.debug(\"Yielding read and write streams\")\n            yield (read_stream, write_stream)\n\n    async def handle_post_message(\n        self, scope: Scope, receive: Receive, send: Send\n    ) -> None:\n        logger.debug(\"Handling POST message\")\n        request = Request(scope, receive)\n\n        session_id_param = request.query_params.get(\"session_id\")\n        if session_id_param is None:\n            logger.warning(\"Received request without session_id\")\n            response = Response(\"session_id is required\", status_code=400)\n            return await response(scope, receive, send)\n\n        try:\n            session_id = UUID(hex=session_id_param)\n            logger.debug(f\"Parsed session ID: {session_id}\")\n        except ValueError:\n            logger.warning(f\"Received invalid session ID: {session_id_param}\")\n            response = Response(\"Invalid session ID\", status_code=400)\n            return await response(scope, receive, send)\n\n        writer = self._read_stream_writers.get(session_id)\n        if not writer:\n            logger.warning(f\"Could not find session for ID: {session_id}\")\n            response = Response(\"Could not find session\", status_code=404)\n            return await response(scope, receive, send)\n\n        body = await request.body()\n        logger.debug(f\"Received JSON: {body}\")\n\n        try:\n            message = types.JSONRPCMessage.model_validate_json(body)\n            logger.debug(f\"Validated client message: {message}\")\n        except ValidationError as err:\n            logger.error(f\"Failed to parse message: {err}\")\n            response = Response(\"Could not parse message\", status_code=400)\n            await response(scope, receive, send)\n            await writer.send(err)\n            return\n\n        logger.debug(f\"Sending message to writer: {message}\")\n        response = Response(\"Accepted\", status_code=202)\n        await response(scope, receive, send)\n        await writer.send(message)\n\n\n--- File: 58 # src/mcp/server/stdio.py ---\n\"\"\"\nStdio Server Transport Module\n\nThis module provides functionality for creating an stdio-based transport layer\nthat can be used to communicate with an MCP client through standard input/output\nstreams.\n\nExample usage:\n```\n    async def run_server():\n        async with stdio_server() as (read_stream, write_stream):\n            # read_stream contains incoming JSONRPCMessages from stdin\n            # write_stream allows sending JSONRPCMessages to stdout\n            server = await create_my_server()\n            await server.run(read_stream, write_stream, init_options)\n\n    anyio.run(run_server)\n```\n\"\"\"\n\nimport sys\nfrom contextlib import asynccontextmanager\nfrom io import TextIOWrapper\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\n\n\n@asynccontextmanager\nasync def stdio_server(\n    stdin: anyio.AsyncFile[str] | None = None,\n    stdout: anyio.AsyncFile[str] | None = None,\n):\n    \"\"\"\n    Server transport for stdio: this communicates with an MCP client by reading\n    from the current process' stdin and writing to stdout.\n    \"\"\"\n    # Purposely not using context managers for these, as we don't want to close\n    # standard process handles. Encoding of stdin/stdout as text streams on\n    # python is platform-dependent (Windows is particularly problematic), so we\n    # re-wrap the underlying binary stream to ensure UTF-8.\n    if not stdin:\n        stdin = anyio.wrap_file(TextIOWrapper(sys.stdin.buffer, encoding=\"utf-8\"))\n    if not stdout:\n        stdout = anyio.wrap_file(TextIOWrapper(sys.stdout.buffer, encoding=\"utf-8\"))\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def stdin_reader():\n        try:\n            async with read_stream_writer:\n                async for line in stdin:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(line)\n                    except Exception as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdout_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await stdout.write(json + \"\\n\")\n                    await stdout.flush()\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(stdin_reader)\n        tg.start_soon(stdout_writer)\n        yield read_stream, write_stream\n\n\n--- File: 59 # src/mcp/server/websocket.py ---\nimport logging\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic_core import ValidationError\nfrom starlette.types import Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_server(scope: Scope, receive: Receive, send: Send):\n    \"\"\"\n    WebSocket server transport for MCP. This is an ASGI application, suitable to be\n    used with a framework like Starlette and a server like Hypercorn.\n    \"\"\"\n\n    websocket = WebSocket(scope, receive, send)\n    await websocket.accept(subprotocol=\"mcp\")\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def ws_reader():\n        try:\n            async with read_stream_writer:\n                async for msg in websocket.iter_text():\n                    try:\n                        client_message = types.JSONRPCMessage.model_validate_json(msg)\n                    except ValidationError as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(client_message)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async def ws_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    obj = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await websocket.send_text(obj)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(ws_reader)\n        tg.start_soon(ws_writer)\n        yield (read_stream, write_stream)\n\n\n--- File: 61 # src/mcp/shared/context.py ---\nfrom dataclasses import dataclass\nfrom typing import Any, Generic\n\nfrom typing_extensions import TypeVar\n\nfrom mcp.shared.session import BaseSession\nfrom mcp.types import RequestId, RequestParams\n\nSessionT = TypeVar(\"SessionT\", bound=BaseSession[Any, Any, Any, Any, Any])\nLifespanContextT = TypeVar(\"LifespanContextT\")\n\n\n@dataclass\nclass RequestContext(Generic[SessionT, LifespanContextT]):\n    request_id: RequestId\n    meta: RequestParams.Meta | None\n    session: SessionT\n    lifespan_context: LifespanContextT\n\n\n--- File: 63 # src/mcp/shared/memory.py ---\n\"\"\"\nIn-memory transports\n\"\"\"\n\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom datetime import timedelta\nfrom typing import Any\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import (\n    ClientSession,\n    ListRootsFnT,\n    LoggingFnT,\n    MessageHandlerFnT,\n    SamplingFnT,\n)\nfrom mcp.server import Server\nfrom mcp.types import JSONRPCMessage\n\nMessageStream = tuple[\n    MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    MemoryObjectSendStream[JSONRPCMessage],\n]\n\n\n@asynccontextmanager\nasync def create_client_server_memory_streams() -> (\n    AsyncGenerator[tuple[MessageStream, MessageStream], None]\n):\n    \"\"\"\n    Creates a pair of bidirectional memory streams for client-server communication.\n\n    Returns:\n        A tuple of (client_streams, server_streams) where each is a tuple of\n        (read_stream, write_stream)\n    \"\"\"\n    # Create streams for both directions\n    server_to_client_send, server_to_client_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n    client_to_server_send, client_to_server_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n\n    client_streams = (server_to_client_receive, client_to_server_send)\n    server_streams = (client_to_server_receive, server_to_client_send)\n\n    async with (\n        server_to_client_receive,\n        client_to_server_send,\n        client_to_server_receive,\n        server_to_client_send,\n    ):\n        yield client_streams, server_streams\n\n\n@asynccontextmanager\nasync def create_connected_server_and_client_session(\n    server: Server[Any],\n    read_timeout_seconds: timedelta | None = None,\n    sampling_callback: SamplingFnT | None = None,\n    list_roots_callback: ListRootsFnT | None = None,\n    logging_callback: LoggingFnT | None = None,\n    message_handler: MessageHandlerFnT | None = None,\n    client_info: types.Implementation | None = None,\n    raise_exceptions: bool = False,\n) -> AsyncGenerator[ClientSession, None]:\n    \"\"\"Creates a ClientSession that is connected to a running MCP server.\"\"\"\n    async with create_client_server_memory_streams() as (\n        client_streams,\n        server_streams,\n    ):\n        client_read, client_write = client_streams\n        server_read, server_write = server_streams\n\n        # Create a cancel scope for the server task\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(\n                lambda: server.run(\n                    server_read,\n                    server_write,\n                    server.create_initialization_options(),\n                    raise_exceptions=raise_exceptions,\n                )\n            )\n\n            try:\n                async with ClientSession(\n                    read_stream=client_read,\n                    write_stream=client_write,\n                    read_timeout_seconds=read_timeout_seconds,\n                    sampling_callback=sampling_callback,\n                    list_roots_callback=list_roots_callback,\n                    logging_callback=logging_callback,\n                    message_handler=message_handler,\n                    client_info=client_info,\n                ) as client_session:\n                    await client_session.initialize()\n                    yield client_session\n            finally:\n                tg.cancel_scope.cancel()\n\n\n--- File: 65 # src/mcp/shared/session.py ---\nimport logging\nfrom collections.abc import Callable\nfrom contextlib import AsyncExitStack\nfrom datetime import timedelta\nfrom types import TracebackType\nfrom typing import Any, Generic, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nimport httpx\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\nfrom mcp.shared.exceptions import McpError\nfrom mcp.types import (\n    CancelledNotification,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    ErrorData,\n    JSONRPCError,\n    JSONRPCMessage,\n    JSONRPCNotification,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    RequestParams,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n)\n\nSendRequestT = TypeVar(\"SendRequestT\", ClientRequest, ServerRequest)\nSendResultT = TypeVar(\"SendResultT\", ClientResult, ServerResult)\nSendNotificationT = TypeVar(\"SendNotificationT\", ClientNotification, ServerNotification)\nReceiveRequestT = TypeVar(\"ReceiveRequestT\", ClientRequest, ServerRequest)\nReceiveResultT = TypeVar(\"ReceiveResultT\", bound=BaseModel)\nReceiveNotificationT = TypeVar(\n    \"ReceiveNotificationT\", ClientNotification, ServerNotification\n)\n\nRequestId = str | int\n\n\nclass RequestResponder(Generic[ReceiveRequestT, SendResultT]):\n    \"\"\"Handles responding to MCP requests and manages request lifecycle.\n\n    This class MUST be used as a context manager to ensure proper cleanup and\n    cancellation handling:\n\n    Example:\n        with request_responder as resp:\n            await resp.respond(result)\n\n    The context manager ensures:\n    1. Proper cancellation scope setup and cleanup\n    2. Request completion tracking\n    3. Cleanup of in-flight requests\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: RequestId,\n        request_meta: RequestParams.Meta | None,\n        request: ReceiveRequestT,\n        session: \"\"\"BaseSession[\n            SendRequestT,\n            SendNotificationT,\n            SendResultT,\n            ReceiveRequestT,\n            ReceiveNotificationT\n        ]\"\"\",\n        on_complete: Callable[[\"RequestResponder[ReceiveRequestT, SendResultT]\"], Any],\n    ) -> None:\n        self.request_id = request_id\n        self.request_meta = request_meta\n        self.request = request\n        self._session = session\n        self._completed = False\n        self._cancel_scope = anyio.CancelScope()\n        self._on_complete = on_complete\n        self._entered = False  # Track if we're in a context manager\n\n    def __enter__(self) -> \"RequestResponder[ReceiveRequestT, SendResultT]\":\n        \"\"\"Enter the context manager, enabling request cancellation tracking.\"\"\"\n        self._entered = True\n        self._cancel_scope = anyio.CancelScope()\n        self._cancel_scope.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit the context manager, performing cleanup and notifying completion.\"\"\"\n        try:\n            if self._completed:\n                self._on_complete(self)\n        finally:\n            self._entered = False\n            if not self._cancel_scope:\n                raise RuntimeError(\"No active cancel scope\")\n            self._cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n\n    async def respond(self, response: SendResultT | ErrorData) -> None:\n        \"\"\"Send a response for this request.\n\n        Must be called within a context manager block.\n        Raises:\n            RuntimeError: If not used within a context manager\n            AssertionError: If request was already responded to\n        \"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        assert not self._completed, \"Request already responded to\"\n\n        if not self.cancelled:\n            self._completed = True\n\n            await self._session._send_response(  # type: ignore[reportPrivateUsage]\n                request_id=self.request_id, response=response\n            )\n\n    async def cancel(self) -> None:\n        \"\"\"Cancel this request and mark it as completed.\"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        if not self._cancel_scope:\n            raise RuntimeError(\"No active cancel scope\")\n\n        self._cancel_scope.cancel()\n        self._completed = True  # Mark as completed so it's removed from in_flight\n        # Send an error response to indicate cancellation\n        await self._session._send_response(  # type: ignore[reportPrivateUsage]\n            request_id=self.request_id,\n            response=ErrorData(code=0, message=\"Request cancelled\", data=None),\n        )\n\n    @property\n    def in_flight(self) -> bool:\n        return not self._completed and not self.cancelled\n\n    @property\n    def cancelled(self) -> bool:\n        return self._cancel_scope.cancel_called\n\n\nclass BaseSession(\n    Generic[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ],\n):\n    \"\"\"\n    Implements an MCP \"session\" on top of read/write streams, including features\n    like request/response linking, notifications, and progress.\n\n    This class is an async context manager that automatically starts processing\n    messages when entered.\n    \"\"\"\n\n    _response_streams: dict[\n        RequestId, MemoryObjectSendStream[JSONRPCResponse | JSONRPCError]\n    ]\n    _request_id: int\n    _in_flight: dict[RequestId, RequestResponder[ReceiveRequestT, SendResultT]]\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[JSONRPCMessage],\n        receive_request_type: type[ReceiveRequestT],\n        receive_notification_type: type[ReceiveNotificationT],\n        # If none, reading will never time out\n        read_timeout_seconds: timedelta | None = None,\n    ) -> None:\n        self._read_stream = read_stream\n        self._write_stream = write_stream\n        self._response_streams = {}\n        self._request_id = 0\n        self._receive_request_type = receive_request_type\n        self._receive_notification_type = receive_notification_type\n        self._read_timeout_seconds = read_timeout_seconds\n        self._in_flight = {}\n\n        self._exit_stack = AsyncExitStack()\n\n    async def __aenter__(self) -> Self:\n        self._task_group = anyio.create_task_group()\n        await self._task_group.__aenter__()\n        self._task_group.start_soon(self._receive_loop)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self._exit_stack.aclose()\n        # Using BaseSession as a context manager should not block on exit (this\n        # would be very surprising behavior), so make sure to cancel the tasks\n        # in the task group.\n        self._task_group.cancel_scope.cancel()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    async def send_request(\n        self,\n        request: SendRequestT,\n        result_type: type[ReceiveResultT],\n    ) -> ReceiveResultT:\n        \"\"\"\n        Sends a request and wait for a response. Raises an McpError if the\n        response contains an error.\n\n        Do not use this method to emit notifications! Use send_notification()\n        instead.\n        \"\"\"\n\n        request_id = self._request_id\n        self._request_id = request_id + 1\n\n        response_stream, response_stream_reader = anyio.create_memory_object_stream[\n            JSONRPCResponse | JSONRPCError\n        ](1)\n        self._response_streams[request_id] = response_stream\n\n        self._exit_stack.push_async_callback(lambda: response_stream.aclose())\n        self._exit_stack.push_async_callback(lambda: response_stream_reader.aclose())\n\n        jsonrpc_request = JSONRPCRequest(\n            jsonrpc=\"2.0\",\n            id=request_id,\n            **request.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        # TODO: Support progress callbacks\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_request))\n\n        try:\n            with anyio.fail_after(\n                None\n                if self._read_timeout_seconds is None\n                else self._read_timeout_seconds.total_seconds()\n            ):\n                response_or_error = await response_stream_reader.receive()\n        except TimeoutError:\n            raise McpError(\n                ErrorData(\n                    code=httpx.codes.REQUEST_TIMEOUT,\n                    message=(\n                        f\"Timed out while waiting for response to \"\n                        f\"{request.__class__.__name__}. Waited \"\n                        f\"{self._read_timeout_seconds} seconds.\"\n                    ),\n                )\n            )\n\n        if isinstance(response_or_error, JSONRPCError):\n            raise McpError(response_or_error.error)\n        else:\n            return result_type.model_validate(response_or_error.result)\n\n    async def send_notification(self, notification: SendNotificationT) -> None:\n        \"\"\"\n        Emits a notification, which is a one-way message that does not expect\n        a response.\n        \"\"\"\n        jsonrpc_notification = JSONRPCNotification(\n            jsonrpc=\"2.0\",\n            **notification.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_notification))\n\n    async def _send_response(\n        self, request_id: RequestId, response: SendResultT | ErrorData\n    ) -> None:\n        if isinstance(response, ErrorData):\n            jsonrpc_error = JSONRPCError(jsonrpc=\"2.0\", id=request_id, error=response)\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_error))\n        else:\n            jsonrpc_response = JSONRPCResponse(\n                jsonrpc=\"2.0\",\n                id=request_id,\n                result=response.model_dump(\n                    by_alias=True, mode=\"json\", exclude_none=True\n                ),\n            )\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_response))\n\n    async def _receive_loop(self) -> None:\n        async with (\n            self._read_stream,\n            self._write_stream,\n        ):\n            async for message in self._read_stream:\n                if isinstance(message, Exception):\n                    await self._handle_incoming(message)\n                elif isinstance(message.root, JSONRPCRequest):\n                    validated_request = self._receive_request_type.model_validate(\n                        message.root.model_dump(\n                            by_alias=True, mode=\"json\", exclude_none=True\n                        )\n                    )\n\n                    responder = RequestResponder(\n                        request_id=message.root.id,\n                        request_meta=validated_request.root.params.meta\n                        if validated_request.root.params\n                        else None,\n                        request=validated_request,\n                        session=self,\n                        on_complete=lambda r: self._in_flight.pop(r.request_id, None),\n                    )\n\n                    self._in_flight[responder.request_id] = responder\n                    await self._received_request(responder)\n\n                    if not responder._completed:  # type: ignore[reportPrivateUsage]\n                        await self._handle_incoming(responder)\n\n                elif isinstance(message.root, JSONRPCNotification):\n                    try:\n                        notification = self._receive_notification_type.model_validate(\n                            message.root.model_dump(\n                                by_alias=True, mode=\"json\", exclude_none=True\n                            )\n                        )\n                        # Handle cancellation notifications\n                        if isinstance(notification.root, CancelledNotification):\n                            cancelled_id = notification.root.params.requestId\n                            if cancelled_id in self._in_flight:\n                                await self._in_flight[cancelled_id].cancel()\n                        else:\n                            await self._received_notification(notification)\n                            await self._handle_incoming(notification)\n                    except Exception as e:\n                        # For other validation errors, log and continue\n                        logging.warning(\n                            f\"Failed to validate notification: {e}. \"\n                            f\"Message was: {message.root}\"\n                        )\n                else:  # Response or error\n                    stream = self._response_streams.pop(message.root.id, None)\n                    if stream:\n                        await stream.send(message.root)\n                    else:\n                        await self._handle_incoming(\n                            RuntimeError(\n                                \"Received response with an unknown \"\n                                f\"request ID: {message}\"\n                            )\n                        )\n\n    async def _received_request(\n        self, responder: RequestResponder[ReceiveRequestT, SendResultT]\n    ) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a request without needing to\n        listen on the message stream.\n\n        If the request is responded to within this method, it will not be\n        forwarded on to the message stream.\n        \"\"\"\n\n    async def _received_notification(self, notification: ReceiveNotificationT) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a notification without needing\n        to listen on the message stream.\n        \"\"\"\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"\n        Sends a progress notification for a request that is currently being\n        processed.\n        \"\"\"\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[ReceiveRequestT, SendResultT]\n        | ReceiveNotificationT\n        | Exception,\n    ) -> None:\n        \"\"\"A generic handler for incoming messages. Overwritten by subclasses.\"\"\"\n        pass\n\n\n--- File: 67 # src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  This project provides a Python implementation for the **Model Context Protocol (MCP)**.\n  It allows developers to easily build both *MCP servers* and *MCP clients*.\n  Servers use the framework (especially **FastMCP**) to expose capabilities like remote functions (**Tools**), data access (**Resources**), or LLM instructions (**Prompts**).\n  Clients can then connect to these servers using different **Transports** (like standard I/O or WebSockets) to interact with those capabilities, often enabling AI models to use external functions and data.\nrelationships:\n  - from_abstraction: 0 # MCP Protocol Types\n    to_abstraction: 5 # Transports\n    label: \"Defines wire format for\"\n  - from_abstraction: 1 # FastMCP Server\n    to_abstraction: 3 # Tools\n    label: \"Defines\"\n  - from_abstraction: 1 # FastMCP Server\n    to_abstraction: 4 # Resources\n    label: \"Defines\"\n  - from_abstraction: 1 # FastMCP Server\n    to_abstraction: 6 # Prompts\n    label: \"Defines\"\n  - from_abstraction: 1 # FastMCP Server\n    to_abstraction: 5 # Transports\n    label: \"Runs over\"\n  - from_abstraction: 1 # FastMCP Server\n    to_abstraction: 7 # FastMCP Context\n    label: \"Provides\"\n  - from_abstraction: 2 # Client Session\n    to_abstraction: 5 # Transports\n    label: \"Connects via\"\n  - from_abstraction: 2 # Client Session\n    to_abstraction: 3 # Tools\n    label: \"Calls\"\n  - from_abstraction: 2 # Client Session\n    to_abstraction: 4 # Resources\n    label: \"Reads\"\n  - from_abstraction: 2 # Client Session\n    to_abstraction: 6 # Prompts\n    label: \"Gets\"\n  - from_abstraction: 3 # Tools\n    to_abstraction: 7 # FastMCP Context\n    label: \"Uses\"\n```", "\n        Given the following project abstractions and their relationships for the project ```` python-sdk ````:\n\n        Abstractions (Index # Name):\n        - 0 # MCP Protocol Types\n- 1 # FastMCP Server\n- 2 # Client Session\n- 3 # Tools\n- 4 # Resources\n- 5 # Transports\n- 6 # Prompts\n- 7 # FastMCP Context\n\n        Context about relationships and project summary:\n        Project Summary:\nThis project provides a Python implementation for the **Model Context Protocol (MCP)**.\nIt allows developers to easily build both *MCP servers* and *MCP clients*.\nServers use the framework (especially **FastMCP**) to expose capabilities like remote functions (**Tools**), data access (**Resources**), or LLM instructions (**Prompts**).\nClients can then connect to these servers using different **Transports** (like standard I/O or WebSockets) to interact with those capabilities, often enabling AI models to use external functions and data.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (MCP Protocol Types) to 5 (Transports): Defines wire format for\n- From 1 (FastMCP Server) to 3 (Tools): Defines\n- From 1 (FastMCP Server) to 4 (Resources): Defines\n- From 1 (FastMCP Server) to 6 (Prompts): Defines\n- From 1 (FastMCP Server) to 5 (Transports): Runs over\n- From 1 (FastMCP Server) to 7 (FastMCP Context): Provides\n- From 2 (Client Session) to 5 (Transports): Connects via\n- From 2 (Client Session) to 3 (Tools): Calls\n- From 2 (Client Session) to 4 (Resources): Reads\n- From 2 (Client Session) to 6 (Prompts): Gets\n- From 3 (Tools) to 7 (FastMCP Context): Uses\n\n\n        If you are going to make a tutorial for ```` python-sdk ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        ": "```yaml\n- 2 # Client Session\n- 3 # Tools\n- 4 # Resources\n- 6 # Prompts\n- 1 # FastMCP Server\n- 5 # Transports\n- 7 # FastMCP Context\n- 0 # MCP Protocol Types\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"Client Session\". This is Chapter 1.\n\n        Concept Details:\n        - Description:\n        Manages the client-side connection to an MCP server. It handles sending requests (like asking to list tools or call a specific tool) and receiving responses and notifications from the server. It abstracts away the low-level communication details, providing Python methods like `list_tools()` or `call_tool()`. Imagine it as the control panel you use to interact with a remote service (the MCP server), translating your Python calls into the correct MCP protocol messages.\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        This is the first chapter.\n\n        Relevant Code Snippets:\n        --- File: examples/clients/simple-chatbot/mcp_simple_chatbot/main.py ---\nimport asyncio\nimport json\nimport logging\nimport os\nimport shutil\nfrom contextlib import AsyncExitStack\nfrom typing import Any\n\nimport httpx\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\n\nclass Configuration:\n    \"\"\"Manages configuration and environment variables for the MCP client.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize configuration with environment variables.\"\"\"\n        self.load_env()\n        self.api_key = os.getenv(\"LLM_API_KEY\")\n\n    @staticmethod\n    def load_env() -> None:\n        \"\"\"Load environment variables from .env file.\"\"\"\n        load_dotenv()\n\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        \"\"\"Load server configuration from JSON file.\n\n        Args:\n            file_path: Path to the JSON configuration file.\n\n        Returns:\n            Dict containing server configuration.\n\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n\n    @property\n    def llm_api_key(self) -> str:\n        \"\"\"Get the LLM API key.\n\n        Returns:\n            The API key as a string.\n\n        Raises:\n            ValueError: If the API key is not found in environment variables.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"LLM_API_KEY not found in environment variables\")\n        return self.api_key\n\n\nclass Server:\n    \"\"\"Manages MCP server connections and tool execution.\"\"\"\n\n    def __init__(self, name: str, config: dict[str, Any]) -> None:\n        self.name: str = name\n        self.config: dict[str, Any] = config\n        self.stdio_context: Any | None = None\n        self.session: ClientSession | None = None\n        self._cleanup_lock: asyncio.Lock = asyncio.Lock()\n        self.exit_stack: AsyncExitStack = AsyncExitStack()\n\n    async def initialize(self) -> None:\n        \"\"\"Initialize the server connection.\"\"\"\n        command = (\n            shutil.which(\"npx\")\n            if self.config[\"command\"] == \"npx\"\n            else self.config[\"command\"]\n        )\n        if command is None:\n            raise ValueError(\"The command must be a valid string and cannot be None.\")\n\n        server_params = StdioServerParameters(\n            command=command,\n            args=self.config[\"args\"],\n            env={**os.environ, **self.config[\"env\"]}\n            if self.config.get(\"env\")\n            else None,\n        )\n        try:\n            stdio_transport = await self.exit_stack.enter_async_context(\n                stdio_client(server_params)\n            )\n            read, write = stdio_transport\n            session = await self.exit_stack.enter_async_context(\n                ClientSession(read, write)\n            )\n            await session.initialize()\n            self.session = session\n        except Exception as e:\n            logging.error(f\"Error initializing server {self.name}: {e}\")\n            await self.cleanup()\n            raise\n\n    async def list_tools(self) -> list[Any]:\n        \"\"\"List available tools from the server.\n\n        Returns:\n            A list of available tools.\n\n        Raises:\n            RuntimeError: If the server is not initialized.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        tools_response = await self.session.list_tools()\n        tools = []\n\n        for item in tools_response:\n            if isinstance(item, tuple) and item[0] == \"tools\":\n                tools.extend(\n                    Tool(tool.name, tool.description, tool.inputSchema)\n                    for tool in item[1]\n                )\n\n        return tools\n\n    async def execute_tool(\n        self,\n        tool_name: str,\n        arguments: dict[str, Any],\n        retries: int = 2,\n        delay: float = 1.0,\n    ) -> Any:\n        \"\"\"Execute a tool with retry mechanism.\n\n        Args:\n            tool_name: Name of the tool to execute.\n            arguments: Tool arguments.\n            retries: Number of retry attempts.\n            delay: Delay between retries in seconds.\n\n        Returns:\n            Tool execution result.\n\n        Raises:\n            RuntimeError: If server is not initialized.\n            Exception: If tool execution fails after all retries.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        attempt = 0\n        while attempt < retries:\n            try:\n                logging.info(f\"Executing {tool_name}...\")\n                result = await self.session.call_tool(tool_name, arguments)\n\n                return result\n\n            except Exception as e:\n                attempt += 1\n                logging.warning(\n                    f\"Error executing tool: {e}. Attempt {attempt} of {retries}.\"\n                )\n                if attempt < retries:\n                    logging.info(f\"Retrying in {delay} seconds...\")\n                    await asyncio.sleep(delay)\n                else:\n                    logging.error(\"Max retries reached. Failing.\")\n                    raise\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up server resources.\"\"\"\n        async with self._cleanup_lock:\n            try:\n                await self.exit_stack.aclose()\n                self.session = None\n                self.stdio_context = None\n            except Exception as e:\n                logging.error(f\"Error during cleanup of server {self.name}: {e}\")\n\n\nclass Tool:\n    \"\"\"Represents a tool with its properties and formatting.\"\"\"\n\n    def __init__(\n        self, name: str, description: str, input_schema: dict[str, Any]\n    ) -> None:\n        self.name: str = name\n        self.description: str = description\n        self.input_schema: dict[str, Any] = input_schema\n\n    def format_for_llm(self) -> str:\n        \"\"\"Format tool information for LLM.\n\n        Returns:\n            A formatted string describing the tool.\n        \"\"\"\n        args_desc = []\n        if \"properties\" in self.input_schema:\n            for param_name, param_info in self.input_schema[\"properties\"].items():\n                arg_desc = (\n                    f\"- {param_name}: {param_info.get('description', 'No description')}\"\n                )\n                if param_name in self.input_schema.get(\"required\", []):\n                    arg_desc += \" (required)\"\n                args_desc.append(arg_desc)\n\n        return f\"\"\"\nTool: {self.name}\nDescription: {self.description}\nArguments:\n{chr(10).join(args_desc)}\n\"\"\"\n\n\nclass LLMClient:\n    \"\"\"Manages communication with the LLM provider.\"\"\"\n\n    def __init__(self, api_key: str) -> None:\n        self.api_key: str = api_key\n\n    def get_response(self, messages: list[dict[str, str]]) -> str:\n        \"\"\"Get a response from the LLM.\n\n        Args:\n            messages: A list of message dictionaries.\n\n        Returns:\n            The LLM's response as a string.\n\n        Raises:\n            httpx.RequestError: If the request to the LLM fails.\n        \"\"\"\n        url = \"https://api.groq.com/openai/v1/chat/completions\"\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n        payload = {\n            \"messages\": messages,\n            \"model\": \"llama-3.2-90b-vision-preview\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096,\n            \"top_p\": 1,\n            \"stream\": False,\n            \"stop\": None,\n        }\n\n        try:\n            with httpx.Client() as client:\n                response = client.post(url, headers=headers, json=payload)\n                response.raise_for_status()\n                data = response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n\n        except httpx.RequestError as e:\n            error_message = f\"Error getting LLM response: {str(e)}\"\n            logging.error(error_message)\n\n            if isinstance(e, httpx.HTTPStatusError):\n                status_code = e.response.status_code\n                logging.error(f\"Status code: {status_code}\")\n                logging.error(f\"Response details: {e.response.text}\")\n\n            return (\n                f\"I encountered an error: {error_message}. \"\n                \"Please try again or rephrase your request.\"\n            )\n\n\nclass ChatSession:\n    \"\"\"Orchestrates the interaction between user, LLM, and tools.\"\"\"\n\n    def __init__(self, servers: list[Server], llm_client: LLMClient) -> None:\n        self.servers: list[Server] = servers\n        self.llm_client: LLMClient = llm_client\n\n    async def cleanup_servers(self) -> None:\n        \"\"\"Clean up all servers properly.\"\"\"\n        cleanup_tasks = [\n            asyncio.create_task(server.cleanup()) for server in self.servers\n        ]\n        if cleanup_tasks:\n            try:\n                await asyncio.gather(*cleanup_tasks, return_exceptions=True)\n            except Exception as e:\n                logging.warning(f\"Warning during final cleanup: {e}\")\n\n    async def process_llm_response(self, llm_response: str) -> str:\n        \"\"\"Process the LLM response and execute tools if needed.\n\n        Args:\n            llm_response: The response from the LLM.\n\n        Returns:\n            The result of tool execution or the original response.\n        \"\"\"\n        import json\n\n        try:\n            tool_call = json.loads(llm_response)\n            if \"tool\" in tool_call and \"arguments\" in tool_call:\n                logging.info(f\"Executing tool: {tool_call['tool']}\")\n                logging.info(f\"With arguments: {tool_call['arguments']}\")\n\n                for server in self.servers:\n                    tools = await server.list_tools()\n                    if any(tool.name == tool_call[\"tool\"] for tool in tools):\n                        try:\n                            result = await server.execute_tool(\n                                tool_call[\"tool\"], tool_call[\"arguments\"]\n                            )\n\n                            if isinstance(result, dict) and \"progress\" in result:\n                                progress = result[\"progress\"]\n                                total = result[\"total\"]\n                                percentage = (progress / total) * 100\n                                logging.info(\n                                    f\"Progress: {progress}/{total} \"\n                                    f\"({percentage:.1f}%)\"\n                                )\n\n                            return f\"Tool execution result: {result}\"\n                        except Exception as e:\n                            error_msg = f\"Error executing tool: {str(e)}\"\n                            logging.error(error_msg)\n                            return error_msg\n\n                return f\"No server found with tool: {tool_call['tool']}\"\n            return llm_response\n        except json.JSONDecodeError:\n            return llm_response\n\n    async def start(self) -> None:\n        \"\"\"Main chat session handler.\"\"\"\n        try:\n            for server in self.servers:\n                try:\n                    await server.initialize()\n                except Exception as e:\n                    logging.error(f\"Failed to initialize server: {e}\")\n                    await self.cleanup_servers()\n                    return\n\n            all_tools = []\n            for server in self.servers:\n                tools = await server.list_tools()\n                all_tools.extend(tools)\n\n            tools_description = \"\\n\".join([tool.format_for_llm() for tool in all_tools])\n\n            system_message = (\n                \"You are a helpful assistant with access to these tools:\\n\\n\"\n                f\"{tools_description}\\n\"\n                \"Choose the appropriate tool based on the user's question. \"\n                \"If no tool is needed, reply directly.\\n\\n\"\n                \"IMPORTANT: When you need to use a tool, you must ONLY respond with \"\n                \"the exact JSON object format below, nothing else:\\n\"\n                \"{\\n\"\n                '    \"tool\": \"tool-name\",\\n'\n                '    \"arguments\": {\\n'\n                '        \"argument-name\": \"value\"\\n'\n                \"    }\\n\"\n                \"}\\n\\n\"\n                \"After receiving a tool's response:\\n\"\n                \"1. Transform the raw data into a natural, conversational response\\n\"\n                \"2. Keep responses concise but informative\\n\"\n                \"3. Focus on the most relevant information\\n\"\n                \"4. Use appropriate context from the user's question\\n\"\n                \"5. Avoid simply repeating the raw data\\n\\n\"\n                \"Please use only the tools that are explicitly defined above.\"\n            )\n\n            messages = [{\"role\": \"system\", \"content\": system_message}]\n\n            while True:\n                try:\n                    user_input = input(\"You: \").strip().lower()\n                    if user_input in [\"quit\", \"exit\"]:\n                        logging.info(\"\\nExiting...\")\n                        break\n\n                    messages.append({\"role\": \"user\", \"content\": user_input})\n\n                    llm_response = self.llm_client.get_response(messages)\n                    logging.info(\"\\nAssistant: %s\", llm_response)\n\n                    result = await self.process_llm_response(llm_response)\n\n                    if result != llm_response:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n                        messages.append({\"role\": \"system\", \"content\": result})\n\n                        final_response = self.llm_client.get_response(messages)\n                        logging.info(\"\\nFinal response: %s\", final_response)\n                        messages.append(\n                            {\"role\": \"assistant\", \"content\": final_response}\n                        )\n                    else:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n\n                except KeyboardInterrupt:\n                    logging.info(\"\\nExiting...\")\n                    break\n\n        finally:\n            await self.cleanup_servers()\n\n\nasync def main() -> None:\n    \"\"\"Initialize and run the chat session.\"\"\"\n    config = Configuration()\n    server_config = config.load_config(\"servers_config.json\")\n    servers = [\n        Server(name, srv_config)\n        for name, srv_config in server_config[\"mcpServers\"].items()\n    ]\n    llm_client = LLMClient(config.llm_api_key)\n    chat_session = ChatSession(servers, llm_client)\n    await chat_session.start()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n--- File: src/mcp/__init__.py ---\nfrom .client.session import ClientSession\nfrom .client.stdio import StdioServerParameters, stdio_client\nfrom .server.session import ServerSession\nfrom .server.stdio import stdio_server\nfrom .shared.exceptions import McpError\nfrom .types import (\n    CallToolRequest,\n    ClientCapabilities,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    CompleteRequest,\n    CreateMessageRequest,\n    CreateMessageResult,\n    ErrorData,\n    GetPromptRequest,\n    GetPromptResult,\n    Implementation,\n    IncludeContext,\n    InitializedNotification,\n    InitializeRequest,\n    InitializeResult,\n    JSONRPCError,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    ListPromptsRequest,\n    ListPromptsResult,\n    ListResourcesRequest,\n    ListResourcesResult,\n    ListToolsResult,\n    LoggingLevel,\n    LoggingMessageNotification,\n    Notification,\n    PingRequest,\n    ProgressNotification,\n    PromptsCapability,\n    ReadResourceRequest,\n    ReadResourceResult,\n    Resource,\n    ResourcesCapability,\n    ResourceUpdatedNotification,\n    RootsCapability,\n    SamplingMessage,\n    ServerCapabilities,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n    SetLevelRequest,\n    StopReason,\n    SubscribeRequest,\n    Tool,\n    ToolsCapability,\n    UnsubscribeRequest,\n)\nfrom .types import (\n    Role as SamplingRole,\n)\n\n__all__ = [\n    \"CallToolRequest\",\n    \"ClientCapabilities\",\n    \"ClientNotification\",\n    \"ClientRequest\",\n    \"ClientResult\",\n    \"ClientSession\",\n    \"CreateMessageRequest\",\n    \"CreateMessageResult\",\n    \"ErrorData\",\n    \"GetPromptRequest\",\n    \"GetPromptResult\",\n    \"Implementation\",\n    \"IncludeContext\",\n    \"InitializeRequest\",\n    \"InitializeResult\",\n    \"InitializedNotification\",\n    \"JSONRPCError\",\n    \"JSONRPCRequest\",\n    \"ListPromptsRequest\",\n    \"ListPromptsResult\",\n    \"ListResourcesRequest\",\n    \"ListResourcesResult\",\n    \"ListToolsResult\",\n    \"LoggingLevel\",\n    \"LoggingMessageNotification\",\n    \"McpError\",\n    \"Notification\",\n    \"PingRequest\",\n    \"ProgressNotification\",\n    \"PromptsCapability\",\n    \"ReadResourceRequest\",\n    \"ReadResourceResult\",\n    \"ResourcesCapability\",\n    \"ResourceUpdatedNotification\",\n    \"Resource\",\n    \"RootsCapability\",\n    \"SamplingMessage\",\n    \"SamplingRole\",\n    \"ServerCapabilities\",\n    \"ServerNotification\",\n    \"ServerRequest\",\n    \"ServerResult\",\n    \"ServerSession\",\n    \"SetLevelRequest\",\n    \"StdioServerParameters\",\n    \"StopReason\",\n    \"SubscribeRequest\",\n    \"Tool\",\n    \"ToolsCapability\",\n    \"UnsubscribeRequest\",\n    \"stdio_client\",\n    \"stdio_server\",\n    \"CompleteRequest\",\n    \"JSONRPCResponse\",\n]\n\n\n--- File: src/mcp/client/__main__.py ---\nimport argparse\nimport logging\nimport sys\nfrom functools import partial\nfrom urllib.parse import urlparse\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import ClientSession\nfrom mcp.client.sse import sse_client\nfrom mcp.client.stdio import StdioServerParameters, stdio_client\nfrom mcp.shared.session import RequestResponder\nfrom mcp.types import JSONRPCMessage\n\nif not sys.warnoptions:\n    import warnings\n\n    warnings.simplefilter(\"ignore\")\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"client\")\n\n\nasync def message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    if isinstance(message, Exception):\n        logger.error(\"Error: %s\", message)\n        return\n\n    logger.info(\"Received message from server: %s\", message)\n\n\nasync def run_session(\n    read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    write_stream: MemoryObjectSendStream[JSONRPCMessage],\n    client_info: types.Implementation | None = None,\n):\n    async with ClientSession(\n        read_stream,\n        write_stream,\n        message_handler=message_handler,\n        client_info=client_info,\n    ) as session:\n        logger.info(\"Initializing session\")\n        await session.initialize()\n        logger.info(\"Initialized\")\n\n\nasync def main(command_or_url: str, args: list[str], env: list[tuple[str, str]]):\n    env_dict = dict(env)\n\n    if urlparse(command_or_url).scheme in (\"http\", \"https\"):\n        # Use SSE client for HTTP(S) URLs\n        async with sse_client(command_or_url) as streams:\n            await run_session(*streams)\n    else:\n        # Use stdio client for commands\n        server_parameters = StdioServerParameters(\n            command=command_or_url, args=args, env=env_dict\n        )\n        async with stdio_client(server_parameters) as streams:\n            await run_session(*streams)\n\n\ndef cli():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"command_or_url\", help=\"Command or URL to connect to\")\n    parser.add_argument(\"args\", nargs=\"*\", help=\"Additional arguments\")\n    parser.add_argument(\n        \"-e\",\n        \"--env\",\n        nargs=2,\n        action=\"append\",\n        metavar=(\"KEY\", \"VALUE\"),\n        help=\"Environment variables to set. Can be used multiple times.\",\n        default=[],\n    )\n\n    args = parser.parse_args()\n    anyio.run(partial(main, args.command_or_url, args.args, args.env), backend=\"trio\")\n\n\nif __name__ == \"__main__\":\n    cli()\n\n\n--- File: src/mcp/client/session.py ---\nfrom datetime import timedelta\nfrom typing import Any, Protocol\n\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl, TypeAdapter\n\nimport mcp.types as types\nfrom mcp.shared.context import RequestContext\nfrom mcp.shared.session import BaseSession, RequestResponder\nfrom mcp.shared.version import SUPPORTED_PROTOCOL_VERSIONS\n\nDEFAULT_CLIENT_INFO = types.Implementation(name=\"mcp\", version=\"0.1.0\")\n\n\nclass SamplingFnT(Protocol):\n    async def __call__(\n        self,\n        context: RequestContext[\"ClientSession\", Any],\n        params: types.CreateMessageRequestParams,\n    ) -> types.CreateMessageResult | types.ErrorData: ...\n\n\nclass ListRootsFnT(Protocol):\n    async def __call__(\n        self, context: RequestContext[\"ClientSession\", Any]\n    ) -> types.ListRootsResult | types.ErrorData: ...\n\n\nclass LoggingFnT(Protocol):\n    async def __call__(\n        self,\n        params: types.LoggingMessageNotificationParams,\n    ) -> None: ...\n\n\nclass MessageHandlerFnT(Protocol):\n    async def __call__(\n        self,\n        message: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None: ...\n\n\nasync def _default_message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    await anyio.lowlevel.checkpoint()\n\n\nasync def _default_sampling_callback(\n    context: RequestContext[\"ClientSession\", Any],\n    params: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"Sampling not supported\",\n    )\n\n\nasync def _default_list_roots_callback(\n    context: RequestContext[\"ClientSession\", Any],\n) -> types.ListRootsResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"List roots not supported\",\n    )\n\n\nasync def _default_logging_callback(\n    params: types.LoggingMessageNotificationParams,\n) -> None:\n    pass\n\n\nClientResponse: TypeAdapter[types.ClientResult | types.ErrorData] = TypeAdapter(\n    types.ClientResult | types.ErrorData\n)\n\n\nclass ClientSession(\n    BaseSession[\n        types.ClientRequest,\n        types.ClientNotification,\n        types.ClientResult,\n        types.ServerRequest,\n        types.ServerNotification,\n    ]\n):\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        read_timeout_seconds: timedelta | None = None,\n        sampling_callback: SamplingFnT | None = None,\n        list_roots_callback: ListRootsFnT | None = None,\n        logging_callback: LoggingFnT | None = None,\n        message_handler: MessageHandlerFnT | None = None,\n        client_info: types.Implementation | None = None,\n    ) -> None:\n        super().__init__(\n            read_stream,\n            write_stream,\n            types.ServerRequest,\n            types.ServerNotification,\n            read_timeout_seconds=read_timeout_seconds,\n        )\n        self._client_info = client_info or DEFAULT_CLIENT_INFO\n        self._sampling_callback = sampling_callback or _default_sampling_callback\n        self._list_roots_callback = list_roots_callback or _default_list_roots_callback\n        self._logging_callback = logging_callback or _default_logging_callback\n        self._message_handler = message_handler or _default_message_handler\n\n    async def initialize(self) -> types.InitializeResult:\n        sampling = types.SamplingCapability()\n        roots = types.RootsCapability(\n            # TODO: Should this be based on whether we\n            # _will_ send notifications, or only whether\n            # they're supported?\n            listChanged=True,\n        )\n\n        result = await self.send_request(\n            types.ClientRequest(\n                types.InitializeRequest(\n                    method=\"initialize\",\n                    params=types.InitializeRequestParams(\n                        protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                        capabilities=types.ClientCapabilities(\n                            sampling=sampling,\n                            experimental=None,\n                            roots=roots,\n                        ),\n                        clientInfo=self._client_info,\n                    ),\n                )\n            ),\n            types.InitializeResult,\n        )\n\n        if result.protocolVersion not in SUPPORTED_PROTOCOL_VERSIONS:\n            raise RuntimeError(\n                \"Unsupported protocol version from the server: \"\n                f\"{result.protocolVersion}\"\n            )\n\n        await self.send_notification(\n            types.ClientNotification(\n                types.InitializedNotification(method=\"notifications/initialized\")\n            )\n        )\n\n        return result\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                ),\n            )\n        )\n\n    async def set_logging_level(self, level: types.LoggingLevel) -> types.EmptyResult:\n        \"\"\"Send a logging/setLevel request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SetLevelRequest(\n                    method=\"logging/setLevel\",\n                    params=types.SetLevelRequestParams(level=level),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def list_resources(self) -> types.ListResourcesResult:\n        \"\"\"Send a resources/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourcesRequest(\n                    method=\"resources/list\",\n                )\n            ),\n            types.ListResourcesResult,\n        )\n\n    async def list_resource_templates(self) -> types.ListResourceTemplatesResult:\n        \"\"\"Send a resources/templates/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourceTemplatesRequest(\n                    method=\"resources/templates/list\",\n                )\n            ),\n            types.ListResourceTemplatesResult,\n        )\n\n    async def read_resource(self, uri: AnyUrl) -> types.ReadResourceResult:\n        \"\"\"Send a resources/read request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ReadResourceRequest(\n                    method=\"resources/read\",\n                    params=types.ReadResourceRequestParams(uri=uri),\n                )\n            ),\n            types.ReadResourceResult,\n        )\n\n    async def subscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/subscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SubscribeRequest(\n                    method=\"resources/subscribe\",\n                    params=types.SubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def unsubscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/unsubscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.UnsubscribeRequest(\n                    method=\"resources/unsubscribe\",\n                    params=types.UnsubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CallToolRequest(\n                    method=\"tools/call\",\n                    params=types.CallToolRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.CallToolResult,\n        )\n\n    async def list_prompts(self) -> types.ListPromptsResult:\n        \"\"\"Send a prompts/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListPromptsRequest(\n                    method=\"prompts/list\",\n                )\n            ),\n            types.ListPromptsResult,\n        )\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.GetPromptRequest(\n                    method=\"prompts/get\",\n                    params=types.GetPromptRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.GetPromptResult,\n        )\n\n    async def complete(\n        self,\n        ref: types.ResourceReference | types.PromptReference,\n        argument: dict[str, str],\n    ) -> types.CompleteResult:\n        \"\"\"Send a completion/complete request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CompleteRequest(\n                    method=\"completion/complete\",\n                    params=types.CompleteRequestParams(\n                        ref=ref,\n                        argument=types.CompletionArgument(**argument),\n                    ),\n                )\n            ),\n            types.CompleteResult,\n        )\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListToolsRequest(\n                    method=\"tools/list\",\n                )\n            ),\n            types.ListToolsResult,\n        )\n\n    async def send_roots_list_changed(self) -> None:\n        \"\"\"Send a roots/list_changed notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.RootsListChangedNotification(\n                    method=\"notifications/roots/list_changed\",\n                )\n            )\n        )\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ServerRequest, types.ClientResult]\n    ) -> None:\n        ctx = RequestContext[ClientSession, Any](\n            request_id=responder.request_id,\n            meta=responder.request_meta,\n            session=self,\n            lifespan_context=None,\n        )\n\n        match responder.request.root:\n            case types.CreateMessageRequest(params=params):\n                with responder:\n                    response = await self._sampling_callback(ctx, params)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.ListRootsRequest():\n                with responder:\n                    response = await self._list_roots_callback(ctx)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.PingRequest():\n                with responder:\n                    return await responder.respond(\n                        types.ClientResult(root=types.EmptyResult())\n                    )\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None:\n        \"\"\"Handle incoming messages by forwarding to the message handler.\"\"\"\n        await self._message_handler(req)\n\n    async def _received_notification(\n        self, notification: types.ServerNotification\n    ) -> None:\n        \"\"\"Handle notifications from the server.\"\"\"\n        # Process specific notification types\n        match notification.root:\n            case types.LoggingMessageNotification(params=params):\n                await self._logging_callback(params)\n            case _:\n                pass\n\n\n--- File: src/mcp/shared/memory.py ---\n\"\"\"\nIn-memory transports\n\"\"\"\n\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom datetime import timedelta\nfrom typing import Any\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\nfrom mcp.client.session import (\n    ClientSession,\n    ListRootsFnT,\n    LoggingFnT,\n    MessageHandlerFnT,\n    SamplingFnT,\n)\nfrom mcp.server import Server\nfrom mcp.types import JSONRPCMessage\n\nMessageStream = tuple[\n    MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n    MemoryObjectSendStream[JSONRPCMessage],\n]\n\n\n@asynccontextmanager\nasync def create_client_server_memory_streams() -> (\n    AsyncGenerator[tuple[MessageStream, MessageStream], None]\n):\n    \"\"\"\n    Creates a pair of bidirectional memory streams for client-server communication.\n\n    Returns:\n        A tuple of (client_streams, server_streams) where each is a tuple of\n        (read_stream, write_stream)\n    \"\"\"\n    # Create streams for both directions\n    server_to_client_send, server_to_client_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n    client_to_server_send, client_to_server_receive = anyio.create_memory_object_stream[\n        JSONRPCMessage | Exception\n    ](1)\n\n    client_streams = (server_to_client_receive, client_to_server_send)\n    server_streams = (client_to_server_receive, server_to_client_send)\n\n    async with (\n        server_to_client_receive,\n        client_to_server_send,\n        client_to_server_receive,\n        server_to_client_send,\n    ):\n        yield client_streams, server_streams\n\n\n@asynccontextmanager\nasync def create_connected_server_and_client_session(\n    server: Server[Any],\n    read_timeout_seconds: timedelta | None = None,\n    sampling_callback: SamplingFnT | None = None,\n    list_roots_callback: ListRootsFnT | None = None,\n    logging_callback: LoggingFnT | None = None,\n    message_handler: MessageHandlerFnT | None = None,\n    client_info: types.Implementation | None = None,\n    raise_exceptions: bool = False,\n) -> AsyncGenerator[ClientSession, None]:\n    \"\"\"Creates a ClientSession that is connected to a running MCP server.\"\"\"\n    async with create_client_server_memory_streams() as (\n        client_streams,\n        server_streams,\n    ):\n        client_read, client_write = client_streams\n        server_read, server_write = server_streams\n\n        # Create a cancel scope for the server task\n        async with anyio.create_task_group() as tg:\n            tg.start_soon(\n                lambda: server.run(\n                    server_read,\n                    server_write,\n                    server.create_initialization_options(),\n                    raise_exceptions=raise_exceptions,\n                )\n            )\n\n            try:\n                async with ClientSession(\n                    read_stream=client_read,\n                    write_stream=client_write,\n                    read_timeout_seconds=read_timeout_seconds,\n                    sampling_callback=sampling_callback,\n                    list_roots_callback=list_roots_callback,\n                    logging_callback=logging_callback,\n                    message_handler=message_handler,\n                    client_info=client_info,\n                ) as client_session:\n                    await client_session.initialize()\n                    yield client_session\n            finally:\n                tg.cancel_scope.cancel()\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 1: Client Session`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"Tools\". This is Chapter 2.\n\n        Concept Details:\n        - Description:\n        Functions exposed by an MCP server that a client can execute remotely. Tools take specific inputs (defined by a JSON schema) and return results. They allow a client (or an AI model via the client) to perform actions or computations using the server's capabilities. Think of them as specialized commands or functions the server makes available, like a calculator offering an `add` tool or a system offering a `take_screenshot` tool.\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n\n        Relevant Code Snippets:\n        --- File: examples/fastmcp/readme-quickstart.py ---\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n--- File: examples/fastmcp/screenshot.py ---\n\"\"\"\nFastMCP Screenshot Example\n\nGive Claude a tool to capture and view screenshots.\n\"\"\"\n\nimport io\n\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.utilities.types import Image\n\n# Create server\nmcp = FastMCP(\"Screenshot Demo\", dependencies=[\"pyautogui\", \"Pillow\"])\n\n\n@mcp.tool()\ndef take_screenshot() -> Image:\n    \"\"\"\n    Take a screenshot of the user's screen and return it as an image. Use\n    this tool anytime the user wants you to look at something they're doing.\n    \"\"\"\n    import pyautogui\n\n    buffer = io.BytesIO()\n\n    # if the file exceeds ~1MB, it will be rejected by Claude\n    screenshot = pyautogui.screenshot()\n    screenshot.convert(\"RGB\").save(buffer, format=\"JPEG\", quality=60, optimize=True)\n    return Image(data=buffer.getvalue(), format=\"jpeg\")\n\n\n--- File: examples/servers/simple-tool/mcp_simple_tool/server.py ---\nimport anyio\nimport click\nimport httpx\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\nasync def fetch_website(\n    url: str,\n) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n    headers = {\n        \"User-Agent\": \"MCP Test Server (github.com/modelcontextprotocol/python-sdk)\"\n    }\n    async with httpx.AsyncClient(follow_redirects=True, headers=headers) as client:\n        response = await client.get(url)\n        response.raise_for_status()\n        return [types.TextContent(type=\"text\", text=response.text)]\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-website-fetcher\")\n\n    @app.call_tool()\n    async def fetch_tool(\n        name: str, arguments: dict\n    ) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n        if name != \"fetch\":\n            raise ValueError(f\"Unknown tool: {name}\")\n        if \"url\" not in arguments:\n            raise ValueError(\"Missing required argument 'url'\")\n        return await fetch_website(arguments[\"url\"])\n\n    @app.list_tools()\n    async def list_tools() -> list[types.Tool]:\n        return [\n            types.Tool(\n                name=\"fetch\",\n                description=\"Fetches a website and returns its content\",\n                inputSchema={\n                    \"type\": \"object\",\n                    \"required\": [\"url\"],\n                    \"properties\": {\n                        \"url\": {\n                            \"type\": \"string\",\n                            \"description\": \"URL to fetch\",\n                        }\n                    },\n                },\n            )\n        ]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File: src/mcp/server/fastmcp/tools/__init__.py ---\nfrom .base import Tool\nfrom .tool_manager import ToolManager\n\n__all__ = [\"Tool\", \"ToolManager\"]\n\n\n--- File: src/mcp/server/fastmcp/tools/base.py ---\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, get_origin\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.utilities.func_metadata import FuncMetadata, func_metadata\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n    from mcp.shared.context import LifespanContextT\n\n\nclass Tool(BaseModel):\n    \"\"\"Internal tool registration info.\"\"\"\n\n    fn: Callable[..., Any] = Field(exclude=True)\n    name: str = Field(description=\"Name of the tool\")\n    description: str = Field(description=\"Description of what the tool does\")\n    parameters: dict[str, Any] = Field(description=\"JSON schema for tool parameters\")\n    fn_metadata: FuncMetadata = Field(\n        description=\"Metadata about the function including a pydantic model for tool\"\n        \" arguments\"\n    )\n    is_async: bool = Field(description=\"Whether the tool is async\")\n    context_kwarg: str | None = Field(\n        None, description=\"Name of the kwarg that should receive context\"\n    )\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n        context_kwarg: str | None = None,\n    ) -> Tool:\n        \"\"\"Create a Tool from a function.\"\"\"\n        from mcp.server.fastmcp import Context\n\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        func_doc = description or fn.__doc__ or \"\"\n        is_async = inspect.iscoroutinefunction(fn)\n\n        if context_kwarg is None:\n            sig = inspect.signature(fn)\n            for param_name, param in sig.parameters.items():\n                if get_origin(param.annotation) is not None:\n                    continue\n                if issubclass(param.annotation, Context):\n                    context_kwarg = param_name\n                    break\n\n        func_arg_metadata = func_metadata(\n            fn,\n            skip_names=[context_kwarg] if context_kwarg is not None else [],\n        )\n        parameters = func_arg_metadata.arg_model.model_json_schema()\n\n        return cls(\n            fn=fn,\n            name=func_name,\n            description=func_doc,\n            parameters=parameters,\n            fn_metadata=func_arg_metadata,\n            is_async=is_async,\n            context_kwarg=context_kwarg,\n        )\n\n    async def run(\n        self,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Run the tool with arguments.\"\"\"\n        try:\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn,\n                self.is_async,\n                arguments,\n                {self.context_kwarg: context}\n                if self.context_kwarg is not None\n                else None,\n            )\n        except Exception as e:\n            raise ToolError(f\"Error executing tool {self.name}: {e}\") from e\n\n\n--- File: src/mcp/server/fastmcp/tools/tool_manager.py ---\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.tools.base import Tool\nfrom mcp.server.fastmcp.utilities.logging import get_logger\nfrom mcp.shared.context import LifespanContextT\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n\nlogger = get_logger(__name__)\n\n\nclass ToolManager:\n    \"\"\"Manages FastMCP tools.\"\"\"\n\n    def __init__(self, warn_on_duplicate_tools: bool = True):\n        self._tools: dict[str, Tool] = {}\n        self.warn_on_duplicate_tools = warn_on_duplicate_tools\n\n    def get_tool(self, name: str) -> Tool | None:\n        \"\"\"Get tool by name.\"\"\"\n        return self._tools.get(name)\n\n    def list_tools(self) -> list[Tool]:\n        \"\"\"List all registered tools.\"\"\"\n        return list(self._tools.values())\n\n    def add_tool(\n        self,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> Tool:\n        \"\"\"Add a tool to the server.\"\"\"\n        tool = Tool.from_function(fn, name=name, description=description)\n        existing = self._tools.get(tool.name)\n        if existing:\n            if self.warn_on_duplicate_tools:\n                logger.warning(f\"Tool already exists: {tool.name}\")\n            return existing\n        self._tools[tool.name] = tool\n        return tool\n\n    async def call_tool(\n        self,\n        name: str,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        tool = self.get_tool(name)\n        if not tool:\n            raise ToolError(f\"Unknown tool: {name}\")\n\n        return await tool.run(arguments, context=context)\n\n\n--- File: src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 2: Tools`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"Resources\". This is Chapter 3.\n\n        Concept Details:\n        - Description:\n        Data sources exposed by an MCP server that a client can read. Resources are identified by URIs (like `file:///path/to/file.txt` or `http://example.com/data`) and can represent text, binary data, files, directories, or even dynamically generated content from functions. Think of resources as the specific files, web pages, or data streams that the server can provide access to upon request, like asking a library server for a specific book (resource) via its catalog number (URI).\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n\n        Relevant Code Snippets:\n        --- File: examples/fastmcp/desktop.py ---\n\"\"\"\nFastMCP Desktop Example\n\nA simple example that exposes the desktop directory as a resource.\n\"\"\"\n\nfrom pathlib import Path\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Demo\")\n\n\n@mcp.resource(\"dir://desktop\")\ndef desktop() -> list[str]:\n    \"\"\"List the files in the user's desktop\"\"\"\n    desktop = Path.home() / \"Desktop\"\n    return [str(f) for f in desktop.iterdir()]\n\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n--- File: examples/servers/simple-resource/mcp_simple_resource/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\nfrom pydantic import FileUrl\n\nSAMPLE_RESOURCES = {\n    \"greeting\": \"Hello! This is a sample text resource.\",\n    \"help\": \"This server provides a few sample text resources for testing.\",\n    \"about\": \"This is the simple-resource MCP server implementation.\",\n}\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-resource\")\n\n    @app.list_resources()\n    async def list_resources() -> list[types.Resource]:\n        return [\n            types.Resource(\n                uri=FileUrl(f\"file:///{name}.txt\"),\n                name=name,\n                description=f\"A sample text resource named {name}\",\n                mimeType=\"text/plain\",\n            )\n            for name in SAMPLE_RESOURCES.keys()\n        ]\n\n    @app.read_resource()\n    async def read_resource(uri: FileUrl) -> str | bytes:\n        name = uri.path.replace(\".txt\", \"\").lstrip(\"/\")\n\n        if name not in SAMPLE_RESOURCES:\n            raise ValueError(f\"Unknown resource: {uri}\")\n\n        return SAMPLE_RESOURCES[name]\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: src/mcp/server/fastmcp/resources/__init__.py ---\nfrom .base import Resource\nfrom .resource_manager import ResourceManager\nfrom .templates import ResourceTemplate\nfrom .types import (\n    BinaryResource,\n    DirectoryResource,\n    FileResource,\n    FunctionResource,\n    HttpResource,\n    TextResource,\n)\n\n__all__ = [\n    \"Resource\",\n    \"TextResource\",\n    \"BinaryResource\",\n    \"FunctionResource\",\n    \"FileResource\",\n    \"HttpResource\",\n    \"DirectoryResource\",\n    \"ResourceTemplate\",\n    \"ResourceManager\",\n]\n\n\n--- File: src/mcp/server/fastmcp/resources/base.py ---\n\"\"\"Base classes and interfaces for FastMCP resources.\"\"\"\n\nimport abc\nfrom typing import Annotated\n\nfrom pydantic import (\n    AnyUrl,\n    BaseModel,\n    ConfigDict,\n    Field,\n    UrlConstraints,\n    ValidationInfo,\n    field_validator,\n)\n\n\nclass Resource(BaseModel, abc.ABC):\n    \"\"\"Base class for all resources.\"\"\"\n\n    model_config = ConfigDict(validate_default=True)\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)] = Field(\n        default=..., description=\"URI of the resource\"\n    )\n    name: str | None = Field(description=\"Name of the resource\", default=None)\n    description: str | None = Field(\n        description=\"Description of the resource\", default=None\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n        pattern=r\"^[a-zA-Z0-9]+/[a-zA-Z0-9\\-+.]+$\",\n    )\n\n    @field_validator(\"name\", mode=\"before\")\n    @classmethod\n    def set_default_name(cls, name: str | None, info: ValidationInfo) -> str:\n        \"\"\"Set default name from URI if not provided.\"\"\"\n        if name:\n            return name\n        if uri := info.data.get(\"uri\"):\n            return str(uri)\n        raise ValueError(\"Either name or uri must be provided\")\n\n    @abc.abstractmethod\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource content.\"\"\"\n        pass\n\n\n--- File: src/mcp/server/fastmcp/resources/resource_manager.py ---\n\"\"\"Resource manager functionality.\"\"\"\n\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom pydantic import AnyUrl\n\nfrom mcp.server.fastmcp.resources.base import Resource\nfrom mcp.server.fastmcp.resources.templates import ResourceTemplate\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass ResourceManager:\n    \"\"\"Manages FastMCP resources.\"\"\"\n\n    def __init__(self, warn_on_duplicate_resources: bool = True):\n        self._resources: dict[str, Resource] = {}\n        self._templates: dict[str, ResourceTemplate] = {}\n        self.warn_on_duplicate_resources = warn_on_duplicate_resources\n\n    def add_resource(self, resource: Resource) -> Resource:\n        \"\"\"Add a resource to the manager.\n\n        Args:\n            resource: A Resource instance to add\n\n        Returns:\n            The added resource. If a resource with the same URI already exists,\n            returns the existing resource.\n        \"\"\"\n        logger.debug(\n            \"Adding resource\",\n            extra={\n                \"uri\": resource.uri,\n                \"type\": type(resource).__name__,\n                \"resource_name\": resource.name,\n            },\n        )\n        existing = self._resources.get(str(resource.uri))\n        if existing:\n            if self.warn_on_duplicate_resources:\n                logger.warning(f\"Resource already exists: {resource.uri}\")\n            return existing\n        self._resources[str(resource.uri)] = resource\n        return resource\n\n    def add_template(\n        self,\n        fn: Callable[..., Any],\n        uri_template: str,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> ResourceTemplate:\n        \"\"\"Add a template from a function.\"\"\"\n        template = ResourceTemplate.from_function(\n            fn,\n            uri_template=uri_template,\n            name=name,\n            description=description,\n            mime_type=mime_type,\n        )\n        self._templates[template.uri_template] = template\n        return template\n\n    async def get_resource(self, uri: AnyUrl | str) -> Resource | None:\n        \"\"\"Get resource by URI, checking concrete resources first, then templates.\"\"\"\n        uri_str = str(uri)\n        logger.debug(\"Getting resource\", extra={\"uri\": uri_str})\n\n        # First check concrete resources\n        if resource := self._resources.get(uri_str):\n            return resource\n\n        # Then check templates\n        for template in self._templates.values():\n            if params := template.matches(uri_str):\n                try:\n                    return await template.create_resource(uri_str, params)\n                except Exception as e:\n                    raise ValueError(f\"Error creating resource from template: {e}\")\n\n        raise ValueError(f\"Unknown resource: {uri}\")\n\n    def list_resources(self) -> list[Resource]:\n        \"\"\"List all registered resources.\"\"\"\n        logger.debug(\"Listing resources\", extra={\"count\": len(self._resources)})\n        return list(self._resources.values())\n\n    def list_templates(self) -> list[ResourceTemplate]:\n        \"\"\"List all registered templates.\"\"\"\n        logger.debug(\"Listing templates\", extra={\"count\": len(self._templates)})\n        return list(self._templates.values())\n\n\n--- File: src/mcp/server/fastmcp/resources/types.py ---\n\"\"\"Concrete resource implementations.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Callable\nfrom pathlib import Path\nfrom typing import Any\n\nimport anyio\nimport anyio.to_thread\nimport httpx\nimport pydantic.json\nimport pydantic_core\nfrom pydantic import Field, ValidationInfo\n\nfrom mcp.server.fastmcp.resources.base import Resource\n\n\nclass TextResource(Resource):\n    \"\"\"A resource that reads from a string.\"\"\"\n\n    text: str = Field(description=\"Text content of the resource\")\n\n    async def read(self) -> str:\n        \"\"\"Read the text content.\"\"\"\n        return self.text\n\n\nclass BinaryResource(Resource):\n    \"\"\"A resource that reads from bytes.\"\"\"\n\n    data: bytes = Field(description=\"Binary content of the resource\")\n\n    async def read(self) -> bytes:\n        \"\"\"Read the binary content.\"\"\"\n        return self.data\n\n\nclass FunctionResource(Resource):\n    \"\"\"A resource that defers data loading by wrapping a function.\n\n    The function is only called when the resource is read, allowing for lazy loading\n    of potentially expensive data. This is particularly useful when listing resources,\n    as the function won't be called until the resource is actually accessed.\n\n    The function can return:\n    - str for text content (default)\n    - bytes for binary content\n    - other types will be converted to JSON\n    \"\"\"\n\n    fn: Callable[[], Any] = Field(exclude=True)\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the resource by calling the wrapped function.\"\"\"\n        try:\n            result = (\n                await self.fn() if inspect.iscoroutinefunction(self.fn) else self.fn()\n            )\n            if isinstance(result, Resource):\n                return await result.read()\n            if isinstance(result, bytes):\n                return result\n            if isinstance(result, str):\n                return result\n            try:\n                return json.dumps(pydantic_core.to_jsonable_python(result))\n            except (TypeError, pydantic_core.PydanticSerializationError):\n                # If JSON serialization fails, try str()\n                return str(result)\n        except Exception as e:\n            raise ValueError(f\"Error reading resource {self.uri}: {e}\")\n\n\nclass FileResource(Resource):\n    \"\"\"A resource that reads from a file.\n\n    Set is_binary=True to read file as binary data instead of text.\n    \"\"\"\n\n    path: Path = Field(description=\"Path to the file\")\n    is_binary: bool = Field(\n        default=False,\n        description=\"Whether to read the file as binary data\",\n    )\n    mime_type: str = Field(\n        default=\"text/plain\",\n        description=\"MIME type of the resource content\",\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    @pydantic.field_validator(\"is_binary\")\n    @classmethod\n    def set_binary_from_mime_type(cls, is_binary: bool, info: ValidationInfo) -> bool:\n        \"\"\"Set is_binary based on mime_type if not explicitly set.\"\"\"\n        if is_binary:\n            return True\n        mime_type = info.data.get(\"mime_type\", \"text/plain\")\n        return not mime_type.startswith(\"text/\")\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the file content.\"\"\"\n        try:\n            if self.is_binary:\n                return await anyio.to_thread.run_sync(self.path.read_bytes)\n            return await anyio.to_thread.run_sync(self.path.read_text)\n        except Exception as e:\n            raise ValueError(f\"Error reading file {self.path}: {e}\")\n\n\nclass HttpResource(Resource):\n    \"\"\"A resource that reads from an HTTP endpoint.\"\"\"\n\n    url: str = Field(description=\"URL to fetch content from\")\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    async def read(self) -> str | bytes:\n        \"\"\"Read the HTTP content.\"\"\"\n        async with httpx.AsyncClient() as client:\n            response = await client.get(self.url)\n            response.raise_for_status()\n            return response.text\n\n\nclass DirectoryResource(Resource):\n    \"\"\"A resource that lists files in a directory.\"\"\"\n\n    path: Path = Field(description=\"Path to the directory\")\n    recursive: bool = Field(\n        default=False, description=\"Whether to list files recursively\"\n    )\n    pattern: str | None = Field(\n        default=None, description=\"Optional glob pattern to filter files\"\n    )\n    mime_type: str = Field(\n        default=\"application/json\", description=\"MIME type of the resource content\"\n    )\n\n    @pydantic.field_validator(\"path\")\n    @classmethod\n    def validate_absolute_path(cls, path: Path) -> Path:\n        \"\"\"Ensure path is absolute.\"\"\"\n        if not path.is_absolute():\n            raise ValueError(\"Path must be absolute\")\n        return path\n\n    def list_files(self) -> list[Path]:\n        \"\"\"List files in the directory.\"\"\"\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Directory not found: {self.path}\")\n        if not self.path.is_dir():\n            raise NotADirectoryError(f\"Not a directory: {self.path}\")\n\n        try:\n            if self.pattern:\n                return (\n                    list(self.path.glob(self.pattern))\n                    if not self.recursive\n                    else list(self.path.rglob(self.pattern))\n                )\n            return (\n                list(self.path.glob(\"*\"))\n                if not self.recursive\n                else list(self.path.rglob(\"*\"))\n            )\n        except Exception as e:\n            raise ValueError(f\"Error listing directory {self.path}: {e}\")\n\n    async def read(self) -> str:  # Always returns JSON string\n        \"\"\"Read the directory listing.\"\"\"\n        try:\n            files = await anyio.to_thread.run_sync(self.list_files)\n            file_list = [str(f.relative_to(self.path)) for f in files if f.is_file()]\n            return json.dumps({\"files\": file_list}, indent=2)\n        except Exception as e:\n            raise ValueError(f\"Error reading directory {self.path}: {e}\")\n\n\n--- File: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File: src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 3: Resources`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"Prompts\". This is Chapter 4.\n\n        Concept Details:\n        - Description:\n        Templates provided by the server for generating sequences of messages, often used to instruct Large Language Models (LLMs). Prompts can be static or dynamic, accepting arguments to customize the generated messages. They allow a server to provide pre-defined interaction structures or instructions to the client/LLM. Think of them as pre-written recipes or instruction manuals that the server provides, which can sometimes be filled in with specific ingredients (arguments).\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n\n        Relevant Code Snippets:\n        --- File: examples/fastmcp/echo.py ---\n\"\"\"\nFastMCP Echo Server\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Echo Server\")\n\n\n@mcp.tool()\ndef echo_tool(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return text\n\n\n@mcp.resource(\"echo://static\")\ndef echo_resource() -> str:\n    return \"Echo!\"\n\n\n@mcp.resource(\"echo://{text}\")\ndef echo_template(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return f\"Echo: {text}\"\n\n\n@mcp.prompt(\"echo\")\ndef echo_prompt(text: str) -> str:\n    return text\n\n\n--- File: examples/servers/simple-prompt/mcp_simple_prompt/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\ndef create_messages(\n    context: str | None = None, topic: str | None = None\n) -> list[types.PromptMessage]:\n    \"\"\"Create the messages for the prompt.\"\"\"\n    messages = []\n\n    # Add context if provided\n    if context:\n        messages.append(\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(\n                    type=\"text\", text=f\"Here is some relevant context: {context}\"\n                ),\n            )\n        )\n\n    # Add the main prompt\n    prompt = \"Please help me with \"\n    if topic:\n        prompt += f\"the following topic: {topic}\"\n    else:\n        prompt += \"whatever questions I may have.\"\n\n    messages.append(\n        types.PromptMessage(\n            role=\"user\", content=types.TextContent(type=\"text\", text=prompt)\n        )\n    )\n\n    return messages\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-prompt\")\n\n    @app.list_prompts()\n    async def list_prompts() -> list[types.Prompt]:\n        return [\n            types.Prompt(\n                name=\"simple\",\n                description=\"A simple prompt that can take optional context and topic \"\n                \"arguments\",\n                arguments=[\n                    types.PromptArgument(\n                        name=\"context\",\n                        description=\"Additional context to consider\",\n                        required=False,\n                    ),\n                    types.PromptArgument(\n                        name=\"topic\",\n                        description=\"Specific topic to focus on\",\n                        required=False,\n                    ),\n                ],\n            )\n        ]\n\n    @app.get_prompt()\n    async def get_prompt(\n        name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        if name != \"simple\":\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        if arguments is None:\n            arguments = {}\n\n        return types.GetPromptResult(\n            messages=create_messages(\n                context=arguments.get(\"context\"), topic=arguments.get(\"topic\")\n            ),\n            description=\"A simple prompt with optional context and topic arguments\",\n        )\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: src/mcp/server/fastmcp/prompts/__init__.py ---\nfrom .base import Prompt\nfrom .manager import PromptManager\n\n__all__ = [\"Prompt\", \"PromptManager\"]\n\n\n--- File: src/mcp/server/fastmcp/prompts/base.py ---\n\"\"\"Base classes for FastMCP prompts.\"\"\"\n\nimport inspect\nimport json\nfrom collections.abc import Awaitable, Callable, Sequence\nfrom typing import Any, Literal\n\nimport pydantic_core\nfrom pydantic import BaseModel, Field, TypeAdapter, validate_call\n\nfrom mcp.types import EmbeddedResource, ImageContent, TextContent\n\nCONTENT_TYPES = TextContent | ImageContent | EmbeddedResource\n\n\nclass Message(BaseModel):\n    \"\"\"Base class for all prompt messages.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"]\n    content: CONTENT_TYPES\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        if isinstance(content, str):\n            content = TextContent(type=\"text\", text=content)\n        super().__init__(content=content, **kwargs)\n\n\nclass UserMessage(Message):\n    \"\"\"A message from the user.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"user\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nclass AssistantMessage(Message):\n    \"\"\"A message from the assistant.\"\"\"\n\n    role: Literal[\"user\", \"assistant\"] = \"assistant\"\n\n    def __init__(self, content: str | CONTENT_TYPES, **kwargs: Any):\n        super().__init__(content=content, **kwargs)\n\n\nmessage_validator = TypeAdapter[UserMessage | AssistantMessage](\n    UserMessage | AssistantMessage\n)\n\nSyncPromptResult = (\n    str | Message | dict[str, Any] | Sequence[str | Message | dict[str, Any]]\n)\nPromptResult = SyncPromptResult | Awaitable[SyncPromptResult]\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument that can be passed to a prompt.\"\"\"\n\n    name: str = Field(description=\"Name of the argument\")\n    description: str | None = Field(\n        None, description=\"Description of what the argument does\"\n    )\n    required: bool = Field(\n        default=False, description=\"Whether the argument is required\"\n    )\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt template that can be rendered with parameters.\"\"\"\n\n    name: str = Field(description=\"Name of the prompt\")\n    description: str | None = Field(\n        None, description=\"Description of what the prompt does\"\n    )\n    arguments: list[PromptArgument] | None = Field(\n        None, description=\"Arguments that can be passed to the prompt\"\n    )\n    fn: Callable[..., PromptResult | Awaitable[PromptResult]] = Field(exclude=True)\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., PromptResult | Awaitable[PromptResult]],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> \"Prompt\":\n        \"\"\"Create a Prompt from a function.\n\n        The function can return:\n        - A string (converted to a message)\n        - A Message object\n        - A dict (converted to a message)\n        - A sequence of any of the above\n        \"\"\"\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        # Get schema from TypeAdapter - will fail if function isn't properly typed\n        parameters = TypeAdapter(fn).json_schema()\n\n        # Convert parameters to PromptArguments\n        arguments: list[PromptArgument] = []\n        if \"properties\" in parameters:\n            for param_name, param in parameters[\"properties\"].items():\n                required = param_name in parameters.get(\"required\", [])\n                arguments.append(\n                    PromptArgument(\n                        name=param_name,\n                        description=param.get(\"description\"),\n                        required=required,\n                    )\n                )\n\n        # ensure the arguments are properly cast\n        fn = validate_call(fn)\n\n        return cls(\n            name=func_name,\n            description=description or fn.__doc__ or \"\",\n            arguments=arguments,\n            fn=fn,\n        )\n\n    async def render(self, arguments: dict[str, Any] | None = None) -> list[Message]:\n        \"\"\"Render the prompt with arguments.\"\"\"\n        # Validate required arguments\n        if self.arguments:\n            required = {arg.name for arg in self.arguments if arg.required}\n            provided = set(arguments or {})\n            missing = required - provided\n            if missing:\n                raise ValueError(f\"Missing required arguments: {missing}\")\n\n        try:\n            # Call function and check if result is a coroutine\n            result = self.fn(**(arguments or {}))\n            if inspect.iscoroutine(result):\n                result = await result\n\n            # Validate messages\n            if not isinstance(result, list | tuple):\n                result = [result]\n\n            # Convert result to messages\n            messages: list[Message] = []\n            for msg in result:  # type: ignore[reportUnknownVariableType]\n                try:\n                    if isinstance(msg, Message):\n                        messages.append(msg)\n                    elif isinstance(msg, dict):\n                        messages.append(message_validator.validate_python(msg))\n                    elif isinstance(msg, str):\n                        content = TextContent(type=\"text\", text=msg)\n                        messages.append(UserMessage(content=content))\n                    else:\n                        content = json.dumps(pydantic_core.to_jsonable_python(msg))\n                        messages.append(Message(role=\"user\", content=content))\n                except Exception:\n                    raise ValueError(\n                        f\"Could not convert prompt result to message: {msg}\"\n                    )\n\n            return messages\n        except Exception as e:\n            raise ValueError(f\"Error rendering prompt {self.name}: {e}\")\n\n\n--- File: src/mcp/server/fastmcp/prompts/manager.py ---\n\"\"\"Prompt management functionality.\"\"\"\n\nfrom typing import Any\n\nfrom mcp.server.fastmcp.prompts.base import Message, Prompt\nfrom mcp.server.fastmcp.utilities.logging import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass PromptManager:\n    \"\"\"Manages FastMCP prompts.\"\"\"\n\n    def __init__(self, warn_on_duplicate_prompts: bool = True):\n        self._prompts: dict[str, Prompt] = {}\n        self.warn_on_duplicate_prompts = warn_on_duplicate_prompts\n\n    def get_prompt(self, name: str) -> Prompt | None:\n        \"\"\"Get prompt by name.\"\"\"\n        return self._prompts.get(name)\n\n    def list_prompts(self) -> list[Prompt]:\n        \"\"\"List all registered prompts.\"\"\"\n        return list(self._prompts.values())\n\n    def add_prompt(\n        self,\n        prompt: Prompt,\n    ) -> Prompt:\n        \"\"\"Add a prompt to the manager.\"\"\"\n\n        # Check for duplicates\n        existing = self._prompts.get(prompt.name)\n        if existing:\n            if self.warn_on_duplicate_prompts:\n                logger.warning(f\"Prompt already exists: {prompt.name}\")\n            return existing\n\n        self._prompts[prompt.name] = prompt\n        return prompt\n\n    async def render_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> list[Message]:\n        \"\"\"Render a prompt by name with arguments.\"\"\"\n        prompt = self.get_prompt(name)\n        if not prompt:\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        return await prompt.render(arguments)\n\n\n--- File: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File: src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 4: Prompts`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"FastMCP Server\". This is Chapter 5.\n\n        Concept Details:\n        - Description:\n        A high-level, user-friendly framework for creating MCP servers. It simplifies server development by letting you define tools, resources, and prompts using Python decorators (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`) on regular functions. It handles the underlying MCP protocol details, JSON schema generation, and request routing automatically. Think of it as a helpful assistant that lets you focus on writing the *logic* for your server's capabilities, rather than the complex communication setup.\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n---\n# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).\n\n        Relevant Code Snippets:\n        --- File: examples/fastmcp/desktop.py ---\n\"\"\"\nFastMCP Desktop Example\n\nA simple example that exposes the desktop directory as a resource.\n\"\"\"\n\nfrom pathlib import Path\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Demo\")\n\n\n@mcp.resource(\"dir://desktop\")\ndef desktop() -> list[str]:\n    \"\"\"List the files in the user's desktop\"\"\"\n    desktop = Path.home() / \"Desktop\"\n    return [str(f) for f in desktop.iterdir()]\n\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n--- File: examples/fastmcp/echo.py ---\n\"\"\"\nFastMCP Echo Server\n\"\"\"\n\nfrom mcp.server.fastmcp import FastMCP\n\n# Create server\nmcp = FastMCP(\"Echo Server\")\n\n\n@mcp.tool()\ndef echo_tool(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return text\n\n\n@mcp.resource(\"echo://static\")\ndef echo_resource() -> str:\n    return \"Echo!\"\n\n\n@mcp.resource(\"echo://{text}\")\ndef echo_template(text: str) -> str:\n    \"\"\"Echo the input text\"\"\"\n    return f\"Echo: {text}\"\n\n\n@mcp.prompt(\"echo\")\ndef echo_prompt(text: str) -> str:\n    return text\n\n\n--- File: examples/fastmcp/readme-quickstart.py ---\nfrom mcp.server.fastmcp import FastMCP\n\n# Create an MCP server\nmcp = FastMCP(\"Demo\")\n\n\n# Add an addition tool\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n\n# Add a dynamic greeting resource\n@mcp.resource(\"greeting://{name}\")\ndef get_greeting(name: str) -> str:\n    \"\"\"Get a personalized greeting\"\"\"\n    return f\"Hello, {name}!\"\n\n\n--- File: examples/fastmcp/screenshot.py ---\n\"\"\"\nFastMCP Screenshot Example\n\nGive Claude a tool to capture and view screenshots.\n\"\"\"\n\nimport io\n\nfrom mcp.server.fastmcp import FastMCP\nfrom mcp.server.fastmcp.utilities.types import Image\n\n# Create server\nmcp = FastMCP(\"Screenshot Demo\", dependencies=[\"pyautogui\", \"Pillow\"])\n\n\n@mcp.tool()\ndef take_screenshot() -> Image:\n    \"\"\"\n    Take a screenshot of the user's screen and return it as an image. Use\n    this tool anytime the user wants you to look at something they're doing.\n    \"\"\"\n    import pyautogui\n\n    buffer = io.BytesIO()\n\n    # if the file exceeds ~1MB, it will be rejected by Claude\n    screenshot = pyautogui.screenshot()\n    screenshot.convert(\"RGB\").save(buffer, format=\"JPEG\", quality=60, optimize=True)\n    return Image(data=buffer.getvalue(), format=\"jpeg\")\n\n\n--- File: src/mcp/server/fastmcp/__init__.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom importlib.metadata import version\n\nfrom .server import Context, FastMCP\nfrom .utilities.types import Image\n\n__version__ = version(\"mcp\")\n__all__ = [\"FastMCP\", \"Context\", \"Image\"]\n\n\n--- File: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 5: FastMCP Server`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 5: FastMCP Server - Building Your Own Assistant\n\nIn the previous chapters ([Client Session](01_client_session.md), [Tools](02_tools.md), [Resources](03_resources.md), [Prompts](04_prompts.md)), we learned how to *use* an existing MCP server from our Python code (the client). We learned how to connect, list capabilities, call tools, read resources, and get prompts.\n\nBut what if *you* want to create your own assistant? Maybe you have a special calculation, access to unique data, or a cool function you want to make available to other programs (or even to an AI like Claude). How do you build the *server* side?\n\n## Motivation: Making Server Creation Easy\n\nBuilding a server that speaks the MCP protocol might sound complicated. You'd need to handle:\n\n*   Receiving messages (like JSON-RPC requests).\n*   Parsing those messages correctly.\n*   Understanding requests like \"tools/list\" or \"tools/call\".\n*   Routing the request to the correct piece of your code.\n*   Generating JSON Schemas to describe your tool inputs.\n*   Formatting your results back into the correct MCP response format.\n*   Sending the response back.\n\nThat's a lot of boilerplate code just to handle the communication! Wouldn't it be great if you could just write your core Python functions (the actual logic for your tools, resources, etc.) and have something else handle all the tricky MCP communication details?\n\nThat's exactly what **FastMCP Server** does!\n\n**`FastMCP` is a high-level, user-friendly framework within `python-sdk` for creating MCP servers.** Think of it like a helpful construction kit for building your MCP assistant. You define the *capabilities* (tools, resources, prompts) using simple Python functions and special markers called **decorators** (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`). `FastMCP` then automatically handles all the underlying MCP protocol details, generates the necessary descriptions (like JSON schemas), and routes incoming client requests to your functions.\n\nIt lets you focus on writing the *logic* for what your server can *do*, not the complex setup of how it *communicates*.\n\n**Our Goal:** Let's build a very simple MCP server that offers just one tool: adding two numbers.\n\n## Building Our Adder Server: Step-by-Step\n\nWe'll use `FastMCP` to create a server that a client can connect to and ask to add numbers.\n\n**1. Import `FastMCP`**\n\nFirst, we need to import the main class from the library.\n\n```python\n# adder_server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# We'll add more code below...\n```\n\nThis line brings the `FastMCP` framework into our script.\n\n**2. Create a `FastMCP` Instance**\n\nNext, we create an instance of the `FastMCP` class. This represents our server. We can give it a name.\n\n```python\n# adder_server.py (continued)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n# We'll define tools next...\n```\n\nThink of `mcp` as the main control object for our server definition.\n\n**3. Define the Tool Logic (A Simple Function)**\n\nWe need the actual code that performs the addition. This is just a regular Python function. Notice the use of type hints (`a: int`, `b: int`, `-> int`). These are important!\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n# This function contains the logic for our tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Now, how do we tell FastMCP this is a tool?\n```\n\nThis is the core logic. It takes two integers and returns their sum. The `print` statements are just for us to see when it gets called.\n\n**4. Register the Function as a Tool (The Magic Decorator!)**\n\nHow does `FastMCP` know that our `add` function should be made available as an MCP tool? We use the `@mcp.tool()` **decorator**. A decorator is like a sticky note you put on a function to give it special powers or register it with a framework.\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n@mcp.tool() # <-- This is the FastMCP decorator!\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# We're almost ready to run the server...\n```\n\nBy adding `@mcp.tool()` right above our `add` function, we tell `FastMCP`:\n\n*   \"Make this function available as an MCP tool.\"\n*   **Automatically use the function name (`add`) as the tool name.** (You can specify a different name like `@mcp.tool(\"adder\")` if you want).\n*   **Automatically use the function's docstring (`\"\"\"Adds two numbers together.\"\"\"`) as the tool's description.**\n*   **Automatically look at the type hints (`a: int`, `b: int`) to generate the JSON `inputSchema`** that tells clients what arguments are needed (an integer `a` and an integer `b`).\n*   **Automatically understand the return type (`-> int`)** to know what kind of result the tool produces.\n\nThis single line saves us from writing a lot of configuration code!\n\n**5. Run the Server**\n\nFinally, we need to tell our server to start running and listening for connections.\n\n```python\n# adder_server.py (complete example)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Add this block to start the server when the script is run\nif __name__ == \"__main__\":\n    print(\"Starting Adder Server...\")\n    # This runs the server, listening for connections via stdio by default\n    mcp.run()\n    print(\"Adder Server stopped.\")\n\n```\n\nThe `if __name__ == \"__main__\":` block is standard Python practice to make code run only when the script is executed directly. `mcp.run()` starts the `FastMCP` server. By default, it uses \"stdio\" (standard input/output) as the communication channel, meaning it behaves like a command-line application waiting for requests on its input and sending responses to its output. We'll learn more about different ways servers and clients can communicate in the next chapter on [Transports](06_transports.md).\n\nIf you save this as `adder_server.py` and run it (e.g., `python adder_server.py`), it will print \"Starting Adder Server...\" and wait. You could then use a client (like the one from [Chapter 2: Tools](02_tools.md), configured to run this script) to connect and call the `add` tool!\n\n## What Happens Under the Hood?\n\nLet's trace what happens when a client (using `ClientSession` from Chapter 1) calls `session.call_tool(\"add\", {\"a\": 5, \"b\": 3})` on our running `adder_server.py`:\n\n1.  **Client Sends Request:** The `ClientSession` creates an MCP `tools/call` request message (like `{\"method\": \"tools/call\", \"params\": {\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}, ...}`) and sends it over the communication channel (e.g., stdio).\n2.  **Transport Delivers:** The underlying transport layer ([Transports](06_transports.md)) delivers this raw message to the running `FastMCP` server.\n3.  **FastMCP Receives & Parses:** The `FastMCP` framework reads the incoming message. It understands the MCP protocol and parses the JSON to identify it's a `tools/call` request for the tool named `add` with arguments `a=5` and `b=3`.\n4.  **Finds the Tool Function:** `FastMCP` looks up the tool name \"add\" in its internal registry (where the `@mcp.tool()` decorator placed our `add` function).\n5.  **Validates Arguments:** It checks if the provided arguments (`{\"a\": 5, \"b\": 3}`) match the `inputSchema` it automatically generated from the `add(a: int, b: int)` type hints. In this case, they do. (If not, it would generate an error response).\n6.  **Calls Your Function:** `FastMCP` calls your actual Python function: `add(a=5, b=3)`. Your `print` statements will execute.\n7.  **Receives Return Value:** Your function returns the integer `8`.\n8.  **Formats Response:** `FastMCP` takes the return value (`8`) and automatically formats it into the standard MCP `CallToolResult` structure, likely as `TextContent`. The response message might look like `{\"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, ...}`.\n9.  **Sends Response:** `FastMCP` sends this JSON response message back over the communication channel.\n10. **Client Receives:** The client's `ClientSession` receives the response, parses it, and returns the `CallToolResult` object to the calling code.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Communication Channel\n    participant FastMCP as FastMCP Framework\n    participant AdderFunc as Your add() function\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"add\", args={a:5, b:3})\n    Transport->>+FastMCP: Deliver raw message\n    FastMCP->>FastMCP: Parse message, find 'add' tool registration\n    FastMCP->>FastMCP: Validate args against auto-generated schema\n    FastMCP->>+AdderFunc: Call add(a=5, b=3)\n    Note over AdderFunc: Prints logs, returns 8\n    AdderFunc-->>-FastMCP: Return result (8)\n    FastMCP->>FastMCP: Format 8 into MCP response (e.g., TextContent)\n    FastMCP->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\nYou don't *need* to know the internals, but it can be helpful. Inside `FastMCP` (see `src/mcp/server/fastmcp/server.py`):\n\n*   The `FastMCP` class initializes internal managers like `ToolManager`, `ResourceManager`, and `PromptManager`.\n*   The `@mcp.tool()` decorator calls the `add_tool` method on the `ToolManager`.\n*   `add_tool` uses Python's `inspect` module to look at your function's signature (parameters, type hints, docstring). It uses this information (often with help from the Pydantic library) to build the `MCPTool` definition, including the `inputSchema`. It stores this definition.\n*   `FastMCP` sets up handlers (like `self._mcp_server.call_tool()(self.call_tool)`) that map incoming MCP methods (`tools/call`) to internal `FastMCP` methods.\n*   The internal `call_tool` method asks the `ToolManager` to execute the registered tool function, passing the arguments.\n*   Helper functions like `_convert_to_content` take your Python function's return value and package it into the standard MCP content types (`TextContent`, `ImageContent`, etc.).\n\nThe key takeaway is that `FastMCP` uses Python's introspection capabilities (looking at function details) and decorators to automate the tedious parts of building an MCP server.\n\n## Adding Resources and Prompts\n\nJust like `@mcp.tool()`, `FastMCP` provides decorators for defining resources and prompts:\n\n*   **`@mcp.resource(uri)`:** Registers a function to provide data for a specific URI. The URI can even include template parameters (like `greeting://{name}`) that match function arguments. `FastMCP` handles mapping the URI request to your function. (See [Resources](03_resources.md)).\n\n    ```python\n    # From examples/fastmcp/readme-quickstart.py\n    @mcp.resource(\"greeting://{name}\")\n    def get_greeting(name: str) -> str:\n        \"\"\"Get a personalized greeting\"\"\"\n        return f\"Hello, {name}!\"\n    ```\n\n*   **`@mcp.prompt(name)`:** Registers a function that generates a list of prompt messages, often taking arguments defined by the function's parameters. `FastMCP` handles generating the `MCPPrompt` definition and rendering the messages when requested. (See [Prompts](04_prompts.md)).\n\n    ```python\n    # From examples/fastmcp/echo.py\n    @mcp.prompt(\"echo\")\n    def echo_prompt(text: str) -> str:\n        # FastMCP automatically converts the returned string\n        # into a User message in the GetPromptResult\n        return text\n    ```\n\nThese work on the same principle: write a Python function with type hints and a docstring, add the decorator, and `FastMCP` handles the MCP integration.\n\n## Conclusion\n\nCongratulations! You've taken your first step into building your own MCP server using the `FastMCP` framework. You learned:\n\n*   `FastMCP` simplifies server development by handling complex MCP protocol details automatically.\n*   You can define server capabilities (Tools, Resources, Prompts) using standard Python functions.\n*   Decorators like `@mcp.tool()`, `@mcp.resource()`, and `@mcp.prompt()` are the magic that registers your functions with `FastMCP`.\n*   `FastMCP` uses type hints and docstrings to automatically generate schemas and descriptions.\n*   Running the server is as simple as calling `mcp.run()`.\n\nYou now have the power to create your own \"assistants\" that can offer custom capabilities over the MCP protocol.\n\nBut how exactly do the client and server *talk* to each other? Whether it's stdio, HTTP/SSE, or something else, the communication channel is crucial. In the next chapter, we'll explore these communication methods: [Transports](06_transports.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"Transports\". This is Chapter 6.\n\n        Concept Details:\n        - Description:\n        The underlying communication mechanism used to connect a client and a server. MCP supports different transports like standard input/output (stdio), Server-Sent Events (SSE over HTTP), and WebSockets. The transport handles the actual sending and receiving of the formatted MCP messages. Think of it as the delivery method for messages between client and server \u2013 like choosing between sending letters via postal mail (stdio), a continuous news ticker feed (SSE), or a direct phone line (WebSocket).\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n---\n# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).\n---\n# Chapter 5: FastMCP Server - Building Your Own Assistant\n\nIn the previous chapters ([Client Session](01_client_session.md), [Tools](02_tools.md), [Resources](03_resources.md), [Prompts](04_prompts.md)), we learned how to *use* an existing MCP server from our Python code (the client). We learned how to connect, list capabilities, call tools, read resources, and get prompts.\n\nBut what if *you* want to create your own assistant? Maybe you have a special calculation, access to unique data, or a cool function you want to make available to other programs (or even to an AI like Claude). How do you build the *server* side?\n\n## Motivation: Making Server Creation Easy\n\nBuilding a server that speaks the MCP protocol might sound complicated. You'd need to handle:\n\n*   Receiving messages (like JSON-RPC requests).\n*   Parsing those messages correctly.\n*   Understanding requests like \"tools/list\" or \"tools/call\".\n*   Routing the request to the correct piece of your code.\n*   Generating JSON Schemas to describe your tool inputs.\n*   Formatting your results back into the correct MCP response format.\n*   Sending the response back.\n\nThat's a lot of boilerplate code just to handle the communication! Wouldn't it be great if you could just write your core Python functions (the actual logic for your tools, resources, etc.) and have something else handle all the tricky MCP communication details?\n\nThat's exactly what **FastMCP Server** does!\n\n**`FastMCP` is a high-level, user-friendly framework within `python-sdk` for creating MCP servers.** Think of it like a helpful construction kit for building your MCP assistant. You define the *capabilities* (tools, resources, prompts) using simple Python functions and special markers called **decorators** (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`). `FastMCP` then automatically handles all the underlying MCP protocol details, generates the necessary descriptions (like JSON schemas), and routes incoming client requests to your functions.\n\nIt lets you focus on writing the *logic* for what your server can *do*, not the complex setup of how it *communicates*.\n\n**Our Goal:** Let's build a very simple MCP server that offers just one tool: adding two numbers.\n\n## Building Our Adder Server: Step-by-Step\n\nWe'll use `FastMCP` to create a server that a client can connect to and ask to add numbers.\n\n**1. Import `FastMCP`**\n\nFirst, we need to import the main class from the library.\n\n```python\n# adder_server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# We'll add more code below...\n```\n\nThis line brings the `FastMCP` framework into our script.\n\n**2. Create a `FastMCP` Instance**\n\nNext, we create an instance of the `FastMCP` class. This represents our server. We can give it a name.\n\n```python\n# adder_server.py (continued)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n# We'll define tools next...\n```\n\nThink of `mcp` as the main control object for our server definition.\n\n**3. Define the Tool Logic (A Simple Function)**\n\nWe need the actual code that performs the addition. This is just a regular Python function. Notice the use of type hints (`a: int`, `b: int`, `-> int`). These are important!\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n# This function contains the logic for our tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Now, how do we tell FastMCP this is a tool?\n```\n\nThis is the core logic. It takes two integers and returns their sum. The `print` statements are just for us to see when it gets called.\n\n**4. Register the Function as a Tool (The Magic Decorator!)**\n\nHow does `FastMCP` know that our `add` function should be made available as an MCP tool? We use the `@mcp.tool()` **decorator**. A decorator is like a sticky note you put on a function to give it special powers or register it with a framework.\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n@mcp.tool() # <-- This is the FastMCP decorator!\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# We're almost ready to run the server...\n```\n\nBy adding `@mcp.tool()` right above our `add` function, we tell `FastMCP`:\n\n*   \"Make this function available as an MCP tool.\"\n*   **Automatically use the function name (`add`) as the tool name.** (You can specify a different name like `@mcp.tool(\"adder\")` if you want).\n*   **Automatically use the function's docstring (`\"\"\"Adds two numbers together.\"\"\"`) as the tool's description.**\n*   **Automatically look at the type hints (`a: int`, `b: int`) to generate the JSON `inputSchema`** that tells clients what arguments are needed (an integer `a` and an integer `b`).\n*   **Automatically understand the return type (`-> int`)** to know what kind of result the tool produces.\n\nThis single line saves us from writing a lot of configuration code!\n\n**5. Run the Server**\n\nFinally, we need to tell our server to start running and listening for connections.\n\n```python\n# adder_server.py (complete example)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Add this block to start the server when the script is run\nif __name__ == \"__main__\":\n    print(\"Starting Adder Server...\")\n    # This runs the server, listening for connections via stdio by default\n    mcp.run()\n    print(\"Adder Server stopped.\")\n\n```\n\nThe `if __name__ == \"__main__\":` block is standard Python practice to make code run only when the script is executed directly. `mcp.run()` starts the `FastMCP` server. By default, it uses \"stdio\" (standard input/output) as the communication channel, meaning it behaves like a command-line application waiting for requests on its input and sending responses to its output. We'll learn more about different ways servers and clients can communicate in the next chapter on [Transports](06_transports.md).\n\nIf you save this as `adder_server.py` and run it (e.g., `python adder_server.py`), it will print \"Starting Adder Server...\" and wait. You could then use a client (like the one from [Chapter 2: Tools](02_tools.md), configured to run this script) to connect and call the `add` tool!\n\n## What Happens Under the Hood?\n\nLet's trace what happens when a client (using `ClientSession` from Chapter 1) calls `session.call_tool(\"add\", {\"a\": 5, \"b\": 3})` on our running `adder_server.py`:\n\n1.  **Client Sends Request:** The `ClientSession` creates an MCP `tools/call` request message (like `{\"method\": \"tools/call\", \"params\": {\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}, ...}`) and sends it over the communication channel (e.g., stdio).\n2.  **Transport Delivers:** The underlying transport layer ([Transports](06_transports.md)) delivers this raw message to the running `FastMCP` server.\n3.  **FastMCP Receives & Parses:** The `FastMCP` framework reads the incoming message. It understands the MCP protocol and parses the JSON to identify it's a `tools/call` request for the tool named `add` with arguments `a=5` and `b=3`.\n4.  **Finds the Tool Function:** `FastMCP` looks up the tool name \"add\" in its internal registry (where the `@mcp.tool()` decorator placed our `add` function).\n5.  **Validates Arguments:** It checks if the provided arguments (`{\"a\": 5, \"b\": 3}`) match the `inputSchema` it automatically generated from the `add(a: int, b: int)` type hints. In this case, they do. (If not, it would generate an error response).\n6.  **Calls Your Function:** `FastMCP` calls your actual Python function: `add(a=5, b=3)`. Your `print` statements will execute.\n7.  **Receives Return Value:** Your function returns the integer `8`.\n8.  **Formats Response:** `FastMCP` takes the return value (`8`) and automatically formats it into the standard MCP `CallToolResult` structure, likely as `TextContent`. The response message might look like `{\"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, ...}`.\n9.  **Sends Response:** `FastMCP` sends this JSON response message back over the communication channel.\n10. **Client Receives:** The client's `ClientSession` receives the response, parses it, and returns the `CallToolResult` object to the calling code.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Communication Channel\n    participant FastMCP as FastMCP Framework\n    participant AdderFunc as Your add() function\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"add\", args={a:5, b:3})\n    Transport->>+FastMCP: Deliver raw message\n    FastMCP->>FastMCP: Parse message, find 'add' tool registration\n    FastMCP->>FastMCP: Validate args against auto-generated schema\n    FastMCP->>+AdderFunc: Call add(a=5, b=3)\n    Note over AdderFunc: Prints logs, returns 8\n    AdderFunc-->>-FastMCP: Return result (8)\n    FastMCP->>FastMCP: Format 8 into MCP response (e.g., TextContent)\n    FastMCP->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\nYou don't *need* to know the internals, but it can be helpful. Inside `FastMCP` (see `src/mcp/server/fastmcp/server.py`):\n\n*   The `FastMCP` class initializes internal managers like `ToolManager`, `ResourceManager`, and `PromptManager`.\n*   The `@mcp.tool()` decorator calls the `add_tool` method on the `ToolManager`.\n*   `add_tool` uses Python's `inspect` module to look at your function's signature (parameters, type hints, docstring). It uses this information (often with help from the Pydantic library) to build the `MCPTool` definition, including the `inputSchema`. It stores this definition.\n*   `FastMCP` sets up handlers (like `self._mcp_server.call_tool()(self.call_tool)`) that map incoming MCP methods (`tools/call`) to internal `FastMCP` methods.\n*   The internal `call_tool` method asks the `ToolManager` to execute the registered tool function, passing the arguments.\n*   Helper functions like `_convert_to_content` take your Python function's return value and package it into the standard MCP content types (`TextContent`, `ImageContent`, etc.).\n\nThe key takeaway is that `FastMCP` uses Python's introspection capabilities (looking at function details) and decorators to automate the tedious parts of building an MCP server.\n\n## Adding Resources and Prompts\n\nJust like `@mcp.tool()`, `FastMCP` provides decorators for defining resources and prompts:\n\n*   **`@mcp.resource(uri)`:** Registers a function to provide data for a specific URI. The URI can even include template parameters (like `greeting://{name}`) that match function arguments. `FastMCP` handles mapping the URI request to your function. (See [Resources](03_resources.md)).\n\n    ```python\n    # From examples/fastmcp/readme-quickstart.py\n    @mcp.resource(\"greeting://{name}\")\n    def get_greeting(name: str) -> str:\n        \"\"\"Get a personalized greeting\"\"\"\n        return f\"Hello, {name}!\"\n    ```\n\n*   **`@mcp.prompt(name)`:** Registers a function that generates a list of prompt messages, often taking arguments defined by the function's parameters. `FastMCP` handles generating the `MCPPrompt` definition and rendering the messages when requested. (See [Prompts](04_prompts.md)).\n\n    ```python\n    # From examples/fastmcp/echo.py\n    @mcp.prompt(\"echo\")\n    def echo_prompt(text: str) -> str:\n        # FastMCP automatically converts the returned string\n        # into a User message in the GetPromptResult\n        return text\n    ```\n\nThese work on the same principle: write a Python function with type hints and a docstring, add the decorator, and `FastMCP` handles the MCP integration.\n\n## Conclusion\n\nCongratulations! You've taken your first step into building your own MCP server using the `FastMCP` framework. You learned:\n\n*   `FastMCP` simplifies server development by handling complex MCP protocol details automatically.\n*   You can define server capabilities (Tools, Resources, Prompts) using standard Python functions.\n*   Decorators like `@mcp.tool()`, `@mcp.resource()`, and `@mcp.prompt()` are the magic that registers your functions with `FastMCP`.\n*   `FastMCP` uses type hints and docstrings to automatically generate schemas and descriptions.\n*   Running the server is as simple as calling `mcp.run()`.\n\nYou now have the power to create your own \"assistants\" that can offer custom capabilities over the MCP protocol.\n\nBut how exactly do the client and server *talk* to each other? Whether it's stdio, HTTP/SSE, or something else, the communication channel is crucial. In the next chapter, we'll explore these communication methods: [Transports](06_transports.md).\n\n        Relevant Code Snippets:\n        --- File: examples/clients/simple-chatbot/mcp_simple_chatbot/main.py ---\nimport asyncio\nimport json\nimport logging\nimport os\nimport shutil\nfrom contextlib import AsyncExitStack\nfrom typing import Any\n\nimport httpx\nfrom dotenv import load_dotenv\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n)\n\n\nclass Configuration:\n    \"\"\"Manages configuration and environment variables for the MCP client.\"\"\"\n\n    def __init__(self) -> None:\n        \"\"\"Initialize configuration with environment variables.\"\"\"\n        self.load_env()\n        self.api_key = os.getenv(\"LLM_API_KEY\")\n\n    @staticmethod\n    def load_env() -> None:\n        \"\"\"Load environment variables from .env file.\"\"\"\n        load_dotenv()\n\n    @staticmethod\n    def load_config(file_path: str) -> dict[str, Any]:\n        \"\"\"Load server configuration from JSON file.\n\n        Args:\n            file_path: Path to the JSON configuration file.\n\n        Returns:\n            Dict containing server configuration.\n\n        Raises:\n            FileNotFoundError: If configuration file doesn't exist.\n            JSONDecodeError: If configuration file is invalid JSON.\n        \"\"\"\n        with open(file_path, \"r\") as f:\n            return json.load(f)\n\n    @property\n    def llm_api_key(self) -> str:\n        \"\"\"Get the LLM API key.\n\n        Returns:\n            The API key as a string.\n\n        Raises:\n            ValueError: If the API key is not found in environment variables.\n        \"\"\"\n        if not self.api_key:\n            raise ValueError(\"LLM_API_KEY not found in environment variables\")\n        return self.api_key\n\n\nclass Server:\n    \"\"\"Manages MCP server connections and tool execution.\"\"\"\n\n    def __init__(self, name: str, config: dict[str, Any]) -> None:\n        self.name: str = name\n        self.config: dict[str, Any] = config\n        self.stdio_context: Any | None = None\n        self.session: ClientSession | None = None\n        self._cleanup_lock: asyncio.Lock = asyncio.Lock()\n        self.exit_stack: AsyncExitStack = AsyncExitStack()\n\n    async def initialize(self) -> None:\n        \"\"\"Initialize the server connection.\"\"\"\n        command = (\n            shutil.which(\"npx\")\n            if self.config[\"command\"] == \"npx\"\n            else self.config[\"command\"]\n        )\n        if command is None:\n            raise ValueError(\"The command must be a valid string and cannot be None.\")\n\n        server_params = StdioServerParameters(\n            command=command,\n            args=self.config[\"args\"],\n            env={**os.environ, **self.config[\"env\"]}\n            if self.config.get(\"env\")\n            else None,\n        )\n        try:\n            stdio_transport = await self.exit_stack.enter_async_context(\n                stdio_client(server_params)\n            )\n            read, write = stdio_transport\n            session = await self.exit_stack.enter_async_context(\n                ClientSession(read, write)\n            )\n            await session.initialize()\n            self.session = session\n        except Exception as e:\n            logging.error(f\"Error initializing server {self.name}: {e}\")\n            await self.cleanup()\n            raise\n\n    async def list_tools(self) -> list[Any]:\n        \"\"\"List available tools from the server.\n\n        Returns:\n            A list of available tools.\n\n        Raises:\n            RuntimeError: If the server is not initialized.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        tools_response = await self.session.list_tools()\n        tools = []\n\n        for item in tools_response:\n            if isinstance(item, tuple) and item[0] == \"tools\":\n                tools.extend(\n                    Tool(tool.name, tool.description, tool.inputSchema)\n                    for tool in item[1]\n                )\n\n        return tools\n\n    async def execute_tool(\n        self,\n        tool_name: str,\n        arguments: dict[str, Any],\n        retries: int = 2,\n        delay: float = 1.0,\n    ) -> Any:\n        \"\"\"Execute a tool with retry mechanism.\n\n        Args:\n            tool_name: Name of the tool to execute.\n            arguments: Tool arguments.\n            retries: Number of retry attempts.\n            delay: Delay between retries in seconds.\n\n        Returns:\n            Tool execution result.\n\n        Raises:\n            RuntimeError: If server is not initialized.\n            Exception: If tool execution fails after all retries.\n        \"\"\"\n        if not self.session:\n            raise RuntimeError(f\"Server {self.name} not initialized\")\n\n        attempt = 0\n        while attempt < retries:\n            try:\n                logging.info(f\"Executing {tool_name}...\")\n                result = await self.session.call_tool(tool_name, arguments)\n\n                return result\n\n            except Exception as e:\n                attempt += 1\n                logging.warning(\n                    f\"Error executing tool: {e}. Attempt {attempt} of {retries}.\"\n                )\n                if attempt < retries:\n                    logging.info(f\"Retrying in {delay} seconds...\")\n                    await asyncio.sleep(delay)\n                else:\n                    logging.error(\"Max retries reached. Failing.\")\n                    raise\n\n    async def cleanup(self) -> None:\n        \"\"\"Clean up server resources.\"\"\"\n        async with self._cleanup_lock:\n            try:\n                await self.exit_stack.aclose()\n                self.session = None\n                self.stdio_context = None\n            except Exception as e:\n                logging.error(f\"Error during cleanup of server {self.name}: {e}\")\n\n\nclass Tool:\n    \"\"\"Represents a tool with its properties and formatting.\"\"\"\n\n    def __init__(\n        self, name: str, description: str, input_schema: dict[str, Any]\n    ) -> None:\n        self.name: str = name\n        self.description: str = description\n        self.input_schema: dict[str, Any] = input_schema\n\n    def format_for_llm(self) -> str:\n        \"\"\"Format tool information for LLM.\n\n        Returns:\n            A formatted string describing the tool.\n        \"\"\"\n        args_desc = []\n        if \"properties\" in self.input_schema:\n            for param_name, param_info in self.input_schema[\"properties\"].items():\n                arg_desc = (\n                    f\"- {param_name}: {param_info.get('description', 'No description')}\"\n                )\n                if param_name in self.input_schema.get(\"required\", []):\n                    arg_desc += \" (required)\"\n                args_desc.append(arg_desc)\n\n        return f\"\"\"\nTool: {self.name}\nDescription: {self.description}\nArguments:\n{chr(10).join(args_desc)}\n\"\"\"\n\n\nclass LLMClient:\n    \"\"\"Manages communication with the LLM provider.\"\"\"\n\n    def __init__(self, api_key: str) -> None:\n        self.api_key: str = api_key\n\n    def get_response(self, messages: list[dict[str, str]]) -> str:\n        \"\"\"Get a response from the LLM.\n\n        Args:\n            messages: A list of message dictionaries.\n\n        Returns:\n            The LLM's response as a string.\n\n        Raises:\n            httpx.RequestError: If the request to the LLM fails.\n        \"\"\"\n        url = \"https://api.groq.com/openai/v1/chat/completions\"\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n        payload = {\n            \"messages\": messages,\n            \"model\": \"llama-3.2-90b-vision-preview\",\n            \"temperature\": 0.7,\n            \"max_tokens\": 4096,\n            \"top_p\": 1,\n            \"stream\": False,\n            \"stop\": None,\n        }\n\n        try:\n            with httpx.Client() as client:\n                response = client.post(url, headers=headers, json=payload)\n                response.raise_for_status()\n                data = response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n\n        except httpx.RequestError as e:\n            error_message = f\"Error getting LLM response: {str(e)}\"\n            logging.error(error_message)\n\n            if isinstance(e, httpx.HTTPStatusError):\n                status_code = e.response.status_code\n                logging.error(f\"Status code: {status_code}\")\n                logging.error(f\"Response details: {e.response.text}\")\n\n            return (\n                f\"I encountered an error: {error_message}. \"\n                \"Please try again or rephrase your request.\"\n            )\n\n\nclass ChatSession:\n    \"\"\"Orchestrates the interaction between user, LLM, and tools.\"\"\"\n\n    def __init__(self, servers: list[Server], llm_client: LLMClient) -> None:\n        self.servers: list[Server] = servers\n        self.llm_client: LLMClient = llm_client\n\n    async def cleanup_servers(self) -> None:\n        \"\"\"Clean up all servers properly.\"\"\"\n        cleanup_tasks = [\n            asyncio.create_task(server.cleanup()) for server in self.servers\n        ]\n        if cleanup_tasks:\n            try:\n                await asyncio.gather(*cleanup_tasks, return_exceptions=True)\n            except Exception as e:\n                logging.warning(f\"Warning during final cleanup: {e}\")\n\n    async def process_llm_response(self, llm_response: str) -> str:\n        \"\"\"Process the LLM response and execute tools if needed.\n\n        Args:\n            llm_response: The response from the LLM.\n\n        Returns:\n            The result of tool execution or the original response.\n        \"\"\"\n        import json\n\n        try:\n            tool_call = json.loads(llm_response)\n            if \"tool\" in tool_call and \"arguments\" in tool_call:\n                logging.info(f\"Executing tool: {tool_call['tool']}\")\n                logging.info(f\"With arguments: {tool_call['arguments']}\")\n\n                for server in self.servers:\n                    tools = await server.list_tools()\n                    if any(tool.name == tool_call[\"tool\"] for tool in tools):\n                        try:\n                            result = await server.execute_tool(\n                                tool_call[\"tool\"], tool_call[\"arguments\"]\n                            )\n\n                            if isinstance(result, dict) and \"progress\" in result:\n                                progress = result[\"progress\"]\n                                total = result[\"total\"]\n                                percentage = (progress / total) * 100\n                                logging.info(\n                                    f\"Progress: {progress}/{total} \"\n                                    f\"({percentage:.1f}%)\"\n                                )\n\n                            return f\"Tool execution result: {result}\"\n                        except Exception as e:\n                            error_msg = f\"Error executing tool: {str(e)}\"\n                            logging.error(error_msg)\n                            return error_msg\n\n                return f\"No server found with tool: {tool_call['tool']}\"\n            return llm_response\n        except json.JSONDecodeError:\n            return llm_response\n\n    async def start(self) -> None:\n        \"\"\"Main chat session handler.\"\"\"\n        try:\n            for server in self.servers:\n                try:\n                    await server.initialize()\n                except Exception as e:\n                    logging.error(f\"Failed to initialize server: {e}\")\n                    await self.cleanup_servers()\n                    return\n\n            all_tools = []\n            for server in self.servers:\n                tools = await server.list_tools()\n                all_tools.extend(tools)\n\n            tools_description = \"\\n\".join([tool.format_for_llm() for tool in all_tools])\n\n            system_message = (\n                \"You are a helpful assistant with access to these tools:\\n\\n\"\n                f\"{tools_description}\\n\"\n                \"Choose the appropriate tool based on the user's question. \"\n                \"If no tool is needed, reply directly.\\n\\n\"\n                \"IMPORTANT: When you need to use a tool, you must ONLY respond with \"\n                \"the exact JSON object format below, nothing else:\\n\"\n                \"{\\n\"\n                '    \"tool\": \"tool-name\",\\n'\n                '    \"arguments\": {\\n'\n                '        \"argument-name\": \"value\"\\n'\n                \"    }\\n\"\n                \"}\\n\\n\"\n                \"After receiving a tool's response:\\n\"\n                \"1. Transform the raw data into a natural, conversational response\\n\"\n                \"2. Keep responses concise but informative\\n\"\n                \"3. Focus on the most relevant information\\n\"\n                \"4. Use appropriate context from the user's question\\n\"\n                \"5. Avoid simply repeating the raw data\\n\\n\"\n                \"Please use only the tools that are explicitly defined above.\"\n            )\n\n            messages = [{\"role\": \"system\", \"content\": system_message}]\n\n            while True:\n                try:\n                    user_input = input(\"You: \").strip().lower()\n                    if user_input in [\"quit\", \"exit\"]:\n                        logging.info(\"\\nExiting...\")\n                        break\n\n                    messages.append({\"role\": \"user\", \"content\": user_input})\n\n                    llm_response = self.llm_client.get_response(messages)\n                    logging.info(\"\\nAssistant: %s\", llm_response)\n\n                    result = await self.process_llm_response(llm_response)\n\n                    if result != llm_response:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n                        messages.append({\"role\": \"system\", \"content\": result})\n\n                        final_response = self.llm_client.get_response(messages)\n                        logging.info(\"\\nFinal response: %s\", final_response)\n                        messages.append(\n                            {\"role\": \"assistant\", \"content\": final_response}\n                        )\n                    else:\n                        messages.append({\"role\": \"assistant\", \"content\": llm_response})\n\n                except KeyboardInterrupt:\n                    logging.info(\"\\nExiting...\")\n                    break\n\n        finally:\n            await self.cleanup_servers()\n\n\nasync def main() -> None:\n    \"\"\"Initialize and run the chat session.\"\"\"\n    config = Configuration()\n    server_config = config.load_config(\"servers_config.json\")\n    servers = [\n        Server(name, srv_config)\n        for name, srv_config in server_config[\"mcpServers\"].items()\n    ]\n    llm_client = LLMClient(config.llm_api_key)\n    chat_session = ChatSession(servers, llm_client)\n    await chat_session.start()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n--- File: examples/servers/simple-prompt/mcp_simple_prompt/server.py ---\nimport anyio\nimport click\nimport mcp.types as types\nfrom mcp.server.lowlevel import Server\n\n\ndef create_messages(\n    context: str | None = None, topic: str | None = None\n) -> list[types.PromptMessage]:\n    \"\"\"Create the messages for the prompt.\"\"\"\n    messages = []\n\n    # Add context if provided\n    if context:\n        messages.append(\n            types.PromptMessage(\n                role=\"user\",\n                content=types.TextContent(\n                    type=\"text\", text=f\"Here is some relevant context: {context}\"\n                ),\n            )\n        )\n\n    # Add the main prompt\n    prompt = \"Please help me with \"\n    if topic:\n        prompt += f\"the following topic: {topic}\"\n    else:\n        prompt += \"whatever questions I may have.\"\n\n    messages.append(\n        types.PromptMessage(\n            role=\"user\", content=types.TextContent(type=\"text\", text=prompt)\n        )\n    )\n\n    return messages\n\n\n@click.command()\n@click.option(\"--port\", default=8000, help=\"Port to listen on for SSE\")\n@click.option(\n    \"--transport\",\n    type=click.Choice([\"stdio\", \"sse\"]),\n    default=\"stdio\",\n    help=\"Transport type\",\n)\ndef main(port: int, transport: str) -> int:\n    app = Server(\"mcp-simple-prompt\")\n\n    @app.list_prompts()\n    async def list_prompts() -> list[types.Prompt]:\n        return [\n            types.Prompt(\n                name=\"simple\",\n                description=\"A simple prompt that can take optional context and topic \"\n                \"arguments\",\n                arguments=[\n                    types.PromptArgument(\n                        name=\"context\",\n                        description=\"Additional context to consider\",\n                        required=False,\n                    ),\n                    types.PromptArgument(\n                        name=\"topic\",\n                        description=\"Specific topic to focus on\",\n                        required=False,\n                    ),\n                ],\n            )\n        ]\n\n    @app.get_prompt()\n    async def get_prompt(\n        name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        if name != \"simple\":\n            raise ValueError(f\"Unknown prompt: {name}\")\n\n        if arguments is None:\n            arguments = {}\n\n        return types.GetPromptResult(\n            messages=create_messages(\n                context=arguments.get(\"context\"), topic=arguments.get(\"topic\")\n            ),\n            description=\"A simple prompt with optional context and topic arguments\",\n        )\n\n    if transport == \"sse\":\n        from mcp.server.sse import SseServerTransport\n        from starlette.applications import Starlette\n        from starlette.routing import Mount, Route\n\n        sse = SseServerTransport(\"/messages/\")\n\n        async def handle_sse(request):\n            async with sse.connect_sse(\n                request.scope, request.receive, request._send\n            ) as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        starlette_app = Starlette(\n            debug=True,\n            routes=[\n                Route(\"/sse\", endpoint=handle_sse),\n                Mount(\"/messages/\", app=sse.handle_post_message),\n            ],\n        )\n\n        import uvicorn\n\n        uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n    else:\n        from mcp.server.stdio import stdio_server\n\n        async def arun():\n            async with stdio_server() as streams:\n                await app.run(\n                    streams[0], streams[1], app.create_initialization_options()\n                )\n\n        anyio.run(arun)\n\n    return 0\n\n\n--- File: src/mcp/client/sse.py ---\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import urljoin, urlparse\n\nimport anyio\nimport httpx\nfrom anyio.abc import TaskStatus\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom httpx_sse import aconnect_sse\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\ndef remove_request_params(url: str) -> str:\n    return urljoin(url, urlparse(url).path)\n\n\n@asynccontextmanager\nasync def sse_client(\n    url: str,\n    headers: dict[str, Any] | None = None,\n    timeout: float = 5,\n    sse_read_timeout: float = 60 * 5,\n):\n    \"\"\"\n    Client transport for SSE.\n\n    `sse_read_timeout` determines how long (in seconds) the client will wait for a new\n    event before disconnecting. All other HTTP operations are controlled by `timeout`.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async with anyio.create_task_group() as tg:\n        try:\n            logger.info(f\"Connecting to SSE endpoint: {remove_request_params(url)}\")\n            async with httpx.AsyncClient(headers=headers) as client:\n                async with aconnect_sse(\n                    client,\n                    \"GET\",\n                    url,\n                    timeout=httpx.Timeout(timeout, read=sse_read_timeout),\n                ) as event_source:\n                    event_source.response.raise_for_status()\n                    logger.debug(\"SSE connection established\")\n\n                    async def sse_reader(\n                        task_status: TaskStatus[str] = anyio.TASK_STATUS_IGNORED,\n                    ):\n                        try:\n                            async for sse in event_source.aiter_sse():\n                                logger.debug(f\"Received SSE event: {sse.event}\")\n                                match sse.event:\n                                    case \"endpoint\":\n                                        endpoint_url = urljoin(url, sse.data)\n                                        logger.info(\n                                            f\"Received endpoint URL: {endpoint_url}\"\n                                        )\n\n                                        url_parsed = urlparse(url)\n                                        endpoint_parsed = urlparse(endpoint_url)\n                                        if (\n                                            url_parsed.netloc != endpoint_parsed.netloc\n                                            or url_parsed.scheme\n                                            != endpoint_parsed.scheme\n                                        ):\n                                            error_msg = (\n                                                \"Endpoint origin does not match \"\n                                                f\"connection origin: {endpoint_url}\"\n                                            )\n                                            logger.error(error_msg)\n                                            raise ValueError(error_msg)\n\n                                        task_status.started(endpoint_url)\n\n                                    case \"message\":\n                                        try:\n                                            message = types.JSONRPCMessage.model_validate_json(  # noqa: E501\n                                                sse.data\n                                            )\n                                            logger.debug(\n                                                f\"Received server message: {message}\"\n                                            )\n                                        except Exception as exc:\n                                            logger.error(\n                                                f\"Error parsing server message: {exc}\"\n                                            )\n                                            await read_stream_writer.send(exc)\n                                            continue\n\n                                        await read_stream_writer.send(message)\n                                    case _:\n                                        logger.warning(\n                                            f\"Unknown SSE event: {sse.event}\"\n                                        )\n                        except Exception as exc:\n                            logger.error(f\"Error in sse_reader: {exc}\")\n                            await read_stream_writer.send(exc)\n                        finally:\n                            await read_stream_writer.aclose()\n\n                    async def post_writer(endpoint_url: str):\n                        try:\n                            async with write_stream_reader:\n                                async for message in write_stream_reader:\n                                    logger.debug(f\"Sending client message: {message}\")\n                                    response = await client.post(\n                                        endpoint_url,\n                                        json=message.model_dump(\n                                            by_alias=True,\n                                            mode=\"json\",\n                                            exclude_none=True,\n                                        ),\n                                    )\n                                    response.raise_for_status()\n                                    logger.debug(\n                                        \"Client message sent successfully: \"\n                                        f\"{response.status_code}\"\n                                    )\n                        except Exception as exc:\n                            logger.error(f\"Error in post_writer: {exc}\")\n                        finally:\n                            await write_stream.aclose()\n\n                    endpoint_url = await tg.start(sse_reader)\n                    logger.info(\n                        f\"Starting post writer with endpoint URL: {endpoint_url}\"\n                    )\n                    tg.start_soon(post_writer, endpoint_url)\n\n                    try:\n                        yield read_stream, write_stream\n                    finally:\n                        tg.cancel_scope.cancel()\n        finally:\n            await read_stream_writer.aclose()\n            await write_stream.aclose()\n\n\n--- File: src/mcp/client/stdio/__init__.py ---\nimport os\nimport sys\nfrom contextlib import asynccontextmanager\nfrom pathlib import Path\nfrom typing import Literal, TextIO\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom anyio.streams.text import TextReceiveStream\nfrom pydantic import BaseModel, Field\n\nimport mcp.types as types\n\nfrom .win32 import (\n    create_windows_process,\n    get_windows_executable_command,\n    terminate_windows_process,\n)\n\n# Environment variables to inherit by default\nDEFAULT_INHERITED_ENV_VARS = (\n    [\n        \"APPDATA\",\n        \"HOMEDRIVE\",\n        \"HOMEPATH\",\n        \"LOCALAPPDATA\",\n        \"PATH\",\n        \"PROCESSOR_ARCHITECTURE\",\n        \"SYSTEMDRIVE\",\n        \"SYSTEMROOT\",\n        \"TEMP\",\n        \"USERNAME\",\n        \"USERPROFILE\",\n    ]\n    if sys.platform == \"win32\"\n    else [\"HOME\", \"LOGNAME\", \"PATH\", \"SHELL\", \"TERM\", \"USER\"]\n)\n\n\ndef get_default_environment() -> dict[str, str]:\n    \"\"\"\n    Returns a default environment object including only environment variables deemed\n    safe to inherit.\n    \"\"\"\n    env: dict[str, str] = {}\n\n    for key in DEFAULT_INHERITED_ENV_VARS:\n        value = os.environ.get(key)\n        if value is None:\n            continue\n\n        if value.startswith(\"()\"):\n            # Skip functions, which are a security risk\n            continue\n\n        env[key] = value\n\n    return env\n\n\nclass StdioServerParameters(BaseModel):\n    command: str\n    \"\"\"The executable to run to start the server.\"\"\"\n\n    args: list[str] = Field(default_factory=list)\n    \"\"\"Command line arguments to pass to the executable.\"\"\"\n\n    env: dict[str, str] | None = None\n    \"\"\"\n    The environment to use when spawning the process.\n\n    If not specified, the result of get_default_environment() will be used.\n    \"\"\"\n\n    cwd: str | Path | None = None\n    \"\"\"The working directory to use when spawning the process.\"\"\"\n\n    encoding: str = \"utf-8\"\n    \"\"\"\n    The text encoding used when sending/receiving messages to the server\n\n    defaults to utf-8\n    \"\"\"\n\n    encoding_error_handler: Literal[\"strict\", \"ignore\", \"replace\"] = \"strict\"\n    \"\"\"\n    The text encoding error handler.\n\n    See https://docs.python.org/3/library/codecs.html#codec-base-classes for\n    explanations of possible values\n    \"\"\"\n\n\n@asynccontextmanager\nasync def stdio_client(server: StdioServerParameters, errlog: TextIO = sys.stderr):\n    \"\"\"\n    Client transport for stdio: this will connect to a server by spawning a\n    process and communicating with it over stdin/stdout.\n    \"\"\"\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    command = _get_executable_command(server.command)\n\n    # Open process with stderr piped for capture\n    process = await _create_platform_compatible_process(\n        command=command,\n        args=server.args,\n        env=(\n            {**get_default_environment(), **server.env}\n            if server.env is not None\n            else get_default_environment()\n        ),\n        errlog=errlog,\n        cwd=server.cwd,\n    )\n\n    async def stdout_reader():\n        assert process.stdout, \"Opened process is missing stdout\"\n\n        try:\n            async with read_stream_writer:\n                buffer = \"\"\n                async for chunk in TextReceiveStream(\n                    process.stdout,\n                    encoding=server.encoding,\n                    errors=server.encoding_error_handler,\n                ):\n                    lines = (buffer + chunk).split(\"\\n\")\n                    buffer = lines.pop()\n\n                    for line in lines:\n                        try:\n                            message = types.JSONRPCMessage.model_validate_json(line)\n                        except Exception as exc:\n                            await read_stream_writer.send(exc)\n                            continue\n\n                        await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdin_writer():\n        assert process.stdin, \"Opened process is missing stdin\"\n\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await process.stdin.send(\n                        (json + \"\\n\").encode(\n                            encoding=server.encoding,\n                            errors=server.encoding_error_handler,\n                        )\n                    )\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with (\n        anyio.create_task_group() as tg,\n        process,\n    ):\n        tg.start_soon(stdout_reader)\n        tg.start_soon(stdin_writer)\n        try:\n            yield read_stream, write_stream\n        finally:\n            # Clean up process to prevent any dangling orphaned processes\n            if sys.platform == \"win32\":\n                await terminate_windows_process(process)\n            else:\n                process.terminate()\n\n\ndef _get_executable_command(command: str) -> str:\n    \"\"\"\n    Get the correct executable command normalized for the current platform.\n\n    Args:\n        command: Base command (e.g., 'uvx', 'npx')\n\n    Returns:\n        str: Platform-appropriate command\n    \"\"\"\n    if sys.platform == \"win32\":\n        return get_windows_executable_command(command)\n    else:\n        return command\n\n\nasync def _create_platform_compatible_process(\n    command: str,\n    args: list[str],\n    env: dict[str, str] | None = None,\n    errlog: TextIO = sys.stderr,\n    cwd: Path | str | None = None,\n):\n    \"\"\"\n    Creates a subprocess in a platform-compatible way.\n    Returns a process handle.\n    \"\"\"\n    if sys.platform == \"win32\":\n        process = await create_windows_process(command, args, env, errlog, cwd)\n    else:\n        process = await anyio.open_process(\n            [command, *args], env=env, stderr=errlog, cwd=cwd\n        )\n\n    return process\n\n\n--- File: src/mcp/client/websocket.py ---\nimport json\nimport logging\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom websockets.asyncio.client import connect as ws_connect\nfrom websockets.typing import Subprotocol\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_client(\n    url: str,\n) -> AsyncGenerator[\n    tuple[\n        MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        MemoryObjectSendStream[types.JSONRPCMessage],\n    ],\n    None,\n]:\n    \"\"\"\n    WebSocket client transport for MCP, symmetrical to the server version.\n\n    Connects to 'url' using the 'mcp' subprotocol, then yields:\n        (read_stream, write_stream)\n\n    - read_stream: As you read from this stream, you'll receive either valid\n      JSONRPCMessage objects or Exception objects (when validation fails).\n    - write_stream: Write JSONRPCMessage objects to this stream to send them\n      over the WebSocket to the server.\n    \"\"\"\n\n    # Create two in-memory streams:\n    # - One for incoming messages (read_stream, written by ws_reader)\n    # - One for outgoing messages (write_stream, read by ws_writer)\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    # Connect using websockets, requesting the \"mcp\" subprotocol\n    async with ws_connect(url, subprotocols=[Subprotocol(\"mcp\")]) as ws:\n\n        async def ws_reader():\n            \"\"\"\n            Reads text messages from the WebSocket, parses them as JSON-RPC messages,\n            and sends them into read_stream_writer.\n            \"\"\"\n            async with read_stream_writer:\n                async for raw_text in ws:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(raw_text)\n                        await read_stream_writer.send(message)\n                    except ValidationError as exc:\n                        # If JSON parse or model validation fails, send the exception\n                        await read_stream_writer.send(exc)\n\n        async def ws_writer():\n            \"\"\"\n            Reads JSON-RPC messages from write_stream_reader and\n            sends them to the server.\n            \"\"\"\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    # Convert to a dict, then to JSON\n                    msg_dict = message.model_dump(\n                        by_alias=True, mode=\"json\", exclude_none=True\n                    )\n                    await ws.send(json.dumps(msg_dict))\n\n        async with anyio.create_task_group() as tg:\n            # Start reader and writer tasks\n            tg.start_soon(ws_reader)\n            tg.start_soon(ws_writer)\n\n            # Yield the receive/send streams\n            yield (read_stream, write_stream)\n\n            # Once the caller's 'async with' block exits, we shut down\n            tg.cancel_scope.cancel()\n\n\n--- File: src/mcp/server/sse.py ---\n\"\"\"\nSSE Server Transport Module\n\nThis module implements a Server-Sent Events (SSE) transport layer for MCP servers.\n\nExample usage:\n```\n    # Create an SSE transport at an endpoint\n    sse = SseServerTransport(\"/messages/\")\n\n    # Create Starlette routes for SSE and message handling\n    routes = [\n        Route(\"/sse\", endpoint=handle_sse),\n        Mount(\"/messages/\", app=sse.handle_post_message),\n    ]\n\n    # Define handler functions\n    async def handle_sse(request):\n        async with sse.connect_sse(\n            request.scope, request.receive, request._send\n        ) as streams:\n            await app.run(\n                streams[0], streams[1], app.create_initialization_options()\n            )\n\n    # Create and run Starlette app\n    starlette_app = Starlette(routes=routes)\n    uvicorn.run(starlette_app, host=\"0.0.0.0\", port=port)\n```\n\nSee SseServerTransport class documentation for more details.\n\"\"\"\n\nimport logging\nfrom contextlib import asynccontextmanager\nfrom typing import Any\nfrom urllib.parse import quote\nfrom uuid import UUID, uuid4\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import ValidationError\nfrom sse_starlette import EventSourceResponse\nfrom starlette.requests import Request\nfrom starlette.responses import Response\nfrom starlette.types import Receive, Scope, Send\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\nclass SseServerTransport:\n    \"\"\"\n    SSE server transport for MCP. This class provides _two_ ASGI applications,\n    suitable to be used with a framework like Starlette and a server like Hypercorn:\n\n        1. connect_sse() is an ASGI application which receives incoming GET requests,\n           and sets up a new SSE stream to send server messages to the client.\n        2. handle_post_message() is an ASGI application which receives incoming POST\n           requests, which should contain client messages that link to a\n           previously-established SSE session.\n    \"\"\"\n\n    _endpoint: str\n    _read_stream_writers: dict[\n        UUID, MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n    ]\n\n    def __init__(self, endpoint: str) -> None:\n        \"\"\"\n        Creates a new SSE server transport, which will direct the client to POST\n        messages to the relative or absolute URL given.\n        \"\"\"\n\n        super().__init__()\n        self._endpoint = endpoint\n        self._read_stream_writers = {}\n        logger.debug(f\"SseServerTransport initialized with endpoint: {endpoint}\")\n\n    @asynccontextmanager\n    async def connect_sse(self, scope: Scope, receive: Receive, send: Send):\n        if scope[\"type\"] != \"http\":\n            logger.error(\"connect_sse received non-HTTP request\")\n            raise ValueError(\"connect_sse can only handle HTTP requests\")\n\n        logger.debug(\"Setting up SSE connection\")\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n        read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n        write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n        read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n        write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n        session_id = uuid4()\n        session_uri = f\"{quote(self._endpoint)}?session_id={session_id.hex}\"\n        self._read_stream_writers[session_id] = read_stream_writer\n        logger.debug(f\"Created new session with ID: {session_id}\")\n\n        sse_stream_writer, sse_stream_reader = anyio.create_memory_object_stream[\n            dict[str, Any]\n        ](0)\n\n        async def sse_writer():\n            logger.debug(\"Starting SSE writer\")\n            async with sse_stream_writer, write_stream_reader:\n                await sse_stream_writer.send({\"event\": \"endpoint\", \"data\": session_uri})\n                logger.debug(f\"Sent endpoint event: {session_uri}\")\n\n                async for message in write_stream_reader:\n                    logger.debug(f\"Sending message via SSE: {message}\")\n                    await sse_stream_writer.send(\n                        {\n                            \"event\": \"message\",\n                            \"data\": message.model_dump_json(\n                                by_alias=True, exclude_none=True\n                            ),\n                        }\n                    )\n\n        async with anyio.create_task_group() as tg:\n            response = EventSourceResponse(\n                content=sse_stream_reader, data_sender_callable=sse_writer\n            )\n            logger.debug(\"Starting SSE response task\")\n            tg.start_soon(response, scope, receive, send)\n\n            logger.debug(\"Yielding read and write streams\")\n            yield (read_stream, write_stream)\n\n    async def handle_post_message(\n        self, scope: Scope, receive: Receive, send: Send\n    ) -> None:\n        logger.debug(\"Handling POST message\")\n        request = Request(scope, receive)\n\n        session_id_param = request.query_params.get(\"session_id\")\n        if session_id_param is None:\n            logger.warning(\"Received request without session_id\")\n            response = Response(\"session_id is required\", status_code=400)\n            return await response(scope, receive, send)\n\n        try:\n            session_id = UUID(hex=session_id_param)\n            logger.debug(f\"Parsed session ID: {session_id}\")\n        except ValueError:\n            logger.warning(f\"Received invalid session ID: {session_id_param}\")\n            response = Response(\"Invalid session ID\", status_code=400)\n            return await response(scope, receive, send)\n\n        writer = self._read_stream_writers.get(session_id)\n        if not writer:\n            logger.warning(f\"Could not find session for ID: {session_id}\")\n            response = Response(\"Could not find session\", status_code=404)\n            return await response(scope, receive, send)\n\n        body = await request.body()\n        logger.debug(f\"Received JSON: {body}\")\n\n        try:\n            message = types.JSONRPCMessage.model_validate_json(body)\n            logger.debug(f\"Validated client message: {message}\")\n        except ValidationError as err:\n            logger.error(f\"Failed to parse message: {err}\")\n            response = Response(\"Could not parse message\", status_code=400)\n            await response(scope, receive, send)\n            await writer.send(err)\n            return\n\n        logger.debug(f\"Sending message to writer: {message}\")\n        response = Response(\"Accepted\", status_code=202)\n        await response(scope, receive, send)\n        await writer.send(message)\n\n\n--- File: src/mcp/server/stdio.py ---\n\"\"\"\nStdio Server Transport Module\n\nThis module provides functionality for creating an stdio-based transport layer\nthat can be used to communicate with an MCP client through standard input/output\nstreams.\n\nExample usage:\n```\n    async def run_server():\n        async with stdio_server() as (read_stream, write_stream):\n            # read_stream contains incoming JSONRPCMessages from stdin\n            # write_stream allows sending JSONRPCMessages to stdout\n            server = await create_my_server()\n            await server.run(read_stream, write_stream, init_options)\n\n    anyio.run(run_server)\n```\n\"\"\"\n\nimport sys\nfrom contextlib import asynccontextmanager\nfrom io import TextIOWrapper\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\n\nimport mcp.types as types\n\n\n@asynccontextmanager\nasync def stdio_server(\n    stdin: anyio.AsyncFile[str] | None = None,\n    stdout: anyio.AsyncFile[str] | None = None,\n):\n    \"\"\"\n    Server transport for stdio: this communicates with an MCP client by reading\n    from the current process' stdin and writing to stdout.\n    \"\"\"\n    # Purposely not using context managers for these, as we don't want to close\n    # standard process handles. Encoding of stdin/stdout as text streams on\n    # python is platform-dependent (Windows is particularly problematic), so we\n    # re-wrap the underlying binary stream to ensure UTF-8.\n    if not stdin:\n        stdin = anyio.wrap_file(TextIOWrapper(sys.stdin.buffer, encoding=\"utf-8\"))\n    if not stdout:\n        stdout = anyio.wrap_file(TextIOWrapper(sys.stdout.buffer, encoding=\"utf-8\"))\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def stdin_reader():\n        try:\n            async with read_stream_writer:\n                async for line in stdin:\n                    try:\n                        message = types.JSONRPCMessage.model_validate_json(line)\n                    except Exception as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(message)\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async def stdout_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    json = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await stdout.write(json + \"\\n\")\n                    await stdout.flush()\n        except anyio.ClosedResourceError:\n            await anyio.lowlevel.checkpoint()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(stdin_reader)\n        tg.start_soon(stdout_writer)\n        yield read_stream, write_stream\n\n\n--- File: src/mcp/server/websocket.py ---\nimport logging\nfrom contextlib import asynccontextmanager\n\nimport anyio\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic_core import ValidationError\nfrom starlette.types import Receive, Scope, Send\nfrom starlette.websockets import WebSocket\n\nimport mcp.types as types\n\nlogger = logging.getLogger(__name__)\n\n\n@asynccontextmanager\nasync def websocket_server(scope: Scope, receive: Receive, send: Send):\n    \"\"\"\n    WebSocket server transport for MCP. This is an ASGI application, suitable to be\n    used with a framework like Starlette and a server like Hypercorn.\n    \"\"\"\n\n    websocket = WebSocket(scope, receive, send)\n    await websocket.accept(subprotocol=\"mcp\")\n\n    read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception]\n    read_stream_writer: MemoryObjectSendStream[types.JSONRPCMessage | Exception]\n\n    write_stream: MemoryObjectSendStream[types.JSONRPCMessage]\n    write_stream_reader: MemoryObjectReceiveStream[types.JSONRPCMessage]\n\n    read_stream_writer, read_stream = anyio.create_memory_object_stream(0)\n    write_stream, write_stream_reader = anyio.create_memory_object_stream(0)\n\n    async def ws_reader():\n        try:\n            async with read_stream_writer:\n                async for msg in websocket.iter_text():\n                    try:\n                        client_message = types.JSONRPCMessage.model_validate_json(msg)\n                    except ValidationError as exc:\n                        await read_stream_writer.send(exc)\n                        continue\n\n                    await read_stream_writer.send(client_message)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async def ws_writer():\n        try:\n            async with write_stream_reader:\n                async for message in write_stream_reader:\n                    obj = message.model_dump_json(by_alias=True, exclude_none=True)\n                    await websocket.send_text(obj)\n        except anyio.ClosedResourceError:\n            await websocket.close()\n\n    async with anyio.create_task_group() as tg:\n        tg.start_soon(ws_reader)\n        tg.start_soon(ws_writer)\n        yield (read_stream, write_stream)\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 6: Transports`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 6: Transports - The Delivery Service\n\nIn the last chapter, [FastMCP Server](05_fastmcp_server.md), we learned how to build our own MCP server assistant using the `FastMCP` framework. We saw how easy it is to define tools and have `FastMCP` handle the MCP protocol details.\n\nBut we skipped over a crucial detail: how does our client code, using the `ClientSession` from [Chapter 1: Client Session](01_client_session.md), actually *talk* to the server we built? They might be running on the same computer, or maybe the server is on a different machine across the internet. How do the request and response messages physically travel between them?\n\nThis is where **Transports** come in!\n\n## Motivation: Choosing Your Delivery Method\n\nImagine you wrote a letter (an MCP request message, like asking to call a tool). You need to send it to your friend (the MCP server). How do you get it there?\n\n*   You could hand it directly to them if they're right next to you (like a local program).\n*   You could use the postal service (like standard input/output for local programs).\n*   You could use a special courier service that provides continuous updates (like Server-Sent Events over the web).\n*   You could establish a direct, instant phone line (like WebSockets).\n\nEach method gets the letter delivered, but they work differently and are suited for different situations.\n\nIn the world of `python-sdk` and MCP, the **Transport** is the underlying communication mechanism chosen to send messages between the client and the server. It's the actual delivery service used for the MCP messages formatted by `ClientSession` and understood by the server.\n\n## What is a Transport?\n\nA **Transport** handles the low-level details of setting up a connection and sending/receiving the actual bytes that make up the MCP messages. Think of it as the plumbing that connects the client and the server.\n\nThe `ClientSession` we learned about in [Chapter 1: Client Session](01_client_session.md) is responsible for *formatting* the messages (like `tools/list` requests) and *interpreting* the replies. But it relies on a Transport to do the actual sending and receiving over the chosen communication channel.\n\nThe `python-sdk` provides built-in support for several common transports:\n\n1.  **Standard Input/Output (Stdio):**\n    *   **Analogy:** Sending letters via postal mail to a local address, or talking to a command-line tool.\n    *   **How it works:** The client starts the server as a local process (like running a command in your terminal). The client sends messages to the server's standard input (\"stdin\") and reads responses from the server's standard output (\"stdout\").\n    *   **Use Case:** Perfect when the client application needs to start and manage a local MCP server process running on the same machine. This is what we implicitly used in Chapter 1 and Chapter 5!\n\n2.  **Server-Sent Events (SSE over HTTP):**\n    *   **Analogy:** A continuous news ticker feed delivered over the web.\n    *   **How it works:** The client connects to a specific web URL on the server using standard HTTP. The server can then *push* messages (events) to the client over this connection whenever it wants. To send messages *to* the server, the client typically makes separate standard HTTP POST requests to another URL defined by the server.\n    *   **Use Case:** Good for web-based scenarios where the server needs to proactively send updates or notifications to the client (like progress updates, new messages).\n\n3.  **WebSockets:**\n    *   **Analogy:** A direct, two-way phone line established over the web.\n    *   **How it works:** The client and server establish a persistent, full-duplex (two-way) connection. Both the client and server can send messages to each other at any time over this single connection once it's established.\n    *   **Use Case:** Ideal for real-time, interactive applications requiring low-latency communication in both directions (e.g., chat applications, live dashboards, collaborative tools).\n\n## Using Transports with `ClientSession`\n\nThe beauty is that `ClientSession` itself doesn't need to know the complex details of *how* stdio, SSE, or WebSockets work. It just needs two things:\n\n*   A way to **read** incoming messages (`read_stream`).\n*   A way to **write** outgoing messages (`write_stream`).\n\nThe `python-sdk` provides helper functions (like `stdio_client`, `sse_client`, `websocket_client`) that set up the specific transport mechanism and give you back these two streams. You then pass these streams to `ClientSession`.\n\nLet's see how you'd use each transport helper.\n\n**1. Stdio Transport**\n\nThis is the one we saw in [Chapter 1: Client Session](01_client_session.md). We define how to run the server command and then use `stdio_client`.\n\n```python\n# Using stdio transport (like in Chapter 1)\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client # <-- Stdio helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_stdio(server_command: str):\n    server_params = StdioServerParameters(command=server_command)\n\n    # stdio_client starts the process and provides streams\n    async with stdio_client(server_params) as (read_stream, write_stream):\n        logging.info(\"Stdio transport connected.\")\n        # ClientSession uses the streams from stdio_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via stdio.\")\n            # ... use session.list_tools(), session.call_tool(), etc. ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via stdio: {tools}\")\n\n# Example: asyncio.run(connect_via_stdio(\"python your_server_script.py\"))\n```\n\n*   `StdioServerParameters` tells `stdio_client` *how* to run the server process.\n*   `stdio_client(...)` starts the server process and yields the `read_stream` (connected to server's stdout) and `write_stream` (connected to server's stdin).\n*   `ClientSession(read_stream, write_stream)` uses these streams to communicate.\n\n**2. SSE Transport**\n\nTo connect to an MCP server running with an SSE endpoint (e.g., hosted by a web server), you use `sse_client`.\n\n```python\n# Using SSE transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client # <-- SSE helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_sse(server_sse_url: str):\n    # sse_client connects to the URL and provides streams\n    # It handles the SSE event listening and POSTing for sending\n    async with sse_client(server_sse_url) as (read_stream, write_stream):\n        logging.info(\"SSE transport connected.\")\n        # ClientSession uses the streams from sse_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via SSE.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via SSE: {tools}\")\n\n# Example: asyncio.run(connect_via_sse(\"http://localhost:8000/sse\"))\n```\n\n*   `sse_client(server_sse_url)` connects to the specified HTTP endpoint, sets up the SSE listener for incoming messages (`read_stream`), and prepares to send outgoing messages via HTTP POST (`write_stream`).\n*   `ClientSession` uses these streams, unaware of the underlying HTTP/SSE mechanics.\n\n**3. WebSocket Transport**\n\nTo connect to an MCP server using a WebSocket endpoint, you use `websocket_client`.\n\n```python\n# Using WebSocket transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.websocket import websocket_client # <-- WebSocket helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_websocket(server_ws_url: str):\n    # websocket_client establishes the WebSocket connection\n    async with websocket_client(server_ws_url) as (read_stream, write_stream):\n        logging.info(\"WebSocket transport connected.\")\n        # ClientSession uses the streams from websocket_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via WebSocket.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via WebSocket: {tools}\")\n\n# Example: asyncio.run(connect_via_websocket(\"ws://localhost:8000/ws\"))\n```\n\n*   `websocket_client(server_ws_url)` connects to the WebSocket URL (`ws://...` or `wss://...`) and yields streams representing the bidirectional connection.\n*   `ClientSession` uses these streams to send and receive MCP messages over the WebSocket.\n\nNotice the pattern: You choose the appropriate `*_client` helper based on how the server is exposed, but the way you create and use `ClientSession` remains the same!\n\n```python\n# General Pattern\n# 1. Choose and configure the transport helper\n# async with stdio_client(...) as (read_stream, write_stream):\n# async with sse_client(...) as (read_stream, write_stream):\n# async with websocket_client(...) as (read_stream, write_stream):\n\n# 2. Pass the provided streams to ClientSession\n#     async with ClientSession(read_stream, write_stream) as session:\n#         # 3. Initialize and use the session as usual\n#         await session.initialize()\n#         result = await session.list_tools()\n#         # ...\n```\n\n## Under the Hood: How Transports Provide Streams\n\nHow do these `*_client` functions hide the complexity? They act as adapters.\n\n1.  **Connection:** Each helper establishes the specific type of connection (starts a process, makes HTTP requests, opens a WebSocket).\n2.  **Internal Queues:** They typically use internal memory queues or buffers (often using `anyio.create_memory_object_stream`).\n3.  **Reading:** They run background tasks that continuously read raw data from the connection (process stdout, SSE events, WebSocket frames), parse it into MCP messages (or detect errors), and put these messages/errors onto the internal *read* queue. The `read_stream` you get simply reads from this queue.\n4.  **Writing:** When you send a message to the `write_stream`, it goes into an internal *write* queue. Another background task reads from this queue, formats the message according to the transport's protocol (e.g., adds newline for stdio, makes HTTP POST for SSE, sends WebSocket frame), and sends it over the actual connection.\n\nThis setup decouples `ClientSession` from the specific transport details. `ClientSession` just interacts with the clean `read_stream` and `write_stream` interfaces.\n\n```mermaid\ngraph LR\n    subgraph Your Code\n        A[ClientSession]\n    end\n    subgraph Transport Helper (e.g., stdio_client)\n        direction LR\n        B[read_stream] -- Reads From --> C(Internal Read Queue)\n        D(Internal Write Queue) -- Writes To --> E[write_stream]\n        F[Background Reader Task] -- Writes To --> C\n        G[Background Writer Task] -- Reads From --> D\n    end\n    subgraph Actual Connection\n        H[Process Pipes / HTTP / WebSocket]\n    end\n    subgraph Server\n        I[MCP Server]\n    end\n\n    A -- Uses --> B\n    A -- Uses --> E\n\n    F -- Reads From --> H\n    G -- Writes To --> H\n    H -- Communicates With --> I\n```\n\nThe `Transport Helper` box manages the `Internal Queues` and `Background Tasks` to bridge the gap between the `Actual Connection` and the simple `read_stream`/`write_stream` used by `ClientSession`.\n\nYou can explore the implementation details in the `mcp.client` subdirectories:\n*   `src/mcp/client/stdio/`: Code for starting processes and managing stdin/stdout.\n*   `src/mcp/client/sse.py`: Code for handling SSE connections and HTTP POST requests.\n*   `src/mcp/client/websocket.py`: Code for managing WebSocket communication.\n\nSimilarly, the `mcp.server` directory contains corresponding transport implementations for the server side (e.g., `src/mcp/server/stdio.py`, `src/mcp/server/sse.py`, `src/mcp/server/websocket.py`) that frameworks like [FastMCP Server](05_fastmcp_server.md) can use.\n\n## Conclusion\n\nIn this chapter, we peeled back a layer to understand **Transports** \u2013 the delivery mechanisms that connect MCP clients and servers.\n\n*   Transports handle the low-level communication (stdio, SSE, WebSocket).\n*   They provide simple `read_stream` and `write_stream` interfaces for `ClientSession`.\n*   Helper functions (`stdio_client`, `sse_client`, `websocket_client`) manage the specifics of each transport type.\n*   You choose the transport based on how your client needs to connect to the server.\n\nUnderstanding transports gives you flexibility in how you deploy and connect your MCP applications.\n\nNow that we've covered the core components \u2013 sessions, tools, resources, prompts, servers, and transports \u2013 let's look at a more advanced server concept: how servers built with `FastMCP` can maintain state and context across multiple requests within a single client session. We'll explore this in the next chapter: [FastMCP Context](07_fastmcp_context.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"FastMCP Context\". This is Chapter 7.\n\n        Concept Details:\n        - Description:\n        An object provided within FastMCP tool and resource functions that gives access to server capabilities during request handling. It allows the function logic to interact with the MCP session, for example, to send log messages back to the client, report progress on long-running tasks, or even read other resources provided by the same server. Think of it as a special toolkit automatically handed to your tool/resource function when it's called, containing utilities relevant to the current request.\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n---\n# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).\n---\n# Chapter 5: FastMCP Server - Building Your Own Assistant\n\nIn the previous chapters ([Client Session](01_client_session.md), [Tools](02_tools.md), [Resources](03_resources.md), [Prompts](04_prompts.md)), we learned how to *use* an existing MCP server from our Python code (the client). We learned how to connect, list capabilities, call tools, read resources, and get prompts.\n\nBut what if *you* want to create your own assistant? Maybe you have a special calculation, access to unique data, or a cool function you want to make available to other programs (or even to an AI like Claude). How do you build the *server* side?\n\n## Motivation: Making Server Creation Easy\n\nBuilding a server that speaks the MCP protocol might sound complicated. You'd need to handle:\n\n*   Receiving messages (like JSON-RPC requests).\n*   Parsing those messages correctly.\n*   Understanding requests like \"tools/list\" or \"tools/call\".\n*   Routing the request to the correct piece of your code.\n*   Generating JSON Schemas to describe your tool inputs.\n*   Formatting your results back into the correct MCP response format.\n*   Sending the response back.\n\nThat's a lot of boilerplate code just to handle the communication! Wouldn't it be great if you could just write your core Python functions (the actual logic for your tools, resources, etc.) and have something else handle all the tricky MCP communication details?\n\nThat's exactly what **FastMCP Server** does!\n\n**`FastMCP` is a high-level, user-friendly framework within `python-sdk` for creating MCP servers.** Think of it like a helpful construction kit for building your MCP assistant. You define the *capabilities* (tools, resources, prompts) using simple Python functions and special markers called **decorators** (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`). `FastMCP` then automatically handles all the underlying MCP protocol details, generates the necessary descriptions (like JSON schemas), and routes incoming client requests to your functions.\n\nIt lets you focus on writing the *logic* for what your server can *do*, not the complex setup of how it *communicates*.\n\n**Our Goal:** Let's build a very simple MCP server that offers just one tool: adding two numbers.\n\n## Building Our Adder Server: Step-by-Step\n\nWe'll use `FastMCP` to create a server that a client can connect to and ask to add numbers.\n\n**1. Import `FastMCP`**\n\nFirst, we need to import the main class from the library.\n\n```python\n# adder_server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# We'll add more code below...\n```\n\nThis line brings the `FastMCP` framework into our script.\n\n**2. Create a `FastMCP` Instance**\n\nNext, we create an instance of the `FastMCP` class. This represents our server. We can give it a name.\n\n```python\n# adder_server.py (continued)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n# We'll define tools next...\n```\n\nThink of `mcp` as the main control object for our server definition.\n\n**3. Define the Tool Logic (A Simple Function)**\n\nWe need the actual code that performs the addition. This is just a regular Python function. Notice the use of type hints (`a: int`, `b: int`, `-> int`). These are important!\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n# This function contains the logic for our tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Now, how do we tell FastMCP this is a tool?\n```\n\nThis is the core logic. It takes two integers and returns their sum. The `print` statements are just for us to see when it gets called.\n\n**4. Register the Function as a Tool (The Magic Decorator!)**\n\nHow does `FastMCP` know that our `add` function should be made available as an MCP tool? We use the `@mcp.tool()` **decorator**. A decorator is like a sticky note you put on a function to give it special powers or register it with a framework.\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n@mcp.tool() # <-- This is the FastMCP decorator!\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# We're almost ready to run the server...\n```\n\nBy adding `@mcp.tool()` right above our `add` function, we tell `FastMCP`:\n\n*   \"Make this function available as an MCP tool.\"\n*   **Automatically use the function name (`add`) as the tool name.** (You can specify a different name like `@mcp.tool(\"adder\")` if you want).\n*   **Automatically use the function's docstring (`\"\"\"Adds two numbers together.\"\"\"`) as the tool's description.**\n*   **Automatically look at the type hints (`a: int`, `b: int`) to generate the JSON `inputSchema`** that tells clients what arguments are needed (an integer `a` and an integer `b`).\n*   **Automatically understand the return type (`-> int`)** to know what kind of result the tool produces.\n\nThis single line saves us from writing a lot of configuration code!\n\n**5. Run the Server**\n\nFinally, we need to tell our server to start running and listening for connections.\n\n```python\n# adder_server.py (complete example)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Add this block to start the server when the script is run\nif __name__ == \"__main__\":\n    print(\"Starting Adder Server...\")\n    # This runs the server, listening for connections via stdio by default\n    mcp.run()\n    print(\"Adder Server stopped.\")\n\n```\n\nThe `if __name__ == \"__main__\":` block is standard Python practice to make code run only when the script is executed directly. `mcp.run()` starts the `FastMCP` server. By default, it uses \"stdio\" (standard input/output) as the communication channel, meaning it behaves like a command-line application waiting for requests on its input and sending responses to its output. We'll learn more about different ways servers and clients can communicate in the next chapter on [Transports](06_transports.md).\n\nIf you save this as `adder_server.py` and run it (e.g., `python adder_server.py`), it will print \"Starting Adder Server...\" and wait. You could then use a client (like the one from [Chapter 2: Tools](02_tools.md), configured to run this script) to connect and call the `add` tool!\n\n## What Happens Under the Hood?\n\nLet's trace what happens when a client (using `ClientSession` from Chapter 1) calls `session.call_tool(\"add\", {\"a\": 5, \"b\": 3})` on our running `adder_server.py`:\n\n1.  **Client Sends Request:** The `ClientSession` creates an MCP `tools/call` request message (like `{\"method\": \"tools/call\", \"params\": {\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}, ...}`) and sends it over the communication channel (e.g., stdio).\n2.  **Transport Delivers:** The underlying transport layer ([Transports](06_transports.md)) delivers this raw message to the running `FastMCP` server.\n3.  **FastMCP Receives & Parses:** The `FastMCP` framework reads the incoming message. It understands the MCP protocol and parses the JSON to identify it's a `tools/call` request for the tool named `add` with arguments `a=5` and `b=3`.\n4.  **Finds the Tool Function:** `FastMCP` looks up the tool name \"add\" in its internal registry (where the `@mcp.tool()` decorator placed our `add` function).\n5.  **Validates Arguments:** It checks if the provided arguments (`{\"a\": 5, \"b\": 3}`) match the `inputSchema` it automatically generated from the `add(a: int, b: int)` type hints. In this case, they do. (If not, it would generate an error response).\n6.  **Calls Your Function:** `FastMCP` calls your actual Python function: `add(a=5, b=3)`. Your `print` statements will execute.\n7.  **Receives Return Value:** Your function returns the integer `8`.\n8.  **Formats Response:** `FastMCP` takes the return value (`8`) and automatically formats it into the standard MCP `CallToolResult` structure, likely as `TextContent`. The response message might look like `{\"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, ...}`.\n9.  **Sends Response:** `FastMCP` sends this JSON response message back over the communication channel.\n10. **Client Receives:** The client's `ClientSession` receives the response, parses it, and returns the `CallToolResult` object to the calling code.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Communication Channel\n    participant FastMCP as FastMCP Framework\n    participant AdderFunc as Your add() function\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"add\", args={a:5, b:3})\n    Transport->>+FastMCP: Deliver raw message\n    FastMCP->>FastMCP: Parse message, find 'add' tool registration\n    FastMCP->>FastMCP: Validate args against auto-generated schema\n    FastMCP->>+AdderFunc: Call add(a=5, b=3)\n    Note over AdderFunc: Prints logs, returns 8\n    AdderFunc-->>-FastMCP: Return result (8)\n    FastMCP->>FastMCP: Format 8 into MCP response (e.g., TextContent)\n    FastMCP->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\nYou don't *need* to know the internals, but it can be helpful. Inside `FastMCP` (see `src/mcp/server/fastmcp/server.py`):\n\n*   The `FastMCP` class initializes internal managers like `ToolManager`, `ResourceManager`, and `PromptManager`.\n*   The `@mcp.tool()` decorator calls the `add_tool` method on the `ToolManager`.\n*   `add_tool` uses Python's `inspect` module to look at your function's signature (parameters, type hints, docstring). It uses this information (often with help from the Pydantic library) to build the `MCPTool` definition, including the `inputSchema`. It stores this definition.\n*   `FastMCP` sets up handlers (like `self._mcp_server.call_tool()(self.call_tool)`) that map incoming MCP methods (`tools/call`) to internal `FastMCP` methods.\n*   The internal `call_tool` method asks the `ToolManager` to execute the registered tool function, passing the arguments.\n*   Helper functions like `_convert_to_content` take your Python function's return value and package it into the standard MCP content types (`TextContent`, `ImageContent`, etc.).\n\nThe key takeaway is that `FastMCP` uses Python's introspection capabilities (looking at function details) and decorators to automate the tedious parts of building an MCP server.\n\n## Adding Resources and Prompts\n\nJust like `@mcp.tool()`, `FastMCP` provides decorators for defining resources and prompts:\n\n*   **`@mcp.resource(uri)`:** Registers a function to provide data for a specific URI. The URI can even include template parameters (like `greeting://{name}`) that match function arguments. `FastMCP` handles mapping the URI request to your function. (See [Resources](03_resources.md)).\n\n    ```python\n    # From examples/fastmcp/readme-quickstart.py\n    @mcp.resource(\"greeting://{name}\")\n    def get_greeting(name: str) -> str:\n        \"\"\"Get a personalized greeting\"\"\"\n        return f\"Hello, {name}!\"\n    ```\n\n*   **`@mcp.prompt(name)`:** Registers a function that generates a list of prompt messages, often taking arguments defined by the function's parameters. `FastMCP` handles generating the `MCPPrompt` definition and rendering the messages when requested. (See [Prompts](04_prompts.md)).\n\n    ```python\n    # From examples/fastmcp/echo.py\n    @mcp.prompt(\"echo\")\n    def echo_prompt(text: str) -> str:\n        # FastMCP automatically converts the returned string\n        # into a User message in the GetPromptResult\n        return text\n    ```\n\nThese work on the same principle: write a Python function with type hints and a docstring, add the decorator, and `FastMCP` handles the MCP integration.\n\n## Conclusion\n\nCongratulations! You've taken your first step into building your own MCP server using the `FastMCP` framework. You learned:\n\n*   `FastMCP` simplifies server development by handling complex MCP protocol details automatically.\n*   You can define server capabilities (Tools, Resources, Prompts) using standard Python functions.\n*   Decorators like `@mcp.tool()`, `@mcp.resource()`, and `@mcp.prompt()` are the magic that registers your functions with `FastMCP`.\n*   `FastMCP` uses type hints and docstrings to automatically generate schemas and descriptions.\n*   Running the server is as simple as calling `mcp.run()`.\n\nYou now have the power to create your own \"assistants\" that can offer custom capabilities over the MCP protocol.\n\nBut how exactly do the client and server *talk* to each other? Whether it's stdio, HTTP/SSE, or something else, the communication channel is crucial. In the next chapter, we'll explore these communication methods: [Transports](06_transports.md).\n---\n# Chapter 6: Transports - The Delivery Service\n\nIn the last chapter, [FastMCP Server](05_fastmcp_server.md), we learned how to build our own MCP server assistant using the `FastMCP` framework. We saw how easy it is to define tools and have `FastMCP` handle the MCP protocol details.\n\nBut we skipped over a crucial detail: how does our client code, using the `ClientSession` from [Chapter 1: Client Session](01_client_session.md), actually *talk* to the server we built? They might be running on the same computer, or maybe the server is on a different machine across the internet. How do the request and response messages physically travel between them?\n\nThis is where **Transports** come in!\n\n## Motivation: Choosing Your Delivery Method\n\nImagine you wrote a letter (an MCP request message, like asking to call a tool). You need to send it to your friend (the MCP server). How do you get it there?\n\n*   You could hand it directly to them if they're right next to you (like a local program).\n*   You could use the postal service (like standard input/output for local programs).\n*   You could use a special courier service that provides continuous updates (like Server-Sent Events over the web).\n*   You could establish a direct, instant phone line (like WebSockets).\n\nEach method gets the letter delivered, but they work differently and are suited for different situations.\n\nIn the world of `python-sdk` and MCP, the **Transport** is the underlying communication mechanism chosen to send messages between the client and the server. It's the actual delivery service used for the MCP messages formatted by `ClientSession` and understood by the server.\n\n## What is a Transport?\n\nA **Transport** handles the low-level details of setting up a connection and sending/receiving the actual bytes that make up the MCP messages. Think of it as the plumbing that connects the client and the server.\n\nThe `ClientSession` we learned about in [Chapter 1: Client Session](01_client_session.md) is responsible for *formatting* the messages (like `tools/list` requests) and *interpreting* the replies. But it relies on a Transport to do the actual sending and receiving over the chosen communication channel.\n\nThe `python-sdk` provides built-in support for several common transports:\n\n1.  **Standard Input/Output (Stdio):**\n    *   **Analogy:** Sending letters via postal mail to a local address, or talking to a command-line tool.\n    *   **How it works:** The client starts the server as a local process (like running a command in your terminal). The client sends messages to the server's standard input (\"stdin\") and reads responses from the server's standard output (\"stdout\").\n    *   **Use Case:** Perfect when the client application needs to start and manage a local MCP server process running on the same machine. This is what we implicitly used in Chapter 1 and Chapter 5!\n\n2.  **Server-Sent Events (SSE over HTTP):**\n    *   **Analogy:** A continuous news ticker feed delivered over the web.\n    *   **How it works:** The client connects to a specific web URL on the server using standard HTTP. The server can then *push* messages (events) to the client over this connection whenever it wants. To send messages *to* the server, the client typically makes separate standard HTTP POST requests to another URL defined by the server.\n    *   **Use Case:** Good for web-based scenarios where the server needs to proactively send updates or notifications to the client (like progress updates, new messages).\n\n3.  **WebSockets:**\n    *   **Analogy:** A direct, two-way phone line established over the web.\n    *   **How it works:** The client and server establish a persistent, full-duplex (two-way) connection. Both the client and server can send messages to each other at any time over this single connection once it's established.\n    *   **Use Case:** Ideal for real-time, interactive applications requiring low-latency communication in both directions (e.g., chat applications, live dashboards, collaborative tools).\n\n## Using Transports with `ClientSession`\n\nThe beauty is that `ClientSession` itself doesn't need to know the complex details of *how* stdio, SSE, or WebSockets work. It just needs two things:\n\n*   A way to **read** incoming messages (`read_stream`).\n*   A way to **write** outgoing messages (`write_stream`).\n\nThe `python-sdk` provides helper functions (like `stdio_client`, `sse_client`, `websocket_client`) that set up the specific transport mechanism and give you back these two streams. You then pass these streams to `ClientSession`.\n\nLet's see how you'd use each transport helper.\n\n**1. Stdio Transport**\n\nThis is the one we saw in [Chapter 1: Client Session](01_client_session.md). We define how to run the server command and then use `stdio_client`.\n\n```python\n# Using stdio transport (like in Chapter 1)\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client # <-- Stdio helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_stdio(server_command: str):\n    server_params = StdioServerParameters(command=server_command)\n\n    # stdio_client starts the process and provides streams\n    async with stdio_client(server_params) as (read_stream, write_stream):\n        logging.info(\"Stdio transport connected.\")\n        # ClientSession uses the streams from stdio_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via stdio.\")\n            # ... use session.list_tools(), session.call_tool(), etc. ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via stdio: {tools}\")\n\n# Example: asyncio.run(connect_via_stdio(\"python your_server_script.py\"))\n```\n\n*   `StdioServerParameters` tells `stdio_client` *how* to run the server process.\n*   `stdio_client(...)` starts the server process and yields the `read_stream` (connected to server's stdout) and `write_stream` (connected to server's stdin).\n*   `ClientSession(read_stream, write_stream)` uses these streams to communicate.\n\n**2. SSE Transport**\n\nTo connect to an MCP server running with an SSE endpoint (e.g., hosted by a web server), you use `sse_client`.\n\n```python\n# Using SSE transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client # <-- SSE helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_sse(server_sse_url: str):\n    # sse_client connects to the URL and provides streams\n    # It handles the SSE event listening and POSTing for sending\n    async with sse_client(server_sse_url) as (read_stream, write_stream):\n        logging.info(\"SSE transport connected.\")\n        # ClientSession uses the streams from sse_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via SSE.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via SSE: {tools}\")\n\n# Example: asyncio.run(connect_via_sse(\"http://localhost:8000/sse\"))\n```\n\n*   `sse_client(server_sse_url)` connects to the specified HTTP endpoint, sets up the SSE listener for incoming messages (`read_stream`), and prepares to send outgoing messages via HTTP POST (`write_stream`).\n*   `ClientSession` uses these streams, unaware of the underlying HTTP/SSE mechanics.\n\n**3. WebSocket Transport**\n\nTo connect to an MCP server using a WebSocket endpoint, you use `websocket_client`.\n\n```python\n# Using WebSocket transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.websocket import websocket_client # <-- WebSocket helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_websocket(server_ws_url: str):\n    # websocket_client establishes the WebSocket connection\n    async with websocket_client(server_ws_url) as (read_stream, write_stream):\n        logging.info(\"WebSocket transport connected.\")\n        # ClientSession uses the streams from websocket_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via WebSocket.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via WebSocket: {tools}\")\n\n# Example: asyncio.run(connect_via_websocket(\"ws://localhost:8000/ws\"))\n```\n\n*   `websocket_client(server_ws_url)` connects to the WebSocket URL (`ws://...` or `wss://...`) and yields streams representing the bidirectional connection.\n*   `ClientSession` uses these streams to send and receive MCP messages over the WebSocket.\n\nNotice the pattern: You choose the appropriate `*_client` helper based on how the server is exposed, but the way you create and use `ClientSession` remains the same!\n\n```python\n# General Pattern\n# 1. Choose and configure the transport helper\n# async with stdio_client(...) as (read_stream, write_stream):\n# async with sse_client(...) as (read_stream, write_stream):\n# async with websocket_client(...) as (read_stream, write_stream):\n\n# 2. Pass the provided streams to ClientSession\n#     async with ClientSession(read_stream, write_stream) as session:\n#         # 3. Initialize and use the session as usual\n#         await session.initialize()\n#         result = await session.list_tools()\n#         # ...\n```\n\n## Under the Hood: How Transports Provide Streams\n\nHow do these `*_client` functions hide the complexity? They act as adapters.\n\n1.  **Connection:** Each helper establishes the specific type of connection (starts a process, makes HTTP requests, opens a WebSocket).\n2.  **Internal Queues:** They typically use internal memory queues or buffers (often using `anyio.create_memory_object_stream`).\n3.  **Reading:** They run background tasks that continuously read raw data from the connection (process stdout, SSE events, WebSocket frames), parse it into MCP messages (or detect errors), and put these messages/errors onto the internal *read* queue. The `read_stream` you get simply reads from this queue.\n4.  **Writing:** When you send a message to the `write_stream`, it goes into an internal *write* queue. Another background task reads from this queue, formats the message according to the transport's protocol (e.g., adds newline for stdio, makes HTTP POST for SSE, sends WebSocket frame), and sends it over the actual connection.\n\nThis setup decouples `ClientSession` from the specific transport details. `ClientSession` just interacts with the clean `read_stream` and `write_stream` interfaces.\n\n```mermaid\ngraph LR\n    subgraph Your Code\n        A[ClientSession]\n    end\n    subgraph Transport Helper (e.g., stdio_client)\n        direction LR\n        B[read_stream] -- Reads From --> C(Internal Read Queue)\n        D(Internal Write Queue) -- Writes To --> E[write_stream]\n        F[Background Reader Task] -- Writes To --> C\n        G[Background Writer Task] -- Reads From --> D\n    end\n    subgraph Actual Connection\n        H[Process Pipes / HTTP / WebSocket]\n    end\n    subgraph Server\n        I[MCP Server]\n    end\n\n    A -- Uses --> B\n    A -- Uses --> E\n\n    F -- Reads From --> H\n    G -- Writes To --> H\n    H -- Communicates With --> I\n```\n\nThe `Transport Helper` box manages the `Internal Queues` and `Background Tasks` to bridge the gap between the `Actual Connection` and the simple `read_stream`/`write_stream` used by `ClientSession`.\n\nYou can explore the implementation details in the `mcp.client` subdirectories:\n*   `src/mcp/client/stdio/`: Code for starting processes and managing stdin/stdout.\n*   `src/mcp/client/sse.py`: Code for handling SSE connections and HTTP POST requests.\n*   `src/mcp/client/websocket.py`: Code for managing WebSocket communication.\n\nSimilarly, the `mcp.server` directory contains corresponding transport implementations for the server side (e.g., `src/mcp/server/stdio.py`, `src/mcp/server/sse.py`, `src/mcp/server/websocket.py`) that frameworks like [FastMCP Server](05_fastmcp_server.md) can use.\n\n## Conclusion\n\nIn this chapter, we peeled back a layer to understand **Transports** \u2013 the delivery mechanisms that connect MCP clients and servers.\n\n*   Transports handle the low-level communication (stdio, SSE, WebSocket).\n*   They provide simple `read_stream` and `write_stream` interfaces for `ClientSession`.\n*   Helper functions (`stdio_client`, `sse_client`, `websocket_client`) manage the specifics of each transport type.\n*   You choose the transport based on how your client needs to connect to the server.\n\nUnderstanding transports gives you flexibility in how you deploy and connect your MCP applications.\n\nNow that we've covered the core components \u2013 sessions, tools, resources, prompts, servers, and transports \u2013 let's look at a more advanced server concept: how servers built with `FastMCP` can maintain state and context across multiple requests within a single client session. We'll explore this in the next chapter: [FastMCP Context](07_fastmcp_context.md).\n\n        Relevant Code Snippets:\n        --- File: examples/fastmcp/memory.py ---\n# /// script\n# dependencies = [\"pydantic-ai-slim[openai]\", \"asyncpg\", \"numpy\", \"pgvector\"]\n# ///\n\n# uv pip install 'pydantic-ai-slim[openai]' asyncpg numpy pgvector\n\n\"\"\"\nRecursive memory system inspired by the human brain's clustering of memories.\nUses OpenAI's 'text-embedding-3-small' model and pgvector for efficient\nsimilarity search.\n\"\"\"\n\nimport asyncio\nimport math\nimport os\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom typing import Annotated, Self\n\nimport asyncpg\nimport numpy as np\nfrom openai import AsyncOpenAI\nfrom pgvector.asyncpg import register_vector  # Import register_vector\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent\n\nfrom mcp.server.fastmcp import FastMCP\n\nMAX_DEPTH = 5\nSIMILARITY_THRESHOLD = 0.7\nDECAY_FACTOR = 0.99\nREINFORCEMENT_FACTOR = 1.1\n\nDEFAULT_LLM_MODEL = \"openai:gpt-4o\"\nDEFAULT_EMBEDDING_MODEL = \"text-embedding-3-small\"\n\nmcp = FastMCP(\n    \"memory\",\n    dependencies=[\n        \"pydantic-ai-slim[openai]\",\n        \"asyncpg\",\n        \"numpy\",\n        \"pgvector\",\n    ],\n)\n\nDB_DSN = \"postgresql://postgres:postgres@localhost:54320/memory_db\"\n# reset memory with rm ~/.fastmcp/{USER}/memory/*\nPROFILE_DIR = (\n    Path.home() / \".fastmcp\" / os.environ.get(\"USER\", \"anon\") / \"memory\"\n).resolve()\nPROFILE_DIR.mkdir(parents=True, exist_ok=True)\n\n\ndef cosine_similarity(a: list[float], b: list[float]) -> float:\n    a_array = np.array(a, dtype=np.float64)\n    b_array = np.array(b, dtype=np.float64)\n    return np.dot(a_array, b_array) / (\n        np.linalg.norm(a_array) * np.linalg.norm(b_array)\n    )\n\n\nasync def do_ai[T](\n    user_prompt: str,\n    system_prompt: str,\n    result_type: type[T] | Annotated,\n    deps=None,\n) -> T:\n    agent = Agent(\n        DEFAULT_LLM_MODEL,\n        system_prompt=system_prompt,\n        result_type=result_type,\n    )\n    result = await agent.run(user_prompt, deps=deps)\n    return result.data\n\n\n@dataclass\nclass Deps:\n    openai: AsyncOpenAI\n    pool: asyncpg.Pool\n\n\nasync def get_db_pool() -> asyncpg.Pool:\n    async def init(conn):\n        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n        await register_vector(conn)\n\n    pool = await asyncpg.create_pool(DB_DSN, init=init)\n    return pool\n\n\nclass MemoryNode(BaseModel):\n    id: int | None = None\n    content: str\n    summary: str = \"\"\n    importance: float = 1.0\n    access_count: int = 0\n    timestamp: float = Field(\n        default_factory=lambda: datetime.now(timezone.utc).timestamp()\n    )\n    embedding: list[float]\n\n    @classmethod\n    async def from_content(cls, content: str, deps: Deps):\n        embedding = await get_embedding(content, deps)\n        return cls(content=content, embedding=embedding)\n\n    async def save(self, deps: Deps):\n        async with deps.pool.acquire() as conn:\n            if self.id is None:\n                result = await conn.fetchrow(\n                    \"\"\"\n                    INSERT INTO memories (content, summary, importance, access_count,\n                        timestamp, embedding)\n                    VALUES ($1, $2, $3, $4, $5, $6)\n                    RETURNING id\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                )\n                self.id = result[\"id\"]\n            else:\n                await conn.execute(\n                    \"\"\"\n                    UPDATE memories\n                    SET content = $1, summary = $2, importance = $3,\n                        access_count = $4, timestamp = $5, embedding = $6\n                    WHERE id = $7\n                    \"\"\",\n                    self.content,\n                    self.summary,\n                    self.importance,\n                    self.access_count,\n                    self.timestamp,\n                    self.embedding,\n                    self.id,\n                )\n\n    async def merge_with(self, other: Self, deps: Deps):\n        self.content = await do_ai(\n            f\"{self.content}\\n\\n{other.content}\",\n            \"Combine the following two texts into a single, coherent text.\",\n            str,\n            deps,\n        )\n        self.importance += other.importance\n        self.access_count += other.access_count\n        self.embedding = [(a + b) / 2 for a, b in zip(self.embedding, other.embedding)]\n        self.summary = await do_ai(\n            self.content, \"Summarize the following text concisely.\", str, deps\n        )\n        await self.save(deps)\n        # Delete the merged node from the database\n        if other.id is not None:\n            await delete_memory(other.id, deps)\n\n    def get_effective_importance(self):\n        return self.importance * (1 + math.log(self.access_count + 1))\n\n\nasync def get_embedding(text: str, deps: Deps) -> list[float]:\n    embedding_response = await deps.openai.embeddings.create(\n        input=text,\n        model=DEFAULT_EMBEDDING_MODEL,\n    )\n    return embedding_response.data[0].embedding\n\n\nasync def delete_memory(memory_id: int, deps: Deps):\n    async with deps.pool.acquire() as conn:\n        await conn.execute(\"DELETE FROM memories WHERE id = $1\", memory_id)\n\n\nasync def add_memory(content: str, deps: Deps):\n    new_memory = await MemoryNode.from_content(content, deps)\n    await new_memory.save(deps)\n\n    similar_memories = await find_similar_memories(new_memory.embedding, deps)\n    for memory in similar_memories:\n        if memory.id != new_memory.id:\n            await new_memory.merge_with(memory, deps)\n\n    await update_importance(new_memory.embedding, deps)\n\n    await prune_memories(deps)\n\n    return f\"Remembered: {content}\"\n\n\nasync def find_similar_memories(embedding: list[float], deps: Deps) -> list[MemoryNode]:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, content, summary, importance, access_count, timestamp, embedding\n            FROM memories\n            ORDER BY embedding <-> $1\n            LIMIT 5\n            \"\"\",\n            embedding,\n        )\n    memories = [\n        MemoryNode(\n            id=row[\"id\"],\n            content=row[\"content\"],\n            summary=row[\"summary\"],\n            importance=row[\"importance\"],\n            access_count=row[\"access_count\"],\n            timestamp=row[\"timestamp\"],\n            embedding=row[\"embedding\"],\n        )\n        for row in rows\n    ]\n    return memories\n\n\nasync def update_importance(user_embedding: list[float], deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"SELECT id, importance, access_count, embedding FROM memories\"\n        )\n        for row in rows:\n            memory_embedding = row[\"embedding\"]\n            similarity = cosine_similarity(user_embedding, memory_embedding)\n            if similarity > SIMILARITY_THRESHOLD:\n                new_importance = row[\"importance\"] * REINFORCEMENT_FACTOR\n                new_access_count = row[\"access_count\"] + 1\n            else:\n                new_importance = row[\"importance\"] * DECAY_FACTOR\n                new_access_count = row[\"access_count\"]\n            await conn.execute(\n                \"\"\"\n                UPDATE memories\n                SET importance = $1, access_count = $2\n                WHERE id = $3\n                \"\"\",\n                new_importance,\n                new_access_count,\n                row[\"id\"],\n            )\n\n\nasync def prune_memories(deps: Deps):\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT id, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            OFFSET $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n        for row in rows:\n            await conn.execute(\"DELETE FROM memories WHERE id = $1\", row[\"id\"])\n\n\nasync def display_memory_tree(deps: Deps) -> str:\n    async with deps.pool.acquire() as conn:\n        rows = await conn.fetch(\n            \"\"\"\n            SELECT content, summary, importance, access_count\n            FROM memories\n            ORDER BY importance DESC\n            LIMIT $1\n            \"\"\",\n            MAX_DEPTH,\n        )\n    result = \"\"\n    for row in rows:\n        effective_importance = row[\"importance\"] * (\n            1 + math.log(row[\"access_count\"] + 1)\n        )\n        summary = row[\"summary\"] or row[\"content\"]\n        result += f\"- {summary} (Importance: {effective_importance:.2f})\\n\"\n    return result\n\n\n@mcp.tool()\nasync def remember(\n    contents: list[str] = Field(\n        description=\"List of observations or memories to store\"\n    ),\n):\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    try:\n        return \"\\n\".join(\n            await asyncio.gather(*[add_memory(content, deps) for content in contents])\n        )\n    finally:\n        await deps.pool.close()\n\n\n@mcp.tool()\nasync def read_profile() -> str:\n    deps = Deps(openai=AsyncOpenAI(), pool=await get_db_pool())\n    profile = await display_memory_tree(deps)\n    await deps.pool.close()\n    return profile\n\n\nasync def initialize_database():\n    pool = await asyncpg.create_pool(\n        \"postgresql://postgres:postgres@localhost:54320/postgres\"\n    )\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"\"\"\n                SELECT pg_terminate_backend(pg_stat_activity.pid)\n                FROM pg_stat_activity\n                WHERE pg_stat_activity.datname = 'memory_db'\n                AND pid <> pg_backend_pid();\n            \"\"\")\n            await conn.execute(\"DROP DATABASE IF EXISTS memory_db;\")\n            await conn.execute(\"CREATE DATABASE memory_db;\")\n    finally:\n        await pool.close()\n\n    pool = await asyncpg.create_pool(DB_DSN)\n    try:\n        async with pool.acquire() as conn:\n            await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n\n            await register_vector(conn)\n\n            await conn.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS memories (\n                    id SERIAL PRIMARY KEY,\n                    content TEXT NOT NULL,\n                    summary TEXT,\n                    importance REAL NOT NULL,\n                    access_count INT NOT NULL,\n                    timestamp DOUBLE PRECISION NOT NULL,\n                    embedding vector(1536) NOT NULL\n                );\n                CREATE INDEX IF NOT EXISTS idx_memories_embedding ON memories\n                    USING hnsw (embedding vector_l2_ops);\n            \"\"\")\n    finally:\n        await pool.close()\n\n\nif __name__ == \"__main__\":\n    asyncio.run(initialize_database())\n\n\n--- File: src/mcp/server/fastmcp/server.py ---\n\"\"\"FastMCP - A more ergonomic interface for MCP servers.\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport inspect\nimport json\nimport re\nfrom collections.abc import AsyncIterator, Callable, Iterable, Sequence\nfrom contextlib import (\n    AbstractAsyncContextManager,\n    asynccontextmanager,\n)\nfrom itertools import chain\nfrom typing import Any, Generic, Literal\n\nimport anyio\nimport pydantic_core\nfrom pydantic import BaseModel, Field\nfrom pydantic.networks import AnyUrl\nfrom pydantic_settings import BaseSettings, SettingsConfigDict\nfrom starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.routing import Mount, Route\n\nfrom mcp.server.fastmcp.exceptions import ResourceError\nfrom mcp.server.fastmcp.prompts import Prompt, PromptManager\nfrom mcp.server.fastmcp.resources import FunctionResource, Resource, ResourceManager\nfrom mcp.server.fastmcp.tools import ToolManager\nfrom mcp.server.fastmcp.utilities.logging import configure_logging, get_logger\nfrom mcp.server.fastmcp.utilities.types import Image\nfrom mcp.server.lowlevel.helper_types import ReadResourceContents\nfrom mcp.server.lowlevel.server import LifespanResultT\nfrom mcp.server.lowlevel.server import Server as MCPServer\nfrom mcp.server.lowlevel.server import lifespan as default_lifespan\nfrom mcp.server.session import ServerSession, ServerSessionT\nfrom mcp.server.sse import SseServerTransport\nfrom mcp.server.stdio import stdio_server\nfrom mcp.shared.context import LifespanContextT, RequestContext\nfrom mcp.types import (\n    AnyFunction,\n    EmbeddedResource,\n    GetPromptResult,\n    ImageContent,\n    TextContent,\n)\nfrom mcp.types import Prompt as MCPPrompt\nfrom mcp.types import PromptArgument as MCPPromptArgument\nfrom mcp.types import Resource as MCPResource\nfrom mcp.types import ResourceTemplate as MCPResourceTemplate\nfrom mcp.types import Tool as MCPTool\n\nlogger = get_logger(__name__)\n\n\nclass Settings(BaseSettings, Generic[LifespanResultT]):\n    \"\"\"FastMCP server settings.\n\n    All settings can be configured via environment variables with the prefix FASTMCP_.\n    For example, FASTMCP_DEBUG=true will set debug=True.\n    \"\"\"\n\n    model_config = SettingsConfigDict(\n        env_prefix=\"FASTMCP_\",\n        env_file=\".env\",\n        extra=\"ignore\",\n    )\n\n    # Server settings\n    debug: bool = False\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = \"INFO\"\n\n    # HTTP settings\n    host: str = \"0.0.0.0\"\n    port: int = 8000\n    sse_path: str = \"/sse\"\n    message_path: str = \"/messages/\"\n\n    # resource settings\n    warn_on_duplicate_resources: bool = True\n\n    # tool settings\n    warn_on_duplicate_tools: bool = True\n\n    # prompt settings\n    warn_on_duplicate_prompts: bool = True\n\n    dependencies: list[str] = Field(\n        default_factory=list,\n        description=\"List of dependencies to install in the server environment\",\n    )\n\n    lifespan: (\n        Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]] | None\n    ) = Field(None, description=\"Lifespan context manager\")\n\n\ndef lifespan_wrapper(\n    app: FastMCP,\n    lifespan: Callable[[FastMCP], AbstractAsyncContextManager[LifespanResultT]],\n) -> Callable[[MCPServer[LifespanResultT]], AbstractAsyncContextManager[object]]:\n    @asynccontextmanager\n    async def wrap(s: MCPServer[LifespanResultT]) -> AsyncIterator[object]:\n        async with lifespan(app) as context:\n            yield context\n\n    return wrap\n\n\nclass FastMCP:\n    def __init__(\n        self, name: str | None = None, instructions: str | None = None, **settings: Any\n    ):\n        self.settings = Settings(**settings)\n\n        self._mcp_server = MCPServer(\n            name=name or \"FastMCP\",\n            instructions=instructions,\n            lifespan=lifespan_wrapper(self, self.settings.lifespan)\n            if self.settings.lifespan\n            else default_lifespan,\n        )\n        self._tool_manager = ToolManager(\n            warn_on_duplicate_tools=self.settings.warn_on_duplicate_tools\n        )\n        self._resource_manager = ResourceManager(\n            warn_on_duplicate_resources=self.settings.warn_on_duplicate_resources\n        )\n        self._prompt_manager = PromptManager(\n            warn_on_duplicate_prompts=self.settings.warn_on_duplicate_prompts\n        )\n        self.dependencies = self.settings.dependencies\n\n        # Set up MCP protocol handlers\n        self._setup_handlers()\n\n        # Configure logging\n        configure_logging(self.settings.log_level)\n\n    @property\n    def name(self) -> str:\n        return self._mcp_server.name\n\n    @property\n    def instructions(self) -> str | None:\n        return self._mcp_server.instructions\n\n    def run(self, transport: Literal[\"stdio\", \"sse\"] = \"stdio\") -> None:\n        \"\"\"Run the FastMCP server. Note this is a synchronous function.\n\n        Args:\n            transport: Transport protocol to use (\"stdio\" or \"sse\")\n        \"\"\"\n        TRANSPORTS = Literal[\"stdio\", \"sse\"]\n        if transport not in TRANSPORTS.__args__:  # type: ignore\n            raise ValueError(f\"Unknown transport: {transport}\")\n\n        if transport == \"stdio\":\n            anyio.run(self.run_stdio_async)\n        else:  # transport == \"sse\"\n            anyio.run(self.run_sse_async)\n\n    def _setup_handlers(self) -> None:\n        \"\"\"Set up core MCP protocol handlers.\"\"\"\n        self._mcp_server.list_tools()(self.list_tools)\n        self._mcp_server.call_tool()(self.call_tool)\n        self._mcp_server.list_resources()(self.list_resources)\n        self._mcp_server.read_resource()(self.read_resource)\n        self._mcp_server.list_prompts()(self.list_prompts)\n        self._mcp_server.get_prompt()(self.get_prompt)\n        self._mcp_server.list_resource_templates()(self.list_resource_templates)\n\n    async def list_tools(self) -> list[MCPTool]:\n        \"\"\"List all available tools.\"\"\"\n        tools = self._tool_manager.list_tools()\n        return [\n            MCPTool(\n                name=info.name,\n                description=info.description,\n                inputSchema=info.parameters,\n            )\n            for info in tools\n        ]\n\n    def get_context(self) -> Context[ServerSession, object]:\n        \"\"\"\n        Returns a Context object. Note that the context will only be valid\n        during a request; outside a request, most methods will error.\n        \"\"\"\n        try:\n            request_context = self._mcp_server.request_context\n        except LookupError:\n            request_context = None\n        return Context(request_context=request_context, fastmcp=self)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any]\n    ) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        context = self.get_context()\n        result = await self._tool_manager.call_tool(name, arguments, context=context)\n        converted_result = _convert_to_content(result)\n        return converted_result\n\n    async def list_resources(self) -> list[MCPResource]:\n        \"\"\"List all available resources.\"\"\"\n\n        resources = self._resource_manager.list_resources()\n        return [\n            MCPResource(\n                uri=resource.uri,\n                name=resource.name or \"\",\n                description=resource.description,\n                mimeType=resource.mime_type,\n            )\n            for resource in resources\n        ]\n\n    async def list_resource_templates(self) -> list[MCPResourceTemplate]:\n        templates = self._resource_manager.list_templates()\n        return [\n            MCPResourceTemplate(\n                uriTemplate=template.uri_template,\n                name=template.name,\n                description=template.description,\n            )\n            for template in templates\n        ]\n\n    async def read_resource(self, uri: AnyUrl | str) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\"\"\"\n\n        resource = await self._resource_manager.get_resource(uri)\n        if not resource:\n            raise ResourceError(f\"Unknown resource: {uri}\")\n\n        try:\n            content = await resource.read()\n            return [ReadResourceContents(content=content, mime_type=resource.mime_type)]\n        except Exception as e:\n            logger.error(f\"Error reading resource {uri}: {e}\")\n            raise ResourceError(str(e))\n\n    def add_tool(\n        self,\n        fn: AnyFunction,\n        name: str | None = None,\n        description: str | None = None,\n    ) -> None:\n        \"\"\"Add a tool to the server.\n\n        The tool function can optionally request a Context object by adding a parameter\n        with the Context type annotation. See the @tool decorator for examples.\n\n        Args:\n            fn: The function to register as a tool\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n        \"\"\"\n        self._tool_manager.add_tool(fn, name=name, description=description)\n\n    def tool(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a tool.\n\n        Tools can optionally request a Context object by adding a parameter with the\n        Context type annotation. The context provides access to MCP capabilities like\n        logging, progress reporting, and resource access.\n\n        Args:\n            name: Optional name for the tool (defaults to function name)\n            description: Optional description of what the tool does\n\n        Example:\n            @server.tool()\n            def my_tool(x: int) -> str:\n                return str(x)\n\n            @server.tool()\n            def tool_with_context(x: int, ctx: Context) -> str:\n                ctx.info(f\"Processing {x}\")\n                return str(x)\n\n            @server.tool()\n            async def async_tool(x: int, context: Context) -> str:\n                await context.report_progress(50, 100)\n                return str(x)\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @tool decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @tool() instead of @tool\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            self.add_tool(fn, name=name, description=description)\n            return fn\n\n        return decorator\n\n    def add_resource(self, resource: Resource) -> None:\n        \"\"\"Add a resource to the server.\n\n        Args:\n            resource: A Resource instance to add\n        \"\"\"\n        self._resource_manager.add_resource(resource)\n\n    def resource(\n        self,\n        uri: str,\n        *,\n        name: str | None = None,\n        description: str | None = None,\n        mime_type: str | None = None,\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a function as a resource.\n\n        The function will be called when the resource is read to generate its content.\n        The function can return:\n        - str for text content\n        - bytes for binary content\n        - other types will be converted to JSON\n\n        If the URI contains parameters (e.g. \"resource://{param}\") or the function\n        has parameters, it will be registered as a template resource.\n\n        Args:\n            uri: URI for the resource (e.g. \"resource://my-resource\" or \"resource://{param}\")\n            name: Optional name for the resource\n            description: Optional description of the resource\n            mime_type: Optional MIME type for the resource\n\n        Example:\n            @server.resource(\"resource://my-resource\")\n            def get_data() -> str:\n                return \"Hello, world!\"\n\n            @server.resource(\"resource://my-resource\")\n            async get_data() -> str:\n                data = await fetch_data()\n                return f\"Hello, world! {data}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            def get_weather(city: str) -> str:\n                return f\"Weather for {city}\"\n\n            @server.resource(\"resource://{city}/weather\")\n            async def get_weather(city: str) -> str:\n                data = await fetch_weather(city)\n                return f\"Weather for {city}: {data}\"\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(uri):\n            raise TypeError(\n                \"The @resource decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @resource('uri') instead of @resource\"\n            )\n\n        def decorator(fn: AnyFunction) -> AnyFunction:\n            # Check if this should be a template\n            has_uri_params = \"{\" in uri and \"}\" in uri\n            has_func_params = bool(inspect.signature(fn).parameters)\n\n            if has_uri_params or has_func_params:\n                # Validate that URI params match function params\n                uri_params = set(re.findall(r\"{(\\w+)}\", uri))\n                func_params = set(inspect.signature(fn).parameters.keys())\n\n                if uri_params != func_params:\n                    raise ValueError(\n                        f\"Mismatch between URI parameters {uri_params} \"\n                        f\"and function parameters {func_params}\"\n                    )\n\n                # Register as template\n                self._resource_manager.add_template(\n                    fn=fn,\n                    uri_template=uri,\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                )\n            else:\n                # Register as regular resource\n                resource = FunctionResource(\n                    uri=AnyUrl(uri),\n                    name=name,\n                    description=description,\n                    mime_type=mime_type or \"text/plain\",\n                    fn=fn,\n                )\n                self.add_resource(resource)\n            return fn\n\n        return decorator\n\n    def add_prompt(self, prompt: Prompt) -> None:\n        \"\"\"Add a prompt to the server.\n\n        Args:\n            prompt: A Prompt instance to add\n        \"\"\"\n        self._prompt_manager.add_prompt(prompt)\n\n    def prompt(\n        self, name: str | None = None, description: str | None = None\n    ) -> Callable[[AnyFunction], AnyFunction]:\n        \"\"\"Decorator to register a prompt.\n\n        Args:\n            name: Optional name for the prompt (defaults to function name)\n            description: Optional description of what the prompt does\n\n        Example:\n            @server.prompt()\n            def analyze_table(table_name: str) -> list[Message]:\n                schema = read_table_schema(table_name)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Analyze this schema:\\n{schema}\"\n                    }\n                ]\n\n            @server.prompt()\n            async def analyze_file(path: str) -> list[Message]:\n                content = await read_file(path)\n                return [\n                    {\n                        \"role\": \"user\",\n                        \"content\": {\n                            \"type\": \"resource\",\n                            \"resource\": {\n                                \"uri\": f\"file://{path}\",\n                                \"text\": content\n                            }\n                        }\n                    }\n                ]\n        \"\"\"\n        # Check if user passed function directly instead of calling decorator\n        if callable(name):\n            raise TypeError(\n                \"The @prompt decorator was used incorrectly. \"\n                \"Did you forget to call it? Use @prompt() instead of @prompt\"\n            )\n\n        def decorator(func: AnyFunction) -> AnyFunction:\n            prompt = Prompt.from_function(func, name=name, description=description)\n            self.add_prompt(prompt)\n            return func\n\n        return decorator\n\n    async def run_stdio_async(self) -> None:\n        \"\"\"Run the server using stdio transport.\"\"\"\n        async with stdio_server() as (read_stream, write_stream):\n            await self._mcp_server.run(\n                read_stream,\n                write_stream,\n                self._mcp_server.create_initialization_options(),\n            )\n\n    async def run_sse_async(self) -> None:\n        \"\"\"Run the server using SSE transport.\"\"\"\n        import uvicorn\n\n        starlette_app = self.sse_app()\n\n        config = uvicorn.Config(\n            starlette_app,\n            host=self.settings.host,\n            port=self.settings.port,\n            log_level=self.settings.log_level.lower(),\n        )\n        server = uvicorn.Server(config)\n        await server.serve()\n\n    def sse_app(self) -> Starlette:\n        \"\"\"Return an instance of the SSE server app.\"\"\"\n        sse = SseServerTransport(self.settings.message_path)\n\n        async def handle_sse(request: Request) -> None:\n            async with sse.connect_sse(\n                request.scope,\n                request.receive,\n                request._send,  # type: ignore[reportPrivateUsage]\n            ) as streams:\n                await self._mcp_server.run(\n                    streams[0],\n                    streams[1],\n                    self._mcp_server.create_initialization_options(),\n                )\n\n        return Starlette(\n            debug=self.settings.debug,\n            routes=[\n                Route(self.settings.sse_path, endpoint=handle_sse),\n                Mount(self.settings.message_path, app=sse.handle_post_message),\n            ],\n        )\n\n    async def list_prompts(self) -> list[MCPPrompt]:\n        \"\"\"List all available prompts.\"\"\"\n        prompts = self._prompt_manager.list_prompts()\n        return [\n            MCPPrompt(\n                name=prompt.name,\n                description=prompt.description,\n                arguments=[\n                    MCPPromptArgument(\n                        name=arg.name,\n                        description=arg.description,\n                        required=arg.required,\n                    )\n                    for arg in (prompt.arguments or [])\n                ],\n            )\n            for prompt in prompts\n        ]\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> GetPromptResult:\n        \"\"\"Get a prompt by name with arguments.\"\"\"\n        try:\n            messages = await self._prompt_manager.render_prompt(name, arguments)\n\n            return GetPromptResult(messages=pydantic_core.to_jsonable_python(messages))\n        except Exception as e:\n            logger.error(f\"Error getting prompt {name}: {e}\")\n            raise ValueError(str(e))\n\n\ndef _convert_to_content(\n    result: Any,\n) -> Sequence[TextContent | ImageContent | EmbeddedResource]:\n    \"\"\"Convert a result to a sequence of content objects.\"\"\"\n    if result is None:\n        return []\n\n    if isinstance(result, TextContent | ImageContent | EmbeddedResource):\n        return [result]\n\n    if isinstance(result, Image):\n        return [result.to_image_content()]\n\n    if isinstance(result, list | tuple):\n        return list(chain.from_iterable(_convert_to_content(item) for item in result))  # type: ignore[reportUnknownVariableType]\n\n    if not isinstance(result, str):\n        try:\n            result = json.dumps(pydantic_core.to_jsonable_python(result))\n        except Exception:\n            result = str(result)\n\n    return [TextContent(type=\"text\", text=result)]\n\n\nclass Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n    \"\"\"Context object providing access to MCP capabilities.\n\n    This provides a cleaner interface to MCP's RequestContext functionality.\n    It gets injected into tool and resource functions that request it via type hints.\n\n    To use context in a tool function, add a parameter with the Context type annotation:\n\n    ```python\n    @server.tool()\n    def my_tool(x: int, ctx: Context) -> str:\n        # Log messages to the client\n        ctx.info(f\"Processing {x}\")\n        ctx.debug(\"Debug info\")\n        ctx.warning(\"Warning message\")\n        ctx.error(\"Error message\")\n\n        # Report progress\n        ctx.report_progress(50, 100)\n\n        # Access resources\n        data = ctx.read_resource(\"resource://data\")\n\n        # Get request info\n        request_id = ctx.request_id\n        client_id = ctx.client_id\n\n        return str(x)\n    ```\n\n    The context parameter name can be anything as long as it's annotated with Context.\n    The context is optional - tools that don't need it can omit the parameter.\n    \"\"\"\n\n    _request_context: RequestContext[ServerSessionT, LifespanContextT] | None\n    _fastmcp: FastMCP | None\n\n    def __init__(\n        self,\n        *,\n        request_context: RequestContext[ServerSessionT, LifespanContextT] | None = None,\n        fastmcp: FastMCP | None = None,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self._request_context = request_context\n        self._fastmcp = fastmcp\n\n    @property\n    def fastmcp(self) -> FastMCP:\n        \"\"\"Access to the FastMCP server.\"\"\"\n        if self._fastmcp is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._fastmcp\n\n    @property\n    def request_context(self) -> RequestContext[ServerSessionT, LifespanContextT]:\n        \"\"\"Access to the underlying request context.\"\"\"\n        if self._request_context is None:\n            raise ValueError(\"Context is not available outside of a request\")\n        return self._request_context\n\n    async def report_progress(\n        self, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Report progress for the current operation.\n\n        Args:\n            progress: Current progress value e.g. 24\n            total: Optional total value e.g. 100\n        \"\"\"\n\n        progress_token = (\n            self.request_context.meta.progressToken\n            if self.request_context.meta\n            else None\n        )\n\n        if progress_token is None:\n            return\n\n        await self.request_context.session.send_progress_notification(\n            progress_token=progress_token, progress=progress, total=total\n        )\n\n    async def read_resource(self, uri: str | AnyUrl) -> Iterable[ReadResourceContents]:\n        \"\"\"Read a resource by URI.\n\n        Args:\n            uri: Resource URI to read\n\n        Returns:\n            The resource content as either text or bytes\n        \"\"\"\n        assert (\n            self._fastmcp is not None\n        ), \"Context is not available outside of a request\"\n        return await self._fastmcp.read_resource(uri)\n\n    async def log(\n        self,\n        level: Literal[\"debug\", \"info\", \"warning\", \"error\"],\n        message: str,\n        *,\n        logger_name: str | None = None,\n    ) -> None:\n        \"\"\"Send a log message to the client.\n\n        Args:\n            level: Log level (debug, info, warning, error)\n            message: Log message\n            logger_name: Optional logger name\n            **extra: Additional structured data to include\n        \"\"\"\n        await self.request_context.session.send_log_message(\n            level=level, data=message, logger=logger_name\n        )\n\n    @property\n    def client_id(self) -> str | None:\n        \"\"\"Get the client ID if available.\"\"\"\n        return (\n            getattr(self.request_context.meta, \"client_id\", None)\n            if self.request_context.meta\n            else None\n        )\n\n    @property\n    def request_id(self) -> str:\n        \"\"\"Get the unique ID for this request.\"\"\"\n        return str(self.request_context.request_id)\n\n    @property\n    def session(self):\n        \"\"\"Access to the underlying session for advanced usage.\"\"\"\n        return self.request_context.session\n\n    # Convenience methods for common log levels\n    async def debug(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a debug log message.\"\"\"\n        await self.log(\"debug\", message, **extra)\n\n    async def info(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an info log message.\"\"\"\n        await self.log(\"info\", message, **extra)\n\n    async def warning(self, message: str, **extra: Any) -> None:\n        \"\"\"Send a warning log message.\"\"\"\n        await self.log(\"warning\", message, **extra)\n\n    async def error(self, message: str, **extra: Any) -> None:\n        \"\"\"Send an error log message.\"\"\"\n        await self.log(\"error\", message, **extra)\n\n\n--- File: src/mcp/server/fastmcp/tools/base.py ---\nfrom __future__ import annotations as _annotations\n\nimport inspect\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any, get_origin\n\nfrom pydantic import BaseModel, Field\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.utilities.func_metadata import FuncMetadata, func_metadata\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n    from mcp.shared.context import LifespanContextT\n\n\nclass Tool(BaseModel):\n    \"\"\"Internal tool registration info.\"\"\"\n\n    fn: Callable[..., Any] = Field(exclude=True)\n    name: str = Field(description=\"Name of the tool\")\n    description: str = Field(description=\"Description of what the tool does\")\n    parameters: dict[str, Any] = Field(description=\"JSON schema for tool parameters\")\n    fn_metadata: FuncMetadata = Field(\n        description=\"Metadata about the function including a pydantic model for tool\"\n        \" arguments\"\n    )\n    is_async: bool = Field(description=\"Whether the tool is async\")\n    context_kwarg: str | None = Field(\n        None, description=\"Name of the kwarg that should receive context\"\n    )\n\n    @classmethod\n    def from_function(\n        cls,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n        context_kwarg: str | None = None,\n    ) -> Tool:\n        \"\"\"Create a Tool from a function.\"\"\"\n        from mcp.server.fastmcp import Context\n\n        func_name = name or fn.__name__\n\n        if func_name == \"<lambda>\":\n            raise ValueError(\"You must provide a name for lambda functions\")\n\n        func_doc = description or fn.__doc__ or \"\"\n        is_async = inspect.iscoroutinefunction(fn)\n\n        if context_kwarg is None:\n            sig = inspect.signature(fn)\n            for param_name, param in sig.parameters.items():\n                if get_origin(param.annotation) is not None:\n                    continue\n                if issubclass(param.annotation, Context):\n                    context_kwarg = param_name\n                    break\n\n        func_arg_metadata = func_metadata(\n            fn,\n            skip_names=[context_kwarg] if context_kwarg is not None else [],\n        )\n        parameters = func_arg_metadata.arg_model.model_json_schema()\n\n        return cls(\n            fn=fn,\n            name=func_name,\n            description=func_doc,\n            parameters=parameters,\n            fn_metadata=func_arg_metadata,\n            is_async=is_async,\n            context_kwarg=context_kwarg,\n        )\n\n    async def run(\n        self,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Run the tool with arguments.\"\"\"\n        try:\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn,\n                self.is_async,\n                arguments,\n                {self.context_kwarg: context}\n                if self.context_kwarg is not None\n                else None,\n            )\n        except Exception as e:\n            raise ToolError(f\"Error executing tool {self.name}: {e}\") from e\n\n\n--- File: src/mcp/server/fastmcp/tools/tool_manager.py ---\nfrom __future__ import annotations as _annotations\n\nfrom collections.abc import Callable\nfrom typing import TYPE_CHECKING, Any\n\nfrom mcp.server.fastmcp.exceptions import ToolError\nfrom mcp.server.fastmcp.tools.base import Tool\nfrom mcp.server.fastmcp.utilities.logging import get_logger\nfrom mcp.shared.context import LifespanContextT\n\nif TYPE_CHECKING:\n    from mcp.server.fastmcp.server import Context\n    from mcp.server.session import ServerSessionT\n\nlogger = get_logger(__name__)\n\n\nclass ToolManager:\n    \"\"\"Manages FastMCP tools.\"\"\"\n\n    def __init__(self, warn_on_duplicate_tools: bool = True):\n        self._tools: dict[str, Tool] = {}\n        self.warn_on_duplicate_tools = warn_on_duplicate_tools\n\n    def get_tool(self, name: str) -> Tool | None:\n        \"\"\"Get tool by name.\"\"\"\n        return self._tools.get(name)\n\n    def list_tools(self) -> list[Tool]:\n        \"\"\"List all registered tools.\"\"\"\n        return list(self._tools.values())\n\n    def add_tool(\n        self,\n        fn: Callable[..., Any],\n        name: str | None = None,\n        description: str | None = None,\n    ) -> Tool:\n        \"\"\"Add a tool to the server.\"\"\"\n        tool = Tool.from_function(fn, name=name, description=description)\n        existing = self._tools.get(tool.name)\n        if existing:\n            if self.warn_on_duplicate_tools:\n                logger.warning(f\"Tool already exists: {tool.name}\")\n            return existing\n        self._tools[tool.name] = tool\n        return tool\n\n    async def call_tool(\n        self,\n        name: str,\n        arguments: dict[str, Any],\n        context: Context[ServerSessionT, LifespanContextT] | None = None,\n    ) -> Any:\n        \"\"\"Call a tool by name with arguments.\"\"\"\n        tool = self.get_tool(name)\n        if not tool:\n            raise ToolError(f\"Unknown tool: {name}\")\n\n        return await tool.run(arguments, context=context)\n\n\n--- File: src/mcp/shared/context.py ---\nfrom dataclasses import dataclass\nfrom typing import Any, Generic\n\nfrom typing_extensions import TypeVar\n\nfrom mcp.shared.session import BaseSession\nfrom mcp.types import RequestId, RequestParams\n\nSessionT = TypeVar(\"SessionT\", bound=BaseSession[Any, Any, Any, Any, Any])\nLifespanContextT = TypeVar(\"LifespanContextT\")\n\n\n@dataclass\nclass RequestContext(Generic[SessionT, LifespanContextT]):\n    request_id: RequestId\n    meta: RequestParams.Meta | None\n    session: SessionT\n    lifespan_context: LifespanContextT\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 7: FastMCP Context`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 7: FastMCP Context - Your In-Request Toolkit\n\nIn the previous chapter, [Transports](06_transports.md), we saw how clients and servers establish communication channels like stdio or SSE to exchange messages. Now, let's focus back on the server side, specifically when using the [FastMCP Server](05_fastmcp_server.md) framework.\n\nImagine you've built a tool for your FastMCP server, maybe one that performs a complex analysis or generates a large report. This might take a while! Wouldn't it be great if, while your tool function is running, it could:\n\n1.  Tell the client, \"Hey, I'm 20% done... now 50%... now 80%...\"?\n2.  Send status messages back to the client, like \"Starting analysis...\" or \"Fetching data...\"?\n3.  Maybe even access another piece of data (a [Resource](03_resources.md)) provided by the *same* server to help with its calculation?\n\nHow can your simple Python tool function, running inside the server, interact with the ongoing request and the server's capabilities?\n\n## Motivation: Giving Your Tool Function Superpowers\n\nWhen the [FastMCP Server](05_fastmcp_server.md) framework calls your tool or resource function to handle a client request, it doesn't just call it in isolation. It can provide a special helper object directly to your function. This helper object is called the **FastMCP Context**.\n\n**Think of the FastMCP Context (`Context`) as a special toolkit automatically handed to your tool or resource function when it's called.** This toolkit contains useful utilities relevant to the *current* request and the server session it belongs to.\n\nIt's like when you hire a plumber (your tool function); they don't just show up empty-handed. They bring their toolbox (the `Context`) which has tools (like `log`, `report_progress`, `read_resource`) specifically needed for the plumbing job (handling the client request).\n\n## What's in the Toolkit? Using the `Context` Object\n\nThe `Context` object gives your function access to several server capabilities during request handling:\n\n1.  **Logging to the Client:** Send log messages (debug, info, warning, error) back to the connected client. This is super useful for debugging or showing status updates.\n2.  **Progress Reporting:** If the client initiated the request with a progress token, your function can send updates like \"X% complete\" back to the client.\n3.  **Reading Resources:** Access other [Resources](03_resources.md) defined on the *same* FastMCP server, just as if you were a client calling `session.read_resource()`.\n4.  **Accessing Request Info:** Get details about the current request, like its unique ID or the client's identifier (if provided).\n\n## How to Get and Use the Context\n\nGetting access to this toolkit in your function is surprisingly simple: you just ask for it using a type hint!\n\n**1. Requesting the Context in Your Function**\n\nWhen defining your tool or resource function, simply add a parameter and type-hint it as `Context`. `FastMCP` will automatically see this and pass the `Context` object to your function when it's called.\n\n```python\n# server_code.py (inside your FastMCP server file)\nfrom mcp.server.fastmcp import FastMCP, Context # Import Context!\n\nmcp = FastMCP(\"MyServer\")\n\n@mcp.tool()\nasync def long_running_task(duration: int, ctx: Context): # <-- Request Context here!\n    \"\"\"Performs a task and reports progress.\"\"\"\n    ctx.info(f\"Starting task that will take {duration} seconds.\") # Use ctx!\n\n    for i in range(duration):\n        await asyncio.sleep(1) # Simulate work\n        progress = ((i + 1) / duration) * 100\n        # Report progress back to the client via the context\n        await ctx.report_progress(progress=i + 1, total=duration)\n        ctx.debug(f\"Progress: {progress:.0f}%\") # Send debug log\n\n    result_message = \"Task completed successfully!\"\n    ctx.info(result_message) # Log completion\n    return result_message\n\n# You don't *have* to call the parameter 'ctx', any name works\n# as long as the type hint is 'Context'.\n```\n\nIn this example, our `long_running_task` tool function now has a parameter `ctx` hinted as `Context`. When a client calls this tool, `FastMCP` will provide the appropriate `Context` object for that specific request.\n\n**2. Using Context Methods**\n\nInside the function, you can now use the methods provided by the `ctx` object:\n\n*   **Logging:**\n    ```python\n    # Inside your tool/resource function:\n    ctx.debug(\"This is detailed debug info.\")\n    ctx.info(\"Task phase 1 complete.\")\n    ctx.warning(\"Input value looks unusual.\")\n    ctx.error(\"Failed to process sub-task XYZ.\")\n    ```\n    These messages are sent as notifications back to the connected client, helping them understand what's happening.\n\n*   **Reporting Progress:**\n    ```python\n    # Inside your async tool/resource function:\n    total_steps = 10\n    for step in range(total_steps):\n        # ... do work for the step ...\n        await ctx.report_progress(progress=step + 1, total=total_steps)\n    ```\n    If the client supports it, they will receive these progress updates.\n\n*   **Reading Server Resources:**\n    ```python\n    # Inside your async tool/resource function:\n    try:\n        config_data_contents = await ctx.read_resource(\"resource://server-config\")\n        # Process the config_data_contents (which is like the result of session.read_resource)\n        if config_data_contents:\n             config_text = config_data_contents[0].content # Assuming text content\n             ctx.info(f\"Loaded config: {config_text[:30]}...\")\n    except Exception as e:\n        ctx.error(f\"Could not read resource 'resource://server-config': {e}\")\n    ```\n    This allows your tool to dynamically fetch data defined elsewhere on the same server.\n\n*   **Getting Request Info:**\n    ```python\n    # Inside your tool/resource function:\n    request_id = ctx.request_id\n    client_id = ctx.client_id # Might be None if client didn't provide one\n    ctx.info(f\"Handling request {request_id} for client {client_id or 'Unknown'}\")\n    ```\n    This can be useful for logging or tracking purposes.\n\n**Important:** Your tool/resource function doesn't *have* to accept the `Context`. If your function doesn't need these capabilities, just define it without a `Context` parameter, and `FastMCP` won't try to pass one.\n\n## Under the Hood: How FastMCP Provides Context\n\nHow does `FastMCP` magically inject this `Context` object?\n\n1.  **Request Arrives:** A client sends a request (e.g., `tools/call` for `long_running_task`) to the server via one of the [Transports](06_transports.md).\n2.  **Server Receives:** The underlying `MCPServer` receives the request and identifies the target method (e.g., `call_tool`).\n3.  **Context Creation:** `FastMCP` (specifically, the `MCPServer` part) creates an internal `RequestContext` object. This object holds information like the `request_id`, details about the specific client `session`, and any shared `lifespan_context` (server-wide state).\n4.  **Handler Lookup:** `FastMCP` routes the request to the appropriate handler method (e.g., its internal `call_tool` method).\n5.  **Tool Lookup:** The `ToolManager` (part of `FastMCP`) finds the registered Python function for the requested tool (`long_running_task`).\n6.  **Signature Inspection:** Before calling your function, the `ToolManager` (using helpers like `func_metadata` from `src/mcp/server/fastmcp/utilities/func_metadata.py`) inspects its signature (the parameters it expects).\n7.  **Context Injection:** It sees the `ctx: Context` parameter. `FastMCP` then creates the user-friendly `Context` object, wrapping the internal `RequestContext`.\n8.  **Function Call:** The `ToolManager` calls your Python function (`long_running_task`), passing the validated arguments from the client request *and* the newly created `Context` object to the `ctx` parameter.\n9.  **Context Usage:** Your function code now runs and can use the methods on the `ctx` object (like `ctx.info`, `ctx.report_progress`). These methods internally use the `RequestContext` to interact with the session (e.g., `session.send_log_message`, `session.send_progress_notification`).\n\nHere\u2019s a simplified view:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport\n    participant FastMCPServer as FastMCP Server Framework\n    participant ToolManager as Server Tool Manager\n    participant YourToolFunc as Your Tool Function (e.g., long_running_task)\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"long_running_task\")\n    Transport->>+FastMCPServer: Deliver request message\n    FastMCPServer->>FastMCPServer: Create internal RequestContext (holds session, req_id)\n    FastMCPServer->>+ToolManager: Request to execute \"long_running_task\" with args & RequestContext\n    ToolManager->>ToolManager: Find 'long_running_task' function registration\n    ToolManager->>ToolManager: Inspect function signature, see 'ctx: Context'\n    ToolManager->>ToolManager: Create user-friendly Context object (wraps RequestContext)\n    ToolManager->>+YourToolFunc: Call long_running_task(args=..., ctx=ContextObject)\n    YourToolFunc->>YourToolFunc: Run your code...\n    YourToolFunc->>ContextObject: Call ctx.info(\"Starting...\")\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_log_message(...)\n    Session->>Transport: Send log notification\n    Transport-->>Client: Deliver log notification\n    YourToolFunc->>ContextObject: Call await ctx.report_progress(...)\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_progress_notification(...)\n    Session->>Transport: Send progress notification\n    Transport-->>Client: Deliver progress notification\n    Note over YourToolFunc: Continues execution...\n    YourToolFunc-->>-ToolManager: Return result\n    ToolManager-->>-FastMCPServer: Return result\n    FastMCPServer->>FastMCPServer: Format MCP response\n    FastMCPServer->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\n*   The `Context` class itself is defined in `src/mcp/server/fastmcp/server.py`. It mostly acts as a wrapper around the lower-level `RequestContext` (defined in `src/mcp/shared/context.py`) and the `FastMCP` instance.\n\n    ```python\n    # src/mcp/server/fastmcp/server.py (Simplified Context Class)\n    class Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n        _request_context: RequestContext[...] | None\n        _fastmcp: FastMCP | None\n        # ... (initializer) ...\n\n        @property\n        def request_context(self) -> RequestContext[...]:\n            # Provides access to the internal context\n            # ... (error handling) ...\n            return self._request_context\n\n        async def report_progress(self, progress: float, total: float | None = None) -> None:\n            # Uses request_context.session to send notification\n            progress_token = self.request_context.meta.progressToken # Simplified\n            if progress_token:\n                await self.request_context.session.send_progress_notification(...)\n\n        async def log(self, level: ..., message: str, ...) -> None:\n            # Uses request_context.session to send log message\n            await self.request_context.session.send_log_message(...)\n\n        # ... other methods like info(), error(), read_resource() ...\n    ```\n\n*   The `Tool` class (in `src/mcp/server/fastmcp/tools/base.py`) stores whether a function expects context (`context_kwarg`). The `ToolManager` (in `src/mcp/server/fastmcp/tools/tool_manager.py`) uses this information when calling the function via `tool.run()`.\n\n    ```python\n    # src/mcp/server/fastmcp/tools/base.py (Simplified Tool.run)\n    class Tool(BaseModel):\n        # ... fields: fn, name, context_kwarg ...\n\n        async def run(self, arguments: dict, context: Context | None = None) -> Any:\n            # Prepare kwargs, including context if needed\n            extra_kwargs = {}\n            if self.context_kwarg is not None and context is not None:\n                extra_kwargs[self.context_kwarg] = context\n\n            # Call the actual function with args and context\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn, self.is_async, arguments, extra_kwargs\n            )\n    ```\n\n*   The main `FastMCP` class (`src/mcp/server/fastmcp/server.py`) provides the `get_context()` method which creates the `Context` object when needed, using the currently active `request_context` managed by the underlying `MCPServer`.\n\nYou don't need to interact with these internal details directly, but seeing them helps understand that the `Context` isn't magic \u2013 it's a well-defined object passed along during request handling by the `FastMCP` framework.\n\n## Conclusion\n\nYou've learned about the **FastMCP Context** (`Context`), a powerful object provided to your server-side tool and resource functions.\n\n*   It acts as a **toolkit** giving access to server capabilities *during* request handling.\n*   You request it simply by adding a parameter with the `Context` type hint to your function.\n*   It allows your function logic to send **logs** and **progress updates** back to the client.\n*   It enables your function to **read other resources** from the same server.\n*   It provides access to **request-specific information**.\n\nThe `Context` object bridges the gap between your isolated function logic and the broader server session, making your FastMCP server tools much more interactive and capable.\n\nWe've now covered the main concepts for both clients (`ClientSession`) and servers (`FastMCP`), including the core capabilities (Tools, Resources, Prompts), communication (Transports), and server-side interaction (Context). To round things off, the next chapter will provide a quick reference guide to the key data structures used in the MCP protocol itself: [MCP Protocol Types](08_mcp_protocol_types.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `python-sdk` about the concept: \"MCP Protocol Types\". This is Chapter 8.\n\n        Concept Details:\n        - Description:\n        These are the fundamental building blocks defining the communication \"language\" between MCP clients and servers. They specify the exact structure of requests (what one side asks for), responses (the answer), and notifications (one-way messages). Think of it like the shared grammar and vocabulary rules that both the client and server must follow to understand each other, ensuring messages like asking for tools or getting results are formatted correctly. All interactions rely on these shared Pydantic models.\n\n\n        Complete Tutorial Structure:\n        1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n---\n# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).\n---\n# Chapter 5: FastMCP Server - Building Your Own Assistant\n\nIn the previous chapters ([Client Session](01_client_session.md), [Tools](02_tools.md), [Resources](03_resources.md), [Prompts](04_prompts.md)), we learned how to *use* an existing MCP server from our Python code (the client). We learned how to connect, list capabilities, call tools, read resources, and get prompts.\n\nBut what if *you* want to create your own assistant? Maybe you have a special calculation, access to unique data, or a cool function you want to make available to other programs (or even to an AI like Claude). How do you build the *server* side?\n\n## Motivation: Making Server Creation Easy\n\nBuilding a server that speaks the MCP protocol might sound complicated. You'd need to handle:\n\n*   Receiving messages (like JSON-RPC requests).\n*   Parsing those messages correctly.\n*   Understanding requests like \"tools/list\" or \"tools/call\".\n*   Routing the request to the correct piece of your code.\n*   Generating JSON Schemas to describe your tool inputs.\n*   Formatting your results back into the correct MCP response format.\n*   Sending the response back.\n\nThat's a lot of boilerplate code just to handle the communication! Wouldn't it be great if you could just write your core Python functions (the actual logic for your tools, resources, etc.) and have something else handle all the tricky MCP communication details?\n\nThat's exactly what **FastMCP Server** does!\n\n**`FastMCP` is a high-level, user-friendly framework within `python-sdk` for creating MCP servers.** Think of it like a helpful construction kit for building your MCP assistant. You define the *capabilities* (tools, resources, prompts) using simple Python functions and special markers called **decorators** (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`). `FastMCP` then automatically handles all the underlying MCP protocol details, generates the necessary descriptions (like JSON schemas), and routes incoming client requests to your functions.\n\nIt lets you focus on writing the *logic* for what your server can *do*, not the complex setup of how it *communicates*.\n\n**Our Goal:** Let's build a very simple MCP server that offers just one tool: adding two numbers.\n\n## Building Our Adder Server: Step-by-Step\n\nWe'll use `FastMCP` to create a server that a client can connect to and ask to add numbers.\n\n**1. Import `FastMCP`**\n\nFirst, we need to import the main class from the library.\n\n```python\n# adder_server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# We'll add more code below...\n```\n\nThis line brings the `FastMCP` framework into our script.\n\n**2. Create a `FastMCP` Instance**\n\nNext, we create an instance of the `FastMCP` class. This represents our server. We can give it a name.\n\n```python\n# adder_server.py (continued)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n# We'll define tools next...\n```\n\nThink of `mcp` as the main control object for our server definition.\n\n**3. Define the Tool Logic (A Simple Function)**\n\nWe need the actual code that performs the addition. This is just a regular Python function. Notice the use of type hints (`a: int`, `b: int`, `-> int`). These are important!\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n# This function contains the logic for our tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Now, how do we tell FastMCP this is a tool?\n```\n\nThis is the core logic. It takes two integers and returns their sum. The `print` statements are just for us to see when it gets called.\n\n**4. Register the Function as a Tool (The Magic Decorator!)**\n\nHow does `FastMCP` know that our `add` function should be made available as an MCP tool? We use the `@mcp.tool()` **decorator**. A decorator is like a sticky note you put on a function to give it special powers or register it with a framework.\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n@mcp.tool() # <-- This is the FastMCP decorator!\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# We're almost ready to run the server...\n```\n\nBy adding `@mcp.tool()` right above our `add` function, we tell `FastMCP`:\n\n*   \"Make this function available as an MCP tool.\"\n*   **Automatically use the function name (`add`) as the tool name.** (You can specify a different name like `@mcp.tool(\"adder\")` if you want).\n*   **Automatically use the function's docstring (`\"\"\"Adds two numbers together.\"\"\"`) as the tool's description.**\n*   **Automatically look at the type hints (`a: int`, `b: int`) to generate the JSON `inputSchema`** that tells clients what arguments are needed (an integer `a` and an integer `b`).\n*   **Automatically understand the return type (`-> int`)** to know what kind of result the tool produces.\n\nThis single line saves us from writing a lot of configuration code!\n\n**5. Run the Server**\n\nFinally, we need to tell our server to start running and listening for connections.\n\n```python\n# adder_server.py (complete example)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Add this block to start the server when the script is run\nif __name__ == \"__main__\":\n    print(\"Starting Adder Server...\")\n    # This runs the server, listening for connections via stdio by default\n    mcp.run()\n    print(\"Adder Server stopped.\")\n\n```\n\nThe `if __name__ == \"__main__\":` block is standard Python practice to make code run only when the script is executed directly. `mcp.run()` starts the `FastMCP` server. By default, it uses \"stdio\" (standard input/output) as the communication channel, meaning it behaves like a command-line application waiting for requests on its input and sending responses to its output. We'll learn more about different ways servers and clients can communicate in the next chapter on [Transports](06_transports.md).\n\nIf you save this as `adder_server.py` and run it (e.g., `python adder_server.py`), it will print \"Starting Adder Server...\" and wait. You could then use a client (like the one from [Chapter 2: Tools](02_tools.md), configured to run this script) to connect and call the `add` tool!\n\n## What Happens Under the Hood?\n\nLet's trace what happens when a client (using `ClientSession` from Chapter 1) calls `session.call_tool(\"add\", {\"a\": 5, \"b\": 3})` on our running `adder_server.py`:\n\n1.  **Client Sends Request:** The `ClientSession` creates an MCP `tools/call` request message (like `{\"method\": \"tools/call\", \"params\": {\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}, ...}`) and sends it over the communication channel (e.g., stdio).\n2.  **Transport Delivers:** The underlying transport layer ([Transports](06_transports.md)) delivers this raw message to the running `FastMCP` server.\n3.  **FastMCP Receives & Parses:** The `FastMCP` framework reads the incoming message. It understands the MCP protocol and parses the JSON to identify it's a `tools/call` request for the tool named `add` with arguments `a=5` and `b=3`.\n4.  **Finds the Tool Function:** `FastMCP` looks up the tool name \"add\" in its internal registry (where the `@mcp.tool()` decorator placed our `add` function).\n5.  **Validates Arguments:** It checks if the provided arguments (`{\"a\": 5, \"b\": 3}`) match the `inputSchema` it automatically generated from the `add(a: int, b: int)` type hints. In this case, they do. (If not, it would generate an error response).\n6.  **Calls Your Function:** `FastMCP` calls your actual Python function: `add(a=5, b=3)`. Your `print` statements will execute.\n7.  **Receives Return Value:** Your function returns the integer `8`.\n8.  **Formats Response:** `FastMCP` takes the return value (`8`) and automatically formats it into the standard MCP `CallToolResult` structure, likely as `TextContent`. The response message might look like `{\"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, ...}`.\n9.  **Sends Response:** `FastMCP` sends this JSON response message back over the communication channel.\n10. **Client Receives:** The client's `ClientSession` receives the response, parses it, and returns the `CallToolResult` object to the calling code.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Communication Channel\n    participant FastMCP as FastMCP Framework\n    participant AdderFunc as Your add() function\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"add\", args={a:5, b:3})\n    Transport->>+FastMCP: Deliver raw message\n    FastMCP->>FastMCP: Parse message, find 'add' tool registration\n    FastMCP->>FastMCP: Validate args against auto-generated schema\n    FastMCP->>+AdderFunc: Call add(a=5, b=3)\n    Note over AdderFunc: Prints logs, returns 8\n    AdderFunc-->>-FastMCP: Return result (8)\n    FastMCP->>FastMCP: Format 8 into MCP response (e.g., TextContent)\n    FastMCP->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\nYou don't *need* to know the internals, but it can be helpful. Inside `FastMCP` (see `src/mcp/server/fastmcp/server.py`):\n\n*   The `FastMCP` class initializes internal managers like `ToolManager`, `ResourceManager`, and `PromptManager`.\n*   The `@mcp.tool()` decorator calls the `add_tool` method on the `ToolManager`.\n*   `add_tool` uses Python's `inspect` module to look at your function's signature (parameters, type hints, docstring). It uses this information (often with help from the Pydantic library) to build the `MCPTool` definition, including the `inputSchema`. It stores this definition.\n*   `FastMCP` sets up handlers (like `self._mcp_server.call_tool()(self.call_tool)`) that map incoming MCP methods (`tools/call`) to internal `FastMCP` methods.\n*   The internal `call_tool` method asks the `ToolManager` to execute the registered tool function, passing the arguments.\n*   Helper functions like `_convert_to_content` take your Python function's return value and package it into the standard MCP content types (`TextContent`, `ImageContent`, etc.).\n\nThe key takeaway is that `FastMCP` uses Python's introspection capabilities (looking at function details) and decorators to automate the tedious parts of building an MCP server.\n\n## Adding Resources and Prompts\n\nJust like `@mcp.tool()`, `FastMCP` provides decorators for defining resources and prompts:\n\n*   **`@mcp.resource(uri)`:** Registers a function to provide data for a specific URI. The URI can even include template parameters (like `greeting://{name}`) that match function arguments. `FastMCP` handles mapping the URI request to your function. (See [Resources](03_resources.md)).\n\n    ```python\n    # From examples/fastmcp/readme-quickstart.py\n    @mcp.resource(\"greeting://{name}\")\n    def get_greeting(name: str) -> str:\n        \"\"\"Get a personalized greeting\"\"\"\n        return f\"Hello, {name}!\"\n    ```\n\n*   **`@mcp.prompt(name)`:** Registers a function that generates a list of prompt messages, often taking arguments defined by the function's parameters. `FastMCP` handles generating the `MCPPrompt` definition and rendering the messages when requested. (See [Prompts](04_prompts.md)).\n\n    ```python\n    # From examples/fastmcp/echo.py\n    @mcp.prompt(\"echo\")\n    def echo_prompt(text: str) -> str:\n        # FastMCP automatically converts the returned string\n        # into a User message in the GetPromptResult\n        return text\n    ```\n\nThese work on the same principle: write a Python function with type hints and a docstring, add the decorator, and `FastMCP` handles the MCP integration.\n\n## Conclusion\n\nCongratulations! You've taken your first step into building your own MCP server using the `FastMCP` framework. You learned:\n\n*   `FastMCP` simplifies server development by handling complex MCP protocol details automatically.\n*   You can define server capabilities (Tools, Resources, Prompts) using standard Python functions.\n*   Decorators like `@mcp.tool()`, `@mcp.resource()`, and `@mcp.prompt()` are the magic that registers your functions with `FastMCP`.\n*   `FastMCP` uses type hints and docstrings to automatically generate schemas and descriptions.\n*   Running the server is as simple as calling `mcp.run()`.\n\nYou now have the power to create your own \"assistants\" that can offer custom capabilities over the MCP protocol.\n\nBut how exactly do the client and server *talk* to each other? Whether it's stdio, HTTP/SSE, or something else, the communication channel is crucial. In the next chapter, we'll explore these communication methods: [Transports](06_transports.md).\n---\n# Chapter 6: Transports - The Delivery Service\n\nIn the last chapter, [FastMCP Server](05_fastmcp_server.md), we learned how to build our own MCP server assistant using the `FastMCP` framework. We saw how easy it is to define tools and have `FastMCP` handle the MCP protocol details.\n\nBut we skipped over a crucial detail: how does our client code, using the `ClientSession` from [Chapter 1: Client Session](01_client_session.md), actually *talk* to the server we built? They might be running on the same computer, or maybe the server is on a different machine across the internet. How do the request and response messages physically travel between them?\n\nThis is where **Transports** come in!\n\n## Motivation: Choosing Your Delivery Method\n\nImagine you wrote a letter (an MCP request message, like asking to call a tool). You need to send it to your friend (the MCP server). How do you get it there?\n\n*   You could hand it directly to them if they're right next to you (like a local program).\n*   You could use the postal service (like standard input/output for local programs).\n*   You could use a special courier service that provides continuous updates (like Server-Sent Events over the web).\n*   You could establish a direct, instant phone line (like WebSockets).\n\nEach method gets the letter delivered, but they work differently and are suited for different situations.\n\nIn the world of `python-sdk` and MCP, the **Transport** is the underlying communication mechanism chosen to send messages between the client and the server. It's the actual delivery service used for the MCP messages formatted by `ClientSession` and understood by the server.\n\n## What is a Transport?\n\nA **Transport** handles the low-level details of setting up a connection and sending/receiving the actual bytes that make up the MCP messages. Think of it as the plumbing that connects the client and the server.\n\nThe `ClientSession` we learned about in [Chapter 1: Client Session](01_client_session.md) is responsible for *formatting* the messages (like `tools/list` requests) and *interpreting* the replies. But it relies on a Transport to do the actual sending and receiving over the chosen communication channel.\n\nThe `python-sdk` provides built-in support for several common transports:\n\n1.  **Standard Input/Output (Stdio):**\n    *   **Analogy:** Sending letters via postal mail to a local address, or talking to a command-line tool.\n    *   **How it works:** The client starts the server as a local process (like running a command in your terminal). The client sends messages to the server's standard input (\"stdin\") and reads responses from the server's standard output (\"stdout\").\n    *   **Use Case:** Perfect when the client application needs to start and manage a local MCP server process running on the same machine. This is what we implicitly used in Chapter 1 and Chapter 5!\n\n2.  **Server-Sent Events (SSE over HTTP):**\n    *   **Analogy:** A continuous news ticker feed delivered over the web.\n    *   **How it works:** The client connects to a specific web URL on the server using standard HTTP. The server can then *push* messages (events) to the client over this connection whenever it wants. To send messages *to* the server, the client typically makes separate standard HTTP POST requests to another URL defined by the server.\n    *   **Use Case:** Good for web-based scenarios where the server needs to proactively send updates or notifications to the client (like progress updates, new messages).\n\n3.  **WebSockets:**\n    *   **Analogy:** A direct, two-way phone line established over the web.\n    *   **How it works:** The client and server establish a persistent, full-duplex (two-way) connection. Both the client and server can send messages to each other at any time over this single connection once it's established.\n    *   **Use Case:** Ideal for real-time, interactive applications requiring low-latency communication in both directions (e.g., chat applications, live dashboards, collaborative tools).\n\n## Using Transports with `ClientSession`\n\nThe beauty is that `ClientSession` itself doesn't need to know the complex details of *how* stdio, SSE, or WebSockets work. It just needs two things:\n\n*   A way to **read** incoming messages (`read_stream`).\n*   A way to **write** outgoing messages (`write_stream`).\n\nThe `python-sdk` provides helper functions (like `stdio_client`, `sse_client`, `websocket_client`) that set up the specific transport mechanism and give you back these two streams. You then pass these streams to `ClientSession`.\n\nLet's see how you'd use each transport helper.\n\n**1. Stdio Transport**\n\nThis is the one we saw in [Chapter 1: Client Session](01_client_session.md). We define how to run the server command and then use `stdio_client`.\n\n```python\n# Using stdio transport (like in Chapter 1)\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client # <-- Stdio helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_stdio(server_command: str):\n    server_params = StdioServerParameters(command=server_command)\n\n    # stdio_client starts the process and provides streams\n    async with stdio_client(server_params) as (read_stream, write_stream):\n        logging.info(\"Stdio transport connected.\")\n        # ClientSession uses the streams from stdio_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via stdio.\")\n            # ... use session.list_tools(), session.call_tool(), etc. ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via stdio: {tools}\")\n\n# Example: asyncio.run(connect_via_stdio(\"python your_server_script.py\"))\n```\n\n*   `StdioServerParameters` tells `stdio_client` *how* to run the server process.\n*   `stdio_client(...)` starts the server process and yields the `read_stream` (connected to server's stdout) and `write_stream` (connected to server's stdin).\n*   `ClientSession(read_stream, write_stream)` uses these streams to communicate.\n\n**2. SSE Transport**\n\nTo connect to an MCP server running with an SSE endpoint (e.g., hosted by a web server), you use `sse_client`.\n\n```python\n# Using SSE transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client # <-- SSE helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_sse(server_sse_url: str):\n    # sse_client connects to the URL and provides streams\n    # It handles the SSE event listening and POSTing for sending\n    async with sse_client(server_sse_url) as (read_stream, write_stream):\n        logging.info(\"SSE transport connected.\")\n        # ClientSession uses the streams from sse_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via SSE.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via SSE: {tools}\")\n\n# Example: asyncio.run(connect_via_sse(\"http://localhost:8000/sse\"))\n```\n\n*   `sse_client(server_sse_url)` connects to the specified HTTP endpoint, sets up the SSE listener for incoming messages (`read_stream`), and prepares to send outgoing messages via HTTP POST (`write_stream`).\n*   `ClientSession` uses these streams, unaware of the underlying HTTP/SSE mechanics.\n\n**3. WebSocket Transport**\n\nTo connect to an MCP server using a WebSocket endpoint, you use `websocket_client`.\n\n```python\n# Using WebSocket transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.websocket import websocket_client # <-- WebSocket helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_websocket(server_ws_url: str):\n    # websocket_client establishes the WebSocket connection\n    async with websocket_client(server_ws_url) as (read_stream, write_stream):\n        logging.info(\"WebSocket transport connected.\")\n        # ClientSession uses the streams from websocket_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via WebSocket.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via WebSocket: {tools}\")\n\n# Example: asyncio.run(connect_via_websocket(\"ws://localhost:8000/ws\"))\n```\n\n*   `websocket_client(server_ws_url)` connects to the WebSocket URL (`ws://...` or `wss://...`) and yields streams representing the bidirectional connection.\n*   `ClientSession` uses these streams to send and receive MCP messages over the WebSocket.\n\nNotice the pattern: You choose the appropriate `*_client` helper based on how the server is exposed, but the way you create and use `ClientSession` remains the same!\n\n```python\n# General Pattern\n# 1. Choose and configure the transport helper\n# async with stdio_client(...) as (read_stream, write_stream):\n# async with sse_client(...) as (read_stream, write_stream):\n# async with websocket_client(...) as (read_stream, write_stream):\n\n# 2. Pass the provided streams to ClientSession\n#     async with ClientSession(read_stream, write_stream) as session:\n#         # 3. Initialize and use the session as usual\n#         await session.initialize()\n#         result = await session.list_tools()\n#         # ...\n```\n\n## Under the Hood: How Transports Provide Streams\n\nHow do these `*_client` functions hide the complexity? They act as adapters.\n\n1.  **Connection:** Each helper establishes the specific type of connection (starts a process, makes HTTP requests, opens a WebSocket).\n2.  **Internal Queues:** They typically use internal memory queues or buffers (often using `anyio.create_memory_object_stream`).\n3.  **Reading:** They run background tasks that continuously read raw data from the connection (process stdout, SSE events, WebSocket frames), parse it into MCP messages (or detect errors), and put these messages/errors onto the internal *read* queue. The `read_stream` you get simply reads from this queue.\n4.  **Writing:** When you send a message to the `write_stream`, it goes into an internal *write* queue. Another background task reads from this queue, formats the message according to the transport's protocol (e.g., adds newline for stdio, makes HTTP POST for SSE, sends WebSocket frame), and sends it over the actual connection.\n\nThis setup decouples `ClientSession` from the specific transport details. `ClientSession` just interacts with the clean `read_stream` and `write_stream` interfaces.\n\n```mermaid\ngraph LR\n    subgraph Your Code\n        A[ClientSession]\n    end\n    subgraph Transport Helper (e.g., stdio_client)\n        direction LR\n        B[read_stream] -- Reads From --> C(Internal Read Queue)\n        D(Internal Write Queue) -- Writes To --> E[write_stream]\n        F[Background Reader Task] -- Writes To --> C\n        G[Background Writer Task] -- Reads From --> D\n    end\n    subgraph Actual Connection\n        H[Process Pipes / HTTP / WebSocket]\n    end\n    subgraph Server\n        I[MCP Server]\n    end\n\n    A -- Uses --> B\n    A -- Uses --> E\n\n    F -- Reads From --> H\n    G -- Writes To --> H\n    H -- Communicates With --> I\n```\n\nThe `Transport Helper` box manages the `Internal Queues` and `Background Tasks` to bridge the gap between the `Actual Connection` and the simple `read_stream`/`write_stream` used by `ClientSession`.\n\nYou can explore the implementation details in the `mcp.client` subdirectories:\n*   `src/mcp/client/stdio/`: Code for starting processes and managing stdin/stdout.\n*   `src/mcp/client/sse.py`: Code for handling SSE connections and HTTP POST requests.\n*   `src/mcp/client/websocket.py`: Code for managing WebSocket communication.\n\nSimilarly, the `mcp.server` directory contains corresponding transport implementations for the server side (e.g., `src/mcp/server/stdio.py`, `src/mcp/server/sse.py`, `src/mcp/server/websocket.py`) that frameworks like [FastMCP Server](05_fastmcp_server.md) can use.\n\n## Conclusion\n\nIn this chapter, we peeled back a layer to understand **Transports** \u2013 the delivery mechanisms that connect MCP clients and servers.\n\n*   Transports handle the low-level communication (stdio, SSE, WebSocket).\n*   They provide simple `read_stream` and `write_stream` interfaces for `ClientSession`.\n*   Helper functions (`stdio_client`, `sse_client`, `websocket_client`) manage the specifics of each transport type.\n*   You choose the transport based on how your client needs to connect to the server.\n\nUnderstanding transports gives you flexibility in how you deploy and connect your MCP applications.\n\nNow that we've covered the core components \u2013 sessions, tools, resources, prompts, servers, and transports \u2013 let's look at a more advanced server concept: how servers built with `FastMCP` can maintain state and context across multiple requests within a single client session. We'll explore this in the next chapter: [FastMCP Context](07_fastmcp_context.md).\n---\n# Chapter 7: FastMCP Context - Your In-Request Toolkit\n\nIn the previous chapter, [Transports](06_transports.md), we saw how clients and servers establish communication channels like stdio or SSE to exchange messages. Now, let's focus back on the server side, specifically when using the [FastMCP Server](05_fastmcp_server.md) framework.\n\nImagine you've built a tool for your FastMCP server, maybe one that performs a complex analysis or generates a large report. This might take a while! Wouldn't it be great if, while your tool function is running, it could:\n\n1.  Tell the client, \"Hey, I'm 20% done... now 50%... now 80%...\"?\n2.  Send status messages back to the client, like \"Starting analysis...\" or \"Fetching data...\"?\n3.  Maybe even access another piece of data (a [Resource](03_resources.md)) provided by the *same* server to help with its calculation?\n\nHow can your simple Python tool function, running inside the server, interact with the ongoing request and the server's capabilities?\n\n## Motivation: Giving Your Tool Function Superpowers\n\nWhen the [FastMCP Server](05_fastmcp_server.md) framework calls your tool or resource function to handle a client request, it doesn't just call it in isolation. It can provide a special helper object directly to your function. This helper object is called the **FastMCP Context**.\n\n**Think of the FastMCP Context (`Context`) as a special toolkit automatically handed to your tool or resource function when it's called.** This toolkit contains useful utilities relevant to the *current* request and the server session it belongs to.\n\nIt's like when you hire a plumber (your tool function); they don't just show up empty-handed. They bring their toolbox (the `Context`) which has tools (like `log`, `report_progress`, `read_resource`) specifically needed for the plumbing job (handling the client request).\n\n## What's in the Toolkit? Using the `Context` Object\n\nThe `Context` object gives your function access to several server capabilities during request handling:\n\n1.  **Logging to the Client:** Send log messages (debug, info, warning, error) back to the connected client. This is super useful for debugging or showing status updates.\n2.  **Progress Reporting:** If the client initiated the request with a progress token, your function can send updates like \"X% complete\" back to the client.\n3.  **Reading Resources:** Access other [Resources](03_resources.md) defined on the *same* FastMCP server, just as if you were a client calling `session.read_resource()`.\n4.  **Accessing Request Info:** Get details about the current request, like its unique ID or the client's identifier (if provided).\n\n## How to Get and Use the Context\n\nGetting access to this toolkit in your function is surprisingly simple: you just ask for it using a type hint!\n\n**1. Requesting the Context in Your Function**\n\nWhen defining your tool or resource function, simply add a parameter and type-hint it as `Context`. `FastMCP` will automatically see this and pass the `Context` object to your function when it's called.\n\n```python\n# server_code.py (inside your FastMCP server file)\nfrom mcp.server.fastmcp import FastMCP, Context # Import Context!\n\nmcp = FastMCP(\"MyServer\")\n\n@mcp.tool()\nasync def long_running_task(duration: int, ctx: Context): # <-- Request Context here!\n    \"\"\"Performs a task and reports progress.\"\"\"\n    ctx.info(f\"Starting task that will take {duration} seconds.\") # Use ctx!\n\n    for i in range(duration):\n        await asyncio.sleep(1) # Simulate work\n        progress = ((i + 1) / duration) * 100\n        # Report progress back to the client via the context\n        await ctx.report_progress(progress=i + 1, total=duration)\n        ctx.debug(f\"Progress: {progress:.0f}%\") # Send debug log\n\n    result_message = \"Task completed successfully!\"\n    ctx.info(result_message) # Log completion\n    return result_message\n\n# You don't *have* to call the parameter 'ctx', any name works\n# as long as the type hint is 'Context'.\n```\n\nIn this example, our `long_running_task` tool function now has a parameter `ctx` hinted as `Context`. When a client calls this tool, `FastMCP` will provide the appropriate `Context` object for that specific request.\n\n**2. Using Context Methods**\n\nInside the function, you can now use the methods provided by the `ctx` object:\n\n*   **Logging:**\n    ```python\n    # Inside your tool/resource function:\n    ctx.debug(\"This is detailed debug info.\")\n    ctx.info(\"Task phase 1 complete.\")\n    ctx.warning(\"Input value looks unusual.\")\n    ctx.error(\"Failed to process sub-task XYZ.\")\n    ```\n    These messages are sent as notifications back to the connected client, helping them understand what's happening.\n\n*   **Reporting Progress:**\n    ```python\n    # Inside your async tool/resource function:\n    total_steps = 10\n    for step in range(total_steps):\n        # ... do work for the step ...\n        await ctx.report_progress(progress=step + 1, total=total_steps)\n    ```\n    If the client supports it, they will receive these progress updates.\n\n*   **Reading Server Resources:**\n    ```python\n    # Inside your async tool/resource function:\n    try:\n        config_data_contents = await ctx.read_resource(\"resource://server-config\")\n        # Process the config_data_contents (which is like the result of session.read_resource)\n        if config_data_contents:\n             config_text = config_data_contents[0].content # Assuming text content\n             ctx.info(f\"Loaded config: {config_text[:30]}...\")\n    except Exception as e:\n        ctx.error(f\"Could not read resource 'resource://server-config': {e}\")\n    ```\n    This allows your tool to dynamically fetch data defined elsewhere on the same server.\n\n*   **Getting Request Info:**\n    ```python\n    # Inside your tool/resource function:\n    request_id = ctx.request_id\n    client_id = ctx.client_id # Might be None if client didn't provide one\n    ctx.info(f\"Handling request {request_id} for client {client_id or 'Unknown'}\")\n    ```\n    This can be useful for logging or tracking purposes.\n\n**Important:** Your tool/resource function doesn't *have* to accept the `Context`. If your function doesn't need these capabilities, just define it without a `Context` parameter, and `FastMCP` won't try to pass one.\n\n## Under the Hood: How FastMCP Provides Context\n\nHow does `FastMCP` magically inject this `Context` object?\n\n1.  **Request Arrives:** A client sends a request (e.g., `tools/call` for `long_running_task`) to the server via one of the [Transports](06_transports.md).\n2.  **Server Receives:** The underlying `MCPServer` receives the request and identifies the target method (e.g., `call_tool`).\n3.  **Context Creation:** `FastMCP` (specifically, the `MCPServer` part) creates an internal `RequestContext` object. This object holds information like the `request_id`, details about the specific client `session`, and any shared `lifespan_context` (server-wide state).\n4.  **Handler Lookup:** `FastMCP` routes the request to the appropriate handler method (e.g., its internal `call_tool` method).\n5.  **Tool Lookup:** The `ToolManager` (part of `FastMCP`) finds the registered Python function for the requested tool (`long_running_task`).\n6.  **Signature Inspection:** Before calling your function, the `ToolManager` (using helpers like `func_metadata` from `src/mcp/server/fastmcp/utilities/func_metadata.py`) inspects its signature (the parameters it expects).\n7.  **Context Injection:** It sees the `ctx: Context` parameter. `FastMCP` then creates the user-friendly `Context` object, wrapping the internal `RequestContext`.\n8.  **Function Call:** The `ToolManager` calls your Python function (`long_running_task`), passing the validated arguments from the client request *and* the newly created `Context` object to the `ctx` parameter.\n9.  **Context Usage:** Your function code now runs and can use the methods on the `ctx` object (like `ctx.info`, `ctx.report_progress`). These methods internally use the `RequestContext` to interact with the session (e.g., `session.send_log_message`, `session.send_progress_notification`).\n\nHere\u2019s a simplified view:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport\n    participant FastMCPServer as FastMCP Server Framework\n    participant ToolManager as Server Tool Manager\n    participant YourToolFunc as Your Tool Function (e.g., long_running_task)\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"long_running_task\")\n    Transport->>+FastMCPServer: Deliver request message\n    FastMCPServer->>FastMCPServer: Create internal RequestContext (holds session, req_id)\n    FastMCPServer->>+ToolManager: Request to execute \"long_running_task\" with args & RequestContext\n    ToolManager->>ToolManager: Find 'long_running_task' function registration\n    ToolManager->>ToolManager: Inspect function signature, see 'ctx: Context'\n    ToolManager->>ToolManager: Create user-friendly Context object (wraps RequestContext)\n    ToolManager->>+YourToolFunc: Call long_running_task(args=..., ctx=ContextObject)\n    YourToolFunc->>YourToolFunc: Run your code...\n    YourToolFunc->>ContextObject: Call ctx.info(\"Starting...\")\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_log_message(...)\n    Session->>Transport: Send log notification\n    Transport-->>Client: Deliver log notification\n    YourToolFunc->>ContextObject: Call await ctx.report_progress(...)\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_progress_notification(...)\n    Session->>Transport: Send progress notification\n    Transport-->>Client: Deliver progress notification\n    Note over YourToolFunc: Continues execution...\n    YourToolFunc-->>-ToolManager: Return result\n    ToolManager-->>-FastMCPServer: Return result\n    FastMCPServer->>FastMCPServer: Format MCP response\n    FastMCPServer->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\n*   The `Context` class itself is defined in `src/mcp/server/fastmcp/server.py`. It mostly acts as a wrapper around the lower-level `RequestContext` (defined in `src/mcp/shared/context.py`) and the `FastMCP` instance.\n\n    ```python\n    # src/mcp/server/fastmcp/server.py (Simplified Context Class)\n    class Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n        _request_context: RequestContext[...] | None\n        _fastmcp: FastMCP | None\n        # ... (initializer) ...\n\n        @property\n        def request_context(self) -> RequestContext[...]:\n            # Provides access to the internal context\n            # ... (error handling) ...\n            return self._request_context\n\n        async def report_progress(self, progress: float, total: float | None = None) -> None:\n            # Uses request_context.session to send notification\n            progress_token = self.request_context.meta.progressToken # Simplified\n            if progress_token:\n                await self.request_context.session.send_progress_notification(...)\n\n        async def log(self, level: ..., message: str, ...) -> None:\n            # Uses request_context.session to send log message\n            await self.request_context.session.send_log_message(...)\n\n        # ... other methods like info(), error(), read_resource() ...\n    ```\n\n*   The `Tool` class (in `src/mcp/server/fastmcp/tools/base.py`) stores whether a function expects context (`context_kwarg`). The `ToolManager` (in `src/mcp/server/fastmcp/tools/tool_manager.py`) uses this information when calling the function via `tool.run()`.\n\n    ```python\n    # src/mcp/server/fastmcp/tools/base.py (Simplified Tool.run)\n    class Tool(BaseModel):\n        # ... fields: fn, name, context_kwarg ...\n\n        async def run(self, arguments: dict, context: Context | None = None) -> Any:\n            # Prepare kwargs, including context if needed\n            extra_kwargs = {}\n            if self.context_kwarg is not None and context is not None:\n                extra_kwargs[self.context_kwarg] = context\n\n            # Call the actual function with args and context\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn, self.is_async, arguments, extra_kwargs\n            )\n    ```\n\n*   The main `FastMCP` class (`src/mcp/server/fastmcp/server.py`) provides the `get_context()` method which creates the `Context` object when needed, using the currently active `request_context` managed by the underlying `MCPServer`.\n\nYou don't need to interact with these internal details directly, but seeing them helps understand that the `Context` isn't magic \u2013 it's a well-defined object passed along during request handling by the `FastMCP` framework.\n\n## Conclusion\n\nYou've learned about the **FastMCP Context** (`Context`), a powerful object provided to your server-side tool and resource functions.\n\n*   It acts as a **toolkit** giving access to server capabilities *during* request handling.\n*   You request it simply by adding a parameter with the `Context` type hint to your function.\n*   It allows your function logic to send **logs** and **progress updates** back to the client.\n*   It enables your function to **read other resources** from the same server.\n*   It provides access to **request-specific information**.\n\nThe `Context` object bridges the gap between your isolated function logic and the broader server session, making your FastMCP server tools much more interactive and capable.\n\nWe've now covered the main concepts for both clients (`ClientSession`) and servers (`FastMCP`), including the core capabilities (Tools, Resources, Prompts), communication (Transports), and server-side interaction (Context). To round things off, the next chapter will provide a quick reference guide to the key data structures used in the MCP protocol itself: [MCP Protocol Types](08_mcp_protocol_types.md).\n\n        Relevant Code Snippets:\n        --- File: src/mcp/__init__.py ---\nfrom .client.session import ClientSession\nfrom .client.stdio import StdioServerParameters, stdio_client\nfrom .server.session import ServerSession\nfrom .server.stdio import stdio_server\nfrom .shared.exceptions import McpError\nfrom .types import (\n    CallToolRequest,\n    ClientCapabilities,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    CompleteRequest,\n    CreateMessageRequest,\n    CreateMessageResult,\n    ErrorData,\n    GetPromptRequest,\n    GetPromptResult,\n    Implementation,\n    IncludeContext,\n    InitializedNotification,\n    InitializeRequest,\n    InitializeResult,\n    JSONRPCError,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    ListPromptsRequest,\n    ListPromptsResult,\n    ListResourcesRequest,\n    ListResourcesResult,\n    ListToolsResult,\n    LoggingLevel,\n    LoggingMessageNotification,\n    Notification,\n    PingRequest,\n    ProgressNotification,\n    PromptsCapability,\n    ReadResourceRequest,\n    ReadResourceResult,\n    Resource,\n    ResourcesCapability,\n    ResourceUpdatedNotification,\n    RootsCapability,\n    SamplingMessage,\n    ServerCapabilities,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n    SetLevelRequest,\n    StopReason,\n    SubscribeRequest,\n    Tool,\n    ToolsCapability,\n    UnsubscribeRequest,\n)\nfrom .types import (\n    Role as SamplingRole,\n)\n\n__all__ = [\n    \"CallToolRequest\",\n    \"ClientCapabilities\",\n    \"ClientNotification\",\n    \"ClientRequest\",\n    \"ClientResult\",\n    \"ClientSession\",\n    \"CreateMessageRequest\",\n    \"CreateMessageResult\",\n    \"ErrorData\",\n    \"GetPromptRequest\",\n    \"GetPromptResult\",\n    \"Implementation\",\n    \"IncludeContext\",\n    \"InitializeRequest\",\n    \"InitializeResult\",\n    \"InitializedNotification\",\n    \"JSONRPCError\",\n    \"JSONRPCRequest\",\n    \"ListPromptsRequest\",\n    \"ListPromptsResult\",\n    \"ListResourcesRequest\",\n    \"ListResourcesResult\",\n    \"ListToolsResult\",\n    \"LoggingLevel\",\n    \"LoggingMessageNotification\",\n    \"McpError\",\n    \"Notification\",\n    \"PingRequest\",\n    \"ProgressNotification\",\n    \"PromptsCapability\",\n    \"ReadResourceRequest\",\n    \"ReadResourceResult\",\n    \"ResourcesCapability\",\n    \"ResourceUpdatedNotification\",\n    \"Resource\",\n    \"RootsCapability\",\n    \"SamplingMessage\",\n    \"SamplingRole\",\n    \"ServerCapabilities\",\n    \"ServerNotification\",\n    \"ServerRequest\",\n    \"ServerResult\",\n    \"ServerSession\",\n    \"SetLevelRequest\",\n    \"StdioServerParameters\",\n    \"StopReason\",\n    \"SubscribeRequest\",\n    \"Tool\",\n    \"ToolsCapability\",\n    \"UnsubscribeRequest\",\n    \"stdio_client\",\n    \"stdio_server\",\n    \"CompleteRequest\",\n    \"JSONRPCResponse\",\n]\n\n\n--- File: src/mcp/client/session.py ---\nfrom datetime import timedelta\nfrom typing import Any, Protocol\n\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl, TypeAdapter\n\nimport mcp.types as types\nfrom mcp.shared.context import RequestContext\nfrom mcp.shared.session import BaseSession, RequestResponder\nfrom mcp.shared.version import SUPPORTED_PROTOCOL_VERSIONS\n\nDEFAULT_CLIENT_INFO = types.Implementation(name=\"mcp\", version=\"0.1.0\")\n\n\nclass SamplingFnT(Protocol):\n    async def __call__(\n        self,\n        context: RequestContext[\"ClientSession\", Any],\n        params: types.CreateMessageRequestParams,\n    ) -> types.CreateMessageResult | types.ErrorData: ...\n\n\nclass ListRootsFnT(Protocol):\n    async def __call__(\n        self, context: RequestContext[\"ClientSession\", Any]\n    ) -> types.ListRootsResult | types.ErrorData: ...\n\n\nclass LoggingFnT(Protocol):\n    async def __call__(\n        self,\n        params: types.LoggingMessageNotificationParams,\n    ) -> None: ...\n\n\nclass MessageHandlerFnT(Protocol):\n    async def __call__(\n        self,\n        message: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None: ...\n\n\nasync def _default_message_handler(\n    message: RequestResponder[types.ServerRequest, types.ClientResult]\n    | types.ServerNotification\n    | Exception,\n) -> None:\n    await anyio.lowlevel.checkpoint()\n\n\nasync def _default_sampling_callback(\n    context: RequestContext[\"ClientSession\", Any],\n    params: types.CreateMessageRequestParams,\n) -> types.CreateMessageResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"Sampling not supported\",\n    )\n\n\nasync def _default_list_roots_callback(\n    context: RequestContext[\"ClientSession\", Any],\n) -> types.ListRootsResult | types.ErrorData:\n    return types.ErrorData(\n        code=types.INVALID_REQUEST,\n        message=\"List roots not supported\",\n    )\n\n\nasync def _default_logging_callback(\n    params: types.LoggingMessageNotificationParams,\n) -> None:\n    pass\n\n\nClientResponse: TypeAdapter[types.ClientResult | types.ErrorData] = TypeAdapter(\n    types.ClientResult | types.ErrorData\n)\n\n\nclass ClientSession(\n    BaseSession[\n        types.ClientRequest,\n        types.ClientNotification,\n        types.ClientResult,\n        types.ServerRequest,\n        types.ServerNotification,\n    ]\n):\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        read_timeout_seconds: timedelta | None = None,\n        sampling_callback: SamplingFnT | None = None,\n        list_roots_callback: ListRootsFnT | None = None,\n        logging_callback: LoggingFnT | None = None,\n        message_handler: MessageHandlerFnT | None = None,\n        client_info: types.Implementation | None = None,\n    ) -> None:\n        super().__init__(\n            read_stream,\n            write_stream,\n            types.ServerRequest,\n            types.ServerNotification,\n            read_timeout_seconds=read_timeout_seconds,\n        )\n        self._client_info = client_info or DEFAULT_CLIENT_INFO\n        self._sampling_callback = sampling_callback or _default_sampling_callback\n        self._list_roots_callback = list_roots_callback or _default_list_roots_callback\n        self._logging_callback = logging_callback or _default_logging_callback\n        self._message_handler = message_handler or _default_message_handler\n\n    async def initialize(self) -> types.InitializeResult:\n        sampling = types.SamplingCapability()\n        roots = types.RootsCapability(\n            # TODO: Should this be based on whether we\n            # _will_ send notifications, or only whether\n            # they're supported?\n            listChanged=True,\n        )\n\n        result = await self.send_request(\n            types.ClientRequest(\n                types.InitializeRequest(\n                    method=\"initialize\",\n                    params=types.InitializeRequestParams(\n                        protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                        capabilities=types.ClientCapabilities(\n                            sampling=sampling,\n                            experimental=None,\n                            roots=roots,\n                        ),\n                        clientInfo=self._client_info,\n                    ),\n                )\n            ),\n            types.InitializeResult,\n        )\n\n        if result.protocolVersion not in SUPPORTED_PROTOCOL_VERSIONS:\n            raise RuntimeError(\n                \"Unsupported protocol version from the server: \"\n                f\"{result.protocolVersion}\"\n            )\n\n        await self.send_notification(\n            types.ClientNotification(\n                types.InitializedNotification(method=\"notifications/initialized\")\n            )\n        )\n\n        return result\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                ),\n            )\n        )\n\n    async def set_logging_level(self, level: types.LoggingLevel) -> types.EmptyResult:\n        \"\"\"Send a logging/setLevel request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SetLevelRequest(\n                    method=\"logging/setLevel\",\n                    params=types.SetLevelRequestParams(level=level),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def list_resources(self) -> types.ListResourcesResult:\n        \"\"\"Send a resources/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourcesRequest(\n                    method=\"resources/list\",\n                )\n            ),\n            types.ListResourcesResult,\n        )\n\n    async def list_resource_templates(self) -> types.ListResourceTemplatesResult:\n        \"\"\"Send a resources/templates/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListResourceTemplatesRequest(\n                    method=\"resources/templates/list\",\n                )\n            ),\n            types.ListResourceTemplatesResult,\n        )\n\n    async def read_resource(self, uri: AnyUrl) -> types.ReadResourceResult:\n        \"\"\"Send a resources/read request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ReadResourceRequest(\n                    method=\"resources/read\",\n                    params=types.ReadResourceRequestParams(uri=uri),\n                )\n            ),\n            types.ReadResourceResult,\n        )\n\n    async def subscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/subscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.SubscribeRequest(\n                    method=\"resources/subscribe\",\n                    params=types.SubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def unsubscribe_resource(self, uri: AnyUrl) -> types.EmptyResult:\n        \"\"\"Send a resources/unsubscribe request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.UnsubscribeRequest(\n                    method=\"resources/unsubscribe\",\n                    params=types.UnsubscribeRequestParams(uri=uri),\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CallToolRequest(\n                    method=\"tools/call\",\n                    params=types.CallToolRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.CallToolResult,\n        )\n\n    async def list_prompts(self) -> types.ListPromptsResult:\n        \"\"\"Send a prompts/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListPromptsRequest(\n                    method=\"prompts/list\",\n                )\n            ),\n            types.ListPromptsResult,\n        )\n\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.GetPromptRequest(\n                    method=\"prompts/get\",\n                    params=types.GetPromptRequestParams(name=name, arguments=arguments),\n                )\n            ),\n            types.GetPromptResult,\n        )\n\n    async def complete(\n        self,\n        ref: types.ResourceReference | types.PromptReference,\n        argument: dict[str, str],\n    ) -> types.CompleteResult:\n        \"\"\"Send a completion/complete request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.CompleteRequest(\n                    method=\"completion/complete\",\n                    params=types.CompleteRequestParams(\n                        ref=ref,\n                        argument=types.CompletionArgument(**argument),\n                    ),\n                )\n            ),\n            types.CompleteResult,\n        )\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        return await self.send_request(\n            types.ClientRequest(\n                types.ListToolsRequest(\n                    method=\"tools/list\",\n                )\n            ),\n            types.ListToolsResult,\n        )\n\n    async def send_roots_list_changed(self) -> None:\n        \"\"\"Send a roots/list_changed notification.\"\"\"\n        await self.send_notification(\n            types.ClientNotification(\n                types.RootsListChangedNotification(\n                    method=\"notifications/roots/list_changed\",\n                )\n            )\n        )\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ServerRequest, types.ClientResult]\n    ) -> None:\n        ctx = RequestContext[ClientSession, Any](\n            request_id=responder.request_id,\n            meta=responder.request_meta,\n            session=self,\n            lifespan_context=None,\n        )\n\n        match responder.request.root:\n            case types.CreateMessageRequest(params=params):\n                with responder:\n                    response = await self._sampling_callback(ctx, params)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.ListRootsRequest():\n                with responder:\n                    response = await self._list_roots_callback(ctx)\n                    client_response = ClientResponse.validate_python(response)\n                    await responder.respond(client_response)\n\n            case types.PingRequest():\n                with responder:\n                    return await responder.respond(\n                        types.ClientResult(root=types.EmptyResult())\n                    )\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[types.ServerRequest, types.ClientResult]\n        | types.ServerNotification\n        | Exception,\n    ) -> None:\n        \"\"\"Handle incoming messages by forwarding to the message handler.\"\"\"\n        await self._message_handler(req)\n\n    async def _received_notification(\n        self, notification: types.ServerNotification\n    ) -> None:\n        \"\"\"Handle notifications from the server.\"\"\"\n        # Process specific notification types\n        match notification.root:\n            case types.LoggingMessageNotification(params=params):\n                await self._logging_callback(params)\n            case _:\n                pass\n\n\n--- File: src/mcp/server/session.py ---\n\"\"\"\nServerSession Module\n\nThis module provides the ServerSession class, which manages communication between the\nserver and client in the MCP (Model Context Protocol) framework. It is most commonly\nused in MCP servers to interact with the client.\n\nCommon usage pattern:\n```\n    server = Server(name)\n\n    @server.call_tool()\n    async def handle_tool_call(ctx: RequestContext, arguments: dict[str, Any]) -> Any:\n        # Check client capabilities before proceeding\n        if ctx.session.check_client_capability(\n            types.ClientCapabilities(experimental={\"advanced_tools\": dict()})\n        ):\n            # Perform advanced tool operations\n            result = await perform_advanced_tool_operation(arguments)\n        else:\n            # Fall back to basic tool operations\n            result = await perform_basic_tool_operation(arguments)\n\n        return result\n\n    @server.list_prompts()\n    async def handle_list_prompts(ctx: RequestContext) -> list[types.Prompt]:\n        # Access session for any necessary checks or operations\n        if ctx.session.client_params:\n            # Customize prompts based on client initialization parameters\n            return generate_custom_prompts(ctx.session.client_params)\n        else:\n            return default_prompts\n```\n\nThe ServerSession class is typically used internally by the Server class and should not\nbe instantiated directly by users of the MCP framework.\n\"\"\"\n\nfrom enum import Enum\nfrom typing import Any, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import AnyUrl\n\nimport mcp.types as types\nfrom mcp.server.models import InitializationOptions\nfrom mcp.shared.session import (\n    BaseSession,\n    RequestResponder,\n)\n\n\nclass InitializationState(Enum):\n    NotInitialized = 1\n    Initializing = 2\n    Initialized = 3\n\n\nServerSessionT = TypeVar(\"ServerSessionT\", bound=\"ServerSession\")\n\nServerRequestResponder = (\n    RequestResponder[types.ClientRequest, types.ServerResult]\n    | types.ClientNotification\n    | Exception\n)\n\n\nclass ServerSession(\n    BaseSession[\n        types.ServerRequest,\n        types.ServerNotification,\n        types.ServerResult,\n        types.ClientRequest,\n        types.ClientNotification,\n    ]\n):\n    _initialized: InitializationState = InitializationState.NotInitialized\n    _client_params: types.InitializeRequestParams | None = None\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[types.JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[types.JSONRPCMessage],\n        init_options: InitializationOptions,\n    ) -> None:\n        super().__init__(\n            read_stream, write_stream, types.ClientRequest, types.ClientNotification\n        )\n        self._initialization_state = InitializationState.NotInitialized\n        self._init_options = init_options\n        self._incoming_message_stream_writer, self._incoming_message_stream_reader = (\n            anyio.create_memory_object_stream[ServerRequestResponder](0)\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_reader.aclose()\n        )\n        self._exit_stack.push_async_callback(\n            lambda: self._incoming_message_stream_writer.aclose()\n        )\n\n    @property\n    def client_params(self) -> types.InitializeRequestParams | None:\n        return self._client_params\n\n    def check_client_capability(self, capability: types.ClientCapabilities) -> bool:\n        \"\"\"Check if the client supports a specific capability.\"\"\"\n        if self._client_params is None:\n            return False\n\n        # Get client capabilities from initialization params\n        client_caps = self._client_params.capabilities\n\n        # Check each specified capability in the passed in capability object\n        if capability.roots is not None:\n            if client_caps.roots is None:\n                return False\n            if capability.roots.listChanged and not client_caps.roots.listChanged:\n                return False\n\n        if capability.sampling is not None:\n            if client_caps.sampling is None:\n                return False\n\n        if capability.experimental is not None:\n            if client_caps.experimental is None:\n                return False\n            # Check each experimental capability\n            for exp_key, exp_value in capability.experimental.items():\n                if (\n                    exp_key not in client_caps.experimental\n                    or client_caps.experimental[exp_key] != exp_value\n                ):\n                    return False\n\n        return True\n\n    async def _received_request(\n        self, responder: RequestResponder[types.ClientRequest, types.ServerResult]\n    ):\n        match responder.request.root:\n            case types.InitializeRequest(params=params):\n                self._initialization_state = InitializationState.Initializing\n                self._client_params = params\n                with responder:\n                    await responder.respond(\n                        types.ServerResult(\n                            types.InitializeResult(\n                                protocolVersion=types.LATEST_PROTOCOL_VERSION,\n                                capabilities=self._init_options.capabilities,\n                                serverInfo=types.Implementation(\n                                    name=self._init_options.server_name,\n                                    version=self._init_options.server_version,\n                                ),\n                                instructions=self._init_options.instructions,\n                            )\n                        )\n                    )\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received request before initialization was complete\"\n                    )\n\n    async def _received_notification(\n        self, notification: types.ClientNotification\n    ) -> None:\n        # Need this to avoid ASYNC910\n        await anyio.lowlevel.checkpoint()\n        match notification.root:\n            case types.InitializedNotification():\n                self._initialization_state = InitializationState.Initialized\n            case _:\n                if self._initialization_state != InitializationState.Initialized:\n                    raise RuntimeError(\n                        \"Received notification before initialization was complete\"\n                    )\n\n    async def send_log_message(\n        self, level: types.LoggingLevel, data: Any, logger: str | None = None\n    ) -> None:\n        \"\"\"Send a log message notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.LoggingMessageNotification(\n                    method=\"notifications/message\",\n                    params=types.LoggingMessageNotificationParams(\n                        level=level,\n                        data=data,\n                        logger=logger,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_updated(self, uri: AnyUrl) -> None:\n        \"\"\"Send a resource updated notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceUpdatedNotification(\n                    method=\"notifications/resources/updated\",\n                    params=types.ResourceUpdatedNotificationParams(uri=uri),\n                )\n            )\n        )\n\n    async def create_message(\n        self,\n        messages: list[types.SamplingMessage],\n        *,\n        max_tokens: int,\n        system_prompt: str | None = None,\n        include_context: types.IncludeContext | None = None,\n        temperature: float | None = None,\n        stop_sequences: list[str] | None = None,\n        metadata: dict[str, Any] | None = None,\n        model_preferences: types.ModelPreferences | None = None,\n    ) -> types.CreateMessageResult:\n        \"\"\"Send a sampling/create_message request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.CreateMessageRequest(\n                    method=\"sampling/createMessage\",\n                    params=types.CreateMessageRequestParams(\n                        messages=messages,\n                        systemPrompt=system_prompt,\n                        includeContext=include_context,\n                        temperature=temperature,\n                        maxTokens=max_tokens,\n                        stopSequences=stop_sequences,\n                        metadata=metadata,\n                        modelPreferences=model_preferences,\n                    ),\n                )\n            ),\n            types.CreateMessageResult,\n        )\n\n    async def list_roots(self) -> types.ListRootsResult:\n        \"\"\"Send a roots/list request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.ListRootsRequest(\n                    method=\"roots/list\",\n                )\n            ),\n            types.ListRootsResult,\n        )\n\n    async def send_ping(self) -> types.EmptyResult:\n        \"\"\"Send a ping request.\"\"\"\n        return await self.send_request(\n            types.ServerRequest(\n                types.PingRequest(\n                    method=\"ping\",\n                )\n            ),\n            types.EmptyResult,\n        )\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"Send a progress notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ProgressNotification(\n                    method=\"notifications/progress\",\n                    params=types.ProgressNotificationParams(\n                        progressToken=progress_token,\n                        progress=progress,\n                        total=total,\n                    ),\n                )\n            )\n        )\n\n    async def send_resource_list_changed(self) -> None:\n        \"\"\"Send a resource list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ResourceListChangedNotification(\n                    method=\"notifications/resources/list_changed\",\n                )\n            )\n        )\n\n    async def send_tool_list_changed(self) -> None:\n        \"\"\"Send a tool list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.ToolListChangedNotification(\n                    method=\"notifications/tools/list_changed\",\n                )\n            )\n        )\n\n    async def send_prompt_list_changed(self) -> None:\n        \"\"\"Send a prompt list changed notification.\"\"\"\n        await self.send_notification(\n            types.ServerNotification(\n                types.PromptListChangedNotification(\n                    method=\"notifications/prompts/list_changed\",\n                )\n            )\n        )\n\n    async def _handle_incoming(self, req: ServerRequestResponder) -> None:\n        await self._incoming_message_stream_writer.send(req)\n\n    @property\n    def incoming_messages(\n        self,\n    ) -> MemoryObjectReceiveStream[ServerRequestResponder]:\n        return self._incoming_message_stream_reader\n\n\n--- File: src/mcp/shared/session.py ---\nimport logging\nfrom collections.abc import Callable\nfrom contextlib import AsyncExitStack\nfrom datetime import timedelta\nfrom types import TracebackType\nfrom typing import Any, Generic, TypeVar\n\nimport anyio\nimport anyio.lowlevel\nimport httpx\nfrom anyio.streams.memory import MemoryObjectReceiveStream, MemoryObjectSendStream\nfrom pydantic import BaseModel\nfrom typing_extensions import Self\n\nfrom mcp.shared.exceptions import McpError\nfrom mcp.types import (\n    CancelledNotification,\n    ClientNotification,\n    ClientRequest,\n    ClientResult,\n    ErrorData,\n    JSONRPCError,\n    JSONRPCMessage,\n    JSONRPCNotification,\n    JSONRPCRequest,\n    JSONRPCResponse,\n    RequestParams,\n    ServerNotification,\n    ServerRequest,\n    ServerResult,\n)\n\nSendRequestT = TypeVar(\"SendRequestT\", ClientRequest, ServerRequest)\nSendResultT = TypeVar(\"SendResultT\", ClientResult, ServerResult)\nSendNotificationT = TypeVar(\"SendNotificationT\", ClientNotification, ServerNotification)\nReceiveRequestT = TypeVar(\"ReceiveRequestT\", ClientRequest, ServerRequest)\nReceiveResultT = TypeVar(\"ReceiveResultT\", bound=BaseModel)\nReceiveNotificationT = TypeVar(\n    \"ReceiveNotificationT\", ClientNotification, ServerNotification\n)\n\nRequestId = str | int\n\n\nclass RequestResponder(Generic[ReceiveRequestT, SendResultT]):\n    \"\"\"Handles responding to MCP requests and manages request lifecycle.\n\n    This class MUST be used as a context manager to ensure proper cleanup and\n    cancellation handling:\n\n    Example:\n        with request_responder as resp:\n            await resp.respond(result)\n\n    The context manager ensures:\n    1. Proper cancellation scope setup and cleanup\n    2. Request completion tracking\n    3. Cleanup of in-flight requests\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: RequestId,\n        request_meta: RequestParams.Meta | None,\n        request: ReceiveRequestT,\n        session: \"\"\"BaseSession[\n            SendRequestT,\n            SendNotificationT,\n            SendResultT,\n            ReceiveRequestT,\n            ReceiveNotificationT\n        ]\"\"\",\n        on_complete: Callable[[\"RequestResponder[ReceiveRequestT, SendResultT]\"], Any],\n    ) -> None:\n        self.request_id = request_id\n        self.request_meta = request_meta\n        self.request = request\n        self._session = session\n        self._completed = False\n        self._cancel_scope = anyio.CancelScope()\n        self._on_complete = on_complete\n        self._entered = False  # Track if we're in a context manager\n\n    def __enter__(self) -> \"RequestResponder[ReceiveRequestT, SendResultT]\":\n        \"\"\"Enter the context manager, enabling request cancellation tracking.\"\"\"\n        self._entered = True\n        self._cancel_scope = anyio.CancelScope()\n        self._cancel_scope.__enter__()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> None:\n        \"\"\"Exit the context manager, performing cleanup and notifying completion.\"\"\"\n        try:\n            if self._completed:\n                self._on_complete(self)\n        finally:\n            self._entered = False\n            if not self._cancel_scope:\n                raise RuntimeError(\"No active cancel scope\")\n            self._cancel_scope.__exit__(exc_type, exc_val, exc_tb)\n\n    async def respond(self, response: SendResultT | ErrorData) -> None:\n        \"\"\"Send a response for this request.\n\n        Must be called within a context manager block.\n        Raises:\n            RuntimeError: If not used within a context manager\n            AssertionError: If request was already responded to\n        \"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        assert not self._completed, \"Request already responded to\"\n\n        if not self.cancelled:\n            self._completed = True\n\n            await self._session._send_response(  # type: ignore[reportPrivateUsage]\n                request_id=self.request_id, response=response\n            )\n\n    async def cancel(self) -> None:\n        \"\"\"Cancel this request and mark it as completed.\"\"\"\n        if not self._entered:\n            raise RuntimeError(\"RequestResponder must be used as a context manager\")\n        if not self._cancel_scope:\n            raise RuntimeError(\"No active cancel scope\")\n\n        self._cancel_scope.cancel()\n        self._completed = True  # Mark as completed so it's removed from in_flight\n        # Send an error response to indicate cancellation\n        await self._session._send_response(  # type: ignore[reportPrivateUsage]\n            request_id=self.request_id,\n            response=ErrorData(code=0, message=\"Request cancelled\", data=None),\n        )\n\n    @property\n    def in_flight(self) -> bool:\n        return not self._completed and not self.cancelled\n\n    @property\n    def cancelled(self) -> bool:\n        return self._cancel_scope.cancel_called\n\n\nclass BaseSession(\n    Generic[\n        SendRequestT,\n        SendNotificationT,\n        SendResultT,\n        ReceiveRequestT,\n        ReceiveNotificationT,\n    ],\n):\n    \"\"\"\n    Implements an MCP \"session\" on top of read/write streams, including features\n    like request/response linking, notifications, and progress.\n\n    This class is an async context manager that automatically starts processing\n    messages when entered.\n    \"\"\"\n\n    _response_streams: dict[\n        RequestId, MemoryObjectSendStream[JSONRPCResponse | JSONRPCError]\n    ]\n    _request_id: int\n    _in_flight: dict[RequestId, RequestResponder[ReceiveRequestT, SendResultT]]\n\n    def __init__(\n        self,\n        read_stream: MemoryObjectReceiveStream[JSONRPCMessage | Exception],\n        write_stream: MemoryObjectSendStream[JSONRPCMessage],\n        receive_request_type: type[ReceiveRequestT],\n        receive_notification_type: type[ReceiveNotificationT],\n        # If none, reading will never time out\n        read_timeout_seconds: timedelta | None = None,\n    ) -> None:\n        self._read_stream = read_stream\n        self._write_stream = write_stream\n        self._response_streams = {}\n        self._request_id = 0\n        self._receive_request_type = receive_request_type\n        self._receive_notification_type = receive_notification_type\n        self._read_timeout_seconds = read_timeout_seconds\n        self._in_flight = {}\n\n        self._exit_stack = AsyncExitStack()\n\n    async def __aenter__(self) -> Self:\n        self._task_group = anyio.create_task_group()\n        await self._task_group.__aenter__()\n        self._task_group.start_soon(self._receive_loop)\n        return self\n\n    async def __aexit__(\n        self,\n        exc_type: type[BaseException] | None,\n        exc_val: BaseException | None,\n        exc_tb: TracebackType | None,\n    ) -> bool | None:\n        await self._exit_stack.aclose()\n        # Using BaseSession as a context manager should not block on exit (this\n        # would be very surprising behavior), so make sure to cancel the tasks\n        # in the task group.\n        self._task_group.cancel_scope.cancel()\n        return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n\n    async def send_request(\n        self,\n        request: SendRequestT,\n        result_type: type[ReceiveResultT],\n    ) -> ReceiveResultT:\n        \"\"\"\n        Sends a request and wait for a response. Raises an McpError if the\n        response contains an error.\n\n        Do not use this method to emit notifications! Use send_notification()\n        instead.\n        \"\"\"\n\n        request_id = self._request_id\n        self._request_id = request_id + 1\n\n        response_stream, response_stream_reader = anyio.create_memory_object_stream[\n            JSONRPCResponse | JSONRPCError\n        ](1)\n        self._response_streams[request_id] = response_stream\n\n        self._exit_stack.push_async_callback(lambda: response_stream.aclose())\n        self._exit_stack.push_async_callback(lambda: response_stream_reader.aclose())\n\n        jsonrpc_request = JSONRPCRequest(\n            jsonrpc=\"2.0\",\n            id=request_id,\n            **request.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        # TODO: Support progress callbacks\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_request))\n\n        try:\n            with anyio.fail_after(\n                None\n                if self._read_timeout_seconds is None\n                else self._read_timeout_seconds.total_seconds()\n            ):\n                response_or_error = await response_stream_reader.receive()\n        except TimeoutError:\n            raise McpError(\n                ErrorData(\n                    code=httpx.codes.REQUEST_TIMEOUT,\n                    message=(\n                        f\"Timed out while waiting for response to \"\n                        f\"{request.__class__.__name__}. Waited \"\n                        f\"{self._read_timeout_seconds} seconds.\"\n                    ),\n                )\n            )\n\n        if isinstance(response_or_error, JSONRPCError):\n            raise McpError(response_or_error.error)\n        else:\n            return result_type.model_validate(response_or_error.result)\n\n    async def send_notification(self, notification: SendNotificationT) -> None:\n        \"\"\"\n        Emits a notification, which is a one-way message that does not expect\n        a response.\n        \"\"\"\n        jsonrpc_notification = JSONRPCNotification(\n            jsonrpc=\"2.0\",\n            **notification.model_dump(by_alias=True, mode=\"json\", exclude_none=True),\n        )\n\n        await self._write_stream.send(JSONRPCMessage(jsonrpc_notification))\n\n    async def _send_response(\n        self, request_id: RequestId, response: SendResultT | ErrorData\n    ) -> None:\n        if isinstance(response, ErrorData):\n            jsonrpc_error = JSONRPCError(jsonrpc=\"2.0\", id=request_id, error=response)\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_error))\n        else:\n            jsonrpc_response = JSONRPCResponse(\n                jsonrpc=\"2.0\",\n                id=request_id,\n                result=response.model_dump(\n                    by_alias=True, mode=\"json\", exclude_none=True\n                ),\n            )\n            await self._write_stream.send(JSONRPCMessage(jsonrpc_response))\n\n    async def _receive_loop(self) -> None:\n        async with (\n            self._read_stream,\n            self._write_stream,\n        ):\n            async for message in self._read_stream:\n                if isinstance(message, Exception):\n                    await self._handle_incoming(message)\n                elif isinstance(message.root, JSONRPCRequest):\n                    validated_request = self._receive_request_type.model_validate(\n                        message.root.model_dump(\n                            by_alias=True, mode=\"json\", exclude_none=True\n                        )\n                    )\n\n                    responder = RequestResponder(\n                        request_id=message.root.id,\n                        request_meta=validated_request.root.params.meta\n                        if validated_request.root.params\n                        else None,\n                        request=validated_request,\n                        session=self,\n                        on_complete=lambda r: self._in_flight.pop(r.request_id, None),\n                    )\n\n                    self._in_flight[responder.request_id] = responder\n                    await self._received_request(responder)\n\n                    if not responder._completed:  # type: ignore[reportPrivateUsage]\n                        await self._handle_incoming(responder)\n\n                elif isinstance(message.root, JSONRPCNotification):\n                    try:\n                        notification = self._receive_notification_type.model_validate(\n                            message.root.model_dump(\n                                by_alias=True, mode=\"json\", exclude_none=True\n                            )\n                        )\n                        # Handle cancellation notifications\n                        if isinstance(notification.root, CancelledNotification):\n                            cancelled_id = notification.root.params.requestId\n                            if cancelled_id in self._in_flight:\n                                await self._in_flight[cancelled_id].cancel()\n                        else:\n                            await self._received_notification(notification)\n                            await self._handle_incoming(notification)\n                    except Exception as e:\n                        # For other validation errors, log and continue\n                        logging.warning(\n                            f\"Failed to validate notification: {e}. \"\n                            f\"Message was: {message.root}\"\n                        )\n                else:  # Response or error\n                    stream = self._response_streams.pop(message.root.id, None)\n                    if stream:\n                        await stream.send(message.root)\n                    else:\n                        await self._handle_incoming(\n                            RuntimeError(\n                                \"Received response with an unknown \"\n                                f\"request ID: {message}\"\n                            )\n                        )\n\n    async def _received_request(\n        self, responder: RequestResponder[ReceiveRequestT, SendResultT]\n    ) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a request without needing to\n        listen on the message stream.\n\n        If the request is responded to within this method, it will not be\n        forwarded on to the message stream.\n        \"\"\"\n\n    async def _received_notification(self, notification: ReceiveNotificationT) -> None:\n        \"\"\"\n        Can be overridden by subclasses to handle a notification without needing\n        to listen on the message stream.\n        \"\"\"\n\n    async def send_progress_notification(\n        self, progress_token: str | int, progress: float, total: float | None = None\n    ) -> None:\n        \"\"\"\n        Sends a progress notification for a request that is currently being\n        processed.\n        \"\"\"\n\n    async def _handle_incoming(\n        self,\n        req: RequestResponder[ReceiveRequestT, SendResultT]\n        | ReceiveNotificationT\n        | Exception,\n    ) -> None:\n        \"\"\"A generic handler for incoming messages. Overwritten by subclasses.\"\"\"\n        pass\n\n\n--- File: src/mcp/types.py ---\nfrom collections.abc import Callable\nfrom typing import (\n    Annotated,\n    Any,\n    Generic,\n    Literal,\n    TypeAlias,\n    TypeVar,\n)\n\nfrom pydantic import BaseModel, ConfigDict, Field, FileUrl, RootModel\nfrom pydantic.networks import AnyUrl, UrlConstraints\n\n\"\"\"\nModel Context Protocol bindings for Python\n\nThese bindings were generated from https://github.com/modelcontextprotocol/specification,\nusing Claude, with a prompt something like the following:\n\nGenerate idiomatic Python bindings for this schema for MCP, or the \"Model Context\nProtocol.\" The schema is defined in TypeScript, but there's also a JSON Schema version\nfor reference.\n\n* For the bindings, let's use Pydantic V2 models.\n* Each model should allow extra fields everywhere, by specifying `model_config =\n  ConfigDict(extra='allow')`. Do this in every case, instead of a custom base class.\n* Union types should be represented with a Pydantic `RootModel`.\n* Define additional model classes instead of using dictionaries. Do this even if they're\n  not separate types in the schema.\n\"\"\"\n\nLATEST_PROTOCOL_VERSION = \"2024-11-05\"\n\nProgressToken = str | int\nCursor = str\nRole = Literal[\"user\", \"assistant\"]\nRequestId = str | int\nAnyFunction: TypeAlias = Callable[..., Any]\n\n\nclass RequestParams(BaseModel):\n    class Meta(BaseModel):\n        progressToken: ProgressToken | None = None\n        \"\"\"\n        If specified, the caller requests out-of-band progress notifications for\n        this request (as represented by notifications/progress). The value of this\n        parameter is an opaque token that will be attached to any subsequent\n        notifications. The receiver is not obligated to provide these notifications.\n        \"\"\"\n\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n\n\nclass NotificationParams(BaseModel):\n    class Meta(BaseModel):\n        model_config = ConfigDict(extra=\"allow\")\n\n    meta: Meta | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This parameter name is reserved by MCP to allow clients and servers to attach\n    additional metadata to their notifications.\n    \"\"\"\n\n\nRequestParamsT = TypeVar(\"RequestParamsT\", bound=RequestParams | dict[str, Any] | None)\nNotificationParamsT = TypeVar(\n    \"NotificationParamsT\", bound=NotificationParams | dict[str, Any] | None\n)\nMethodT = TypeVar(\"MethodT\", bound=str)\n\n\nclass Request(BaseModel, Generic[RequestParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC requests.\"\"\"\n\n    method: MethodT\n    params: RequestParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PaginatedRequest(Request[RequestParamsT, MethodT]):\n    cursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the current pagination position.\n    If provided, the server should return results starting after this cursor.\n    \"\"\"\n\n\nclass Notification(BaseModel, Generic[NotificationParamsT, MethodT]):\n    \"\"\"Base class for JSON-RPC notifications.\"\"\"\n\n    method: MethodT\n    params: NotificationParamsT\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Result(BaseModel):\n    \"\"\"Base class for JSON-RPC results.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n    meta: dict[str, Any] | None = Field(alias=\"_meta\", default=None)\n    \"\"\"\n    This result property is reserved by the protocol to allow clients and servers to\n    attach additional metadata to their responses.\n    \"\"\"\n\n\nclass PaginatedResult(Result):\n    nextCursor: Cursor | None = None\n    \"\"\"\n    An opaque token representing the pagination position after the last returned result.\n    If present, there may be more results available.\n    \"\"\"\n\n\nclass JSONRPCRequest(Request[dict[str, Any] | None, str]):\n    \"\"\"A request that expects a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    method: str\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCNotification(Notification[dict[str, Any] | None, str]):\n    \"\"\"A notification which does not expect a response.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    params: dict[str, Any] | None = None\n\n\nclass JSONRPCResponse(BaseModel):\n    \"\"\"A successful (non-error) response to a request.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: RequestId\n    result: dict[str, Any]\n    model_config = ConfigDict(extra=\"allow\")\n\n\n# Standard JSON-RPC error codes\nPARSE_ERROR = -32700\nINVALID_REQUEST = -32600\nMETHOD_NOT_FOUND = -32601\nINVALID_PARAMS = -32602\nINTERNAL_ERROR = -32603\n\n\nclass ErrorData(BaseModel):\n    \"\"\"Error information for JSON-RPC error responses.\"\"\"\n\n    code: int\n    \"\"\"The error type that occurred.\"\"\"\n\n    message: str\n    \"\"\"\n    A short description of the error. The message SHOULD be limited to a concise single\n    sentence.\n    \"\"\"\n\n    data: Any | None = None\n    \"\"\"\n    Additional information about the error. The value of this member is defined by the\n    sender (e.g. detailed error information, nested errors etc.).\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCError(BaseModel):\n    \"\"\"A response to a request that indicates an error occurred.\"\"\"\n\n    jsonrpc: Literal[\"2.0\"]\n    id: str | int\n    error: ErrorData\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass JSONRPCMessage(\n    RootModel[JSONRPCRequest | JSONRPCNotification | JSONRPCResponse | JSONRPCError]\n):\n    pass\n\n\nclass EmptyResult(Result):\n    \"\"\"A response that indicates success but carries no data.\"\"\"\n\n\nclass Implementation(BaseModel):\n    \"\"\"Describes the name and version of an MCP implementation.\"\"\"\n\n    name: str\n    version: str\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass RootsCapability(BaseModel):\n    \"\"\"Capability for root operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether the client supports notifications for changes to the roots list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ClientCapabilities(BaseModel):\n    \"\"\"Capabilities a client may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the client supports.\"\"\"\n    sampling: SamplingCapability | None = None\n    \"\"\"Present if the client supports sampling from an LLM.\"\"\"\n    roots: RootsCapability | None = None\n    \"\"\"Present if the client supports listing roots.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptsCapability(BaseModel):\n    \"\"\"Capability for prompts operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the prompt list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourcesCapability(BaseModel):\n    \"\"\"Capability for resources operations.\"\"\"\n\n    subscribe: bool | None = None\n    \"\"\"Whether this server supports subscribing to resource updates.\"\"\"\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the resource list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ToolsCapability(BaseModel):\n    \"\"\"Capability for tools operations.\"\"\"\n\n    listChanged: bool | None = None\n    \"\"\"Whether this server supports notifications for changes to the tool list.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingCapability(BaseModel):\n    \"\"\"Capability for logging operations.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ServerCapabilities(BaseModel):\n    \"\"\"Capabilities that a server may support.\"\"\"\n\n    experimental: dict[str, dict[str, Any]] | None = None\n    \"\"\"Experimental, non-standard capabilities that the server supports.\"\"\"\n    logging: LoggingCapability | None = None\n    \"\"\"Present if the server supports sending log messages to the client.\"\"\"\n    prompts: PromptsCapability | None = None\n    \"\"\"Present if the server offers any prompt templates.\"\"\"\n    resources: ResourcesCapability | None = None\n    \"\"\"Present if the server offers any resources to read.\"\"\"\n    tools: ToolsCapability | None = None\n    \"\"\"Present if the server offers any tools to call.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequestParams(RequestParams):\n    \"\"\"Parameters for the initialize request.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The latest version of the Model Context Protocol that the client supports.\"\"\"\n    capabilities: ClientCapabilities\n    clientInfo: Implementation\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass InitializeRequest(Request[InitializeRequestParams, Literal[\"initialize\"]]):\n    \"\"\"\n    This request is sent from the client to the server when it first connects, asking it\n    to begin initialization.\n    \"\"\"\n\n    method: Literal[\"initialize\"]\n    params: InitializeRequestParams\n\n\nclass InitializeResult(Result):\n    \"\"\"After receiving an initialize request from the client, the server sends this.\"\"\"\n\n    protocolVersion: str | int\n    \"\"\"The version of the Model Context Protocol that the server wants to use.\"\"\"\n    capabilities: ServerCapabilities\n    serverInfo: Implementation\n    instructions: str | None = None\n    \"\"\"Instructions describing how to use the server and its features.\"\"\"\n\n\nclass InitializedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/initialized\"]]\n):\n    \"\"\"\n    This notification is sent from the client to the server after initialization has\n    finished.\n    \"\"\"\n\n    method: Literal[\"notifications/initialized\"]\n    params: NotificationParams | None = None\n\n\nclass PingRequest(Request[RequestParams | None, Literal[\"ping\"]]):\n    \"\"\"\n    A ping, issued by either the server or the client, to check that the other party is\n    still alive.\n    \"\"\"\n\n    method: Literal[\"ping\"]\n    params: RequestParams | None = None\n\n\nclass ProgressNotificationParams(NotificationParams):\n    \"\"\"Parameters for progress notifications.\"\"\"\n\n    progressToken: ProgressToken\n    \"\"\"\n    The progress token which was given in the initial request, used to associate this\n    notification with the request that is proceeding.\n    \"\"\"\n    progress: float\n    \"\"\"\n    The progress thus far. This should increase every time progress is made, even if the\n    total is unknown.\n    \"\"\"\n    total: float | None = None\n    \"\"\"Total number of items to process (or total progress required), if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ProgressNotification(\n    Notification[ProgressNotificationParams, Literal[\"notifications/progress\"]]\n):\n    \"\"\"\n    An out-of-band notification used to inform the receiver of a progress update for a\n    long-running request.\n    \"\"\"\n\n    method: Literal[\"notifications/progress\"]\n    params: ProgressNotificationParams\n\n\nclass ListResourcesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resources the server has.\"\"\"\n\n    method: Literal[\"resources/list\"]\n    params: RequestParams | None = None\n\n\nclass Annotations(BaseModel):\n    audience: list[Role] | None = None\n    priority: Annotated[float, Field(ge=0.0, le=1.0)] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Resource(BaseModel):\n    \"\"\"A known resource that the server is capable of reading.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    name: str\n    \"\"\"A human-readable name for this resource.\"\"\"\n    description: str | None = None\n    \"\"\"A description of what this resource represents.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    size: int | None = None\n    \"\"\"\n    The size of the raw resource content, in bytes (i.e., before base64 encoding\n    or any tokenization), if known.\n\n    This can be used by Hosts to display file sizes and estimate context window usage.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceTemplate(BaseModel):\n    \"\"\"A template description for resources available on the server.\"\"\"\n\n    uriTemplate: str\n    \"\"\"\n    A URI template (according to RFC 6570) that can be used to construct resource\n    URIs.\n    \"\"\"\n    name: str\n    \"\"\"A human-readable name for the type of resource this template refers to.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of what this template is for.\"\"\"\n    mimeType: str | None = None\n    \"\"\"\n    The MIME type for all resources that match this template. This should only be\n    included if all resources matching this template have the same type.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListResourcesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/list request from the client.\"\"\"\n\n    resources: list[Resource]\n\n\nclass ListResourceTemplatesRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"resources/templates/list\"]]\n):\n    \"\"\"Sent from the client to request a list of resource templates the server has.\"\"\"\n\n    method: Literal[\"resources/templates/list\"]\n    params: RequestParams | None = None\n\n\nclass ListResourceTemplatesResult(PaginatedResult):\n    \"\"\"The server's response to a resources/templates/list request from the client.\"\"\"\n\n    resourceTemplates: list[ResourceTemplate]\n\n\nclass ReadResourceRequestParams(RequestParams):\n    \"\"\"Parameters for reading a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to read. The URI can use any protocol; it is up to the\n    server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ReadResourceRequest(\n    Request[ReadResourceRequestParams, Literal[\"resources/read\"]]\n):\n    \"\"\"Sent from the client to the server, to read a specific resource URI.\"\"\"\n\n    method: Literal[\"resources/read\"]\n    params: ReadResourceRequestParams\n\n\nclass ResourceContents(BaseModel):\n    \"\"\"The contents of a specific resource or sub-resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of this resource.\"\"\"\n    mimeType: str | None = None\n    \"\"\"The MIME type of this resource, if known.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass TextResourceContents(ResourceContents):\n    \"\"\"Text contents of a resource.\"\"\"\n\n    text: str\n    \"\"\"\n    The text of the item. This must only be set if the item can actually be represented\n    as text (not binary data).\n    \"\"\"\n\n\nclass BlobResourceContents(ResourceContents):\n    \"\"\"Binary contents of a resource.\"\"\"\n\n    blob: str\n    \"\"\"A base64-encoded string representing the binary data of the item.\"\"\"\n\n\nclass ReadResourceResult(Result):\n    \"\"\"The server's response to a resources/read request from the client.\"\"\"\n\n    contents: list[TextResourceContents | BlobResourceContents]\n\n\nclass ResourceListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/resources/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of resources it can read from has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass SubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for subscribing to a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource to subscribe to. The URI can use any protocol; it is up to\n    the server how to interpret it.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SubscribeRequest(Request[SubscribeRequestParams, Literal[\"resources/subscribe\"]]):\n    \"\"\"\n    Sent from the client to request resources/updated notifications from the server\n    whenever a particular resource changes.\n    \"\"\"\n\n    method: Literal[\"resources/subscribe\"]\n    params: SubscribeRequestParams\n\n\nclass UnsubscribeRequestParams(RequestParams):\n    \"\"\"Parameters for unsubscribing from a resource.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"The URI of the resource to unsubscribe from.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass UnsubscribeRequest(\n    Request[UnsubscribeRequestParams, Literal[\"resources/unsubscribe\"]]\n):\n    \"\"\"\n    Sent from the client to request cancellation of resources/updated notifications from\n    the server.\n    \"\"\"\n\n    method: Literal[\"resources/unsubscribe\"]\n    params: UnsubscribeRequestParams\n\n\nclass ResourceUpdatedNotificationParams(NotificationParams):\n    \"\"\"Parameters for resource update notifications.\"\"\"\n\n    uri: Annotated[AnyUrl, UrlConstraints(host_required=False)]\n    \"\"\"\n    The URI of the resource that has been updated. This might be a sub-resource of the\n    one that the client actually subscribed to.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ResourceUpdatedNotification(\n    Notification[\n        ResourceUpdatedNotificationParams, Literal[\"notifications/resources/updated\"]\n    ]\n):\n    \"\"\"\n    A notification from the server to the client, informing it that a resource has\n    changed and may need to be read again.\n    \"\"\"\n\n    method: Literal[\"notifications/resources/updated\"]\n    params: ResourceUpdatedNotificationParams\n\n\nclass ListPromptsRequest(\n    PaginatedRequest[RequestParams | None, Literal[\"prompts/list\"]]\n):\n    \"\"\"Sent from the client to request a list of prompts and prompt templates.\"\"\"\n\n    method: Literal[\"prompts/list\"]\n    params: RequestParams | None = None\n\n\nclass PromptArgument(BaseModel):\n    \"\"\"An argument for a prompt template.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the argument.\"\"\"\n    required: bool | None = None\n    \"\"\"Whether this argument must be provided.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass Prompt(BaseModel):\n    \"\"\"A prompt or prompt template that the server offers.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    description: str | None = None\n    \"\"\"An optional description of what this prompt provides.\"\"\"\n    arguments: list[PromptArgument] | None = None\n    \"\"\"A list of arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListPromptsResult(PaginatedResult):\n    \"\"\"The server's response to a prompts/list request from the client.\"\"\"\n\n    prompts: list[Prompt]\n\n\nclass GetPromptRequestParams(RequestParams):\n    \"\"\"Parameters for getting a prompt.\"\"\"\n\n    name: str\n    \"\"\"The name of the prompt or prompt template.\"\"\"\n    arguments: dict[str, str] | None = None\n    \"\"\"Arguments to use for templating the prompt.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptRequest(Request[GetPromptRequestParams, Literal[\"prompts/get\"]]):\n    \"\"\"Used by the client to get a prompt provided by the server.\"\"\"\n\n    method: Literal[\"prompts/get\"]\n    params: GetPromptRequestParams\n\n\nclass TextContent(BaseModel):\n    \"\"\"Text content for a message.\"\"\"\n\n    type: Literal[\"text\"]\n    text: str\n    \"\"\"The text content of the message.\"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ImageContent(BaseModel):\n    \"\"\"Image content for a message.\"\"\"\n\n    type: Literal[\"image\"]\n    data: str\n    \"\"\"The base64-encoded image data.\"\"\"\n    mimeType: str\n    \"\"\"\n    The MIME type of the image. Different providers may support different\n    image types.\n    \"\"\"\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SamplingMessage(BaseModel):\n    \"\"\"Describes a message issued to or received from an LLM API.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass EmbeddedResource(BaseModel):\n    \"\"\"\n    The contents of a resource, embedded into a prompt or tool call result.\n\n    It is up to the client how best to render embedded resources for the benefit\n    of the LLM and/or the user.\n    \"\"\"\n\n    type: Literal[\"resource\"]\n    resource: TextResourceContents | BlobResourceContents\n    annotations: Annotations | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptMessage(BaseModel):\n    \"\"\"Describes a message returned as part of a prompt.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent | EmbeddedResource\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass GetPromptResult(Result):\n    \"\"\"The server's response to a prompts/get request from the client.\"\"\"\n\n    description: str | None = None\n    \"\"\"An optional description for the prompt.\"\"\"\n    messages: list[PromptMessage]\n\n\nclass PromptListChangedNotification(\n    Notification[\n        NotificationParams | None, Literal[\"notifications/prompts/list_changed\"]\n    ]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of prompts it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/prompts/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass ListToolsRequest(PaginatedRequest[RequestParams | None, Literal[\"tools/list\"]]):\n    \"\"\"Sent from the client to request a list of tools the server has.\"\"\"\n\n    method: Literal[\"tools/list\"]\n    params: RequestParams | None = None\n\n\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n\n    name: str\n    \"\"\"The name of the tool.\"\"\"\n    description: str | None = None\n    \"\"\"A human-readable description of the tool.\"\"\"\n    inputSchema: dict[str, Any]\n    \"\"\"A JSON Schema object defining the expected parameters for the tool.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListToolsResult(PaginatedResult):\n    \"\"\"The server's response to a tools/list request from the client.\"\"\"\n\n    tools: list[Tool]\n\n\nclass CallToolRequestParams(RequestParams):\n    \"\"\"Parameters for calling a tool.\"\"\"\n\n    name: str\n    arguments: dict[str, Any] | None = None\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CallToolRequest(Request[CallToolRequestParams, Literal[\"tools/call\"]]):\n    \"\"\"Used by the client to invoke a tool provided by the server.\"\"\"\n\n    method: Literal[\"tools/call\"]\n    params: CallToolRequestParams\n\n\nclass CallToolResult(Result):\n    \"\"\"The server's response to a tool call.\"\"\"\n\n    content: list[TextContent | ImageContent | EmbeddedResource]\n    isError: bool = False\n\n\nclass ToolListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/tools/list_changed\"]]\n):\n    \"\"\"\n    An optional notification from the server to the client, informing it that the list\n    of tools it offers has changed.\n    \"\"\"\n\n    method: Literal[\"notifications/tools/list_changed\"]\n    params: NotificationParams | None = None\n\n\nLoggingLevel = Literal[\n    \"debug\", \"info\", \"notice\", \"warning\", \"error\", \"critical\", \"alert\", \"emergency\"\n]\n\n\nclass SetLevelRequestParams(RequestParams):\n    \"\"\"Parameters for setting the logging level.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The level of logging that the client wants to receive from the server.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass SetLevelRequest(Request[SetLevelRequestParams, Literal[\"logging/setLevel\"]]):\n    \"\"\"A request from the client to the server, to enable or adjust logging.\"\"\"\n\n    method: Literal[\"logging/setLevel\"]\n    params: SetLevelRequestParams\n\n\nclass LoggingMessageNotificationParams(NotificationParams):\n    \"\"\"Parameters for logging message notifications.\"\"\"\n\n    level: LoggingLevel\n    \"\"\"The severity of this log message.\"\"\"\n    logger: str | None = None\n    \"\"\"An optional name of the logger issuing this message.\"\"\"\n    data: Any\n    \"\"\"\n    The data to be logged, such as a string message or an object. Any JSON serializable\n    type is allowed here.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass LoggingMessageNotification(\n    Notification[LoggingMessageNotificationParams, Literal[\"notifications/message\"]]\n):\n    \"\"\"Notification of a log message passed from server to client.\"\"\"\n\n    method: Literal[\"notifications/message\"]\n    params: LoggingMessageNotificationParams\n\n\nIncludeContext = Literal[\"none\", \"thisServer\", \"allServers\"]\n\n\nclass ModelHint(BaseModel):\n    \"\"\"Hints to use for model selection.\"\"\"\n\n    name: str | None = None\n    \"\"\"A hint for a model name.\"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ModelPreferences(BaseModel):\n    \"\"\"\n    The server's preferences for model selection, requested by the client during\n    sampling.\n\n    Because LLMs can vary along multiple dimensions, choosing the \"best\" model is\n    rarely straightforward.  Different models excel in different areas\u2014some are\n    faster but less capable, others are more capable but more expensive, and so\n    on. This interface allows servers to express their priorities across multiple\n    dimensions to help clients make an appropriate selection for their use case.\n\n    These preferences are always advisory. The client MAY ignore them. It is also\n    up to the client to decide how to interpret these preferences and how to\n    balance them against other considerations.\n    \"\"\"\n\n    hints: list[ModelHint] | None = None\n    \"\"\"\n    Optional hints to use for model selection.\n\n    If multiple hints are specified, the client MUST evaluate them in order\n    (such that the first match is taken).\n\n    The client SHOULD prioritize these hints over the numeric priorities, but\n    MAY still use the priorities to select from ambiguous matches.\n    \"\"\"\n\n    costPriority: float | None = None\n    \"\"\"\n    How much to prioritize cost when selecting a model. A value of 0 means cost\n    is not important, while a value of 1 means cost is the most important\n    factor.\n    \"\"\"\n\n    speedPriority: float | None = None\n    \"\"\"\n    How much to prioritize sampling speed (latency) when selecting a model. A\n    value of 0 means speed is not important, while a value of 1 means speed is\n    the most important factor.\n    \"\"\"\n\n    intelligencePriority: float | None = None\n    \"\"\"\n    How much to prioritize intelligence and capabilities when selecting a\n    model. A value of 0 means intelligence is not important, while a value of 1\n    means intelligence is the most important factor.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequestParams(RequestParams):\n    \"\"\"Parameters for creating a message.\"\"\"\n\n    messages: list[SamplingMessage]\n    modelPreferences: ModelPreferences | None = None\n    \"\"\"\n    The server's preferences for which model to select. The client MAY ignore\n    these preferences.\n    \"\"\"\n    systemPrompt: str | None = None\n    \"\"\"An optional system prompt the server wants to use for sampling.\"\"\"\n    includeContext: IncludeContext | None = None\n    \"\"\"\n    A request to include context from one or more MCP servers (including the caller), to\n    be attached to the prompt.\n    \"\"\"\n    temperature: float | None = None\n    maxTokens: int\n    \"\"\"The maximum number of tokens to sample, as requested by the server.\"\"\"\n    stopSequences: list[str] | None = None\n    metadata: dict[str, Any] | None = None\n    \"\"\"Optional metadata to pass through to the LLM provider.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CreateMessageRequest(\n    Request[CreateMessageRequestParams, Literal[\"sampling/createMessage\"]]\n):\n    \"\"\"A request from the server to sample an LLM via the client.\"\"\"\n\n    method: Literal[\"sampling/createMessage\"]\n    params: CreateMessageRequestParams\n\n\nStopReason = Literal[\"endTurn\", \"stopSequence\", \"maxTokens\"] | str\n\n\nclass CreateMessageResult(Result):\n    \"\"\"The client's response to a sampling/create_message request from the server.\"\"\"\n\n    role: Role\n    content: TextContent | ImageContent\n    model: str\n    \"\"\"The name of the model that generated the message.\"\"\"\n    stopReason: StopReason | None = None\n    \"\"\"The reason why sampling stopped, if known.\"\"\"\n\n\nclass ResourceReference(BaseModel):\n    \"\"\"A reference to a resource or resource template definition.\"\"\"\n\n    type: Literal[\"ref/resource\"]\n    uri: str\n    \"\"\"The URI or URI template of the resource.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass PromptReference(BaseModel):\n    \"\"\"Identifies a prompt.\"\"\"\n\n    type: Literal[\"ref/prompt\"]\n    name: str\n    \"\"\"The name of the prompt or prompt template\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompletionArgument(BaseModel):\n    \"\"\"The argument's information for completion requests.\"\"\"\n\n    name: str\n    \"\"\"The name of the argument\"\"\"\n    value: str\n    \"\"\"The value of the argument to use for completion matching.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequestParams(RequestParams):\n    \"\"\"Parameters for completion requests.\"\"\"\n\n    ref: ResourceReference | PromptReference\n    argument: CompletionArgument\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteRequest(Request[CompleteRequestParams, Literal[\"completion/complete\"]]):\n    \"\"\"A request from the client to the server, to ask for completion options.\"\"\"\n\n    method: Literal[\"completion/complete\"]\n    params: CompleteRequestParams\n\n\nclass Completion(BaseModel):\n    \"\"\"Completion information.\"\"\"\n\n    values: list[str]\n    \"\"\"An array of completion values. Must not exceed 100 items.\"\"\"\n    total: int | None = None\n    \"\"\"\n    The total number of completion options available. This can exceed the number of\n    values actually sent in the response.\n    \"\"\"\n    hasMore: bool | None = None\n    \"\"\"\n    Indicates whether there are additional completion options beyond those provided in\n    the current response, even if the exact total is unknown.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CompleteResult(Result):\n    \"\"\"The server's response to a completion/complete request\"\"\"\n\n    completion: Completion\n\n\nclass ListRootsRequest(Request[RequestParams | None, Literal[\"roots/list\"]]):\n    \"\"\"\n    Sent from the server to request a list of root URIs from the client. Roots allow\n    servers to ask for specific directories or files to operate on. A common example\n    for roots is providing a set of repositories or directories a server should operate\n    on.\n\n    This request is typically used when the server needs to understand the file system\n    structure or access specific locations that the client has permission to read from.\n    \"\"\"\n\n    method: Literal[\"roots/list\"]\n    params: RequestParams | None = None\n\n\nclass Root(BaseModel):\n    \"\"\"Represents a root directory or file that the server can operate on.\"\"\"\n\n    uri: FileUrl\n    \"\"\"\n    The URI identifying the root. This *must* start with file:// for now.\n    This restriction may be relaxed in future versions of the protocol to allow\n    other URI schemes.\n    \"\"\"\n    name: str | None = None\n    \"\"\"\n    An optional name for the root. This can be used to provide a human-readable\n    identifier for the root, which may be useful for display purposes or for\n    referencing the root in other parts of the application.\n    \"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass ListRootsResult(Result):\n    \"\"\"\n    The client's response to a roots/list request from the server.\n    This result contains an array of Root objects, each representing a root directory\n    or file that the server can operate on.\n    \"\"\"\n\n    roots: list[Root]\n\n\nclass RootsListChangedNotification(\n    Notification[NotificationParams | None, Literal[\"notifications/roots/list_changed\"]]\n):\n    \"\"\"\n    A notification from the client to the server, informing it that the list of\n    roots has changed.\n\n    This notification should be sent whenever the client adds, removes, or\n    modifies any root. The server should then request an updated list of roots\n    using the ListRootsRequest.\n    \"\"\"\n\n    method: Literal[\"notifications/roots/list_changed\"]\n    params: NotificationParams | None = None\n\n\nclass CancelledNotificationParams(NotificationParams):\n    \"\"\"Parameters for cancellation notifications.\"\"\"\n\n    requestId: RequestId\n    \"\"\"The ID of the request to cancel.\"\"\"\n    reason: str | None = None\n    \"\"\"An optional string describing the reason for the cancellation.\"\"\"\n    model_config = ConfigDict(extra=\"allow\")\n\n\nclass CancelledNotification(\n    Notification[CancelledNotificationParams, Literal[\"notifications/cancelled\"]]\n):\n    \"\"\"\n    This notification can be sent by either side to indicate that it is canceling a\n    previously-issued request.\n    \"\"\"\n\n    method: Literal[\"notifications/cancelled\"]\n    params: CancelledNotificationParams\n\n\nclass ClientRequest(\n    RootModel[\n        PingRequest\n        | InitializeRequest\n        | CompleteRequest\n        | SetLevelRequest\n        | GetPromptRequest\n        | ListPromptsRequest\n        | ListResourcesRequest\n        | ListResourceTemplatesRequest\n        | ReadResourceRequest\n        | SubscribeRequest\n        | UnsubscribeRequest\n        | CallToolRequest\n        | ListToolsRequest\n    ]\n):\n    pass\n\n\nclass ClientNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | InitializedNotification\n        | RootsListChangedNotification\n    ]\n):\n    pass\n\n\nclass ClientResult(RootModel[EmptyResult | CreateMessageResult | ListRootsResult]):\n    pass\n\n\nclass ServerRequest(RootModel[PingRequest | CreateMessageRequest | ListRootsRequest]):\n    pass\n\n\nclass ServerNotification(\n    RootModel[\n        CancelledNotification\n        | ProgressNotification\n        | LoggingMessageNotification\n        | ResourceUpdatedNotification\n        | ResourceListChangedNotification\n        | ToolListChangedNotification\n        | PromptListChangedNotification\n    ]\n):\n    pass\n\n\nclass ServerResult(\n    RootModel[\n        EmptyResult\n        | InitializeResult\n        | CompleteResult\n        | GetPromptResult\n        | ListPromptsResult\n        | ListResourcesResult\n        | ListResourceTemplatesResult\n        | ReadResourceResult\n        | CallToolResult\n        | ListToolsResult\n    ]\n):\n    pass\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 8: MCP Protocol Types`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 8: MCP Protocol Types - The Shared Language\n\nWelcome to the final core chapter! In [Chapter 7: FastMCP Context](07_fastmcp_context.md), we explored how server-side functions can interact with the ongoing request using the `Context` object. We've covered clients, servers, tools, resources, prompts, transports, and server context.\n\nBut how do the client and server *know* exactly what a \"list tools request\" looks like, or how a \"tool call result\" should be structured? They need a precisely defined, shared language. This chapter introduces the **MCP Protocol Types**, the fundamental building blocks of that language.\n\n## Motivation: Why Do We Need Defined Types?\n\nImagine trying to communicate with someone who speaks a different language, and you don't have a shared dictionary or grammar rules. You might say \"Get food,\" but they might expect you to specify *what* food, *where* from, and *how much*. Without agreed-upon structures, communication breaks down.\n\nSimilarly, when a `python-sdk` client talks to an MCP server (like one built with [FastMCP Server](05_fastmcp_server.md)), they need to agree on the *exact* format of every message.\n\n*   When the client calls `session.list_tools()`, what information does the server expect? (Nothing, just the request itself).\n*   When the server responds, what fields will be in the response? (A list of `Tool` objects).\n*   When the client calls `session.call_tool(\"calculator\", {\"a\": 5, \"b\": 3})`, how must the arguments `{\"a\": 5, \"b\": 3}` be structured?\n*   When the server sends back the result `8`, how is that formatted? (As `TextContent` inside a `CallToolResult`).\n\n**MCP Protocol Types define this shared language.** They specify the precise structure of:\n\n1.  **Requests:** Messages asking the other side to do something (e.g., \"list tools,\" \"read resource\").\n2.  **Responses (Results):** The answers to requests (e.g., the list of tools, the resource content).\n3.  **Notifications:** One-way messages that don't expect a direct answer (e.g., progress updates, log messages).\n\nThink of them as the strict grammar and vocabulary rules both the client and server *must* follow. This ensures that when the client sends a message, the server knows exactly how to interpret it, and vice-versa. In `python-sdk`, these rules are defined using **Pydantic models**.\n\n## Key Concepts: Requests, Results, and Notifications\n\nThe core idea is that every interaction follows a predefined structure. Let's look at the main categories:\n\n1.  **Requests (`*Request`)**:\n    *   Sent when one side wants the other to perform an action or provide information.\n    *   Always expect a corresponding **Response** (either a Result or an Error).\n    *   Examples:\n        *   `InitializeRequest`: Sent by the client to start the session.\n        *   `ListToolsRequest`: Sent by the client to ask for available tools.\n        *   `CallToolRequest`: Sent by the client to execute a tool.\n        *   `ReadResourceRequest`: Sent by the client to get resource content.\n        *   `GetPromptRequest`: Sent by the client to get a rendered prompt.\n        *   `CreateMessageRequest`: Sent by the *server* to ask the *client* to generate text (e.g., via an LLM).\n\n2.  **Responses/Results (`*Result`)**:\n    *   Sent in direct reply to a **Request**.\n    *   Contain the information requested or confirm an action was taken.\n    *   Must match the specific `Request` they are answering.\n    *   Examples:\n        *   `InitializeResult`: Server's reply to `InitializeRequest`, contains server capabilities.\n        *   `ListToolsResult`: Server's reply to `ListToolsRequest`, contains a list of `Tool` objects.\n        *   `CallToolResult`: Server's reply to `CallToolRequest`, contains the tool's output (e.g., `TextContent`).\n        *   `ReadResourceResult`: Server's reply to `ReadResourceRequest`, contains the resource `contents`.\n        *   `GetPromptResult`: Server's reply to `GetPromptRequest`, contains the list of `PromptMessage` objects.\n        *   `CreateMessageResult`: Client's reply to `CreateMessageRequest`, contains the generated message.\n    *   **Errors:** If a request fails, the response is a `JSONRPCError` containing an `ErrorData` object instead of a `*Result`.\n\n3.  **Notifications (`*Notification`)**:\n    *   Sent one-way; they do *not* expect a direct response.\n    *   Used for status updates, events, or information that doesn't require an explicit request.\n    *   Examples:\n        *   `InitializedNotification`: Client tells the server it's ready after initialization.\n        *   `ProgressNotification`: Server sends progress updates for a long request.\n        *   `LoggingMessageNotification`: Server sends log messages to the client.\n        *   `ResourceUpdatedNotification`: Server tells the client a subscribed resource has changed.\n        *   `ToolListChangedNotification`: Server tells the client the list of tools has changed.\n\n## Using the Types (Implicitly)\n\nWhen you use methods like `session.list_tools()` or `session.call_tool()`, you are *implicitly* using these protocol types.\n\n*   When you call `await session.list_tools()`, the `ClientSession` internally creates a `ListToolsRequest` object.\n*   It serializes this object into JSON and sends it via the [Transports](06_transports.md).\n*   The server receives the JSON, deserializes it back into a `ListToolsRequest` object (using the same Pydantic definition).\n*   The server processes the request and prepares a `ListToolsResult` object containing `Tool` objects.\n*   The server serializes the `ListToolsResult` to JSON and sends it back.\n*   Your `ClientSession` receives the JSON, deserializes it into a `ListToolsResult` object, and returns it to your code.\n\n**Pydantic** is the library that makes this easy. It allows defining these data structures as Python classes with type hints. Pydantic handles:\n*   **Validation:** Ensuring incoming data matches the expected structure and types. If a server sends back a malformed `ListToolsResult`, Pydantic will raise an error on the client side during deserialization.\n*   **Serialization:** Converting Python objects (like `ListToolsRequest`) into JSON.\n*   **Deserialization:** Converting JSON back into Python objects.\n\nLet's look at a simplified example structure:\n\n```python\n# Simplified view from mcp.types\n\nfrom pydantic import BaseModel, Field\nfrom typing import Any, List, Literal\n\n# --- Tool Structures ---\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n    name: str\n    description: str | None = None\n    inputSchema: dict[str, Any] # JSON schema for tool arguments\n\n# --- Request for Listing Tools ---\nclass ListToolsRequest(BaseModel):\n    \"\"\"Internal representation of a tools/list request\"\"\"\n    method: Literal[\"tools/list\"] = \"tools/list\"\n    # No specific parameters needed for this request\n\n# --- Result for Listing Tools ---\nclass ListToolsResult(BaseModel):\n    \"\"\"The server's response to a tools/list request.\"\"\"\n    tools: List[Tool] # Contains a list of Tool objects\n```\n\nThis shows how `ListToolsRequest` is just a marker for the request, while `ListToolsResult` clearly defines that the response *must* contain a list named `tools`, where each item is a `Tool` object conforming to the `Tool` model.\n\nYou can find all these Pydantic models defined in `src/mcp/types.py`. You don't usually need to create these objects manually when using `ClientSession` or `FastMCP`, but understanding their structure helps you know what data to expect.\n\n## Under the Hood: The Journey of a Typed Message\n\nWhat happens when `ClientSession` sends a request like `list_tools`?\n\n1.  **Python Object Creation:** `ClientSession` creates an instance of the Pydantic model `ListToolsRequest`.\n2.  **Serialization (Pydantic):** Pydantic converts the `ListToolsRequest` object into its JSON representation (e.g., `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`).\n3.  **Transport Sending:** The JSON string is sent over the active [Transports](06_transports.md) (e.g., written to stdio, sent via WebSocket).\n4.  **Transport Receiving:** The server's transport layer receives the raw JSON bytes/string.\n5.  **Deserialization (Pydantic):** The server uses Pydantic to parse the JSON string back into a `ListToolsRequest` Python object. Pydantic validates that the JSON matches the expected structure.\n6.  **Server Logic:** The server processes the request.\n7.  **Response Object Creation:** The server creates a `ListToolsResult` object containing the list of `Tool` objects.\n8.  **Serialization (Pydantic):** Pydantic converts the `ListToolsResult` object into its JSON representation.\n9.  **Transport Sending:** The response JSON is sent back to the client.\n10. **Transport Receiving:** The client's transport layer receives the response JSON.\n11. **Deserialization (Pydantic):** The `ClientSession` uses Pydantic to parse the JSON back into a `ListToolsResult` Python object, again validating the structure.\n12. **Return Value:** The validated `ListToolsResult` object is returned by the `await session.list_tools()` call.\n\n```mermaid\nsequenceDiagram\n    participant ClientCode as Your Python Code\n    participant ClientSession as ClientSession (Python SDK)\n    participant Transport\n    participant Server as MCP Server (Python SDK)\n    participant ServerLogic as Server's Tool Handling\n\n    ClientCode->>+ClientSession: await session.list_tools()\n    ClientSession->>ClientSession: Create ListToolsRequest object\n    ClientSession->>ClientSession: Pydantic: Serialize object to JSON\n    ClientSession->>+Transport: Send JSON Request\n    Transport->>+Server: Deliver JSON Request\n    Server->>Server: Pydantic: Deserialize JSON to ListToolsRequest object\n    Server->>+ServerLogic: Process request\n    ServerLogic-->>-Server: Return list of Tool objects\n    Server->>Server: Create ListToolsResult object\n    Server->>Server: Pydantic: Serialize object to JSON\n    Server->>-Transport: Send JSON Response\n    Transport-->>-ClientSession: Deliver JSON Response\n    ClientSession->>ClientSession: Pydantic: Deserialize JSON to ListToolsResult object\n    ClientSession-->>-ClientCode: Return ListToolsResult object\n```\n\nThe MCP Protocol Types, defined as Pydantic models, are essential at the points where JSON is converted to Python objects and vice-versa, ensuring both sides adhere to the agreed-upon \"language\".\n\n## Code Glimpse: Where Types are Used\n\nWe saw the simplified Pydantic models above. Now let's see how `ClientSession` uses them (simplified from `src/mcp/client/session.py`):\n\n```python\n# src/mcp/client/session.py (Simplified Snippet)\nimport mcp.types as types # Import the protocol types\n\nclass ClientSession(...):\n    # ... (other methods) ...\n\n    async def list_tools(self) -> types.ListToolsResult: # Declares return type!\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific Request object\n        request_obj = types.ClientRequest( # Wrapper for specific requests\n            types.ListToolsRequest(        # The actual request type\n                method=\"tools/list\",\n            )\n        )\n        # 2. Send it and expect the specific Result type back\n        # (internal send_request handles JSON conversion and validation)\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # Tell send_request what Pydantic model to expect\n        )\n```\n\nHere you can clearly see:\n1.  The function signature `-> types.ListToolsResult` explicitly states what type of Pydantic object this method will return.\n2.  Inside, it creates a `types.ListToolsRequest` object.\n3.  It calls the internal `send_request`, telling it to expect a response that can be validated and parsed into a `types.ListToolsResult`.\n\nThe `BaseSession` (which `ClientSession` inherits from) handles the details of using Pydantic for serialization and deserialization within its `_send_response`, `_receive_loop`, and related methods (see `src/mcp/shared/session.py`).\n\n## Conclusion\n\nYou've learned about the **MCP Protocol Types**, the precisely defined data structures that form the communication language between MCP clients and servers.\n\n*   They define the exact format for **Requests**, **Responses (Results)**, and **Notifications**.\n*   They act like a shared grammar and vocabulary, ensuring mutual understanding.\n*   In `python-sdk`, these types are implemented as **Pydantic models** (primarily in `src/mcp/types.py`).\n*   Pydantic handles the **validation**, **serialization** (Python object -> JSON), and **deserialization** (JSON -> Python object).\n*   While often used implicitly by `ClientSession` and `FastMCP`, understanding these types helps clarify what data is being exchanged.\n\nThis chapter concludes our exploration of the core concepts in the `python-sdk`. You now have a solid foundation for understanding how to build clients that interact with MCP servers, how to build servers using `FastMCP`, and the underlying principles of communication and data structures involved. From here, you can dive deeper into specific examples or advanced features!", "\n        For the project `SC4002-Group-Assignment`:\n\n        Codebase Context:\n        --- File Index 0: constants.py ---\nBATCH_SIZE = 32\nADAM_LR = 0.001\nSGD_LR = 0.01\n\n\n--- File Index 1: requirements.txt ---\nabsl-py==2.1.0\naiohappyeyeballs==2.4.3\naiohttp==3.10.10\naiosignal==1.3.1\nasttokens==2.4.1\nastunparse==1.6.3\nattrs==24.2.0\ncertifi==2024.8.30\ncharset-normalizer==3.4.0\nclick==8.1.7\ncolorama==0.4.6\ncomm==0.2.2\ncontourpy==1.3.0\ncycler==0.12.1\ndatasets==3.1.0\ndebugpy==1.8.7\ndecorator==5.1.1\ndill==0.3.8\nexecuting==2.1.0\nfilelock==3.16.1\nflatbuffers==24.3.25\nfonttools==4.54.1\nfrozenlist==1.5.0\nfsspec==2024.9.0\ngast==0.6.0\ngensim==4.3.3\ngoogle-pasta==0.2.0\ngrpcio==1.67.1\nh5py==3.12.1\nhuggingface-hub==0.26.2\nidna==3.10\nipykernel==6.29.5\nipython==8.29.0\njedi==0.19.1\njoblib==1.4.2\njupyter_client==8.6.3\njupyter_core==5.7.2\nkeras==3.6.0\nkeras-tuner==1.4.7\nkiwisolver==1.4.7\nkt-legacy==1.0.5\nlibclang==18.1.1\nMarkdown==3.7\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib==3.9.2\nmatplotlib-inline==0.1.7\nmdurl==0.1.2\nml-dtypes==0.4.1\nmultidict==6.1.0\nmultiprocess==0.70.16\nnamex==0.0.8\nnest-asyncio==1.6.0\nnltk==3.9.1\nnumpy==1.26.4\nopt_einsum==3.4.0\noptree==0.13.0\npackaging==24.1\npandas==2.2.3\nparso==0.8.4\npillow==11.0.0\nplatformdirs==4.3.6\nprompt_toolkit==3.0.48\npropcache==0.2.0\nprotobuf==5.28.3\npsutil==6.1.0\npure_eval==0.2.3\npyarrow==18.0.0\nPygments==2.18.0\npyparsing==3.2.0\npython-dateutil==2.9.0.post0\npytz==2024.2\npywin32==308\nPyYAML==6.0.2\npyzmq==26.2.0\nregex==2024.11.6\nrequests==2.32.3\nrich==13.9.4\nscipy==1.13.1\nsentencepiece==0.2.0\nsetuptools==75.3.0\nsix==1.16.0\nsmart-open==7.0.5\nstack-data==0.6.3\ntransformers==4.44.2\ntensorboard==2.18.0\ntensorboard-data-server==0.7.2\ntensorflow==2.18.0\ntensorflow_intel==2.18.0\ntermcolor==2.5.0\ntornado==6.4.1\ntorch==2.5.0\ntqdm==4.67.0\ntraitlets==5.14.3\ntyping_extensions==4.12.2\ntzdata==2024.2\nurllib3==2.2.3\nwcwidth==0.2.13\nWerkzeug==3.1.2\nwheel==0.44.0\nwrapt==1.16.0\nxxhash==3.5.0\nyarl==1.17.1\n\n\n--- File Index 2: sentiment_transformer_tuning/README.md ---\nThis folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n\n\n--- File Index 3: sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n--- File Index 4: sentiment_transformer_tuning/trial_00/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 5: sentiment_transformer_tuning/trial_01/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 6: sentiment_transformer_tuning/trial_02/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 7: sentiment_transformer_tuning/trial_03/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 8: sentiment_transformer_tuning/trial_04/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 9: sentiment_transformer_tuning/trial_05/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 10: sentiment_transformer_tuning/trial_06/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 11: sentiment_transformer_tuning/trial_07/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 12: sentiment_transformer_tuning/trial_08/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 13: sentiment_transformer_tuning/trial_09/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File Index 14: sentiment_transformer_tuning/tuner0.json ---\n{}\n\n\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        - 0 # constants.py\n- 1 # requirements.txt\n- 2 # sentiment_transformer_tuning/README.md\n- 3 # sentiment_transformer_tuning/oracle.json\n- 4 # sentiment_transformer_tuning/trial_00/build_config.json\n- 5 # sentiment_transformer_tuning/trial_01/build_config.json\n- 6 # sentiment_transformer_tuning/trial_02/build_config.json\n- 7 # sentiment_transformer_tuning/trial_03/build_config.json\n- 8 # sentiment_transformer_tuning/trial_04/build_config.json\n- 9 # sentiment_transformer_tuning/trial_05/build_config.json\n- 10 # sentiment_transformer_tuning/trial_06/build_config.json\n- 11 # sentiment_transformer_tuning/trial_07/build_config.json\n- 12 # sentiment_transformer_tuning/trial_08/build_config.json\n- 13 # sentiment_transformer_tuning/trial_09/build_config.json\n- 14 # sentiment_transformer_tuning/tuner0.json\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```": "```yaml\n- name: Hyperparameter Tuning Setup\n  description: |\n    This represents the overall process of automatically finding the best settings (hyperparameters) for the sentiment analysis transformer model. It involves running multiple training experiments with different configurations.\n    Think of it like trying different recipes (hyperparameter combinations) in a kitchen (`sentiment_transformer_tuning` folder) to discover which one makes the best cake (most accurate model). The goal is to optimize performance without manual guesswork.\n  file_indices:\n    - 2 # sentiment_transformer_tuning/README.md\n    - 3 # sentiment_transformer_tuning/oracle.json\n    - 4 # sentiment_transformer_tuning/trial_00/build_config.json\n    - 14 # sentiment_transformer_tuning/tuner0.json\n\n- name: Keras Tuner Oracle\n  description: |\n    The central component managing the hyperparameter search, specifically within the Keras Tuner library. It keeps track of which hyperparameter combinations have been tried, their results, and decides which combinations to try next.\n    It's like the head chef who directs the baking experiments, meticulously logging each recipe tested (`tried_so_far`), its outcome (stored within the tuner's state, reflected here), and planning the next experiment based on the defined strategy (e.g., random search).\n  file_indices:\n    - 3 # sentiment_transformer_tuning/oracle.json\n\n- name: Tuning Hyperparameter Space\n  description: |\n    Defines the range of possible values for each hyperparameter being optimized during the tuning process. This includes parameters like the number of attention heads, layer sizes, dropout rates, and learning rates.\n    Imagine this as the section of a cookbook outlining the possible variations for a recipe: e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\". The tuner explores combinations within these defined boundaries.\n  file_indices:\n    - 3 # sentiment_transformer_tuning/oracle.json\n\n- name: Tuning Trial Configuration & State\n  description: |\n    Represents a single experiment run within the hyperparameter tuning process. Each trial uses a specific combination of hyperparameters selected by the Tuner Oracle from the defined space. The results and configuration of each trial are stored.\n    This is like one specific attempt at baking a cake using one chosen recipe variation. The `trial_*` folders contain the notes (`build_config.json`) and outcomes for that specific attempt. The `oracle.json` tracks the status (start/end times) of all trials.\n  file_indices:\n    - 3 # sentiment_transformer_tuning/oracle.json\n    - 4 # sentiment_transformer_tuning/trial_00/build_config.json\n    - 5 # sentiment_transformer_tuning/trial_01/build_config.json\n    - 6 # sentiment_transformer_tuning/trial_02/build_config.json\n    - 7 # sentiment_transformer_tuning/trial_03/build_config.json\n    - 8 # sentiment_transformer_tuning/trial_04/build_config.json\n    - 9 # sentiment_transformer_tuning/trial_05/build_config.json\n    - 10 # sentiment_transformer_tuning/trial_06/build_config.json\n    - 11 # sentiment_transformer_tuning/trial_07/build_config.json\n    - 12 # sentiment_transformer_tuning/trial_08/build_config.json\n    - 13 # sentiment_transformer_tuning/trial_09/build_config.json\n\n- name: Model Input Shape\n  description: |\n    Defines the expected dimensions of the data that the machine learning model accepts as input. In this case, it's specified as `[null, 62]`, likely meaning variable batch size (`null`) and sequences of length 62.\n    This is like a specific mold or baking pan size requirement for the cake recipe. The data (ingredients) must fit this shape (pan) before the model (oven) can process it. Consistent input shape is crucial for the model architecture.\n  file_indices:\n    - 4 # sentiment_transformer_tuning/trial_00/build_config.json\n    - 5 # sentiment_transformer_tuning/trial_01/build_config.json\n    - 6 # sentiment_transformer_tuning/trial_02/build_config.json\n    - 7 # sentiment_transformer_tuning/trial_03/build_config.json\n    - 8 # sentiment_transformer_tuning/trial_04/build_config.json\n    - 9 # sentiment_transformer_tuning/trial_05/build_config.json\n    - 10 # sentiment_transformer_tuning/trial_06/build_config.json\n    - 11 # sentiment_transformer_tuning/trial_07/build_config.json\n    - 12 # sentiment_transformer_tuning/trial_08/build_config.json\n    - 13 # sentiment_transformer_tuning/trial_09/build_config.json\n\n- name: Project Dependencies\n  description: |\n    Lists all the external software libraries and their specific versions required for the project to run correctly. This ensures reproducibility and avoids conflicts between library versions.\n    Think of this as the list of essential tools and appliances (like Python, TensorFlow, Keras Tuner, NLTK, etc.) needed in your kitchen. Without the correct tools listed in `requirements.txt`, you can't execute the code (bake the cake).\n  file_indices:\n    - 1 # requirements.txt\n\n- name: Training Constants\n  description: |\n    Defines fixed numerical values used during the model training process, such as default learning rates for optimizers (Adam, SGD) and the batch size. These might be baseline values used before tuning or fixed settings.\n    These are like standard kitchen constants \u2013 perhaps the default oven preheat temperature or the standard size of a measuring cup used across many recipes. They provide consistent starting points or fixed parameters for parts of the process.\n  file_indices:\n    - 0 # constants.py\n```", "\nBased on the following abstractions and relevant code snippets from the project `SC4002-Group-Assignment`:\n\nList of Abstraction Indices and Names:\n0 # Hyperparameter Tuning Setup\n1 # Keras Tuner Oracle\n2 # Tuning Hyperparameter Space\n3 # Tuning Trial Configuration & State\n4 # Model Input Shape\n5 # Project Dependencies\n6 # Training Constants\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Hyperparameter Tuning Setup (Relevant file indices: [2, 3, 4, 14])\n  Description: This represents the overall process of automatically finding the best settings (hyperparameters) for the sentiment analysis transformer model. It involves running multiple training experiments with different configurations.\nThink of it like trying different recipes (hyperparameter combinations) in a kitchen (`sentiment_transformer_tuning` folder) to discover which one makes the best cake (most accurate model). The goal is to optimize performance without manual guesswork.\n\n- Index 1: Keras Tuner Oracle (Relevant file indices: [3])\n  Description: The central component managing the hyperparameter search, specifically within the Keras Tuner library. It keeps track of which hyperparameter combinations have been tried, their results, and decides which combinations to try next.\nIt's like the head chef who directs the baking experiments, meticulously logging each recipe tested (`tried_so_far`), its outcome (stored within the tuner's state, reflected here), and planning the next experiment based on the defined strategy (e.g., random search).\n\n- Index 2: Tuning Hyperparameter Space (Relevant file indices: [3])\n  Description: Defines the range of possible values for each hyperparameter being optimized during the tuning process. This includes parameters like the number of attention heads, layer sizes, dropout rates, and learning rates.\nImagine this as the section of a cookbook outlining the possible variations for a recipe: e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\". The tuner explores combinations within these defined boundaries.\n\n- Index 3: Tuning Trial Configuration & State (Relevant file indices: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13])\n  Description: Represents a single experiment run within the hyperparameter tuning process. Each trial uses a specific combination of hyperparameters selected by the Tuner Oracle from the defined space. The results and configuration of each trial are stored.\nThis is like one specific attempt at baking a cake using one chosen recipe variation. The `trial_*` folders contain the notes (`build_config.json`) and outcomes for that specific attempt. The `oracle.json` tracks the status (start/end times) of all trials.\n\n- Index 4: Model Input Shape (Relevant file indices: [4, 5, 6, 7, 8, 9, 10, 11, 12, 13])\n  Description: Defines the expected dimensions of the data that the machine learning model accepts as input. In this case, it's specified as `[null, 62]`, likely meaning variable batch size (`null`) and sequences of length 62.\nThis is like a specific mold or baking pan size requirement for the cake recipe. The data (ingredients) must fit this shape (pan) before the model (oven) can process it. Consistent input shape is crucial for the model architecture.\n\n- Index 5: Project Dependencies (Relevant file indices: [1])\n  Description: Lists all the external software libraries and their specific versions required for the project to run correctly. This ensures reproducibility and avoids conflicts between library versions.\nThink of this as the list of essential tools and appliances (like Python, TensorFlow, Keras Tuner, NLTK, etc.) needed in your kitchen. Without the correct tools listed in `requirements.txt`, you can't execute the code (bake the cake).\n\n- Index 6: Training Constants (Relevant file indices: [0])\n  Description: Defines fixed numerical values used during the model training process, such as default learning rates for optimizers (Adam, SGD) and the batch size. These might be baseline values used before tuning or fixed settings.\nThese are like standard kitchen constants \u2013 perhaps the default oven preheat temperature or the standard size of a measuring cup used across many recipes. They provide consistent starting points or fixed parameters for parts of the process.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 0 # constants.py ---\nBATCH_SIZE = 32\nADAM_LR = 0.001\nSGD_LR = 0.01\n\n\n--- File: 1 # requirements.txt ---\nabsl-py==2.1.0\naiohappyeyeballs==2.4.3\naiohttp==3.10.10\naiosignal==1.3.1\nasttokens==2.4.1\nastunparse==1.6.3\nattrs==24.2.0\ncertifi==2024.8.30\ncharset-normalizer==3.4.0\nclick==8.1.7\ncolorama==0.4.6\ncomm==0.2.2\ncontourpy==1.3.0\ncycler==0.12.1\ndatasets==3.1.0\ndebugpy==1.8.7\ndecorator==5.1.1\ndill==0.3.8\nexecuting==2.1.0\nfilelock==3.16.1\nflatbuffers==24.3.25\nfonttools==4.54.1\nfrozenlist==1.5.0\nfsspec==2024.9.0\ngast==0.6.0\ngensim==4.3.3\ngoogle-pasta==0.2.0\ngrpcio==1.67.1\nh5py==3.12.1\nhuggingface-hub==0.26.2\nidna==3.10\nipykernel==6.29.5\nipython==8.29.0\njedi==0.19.1\njoblib==1.4.2\njupyter_client==8.6.3\njupyter_core==5.7.2\nkeras==3.6.0\nkeras-tuner==1.4.7\nkiwisolver==1.4.7\nkt-legacy==1.0.5\nlibclang==18.1.1\nMarkdown==3.7\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib==3.9.2\nmatplotlib-inline==0.1.7\nmdurl==0.1.2\nml-dtypes==0.4.1\nmultidict==6.1.0\nmultiprocess==0.70.16\nnamex==0.0.8\nnest-asyncio==1.6.0\nnltk==3.9.1\nnumpy==1.26.4\nopt_einsum==3.4.0\noptree==0.13.0\npackaging==24.1\npandas==2.2.3\nparso==0.8.4\npillow==11.0.0\nplatformdirs==4.3.6\nprompt_toolkit==3.0.48\npropcache==0.2.0\nprotobuf==5.28.3\npsutil==6.1.0\npure_eval==0.2.3\npyarrow==18.0.0\nPygments==2.18.0\npyparsing==3.2.0\npython-dateutil==2.9.0.post0\npytz==2024.2\npywin32==308\nPyYAML==6.0.2\npyzmq==26.2.0\nregex==2024.11.6\nrequests==2.32.3\nrich==13.9.4\nscipy==1.13.1\nsentencepiece==0.2.0\nsetuptools==75.3.0\nsix==1.16.0\nsmart-open==7.0.5\nstack-data==0.6.3\ntransformers==4.44.2\ntensorboard==2.18.0\ntensorboard-data-server==0.7.2\ntensorflow==2.18.0\ntensorflow_intel==2.18.0\ntermcolor==2.5.0\ntornado==6.4.1\ntorch==2.5.0\ntqdm==4.67.0\ntraitlets==5.14.3\ntyping_extensions==4.12.2\ntzdata==2024.2\nurllib3==2.2.3\nwcwidth==0.2.13\nWerkzeug==3.1.2\nwheel==0.44.0\nwrapt==1.16.0\nxxhash==3.5.0\nyarl==1.17.1\n\n\n--- File: 2 # sentiment_transformer_tuning/README.md ---\nThis folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n\n\n--- File: 3 # sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n--- File: 4 # sentiment_transformer_tuning/trial_00/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 5 # sentiment_transformer_tuning/trial_01/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 6 # sentiment_transformer_tuning/trial_02/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 7 # sentiment_transformer_tuning/trial_03/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 8 # sentiment_transformer_tuning/trial_04/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 9 # sentiment_transformer_tuning/trial_05/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 10 # sentiment_transformer_tuning/trial_06/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 11 # sentiment_transformer_tuning/trial_07/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 12 # sentiment_transformer_tuning/trial_08/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 13 # sentiment_transformer_tuning/trial_09/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: 14 # sentiment_transformer_tuning/tuner0.json ---\n{}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  This project focuses on automatically finding the best settings (*hyperparameters*) for a **sentiment analysis transformer model**.\n  It uses the *Keras Tuner library* to run multiple **training experiments** (trials) with different configurations, exploring a defined **hyperparameter space** (e.g., learning rates, layer sizes).\n  The goal is to discover the *optimal model setup* for better performance without manual adjustment. The project relies on specific **software dependencies** listed in `requirements.txt`.\nrelationships:\n  - from_abstraction: 0 # Hyperparameter Tuning Setup\n    to_abstraction: 1 # Keras Tuner Oracle\n    label: \"Uses\"\n  - from_abstraction: 1 # Keras Tuner Oracle\n    to_abstraction: 2 # Tuning Hyperparameter Space\n    label: \"Consults\"\n  - from_abstraction: 1 # Keras Tuner Oracle\n    to_abstraction: 3 # Tuning Trial Configuration & State\n    label: \"Manages Trials\"\n  - from_abstraction: 3 # Tuning Trial Configuration & State\n    to_abstraction: 4 # Model Input Shape\n    label: \"Specifies Input\"\n  - from_abstraction: 0 # Hyperparameter Tuning Setup\n    to_abstraction: 5 # Project Dependencies\n    label: \"Requires\"\n  - from_abstraction: 0 # Hyperparameter Tuning Setup\n    to_abstraction: 6 # Training Constants\n    label: \"Uses Defaults From\"\n```", "\n        Given the following project abstractions and their relationships for the project ```` SC4002-Group-Assignment ````:\n\n        Abstractions (Index # Name):\n        - 0 # Hyperparameter Tuning Setup\n- 1 # Keras Tuner Oracle\n- 2 # Tuning Hyperparameter Space\n- 3 # Tuning Trial Configuration & State\n- 4 # Model Input Shape\n- 5 # Project Dependencies\n- 6 # Training Constants\n\n        Context about relationships and project summary:\n        Project Summary:\nThis project focuses on automatically finding the best settings (*hyperparameters*) for a **sentiment analysis transformer model**.\nIt uses the *Keras Tuner library* to run multiple **training experiments** (trials) with different configurations, exploring a defined **hyperparameter space** (e.g., learning rates, layer sizes).\nThe goal is to discover the *optimal model setup* for better performance without manual adjustment. The project relies on specific **software dependencies** listed in `requirements.txt`.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (Hyperparameter Tuning Setup) to 1 (Keras Tuner Oracle): Uses\n- From 1 (Keras Tuner Oracle) to 2 (Tuning Hyperparameter Space): Consults\n- From 1 (Keras Tuner Oracle) to 3 (Tuning Trial Configuration & State): Manages Trials\n- From 3 (Tuning Trial Configuration & State) to 4 (Model Input Shape): Specifies Input\n- From 0 (Hyperparameter Tuning Setup) to 5 (Project Dependencies): Requires\n- From 0 (Hyperparameter Tuning Setup) to 6 (Training Constants): Uses Defaults From\n\n\n        If you are going to make a tutorial for ```` SC4002-Group-Assignment ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        ": "```yaml\n- 4 # Model Input Shape\n- 2 # Tuning Hyperparameter Space\n- 6 # Training Constants\n- 0 # Hyperparameter Tuning Setup\n- 1 # Keras Tuner Oracle\n- 3 # Tuning Trial Configuration & State\n- 5 # Project Dependencies\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Model Input Shape\". This is Chapter 1.\n\n        Concept Details:\n        - Description:\n        Defines the expected dimensions of the data that the machine learning model accepts as input. In this case, it's specified as `[null, 62]`, likely meaning variable batch size (`null`) and sequences of length 62.\nThis is like a specific mold or baking pan size requirement for the cake recipe. The data (ingredients) must fit this shape (pan) before the model (oven) can process it. Consistent input shape is crucial for the model architecture.\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        This is the first chapter.\n\n        Relevant Code Snippets:\n        --- File: sentiment_transformer_tuning/trial_00/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_01/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_02/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_03/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_04/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_05/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_06/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_07/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_08/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_09/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 1: Model Input Shape`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Tuning Hyperparameter Space\". This is Chapter 2.\n\n        Concept Details:\n        - Description:\n        Defines the range of possible values for each hyperparameter being optimized during the tuning process. This includes parameters like the number of attention heads, layer sizes, dropout rates, and learning rates.\nImagine this as the section of a cookbook outlining the possible variations for a recipe: e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\". The tuner explores combinations within these defined boundaries.\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n\n        Relevant Code Snippets:\n        --- File: sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 2: Tuning Hyperparameter Space`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Training Constants\". This is Chapter 3.\n\n        Concept Details:\n        - Description:\n        Defines fixed numerical values used during the model training process, such as default learning rates for optimizers (Adam, SGD) and the batch size. These might be baseline values used before tuning or fixed settings.\nThese are like standard kitchen constants \u2013 perhaps the default oven preheat temperature or the standard size of a measuring cup used across many recipes. They provide consistent starting points or fixed parameters for parts of the process.\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n\n        Relevant Code Snippets:\n        --- File: constants.py ---\nBATCH_SIZE = 32\nADAM_LR = 0.001\nSGD_LR = 0.01\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 3: Training Constants`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Hyperparameter Tuning Setup\". This is Chapter 4.\n\n        Concept Details:\n        - Description:\n        This represents the overall process of automatically finding the best settings (hyperparameters) for the sentiment analysis transformer model. It involves running multiple training experiments with different configurations.\nThink of it like trying different recipes (hyperparameter combinations) in a kitchen (`sentiment_transformer_tuning` folder) to discover which one makes the best cake (most accurate model). The goal is to optimize performance without manual guesswork.\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n---\n# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).\n\n        Relevant Code Snippets:\n        --- File: sentiment_transformer_tuning/README.md ---\nThis folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n\n\n--- File: sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n--- File: sentiment_transformer_tuning/trial_00/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/tuner0.json ---\n{}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 4: Hyperparameter Tuning Setup`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 4: Hyperparameter Tuning Setup\n\nHi there! In the [previous chapter](03_training_constants.md), we learned about **Training Constants** \u2013 the fixed settings like `BATCH_SIZE` that act as our standard tools. We now have:\n\n1.  The required data format: [Model Input Shape](01_model_input_shape.md) (`[null, 62]`).\n2.  The range of experimental settings: [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) (e.g., how many layers, learning rate).\n3.  The standard, fixed settings: [Training Constants](03_training_constants.md) (e.g., batch size).\n\nNow, how do we actually put all this together and *start the experiments* to find the best combination of hyperparameters? We need a system, a setup, to manage this process automatically. That's what this chapter is about!\n\n## The Problem: Running Many Experiments Systematically\n\nImagine you're determined to bake the absolute best chocolate cake. You have a basic recipe, but you know you can improve it by tweaking things like the amount of sugar, the number of eggs, or the baking time (these are like our hyperparameters).\n\nTrying just one variation isn't enough. You need to:\n*   Try *many* different combinations (recipes).\n*   Keep track of exactly what you did for each cake (which recipe variation).\n*   Record how well each cake turned out (taste test!).\n*   Compare the results to find the winning recipe.\n\nDoing this manually would be slow and messy. You need an organized *setup* for your baking experiments \u2013 maybe a dedicated corner of the kitchen, a notebook for results, and a clear plan.\n\nSimilarly, finding the best settings for our sentiment analysis model requires running many training experiments with different hyperparameter combinations. We need an organized way to manage these experiments, track their configurations, and store their results. This organized system is our **Hyperparameter Tuning Setup**.\n\n## What is the Hyperparameter Tuning Setup?\n\nThe **Hyperparameter Tuning Setup** is the overall **process and infrastructure** we use to automatically:\n\n1.  Define the experiments we want to run (using the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)).\n2.  Execute each experiment (called a \"trial\"). Each trial builds and trains a model with a specific set of hyperparameters.\n3.  Keep track of the hyperparameters used in each trial.\n4.  Record the performance (e.g., accuracy) of the model from each trial.\n5.  Organize all this information so we can later identify the best hyperparameters found.\n\nThink of it as setting up your **experimental kitchen** for finding the best cake recipe:\n\n*   **The Kitchen Space:** A dedicated place where all the experiments happen. In our project, this is the `sentiment_transformer_tuning` folder.\n*   **The Master Recipe Book:** A central place defining the range of ingredients to try ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)) and tracking the overall progress ([Keras Tuner Oracle](05_keras_tuner_oracle.md)).\n*   **Individual Recipe Cards & Notes:** For each specific cake baked (each trial), a record of the exact recipe used and how it turned out ([Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)).\n\n## Our Experimental Kitchen: The `sentiment_transformer_tuning` Folder\n\nIn our project, the main place where all the tuning experiments happen and their results are stored is a folder named `sentiment_transformer_tuning`.\n\nLet's peek at its structure and purpose:\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 README.md                    # A note about this folder\n\u251c\u2500\u2500 oracle.json                  # The 'master plan' / tuner's brain state\n\u251c\u2500\u2500 tuner0.json                  # Tuner configuration status (often empty or minimal)\n\u251c\u2500\u2500 trial_00/                    # Results & config for the 1st experiment\n\u2502   \u251c\u2500\u2500 build_config.json        # Input shape used for this trial\n\u2502   \u251c\u2500\u2500 trial.json               # Details of this specific trial (hyperparameters, score)\n\u2502   \u2514\u2500\u2500 ... (other files like model checkpoints)\n\u251c\u2500\u2500 trial_01/                    # Results & config for the 2nd experiment\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_02/\n...\n\u2514\u2500\u2500 trial_09/                    # Results & config for the 10th experiment\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\n**Explanation:**\n\n*   **`sentiment_transformer_tuning/`**: This is the main directory for our tuning process. Think of it as the logbook for our recipe experiments. The `README.md` inside just gives a brief description.\n    --- File: `sentiment_transformer_tuning/README.md` ---\n    ```\n    This folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n    ```\n*   **`oracle.json`**: This is a crucial file managed by the tuning tool (Keras Tuner). It stores the overall state of the tuning process, including the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined, which trials have been run, and maybe the best results found so far. We'll dive deeper into this in the next chapter: [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   **`tuner0.json`**: Stores some internal state for the tuner itself. For beginners, it's less important than `oracle.json`.\n*   **`trial_XX/` folders**: Each folder represents *one complete experiment* (one \"trial\") testing a specific combination of hyperparameters.\n    *   Inside each `trial_XX` folder, files like `trial.json` store the specific hyperparameter values used for *that trial* and the resulting performance score. Other files might store the trained model weights or configuration details like the input shape (`build_config.json`). We'll explore these in more detail in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\nThis folder structure is automatically created and managed by the hyperparameter tuning tool (like Keras Tuner) when we run the tuning script. It provides a clean and organized way to store the results of potentially hundreds of experiments.\n\n## How the Tuning Setup Works (The Big Picture)\n\nImagine you start the automatic tuning process (usually by running a Python script, which we won't detail here). Here\u2019s a simplified step-by-step flow:\n\n1.  **Start:** You initiate the tuning process.\n2.  **Ask Oracle:** The system asks the [Keras Tuner Oracle](05_keras_tuner_oracle.md) (using the info in `oracle.json`) for the next set of hyperparameters to try, based on the defined [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) and past results.\n3.  **Run Trial:** A new trial (e.g., `trial_10`) is started:\n    *   A model is built using the specific hyperparameters suggested by the Oracle.\n    *   The model is trained using the data, respecting the [Model Input Shape](01_model_input_shape.md) and using [Training Constants](03_training_constants.md) where needed.\n    *   The model's performance is evaluated (e.g., accuracy is measured).\n4.  **Save Results:** The hyperparameters used, the final performance score, and other relevant information for this trial are saved inside the corresponding `trial_XX` folder (e.g., in `sentiment_transformer_tuning/trial_10/trial.json`).\n5.  **Update Oracle:** The performance result is reported back to the Oracle, which updates its state in `oracle.json`. This helps it make better decisions about which hyperparameters to try next.\n6.  **Repeat:** Steps 2-5 are repeated for a predefined number of trials or until a certain condition is met.\n7.  **Finish:** Once done, you can examine the `oracle.json` file or the `trial_XX` folders (especially their `trial.json` files) to find the trial that achieved the best performance and the hyperparameters that led to it.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant TuningScript as Tuning Process\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    User->>TuningScript: Start Hyperparameter Tuning\n    loop For each trial\n        TuningScript->>Oracle: Get next Hyperparameters to test\n        Oracle-->>TuningScript: Provide Hyperparameter set\n        TuningScript->>TrialRunner: Run trial with these Hyperparameters\n        TrialRunner->>TrialRunner: Build Model\n        TrialRunner->>TrialRunner: Train Model\n        TrialRunner->>TrialRunner: Evaluate Performance (e.g., Accuracy)\n        TrialRunner->>TrialFolder: Save Trial Config & Results (trial.json)\n        TrialRunner-->>TuningScript: Return Performance Score\n        TuningScript->>Oracle: Report Score for these Hyperparameters\n        Oracle->>Oracle: Update internal state (oracle.json)\n    end\n    TuningScript-->>User: Tuning Finished (Results in folder)\n\n```\n\nThis whole setup allows us to systematically explore different model configurations without manual intervention, saving time and effort while increasing the chances of finding a high-performing model.\n\n## Conclusion\n\nYou've now learned about the **Hyperparameter Tuning Setup**:\n\n*   It's the **overall process and infrastructure** for automatically finding the best model settings (hyperparameters).\n*   It involves running multiple **trials** (experiments), each testing a different hyperparameter combination.\n*   It's like setting up an **experimental kitchen** to systematically test many cake recipes.\n*   In our project, the results and state are primarily stored in the **`sentiment_transformer_tuning` folder**.\n*   This folder contains the **`oracle.json`** file (the master plan) and individual **`trial_XX` folders** (results of each specific experiment).\n*   This setup automates the search, organizes results, and helps us find the best-performing model configuration.\n\nNow that we understand the overall structure, let's zoom in on the \"brain\" of this operation \u2013 the component that decides which hyperparameters to try next.\n\nReady to meet the mastermind? Let's explore the [Keras Tuner Oracle](05_keras_tuner_oracle.md) in the next chapter!", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Keras Tuner Oracle\". This is Chapter 5.\n\n        Concept Details:\n        - Description:\n        The central component managing the hyperparameter search, specifically within the Keras Tuner library. It keeps track of which hyperparameter combinations have been tried, their results, and decides which combinations to try next.\nIt's like the head chef who directs the baking experiments, meticulously logging each recipe tested (`tried_so_far`), its outcome (stored within the tuner's state, reflected here), and planning the next experiment based on the defined strategy (e.g., random search).\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n---\n# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).\n---\n# Chapter 4: Hyperparameter Tuning Setup\n\nHi there! In the [previous chapter](03_training_constants.md), we learned about **Training Constants** \u2013 the fixed settings like `BATCH_SIZE` that act as our standard tools. We now have:\n\n1.  The required data format: [Model Input Shape](01_model_input_shape.md) (`[null, 62]`).\n2.  The range of experimental settings: [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) (e.g., how many layers, learning rate).\n3.  The standard, fixed settings: [Training Constants](03_training_constants.md) (e.g., batch size).\n\nNow, how do we actually put all this together and *start the experiments* to find the best combination of hyperparameters? We need a system, a setup, to manage this process automatically. That's what this chapter is about!\n\n## The Problem: Running Many Experiments Systematically\n\nImagine you're determined to bake the absolute best chocolate cake. You have a basic recipe, but you know you can improve it by tweaking things like the amount of sugar, the number of eggs, or the baking time (these are like our hyperparameters).\n\nTrying just one variation isn't enough. You need to:\n*   Try *many* different combinations (recipes).\n*   Keep track of exactly what you did for each cake (which recipe variation).\n*   Record how well each cake turned out (taste test!).\n*   Compare the results to find the winning recipe.\n\nDoing this manually would be slow and messy. You need an organized *setup* for your baking experiments \u2013 maybe a dedicated corner of the kitchen, a notebook for results, and a clear plan.\n\nSimilarly, finding the best settings for our sentiment analysis model requires running many training experiments with different hyperparameter combinations. We need an organized way to manage these experiments, track their configurations, and store their results. This organized system is our **Hyperparameter Tuning Setup**.\n\n## What is the Hyperparameter Tuning Setup?\n\nThe **Hyperparameter Tuning Setup** is the overall **process and infrastructure** we use to automatically:\n\n1.  Define the experiments we want to run (using the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)).\n2.  Execute each experiment (called a \"trial\"). Each trial builds and trains a model with a specific set of hyperparameters.\n3.  Keep track of the hyperparameters used in each trial.\n4.  Record the performance (e.g., accuracy) of the model from each trial.\n5.  Organize all this information so we can later identify the best hyperparameters found.\n\nThink of it as setting up your **experimental kitchen** for finding the best cake recipe:\n\n*   **The Kitchen Space:** A dedicated place where all the experiments happen. In our project, this is the `sentiment_transformer_tuning` folder.\n*   **The Master Recipe Book:** A central place defining the range of ingredients to try ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)) and tracking the overall progress ([Keras Tuner Oracle](05_keras_tuner_oracle.md)).\n*   **Individual Recipe Cards & Notes:** For each specific cake baked (each trial), a record of the exact recipe used and how it turned out ([Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)).\n\n## Our Experimental Kitchen: The `sentiment_transformer_tuning` Folder\n\nIn our project, the main place where all the tuning experiments happen and their results are stored is a folder named `sentiment_transformer_tuning`.\n\nLet's peek at its structure and purpose:\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 README.md                    # A note about this folder\n\u251c\u2500\u2500 oracle.json                  # The 'master plan' / tuner's brain state\n\u251c\u2500\u2500 tuner0.json                  # Tuner configuration status (often empty or minimal)\n\u251c\u2500\u2500 trial_00/                    # Results & config for the 1st experiment\n\u2502   \u251c\u2500\u2500 build_config.json        # Input shape used for this trial\n\u2502   \u251c\u2500\u2500 trial.json               # Details of this specific trial (hyperparameters, score)\n\u2502   \u2514\u2500\u2500 ... (other files like model checkpoints)\n\u251c\u2500\u2500 trial_01/                    # Results & config for the 2nd experiment\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_02/\n...\n\u2514\u2500\u2500 trial_09/                    # Results & config for the 10th experiment\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\n**Explanation:**\n\n*   **`sentiment_transformer_tuning/`**: This is the main directory for our tuning process. Think of it as the logbook for our recipe experiments. The `README.md` inside just gives a brief description.\n    --- File: `sentiment_transformer_tuning/README.md` ---\n    ```\n    This folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n    ```\n*   **`oracle.json`**: This is a crucial file managed by the tuning tool (Keras Tuner). It stores the overall state of the tuning process, including the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined, which trials have been run, and maybe the best results found so far. We'll dive deeper into this in the next chapter: [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   **`tuner0.json`**: Stores some internal state for the tuner itself. For beginners, it's less important than `oracle.json`.\n*   **`trial_XX/` folders**: Each folder represents *one complete experiment* (one \"trial\") testing a specific combination of hyperparameters.\n    *   Inside each `trial_XX` folder, files like `trial.json` store the specific hyperparameter values used for *that trial* and the resulting performance score. Other files might store the trained model weights or configuration details like the input shape (`build_config.json`). We'll explore these in more detail in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\nThis folder structure is automatically created and managed by the hyperparameter tuning tool (like Keras Tuner) when we run the tuning script. It provides a clean and organized way to store the results of potentially hundreds of experiments.\n\n## How the Tuning Setup Works (The Big Picture)\n\nImagine you start the automatic tuning process (usually by running a Python script, which we won't detail here). Here\u2019s a simplified step-by-step flow:\n\n1.  **Start:** You initiate the tuning process.\n2.  **Ask Oracle:** The system asks the [Keras Tuner Oracle](05_keras_tuner_oracle.md) (using the info in `oracle.json`) for the next set of hyperparameters to try, based on the defined [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) and past results.\n3.  **Run Trial:** A new trial (e.g., `trial_10`) is started:\n    *   A model is built using the specific hyperparameters suggested by the Oracle.\n    *   The model is trained using the data, respecting the [Model Input Shape](01_model_input_shape.md) and using [Training Constants](03_training_constants.md) where needed.\n    *   The model's performance is evaluated (e.g., accuracy is measured).\n4.  **Save Results:** The hyperparameters used, the final performance score, and other relevant information for this trial are saved inside the corresponding `trial_XX` folder (e.g., in `sentiment_transformer_tuning/trial_10/trial.json`).\n5.  **Update Oracle:** The performance result is reported back to the Oracle, which updates its state in `oracle.json`. This helps it make better decisions about which hyperparameters to try next.\n6.  **Repeat:** Steps 2-5 are repeated for a predefined number of trials or until a certain condition is met.\n7.  **Finish:** Once done, you can examine the `oracle.json` file or the `trial_XX` folders (especially their `trial.json` files) to find the trial that achieved the best performance and the hyperparameters that led to it.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant TuningScript as Tuning Process\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    User->>TuningScript: Start Hyperparameter Tuning\n    loop For each trial\n        TuningScript->>Oracle: Get next Hyperparameters to test\n        Oracle-->>TuningScript: Provide Hyperparameter set\n        TuningScript->>TrialRunner: Run trial with these Hyperparameters\n        TrialRunner->>TrialRunner: Build Model\n        TrialRunner->>TrialRunner: Train Model\n        TrialRunner->>TrialRunner: Evaluate Performance (e.g., Accuracy)\n        TrialRunner->>TrialFolder: Save Trial Config & Results (trial.json)\n        TrialRunner-->>TuningScript: Return Performance Score\n        TuningScript->>Oracle: Report Score for these Hyperparameters\n        Oracle->>Oracle: Update internal state (oracle.json)\n    end\n    TuningScript-->>User: Tuning Finished (Results in folder)\n\n```\n\nThis whole setup allows us to systematically explore different model configurations without manual intervention, saving time and effort while increasing the chances of finding a high-performing model.\n\n## Conclusion\n\nYou've now learned about the **Hyperparameter Tuning Setup**:\n\n*   It's the **overall process and infrastructure** for automatically finding the best model settings (hyperparameters).\n*   It involves running multiple **trials** (experiments), each testing a different hyperparameter combination.\n*   It's like setting up an **experimental kitchen** to systematically test many cake recipes.\n*   In our project, the results and state are primarily stored in the **`sentiment_transformer_tuning` folder**.\n*   This folder contains the **`oracle.json`** file (the master plan) and individual **`trial_XX` folders** (results of each specific experiment).\n*   This setup automates the search, organizes results, and helps us find the best-performing model configuration.\n\nNow that we understand the overall structure, let's zoom in on the \"brain\" of this operation \u2013 the component that decides which hyperparameters to try next.\n\nReady to meet the mastermind? Let's explore the [Keras Tuner Oracle](05_keras_tuner_oracle.md) in the next chapter!\n\n        Relevant Code Snippets:\n        --- File: sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 5: Keras Tuner Oracle`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 5: Keras Tuner Oracle\n\nWelcome back! In the [previous chapter](04_hyperparameter_tuning_setup.md), we learned about the overall **Hyperparameter Tuning Setup** \u2013 our organized \"experimental kitchen\" (`sentiment_transformer_tuning/` folder) for automatically running many model training experiments (trials) to find the best settings. We saw that this setup involves asking for hyperparameter combinations, running trials, and recording results.\n\nBut who decides *which* combination of hyperparameters to try next? And who keeps track of all the experiments and their outcomes? That's the job of the **Keras Tuner Oracle**!\n\n## The Problem: Who's Directing the Experiments?\n\nImagine you're in your experimental kitchen, ready to bake dozens of cakes to find the perfect recipe. You've defined the possible ranges for your ingredients ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)): 2-8 eggs, 100g-500g sugar, etc.\n\nYou *could* just randomly pick values from these ranges for each cake. But maybe there's a smarter way? Perhaps after baking a few cakes, you notice that cakes with more sugar tend to taste better. Shouldn't your next experiments focus more on the higher end of the sugar range?\n\nYou need a **director** or a **head chef** for your experiments. Someone (or something!) that:\n\n1.  Knows the allowed ingredient ranges (the hyperparameter space).\n2.  Decides the *exact* recipe (hyperparameter combination) for the *next* cake (trial).\n3.  Keeps a detailed logbook of every recipe tried and how well the cake turned out (the performance score).\n4.  Uses this information (and possibly a clever strategy) to guide future experiments towards better recipes.\n\nThis \"director\" role in Keras Tuner is filled by the **Oracle**.\n\n## What is the Keras Tuner Oracle?\n\nThe **Keras Tuner Oracle** is the central component that **manages the hyperparameter search process**. It's the \"brain\" behind the tuning operation within the Keras Tuner library.\n\nThink of it as the **Head Chef** in our experimental kitchen:\n\n*   **Knows the Recipe Boundaries:** It holds the definition of the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n*   **Keeps the Master Logbook:** It tracks which hyperparameter combinations have been tried (`tried_so_far`), the results (scores) of those trials, and the overall state of the search.\n*   **Plans the Next Experiment:** Based on a defined strategy (like random search, or more advanced methods), it decides which specific combination of hyperparameters should be tested in the next trial.\n\nThe Oracle ensures that the search is conducted systematically and efficiently, guiding the process towards finding the best possible hyperparameters within the defined space.\n\n## The Oracle's Logbook: Peeking Inside `oracle.json`\n\nThe Oracle needs to store its knowledge and the history of the search somewhere. In our project setup ([Chapter 4](04_hyperparameter_tuning_setup.md)), this information is primarily stored in the `sentiment_transformer_tuning/oracle.json` file. This file acts as the Oracle's master logbook and state tracker.\n\nLet's look at some key parts of this file (we saw the full file in Chapter 4, here are the highlights):\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Simplified Snippet) ---\n```json\n{\n  \"hyperparameters\": { // Section about the settings being tuned\n    \"space\": [ // The allowed ranges (defined in Chapter 2)\n      // ... definitions for num_heads, dff, num_layers, etc. ...\n    ]\n  },\n  \"tried_so_far\": [ // A list representing unique recipes already tried\n    \"198210495baf2451b73e907a02b1546c\", // Hash representing one combo\n    \"c4be04584b5ae928a62b872b508505ad\", // Another unique combo\n    // ... more hashes ...\n    \"29c605d5e8a77f9296610df81f15c87c\"\n  ],\n  \"id_to_hash\": { // Maps trial IDs (like folder names) to the recipe hash\n    \"00\": \"29c605d5e8a77f9296610df81f15c87c\",\n    \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\",\n    // ... mapping for all trials (00 to 09) ...\n    \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"\n  },\n  \"seed\": 2999, // Starting point for random number generation\n  \"seed_state\": 3049, // Current state of random generation\n  // ... other details like trial start/end times ...\n}\n```\n\n**What this tells us about the Oracle:**\n\n*   **It knows the rules:** The `\"hyperparameters\"` section shows the Oracle has stored the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined \u2013 the allowed ranges for each setting.\n*   **It remembers the past:** The `\"tried_so_far\"` list contains unique identifiers (hashes) for each hyperparameter combination that has already been tested. This prevents the Oracle from trying the exact same recipe twice (unless intended). Like the chef crossing off recipes in the logbook.\n*   **It connects trials to recipes:** The `\"id_to_hash\"` dictionary links the trial number (like `00`, `01`) to the specific recipe (hash) used in that trial. This helps organize the results.\n*   **It manages randomness:** The `\"seed\"` and `\"seed_state\"` relate to how the Oracle uses random numbers (important for strategies like Random Search or breaking ties). Using seeds helps make experiments reproducible.\n\nThis `oracle.json` file essentially captures the Oracle's memory and current understanding of the search process.\n\n## How the Oracle Decides: The Strategy\n\nOkay, the Oracle remembers the past, but how does it decide what to try *next*? This depends on the **type** of Oracle being used, which corresponds to a specific search strategy.\n\nCommon strategies (types of Oracles) include:\n\n1.  **Random Search:** The Oracle simply picks the next hyperparameter combination randomly from the allowed space. It's like the chef randomly choosing ingredient amounts within the defined ranges for each cake. This is simple and surprisingly effective sometimes. The `oracle.json` we looked at likely came from a Random Search, as it explores different combinations without a clear pattern based only on the hashes shown.\n2.  **Bayesian Optimization:** A more advanced strategy. The Oracle builds a statistical model based on the results of previous trials. It uses this model to predict which *untried* combinations are most likely to yield good results. It tries to balance exploring new areas of the search space with exploiting areas that have already shown promise. Like the chef noticing that 3-egg cakes were good, and deciding to try 4 eggs next, based on that trend.\n3.  **Hyperband:** Another clever strategy that tries many combinations quickly with limited resources (e.g., training for fewer epochs initially) and then dedicates more resources to the most promising ones.\n\nThe choice of which Oracle (and thus strategy) to use is made when you set up the tuner in your Python code. The Oracle then implements that strategy, using the information in `oracle.json` to guide its decisions.\n\n## How it Works Under the Hood\n\nLet's visualize the Oracle's role in the tuning loop:\n\n1.  The main tuning process needs parameters for a new trial.\n2.  It asks the Oracle.\n3.  The Oracle consults its internal state (strategy, history from `oracle.json`, defined space).\n4.  It decides on a new set of hyperparameters.\n5.  It gives these back to the tuning process.\n6.  The trial runs with these parameters.\n7.  The tuning process reports the final score (e.g., accuracy) back to the Oracle.\n8.  The Oracle updates its state (`oracle.json`) with the results of this trial.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant Tuner as Tuning Process\n    participant Oracle as Keras Tuner Oracle (manages oracle.json)\n    participant Trial as Trial Execution\n\n    Tuner->>Oracle: Request hyperparameters for next trial\n    Oracle->>Oracle: Consult space, history, strategy\n    Oracle->>Oracle: Decide next hyperparameters (e.g., num_heads=4, lr=0.0005)\n    Oracle-->>Tuner: Provide chosen hyperparameters\n    Tuner->>Trial: Run trial with these parameters\n    Trial->>Trial: Build, Train, Evaluate Model\n    Trial-->>Tuner: Return performance score (e.g., 0.85 accuracy)\n    Tuner->>Oracle: Report score for the tried hyperparameters\n    Oracle->>Oracle: Update history & state (in oracle.json)\n```\n\n## Using the Oracle (Implicitly)\n\nAs a beginner, you usually don't interact with the Oracle object directly *while* the tuning is running. You interact with it primarily during the initial **setup**:\n\n```python\n# Conceptual Python code for setting up a Tuner with an Oracle\n\nimport keras_tuner as kt\nimport tensorflow as tf # Just for context\n\n# Assume build_model is a function that defines your model\n# and uses hp.Int, hp.Float etc. to define the search space\n# (as shown in Chapter 2)\n# def build_model(hp):\n#   ... define model using hp ...\n#   return model\n\n# 1. CHOOSE the Oracle/Strategy: Here we choose RandomSearch\ntuner = kt.RandomSearch(\n    hypermodel=build_model,           # Function that builds the model\n    objective=\"val_accuracy\",        # What score to maximize\n    max_trials=10,                   # How many different combinations to try\n    executions_per_trial=1,          # How many times to train each combination\n    directory=\"sentiment_transformer_tuning\", # Where to store results (incl. oracle.json)\n    project_name=\"my_first_tuning\"   # Subfolder within the directory\n)\n\n# 2. START the search (The Tuner uses the Oracle internally)\n# tuner.search(train_data, epochs=5, validation_data=val_data)\n# ... The Oracle manages things behind the scenes during search ...\n\nprint(\"Tuning setup complete. The Oracle will manage the search.\")\n# When tuner.search() runs, the RandomSearch Oracle inside the 'tuner'\n# object will be consulted repeatedly.\n```\n\n**Explanation:**\n\n1.  We choose `kt.RandomSearch`. This creates a Tuner that *contains* a `RandomSearchOracle`. If we wanted Bayesian Optimization, we'd use `kt.BayesianOptimization(...)`.\n2.  We configure the Tuner, telling it which function defines our model and search space (`build_model`), what metric to optimize (`val_accuracy`), how many trials to run (`max_trials`), and where to save the results (`directory`).\n3.  When we would call `tuner.search(...)` (commented out here), the Tuner object takes over. It repeatedly asks its internal Oracle for hyperparameters, runs the trial, and reports back the score, all automatically. The `oracle.json` file is created and updated within the specified `directory`.\n\nSo, you choose the *type* of Oracle by choosing the type of Tuner, and the framework handles the rest!\n\n## Conclusion\n\nYou've now met the **Keras Tuner Oracle**, the mastermind behind hyperparameter tuning!\n\n*   It's the **central component managing the search**, like the head chef directing recipe experiments.\n*   It **knows the hyperparameter space**, **tracks tried combinations** and their **results**, and **decides which parameters to try next**.\n*   Its state and history are stored in the **`oracle.json`** file.\n*   The **strategy** it uses (e.g., Random Search, Bayesian Optimization) depends on the type of Oracle/Tuner you choose during setup.\n*   It works behind the scenes during the `tuner.search()` process to guide the experiments intelligently.\n\nThe Oracle manages the overall search, but what information is stored for each *individual* experiment or \"trial\" that it launches?\n\nLet's dive into the details of each specific experiment in the next chapter: [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Tuning Trial Configuration & State\". This is Chapter 6.\n\n        Concept Details:\n        - Description:\n        Represents a single experiment run within the hyperparameter tuning process. Each trial uses a specific combination of hyperparameters selected by the Tuner Oracle from the defined space. The results and configuration of each trial are stored.\nThis is like one specific attempt at baking a cake using one chosen recipe variation. The `trial_*` folders contain the notes (`build_config.json`) and outcomes for that specific attempt. The `oracle.json` tracks the status (start/end times) of all trials.\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n---\n# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).\n---\n# Chapter 4: Hyperparameter Tuning Setup\n\nHi there! In the [previous chapter](03_training_constants.md), we learned about **Training Constants** \u2013 the fixed settings like `BATCH_SIZE` that act as our standard tools. We now have:\n\n1.  The required data format: [Model Input Shape](01_model_input_shape.md) (`[null, 62]`).\n2.  The range of experimental settings: [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) (e.g., how many layers, learning rate).\n3.  The standard, fixed settings: [Training Constants](03_training_constants.md) (e.g., batch size).\n\nNow, how do we actually put all this together and *start the experiments* to find the best combination of hyperparameters? We need a system, a setup, to manage this process automatically. That's what this chapter is about!\n\n## The Problem: Running Many Experiments Systematically\n\nImagine you're determined to bake the absolute best chocolate cake. You have a basic recipe, but you know you can improve it by tweaking things like the amount of sugar, the number of eggs, or the baking time (these are like our hyperparameters).\n\nTrying just one variation isn't enough. You need to:\n*   Try *many* different combinations (recipes).\n*   Keep track of exactly what you did for each cake (which recipe variation).\n*   Record how well each cake turned out (taste test!).\n*   Compare the results to find the winning recipe.\n\nDoing this manually would be slow and messy. You need an organized *setup* for your baking experiments \u2013 maybe a dedicated corner of the kitchen, a notebook for results, and a clear plan.\n\nSimilarly, finding the best settings for our sentiment analysis model requires running many training experiments with different hyperparameter combinations. We need an organized way to manage these experiments, track their configurations, and store their results. This organized system is our **Hyperparameter Tuning Setup**.\n\n## What is the Hyperparameter Tuning Setup?\n\nThe **Hyperparameter Tuning Setup** is the overall **process and infrastructure** we use to automatically:\n\n1.  Define the experiments we want to run (using the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)).\n2.  Execute each experiment (called a \"trial\"). Each trial builds and trains a model with a specific set of hyperparameters.\n3.  Keep track of the hyperparameters used in each trial.\n4.  Record the performance (e.g., accuracy) of the model from each trial.\n5.  Organize all this information so we can later identify the best hyperparameters found.\n\nThink of it as setting up your **experimental kitchen** for finding the best cake recipe:\n\n*   **The Kitchen Space:** A dedicated place where all the experiments happen. In our project, this is the `sentiment_transformer_tuning` folder.\n*   **The Master Recipe Book:** A central place defining the range of ingredients to try ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)) and tracking the overall progress ([Keras Tuner Oracle](05_keras_tuner_oracle.md)).\n*   **Individual Recipe Cards & Notes:** For each specific cake baked (each trial), a record of the exact recipe used and how it turned out ([Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)).\n\n## Our Experimental Kitchen: The `sentiment_transformer_tuning` Folder\n\nIn our project, the main place where all the tuning experiments happen and their results are stored is a folder named `sentiment_transformer_tuning`.\n\nLet's peek at its structure and purpose:\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 README.md                    # A note about this folder\n\u251c\u2500\u2500 oracle.json                  # The 'master plan' / tuner's brain state\n\u251c\u2500\u2500 tuner0.json                  # Tuner configuration status (often empty or minimal)\n\u251c\u2500\u2500 trial_00/                    # Results & config for the 1st experiment\n\u2502   \u251c\u2500\u2500 build_config.json        # Input shape used for this trial\n\u2502   \u251c\u2500\u2500 trial.json               # Details of this specific trial (hyperparameters, score)\n\u2502   \u2514\u2500\u2500 ... (other files like model checkpoints)\n\u251c\u2500\u2500 trial_01/                    # Results & config for the 2nd experiment\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_02/\n...\n\u2514\u2500\u2500 trial_09/                    # Results & config for the 10th experiment\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\n**Explanation:**\n\n*   **`sentiment_transformer_tuning/`**: This is the main directory for our tuning process. Think of it as the logbook for our recipe experiments. The `README.md` inside just gives a brief description.\n    --- File: `sentiment_transformer_tuning/README.md` ---\n    ```\n    This folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n    ```\n*   **`oracle.json`**: This is a crucial file managed by the tuning tool (Keras Tuner). It stores the overall state of the tuning process, including the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined, which trials have been run, and maybe the best results found so far. We'll dive deeper into this in the next chapter: [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   **`tuner0.json`**: Stores some internal state for the tuner itself. For beginners, it's less important than `oracle.json`.\n*   **`trial_XX/` folders**: Each folder represents *one complete experiment* (one \"trial\") testing a specific combination of hyperparameters.\n    *   Inside each `trial_XX` folder, files like `trial.json` store the specific hyperparameter values used for *that trial* and the resulting performance score. Other files might store the trained model weights or configuration details like the input shape (`build_config.json`). We'll explore these in more detail in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\nThis folder structure is automatically created and managed by the hyperparameter tuning tool (like Keras Tuner) when we run the tuning script. It provides a clean and organized way to store the results of potentially hundreds of experiments.\n\n## How the Tuning Setup Works (The Big Picture)\n\nImagine you start the automatic tuning process (usually by running a Python script, which we won't detail here). Here\u2019s a simplified step-by-step flow:\n\n1.  **Start:** You initiate the tuning process.\n2.  **Ask Oracle:** The system asks the [Keras Tuner Oracle](05_keras_tuner_oracle.md) (using the info in `oracle.json`) for the next set of hyperparameters to try, based on the defined [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) and past results.\n3.  **Run Trial:** A new trial (e.g., `trial_10`) is started:\n    *   A model is built using the specific hyperparameters suggested by the Oracle.\n    *   The model is trained using the data, respecting the [Model Input Shape](01_model_input_shape.md) and using [Training Constants](03_training_constants.md) where needed.\n    *   The model's performance is evaluated (e.g., accuracy is measured).\n4.  **Save Results:** The hyperparameters used, the final performance score, and other relevant information for this trial are saved inside the corresponding `trial_XX` folder (e.g., in `sentiment_transformer_tuning/trial_10/trial.json`).\n5.  **Update Oracle:** The performance result is reported back to the Oracle, which updates its state in `oracle.json`. This helps it make better decisions about which hyperparameters to try next.\n6.  **Repeat:** Steps 2-5 are repeated for a predefined number of trials or until a certain condition is met.\n7.  **Finish:** Once done, you can examine the `oracle.json` file or the `trial_XX` folders (especially their `trial.json` files) to find the trial that achieved the best performance and the hyperparameters that led to it.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant TuningScript as Tuning Process\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    User->>TuningScript: Start Hyperparameter Tuning\n    loop For each trial\n        TuningScript->>Oracle: Get next Hyperparameters to test\n        Oracle-->>TuningScript: Provide Hyperparameter set\n        TuningScript->>TrialRunner: Run trial with these Hyperparameters\n        TrialRunner->>TrialRunner: Build Model\n        TrialRunner->>TrialRunner: Train Model\n        TrialRunner->>TrialRunner: Evaluate Performance (e.g., Accuracy)\n        TrialRunner->>TrialFolder: Save Trial Config & Results (trial.json)\n        TrialRunner-->>TuningScript: Return Performance Score\n        TuningScript->>Oracle: Report Score for these Hyperparameters\n        Oracle->>Oracle: Update internal state (oracle.json)\n    end\n    TuningScript-->>User: Tuning Finished (Results in folder)\n\n```\n\nThis whole setup allows us to systematically explore different model configurations without manual intervention, saving time and effort while increasing the chances of finding a high-performing model.\n\n## Conclusion\n\nYou've now learned about the **Hyperparameter Tuning Setup**:\n\n*   It's the **overall process and infrastructure** for automatically finding the best model settings (hyperparameters).\n*   It involves running multiple **trials** (experiments), each testing a different hyperparameter combination.\n*   It's like setting up an **experimental kitchen** to systematically test many cake recipes.\n*   In our project, the results and state are primarily stored in the **`sentiment_transformer_tuning` folder**.\n*   This folder contains the **`oracle.json`** file (the master plan) and individual **`trial_XX` folders** (results of each specific experiment).\n*   This setup automates the search, organizes results, and helps us find the best-performing model configuration.\n\nNow that we understand the overall structure, let's zoom in on the \"brain\" of this operation \u2013 the component that decides which hyperparameters to try next.\n\nReady to meet the mastermind? Let's explore the [Keras Tuner Oracle](05_keras_tuner_oracle.md) in the next chapter!\n---\n# Chapter 5: Keras Tuner Oracle\n\nWelcome back! In the [previous chapter](04_hyperparameter_tuning_setup.md), we learned about the overall **Hyperparameter Tuning Setup** \u2013 our organized \"experimental kitchen\" (`sentiment_transformer_tuning/` folder) for automatically running many model training experiments (trials) to find the best settings. We saw that this setup involves asking for hyperparameter combinations, running trials, and recording results.\n\nBut who decides *which* combination of hyperparameters to try next? And who keeps track of all the experiments and their outcomes? That's the job of the **Keras Tuner Oracle**!\n\n## The Problem: Who's Directing the Experiments?\n\nImagine you're in your experimental kitchen, ready to bake dozens of cakes to find the perfect recipe. You've defined the possible ranges for your ingredients ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)): 2-8 eggs, 100g-500g sugar, etc.\n\nYou *could* just randomly pick values from these ranges for each cake. But maybe there's a smarter way? Perhaps after baking a few cakes, you notice that cakes with more sugar tend to taste better. Shouldn't your next experiments focus more on the higher end of the sugar range?\n\nYou need a **director** or a **head chef** for your experiments. Someone (or something!) that:\n\n1.  Knows the allowed ingredient ranges (the hyperparameter space).\n2.  Decides the *exact* recipe (hyperparameter combination) for the *next* cake (trial).\n3.  Keeps a detailed logbook of every recipe tried and how well the cake turned out (the performance score).\n4.  Uses this information (and possibly a clever strategy) to guide future experiments towards better recipes.\n\nThis \"director\" role in Keras Tuner is filled by the **Oracle**.\n\n## What is the Keras Tuner Oracle?\n\nThe **Keras Tuner Oracle** is the central component that **manages the hyperparameter search process**. It's the \"brain\" behind the tuning operation within the Keras Tuner library.\n\nThink of it as the **Head Chef** in our experimental kitchen:\n\n*   **Knows the Recipe Boundaries:** It holds the definition of the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n*   **Keeps the Master Logbook:** It tracks which hyperparameter combinations have been tried (`tried_so_far`), the results (scores) of those trials, and the overall state of the search.\n*   **Plans the Next Experiment:** Based on a defined strategy (like random search, or more advanced methods), it decides which specific combination of hyperparameters should be tested in the next trial.\n\nThe Oracle ensures that the search is conducted systematically and efficiently, guiding the process towards finding the best possible hyperparameters within the defined space.\n\n## The Oracle's Logbook: Peeking Inside `oracle.json`\n\nThe Oracle needs to store its knowledge and the history of the search somewhere. In our project setup ([Chapter 4](04_hyperparameter_tuning_setup.md)), this information is primarily stored in the `sentiment_transformer_tuning/oracle.json` file. This file acts as the Oracle's master logbook and state tracker.\n\nLet's look at some key parts of this file (we saw the full file in Chapter 4, here are the highlights):\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Simplified Snippet) ---\n```json\n{\n  \"hyperparameters\": { // Section about the settings being tuned\n    \"space\": [ // The allowed ranges (defined in Chapter 2)\n      // ... definitions for num_heads, dff, num_layers, etc. ...\n    ]\n  },\n  \"tried_so_far\": [ // A list representing unique recipes already tried\n    \"198210495baf2451b73e907a02b1546c\", // Hash representing one combo\n    \"c4be04584b5ae928a62b872b508505ad\", // Another unique combo\n    // ... more hashes ...\n    \"29c605d5e8a77f9296610df81f15c87c\"\n  ],\n  \"id_to_hash\": { // Maps trial IDs (like folder names) to the recipe hash\n    \"00\": \"29c605d5e8a77f9296610df81f15c87c\",\n    \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\",\n    // ... mapping for all trials (00 to 09) ...\n    \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"\n  },\n  \"seed\": 2999, // Starting point for random number generation\n  \"seed_state\": 3049, // Current state of random generation\n  // ... other details like trial start/end times ...\n}\n```\n\n**What this tells us about the Oracle:**\n\n*   **It knows the rules:** The `\"hyperparameters\"` section shows the Oracle has stored the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined \u2013 the allowed ranges for each setting.\n*   **It remembers the past:** The `\"tried_so_far\"` list contains unique identifiers (hashes) for each hyperparameter combination that has already been tested. This prevents the Oracle from trying the exact same recipe twice (unless intended). Like the chef crossing off recipes in the logbook.\n*   **It connects trials to recipes:** The `\"id_to_hash\"` dictionary links the trial number (like `00`, `01`) to the specific recipe (hash) used in that trial. This helps organize the results.\n*   **It manages randomness:** The `\"seed\"` and `\"seed_state\"` relate to how the Oracle uses random numbers (important for strategies like Random Search or breaking ties). Using seeds helps make experiments reproducible.\n\nThis `oracle.json` file essentially captures the Oracle's memory and current understanding of the search process.\n\n## How the Oracle Decides: The Strategy\n\nOkay, the Oracle remembers the past, but how does it decide what to try *next*? This depends on the **type** of Oracle being used, which corresponds to a specific search strategy.\n\nCommon strategies (types of Oracles) include:\n\n1.  **Random Search:** The Oracle simply picks the next hyperparameter combination randomly from the allowed space. It's like the chef randomly choosing ingredient amounts within the defined ranges for each cake. This is simple and surprisingly effective sometimes. The `oracle.json` we looked at likely came from a Random Search, as it explores different combinations without a clear pattern based only on the hashes shown.\n2.  **Bayesian Optimization:** A more advanced strategy. The Oracle builds a statistical model based on the results of previous trials. It uses this model to predict which *untried* combinations are most likely to yield good results. It tries to balance exploring new areas of the search space with exploiting areas that have already shown promise. Like the chef noticing that 3-egg cakes were good, and deciding to try 4 eggs next, based on that trend.\n3.  **Hyperband:** Another clever strategy that tries many combinations quickly with limited resources (e.g., training for fewer epochs initially) and then dedicates more resources to the most promising ones.\n\nThe choice of which Oracle (and thus strategy) to use is made when you set up the tuner in your Python code. The Oracle then implements that strategy, using the information in `oracle.json` to guide its decisions.\n\n## How it Works Under the Hood\n\nLet's visualize the Oracle's role in the tuning loop:\n\n1.  The main tuning process needs parameters for a new trial.\n2.  It asks the Oracle.\n3.  The Oracle consults its internal state (strategy, history from `oracle.json`, defined space).\n4.  It decides on a new set of hyperparameters.\n5.  It gives these back to the tuning process.\n6.  The trial runs with these parameters.\n7.  The tuning process reports the final score (e.g., accuracy) back to the Oracle.\n8.  The Oracle updates its state (`oracle.json`) with the results of this trial.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant Tuner as Tuning Process\n    participant Oracle as Keras Tuner Oracle (manages oracle.json)\n    participant Trial as Trial Execution\n\n    Tuner->>Oracle: Request hyperparameters for next trial\n    Oracle->>Oracle: Consult space, history, strategy\n    Oracle->>Oracle: Decide next hyperparameters (e.g., num_heads=4, lr=0.0005)\n    Oracle-->>Tuner: Provide chosen hyperparameters\n    Tuner->>Trial: Run trial with these parameters\n    Trial->>Trial: Build, Train, Evaluate Model\n    Trial-->>Tuner: Return performance score (e.g., 0.85 accuracy)\n    Tuner->>Oracle: Report score for the tried hyperparameters\n    Oracle->>Oracle: Update history & state (in oracle.json)\n```\n\n## Using the Oracle (Implicitly)\n\nAs a beginner, you usually don't interact with the Oracle object directly *while* the tuning is running. You interact with it primarily during the initial **setup**:\n\n```python\n# Conceptual Python code for setting up a Tuner with an Oracle\n\nimport keras_tuner as kt\nimport tensorflow as tf # Just for context\n\n# Assume build_model is a function that defines your model\n# and uses hp.Int, hp.Float etc. to define the search space\n# (as shown in Chapter 2)\n# def build_model(hp):\n#   ... define model using hp ...\n#   return model\n\n# 1. CHOOSE the Oracle/Strategy: Here we choose RandomSearch\ntuner = kt.RandomSearch(\n    hypermodel=build_model,           # Function that builds the model\n    objective=\"val_accuracy\",        # What score to maximize\n    max_trials=10,                   # How many different combinations to try\n    executions_per_trial=1,          # How many times to train each combination\n    directory=\"sentiment_transformer_tuning\", # Where to store results (incl. oracle.json)\n    project_name=\"my_first_tuning\"   # Subfolder within the directory\n)\n\n# 2. START the search (The Tuner uses the Oracle internally)\n# tuner.search(train_data, epochs=5, validation_data=val_data)\n# ... The Oracle manages things behind the scenes during search ...\n\nprint(\"Tuning setup complete. The Oracle will manage the search.\")\n# When tuner.search() runs, the RandomSearch Oracle inside the 'tuner'\n# object will be consulted repeatedly.\n```\n\n**Explanation:**\n\n1.  We choose `kt.RandomSearch`. This creates a Tuner that *contains* a `RandomSearchOracle`. If we wanted Bayesian Optimization, we'd use `kt.BayesianOptimization(...)`.\n2.  We configure the Tuner, telling it which function defines our model and search space (`build_model`), what metric to optimize (`val_accuracy`), how many trials to run (`max_trials`), and where to save the results (`directory`).\n3.  When we would call `tuner.search(...)` (commented out here), the Tuner object takes over. It repeatedly asks its internal Oracle for hyperparameters, runs the trial, and reports back the score, all automatically. The `oracle.json` file is created and updated within the specified `directory`.\n\nSo, you choose the *type* of Oracle by choosing the type of Tuner, and the framework handles the rest!\n\n## Conclusion\n\nYou've now met the **Keras Tuner Oracle**, the mastermind behind hyperparameter tuning!\n\n*   It's the **central component managing the search**, like the head chef directing recipe experiments.\n*   It **knows the hyperparameter space**, **tracks tried combinations** and their **results**, and **decides which parameters to try next**.\n*   Its state and history are stored in the **`oracle.json`** file.\n*   The **strategy** it uses (e.g., Random Search, Bayesian Optimization) depends on the type of Oracle/Tuner you choose during setup.\n*   It works behind the scenes during the `tuner.search()` process to guide the experiments intelligently.\n\nThe Oracle manages the overall search, but what information is stored for each *individual* experiment or \"trial\" that it launches?\n\nLet's dive into the details of each specific experiment in the next chapter: [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\n        Relevant Code Snippets:\n        --- File: sentiment_transformer_tuning/oracle.json ---\n{\"ongoing_trials\": {}, \"hyperparameters\": {\"space\": [{\"class_name\": \"Int\", \"config\": {\"name\": \"num_heads\", \"default\": null, \"conditions\": [], \"min_value\": 2, \"max_value\": 8, \"step\": 2, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"dff\", \"default\": null, \"conditions\": [], \"min_value\": 64, \"max_value\": 512, \"step\": 64, \"sampling\": \"linear\"}}, {\"class_name\": \"Int\", \"config\": {\"name\": \"num_layers\", \"default\": null, \"conditions\": [], \"min_value\": 1, \"max_value\": 4, \"step\": 1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"dropout_rate\", \"default\": 0.1, \"conditions\": [], \"min_value\": 0.1, \"max_value\": 0.5, \"step\": 0.1, \"sampling\": \"linear\"}}, {\"class_name\": \"Float\", \"config\": {\"name\": \"learning_rate\", \"default\": 1e-05, \"conditions\": [], \"min_value\": 1e-05, \"max_value\": 0.001, \"step\": null, \"sampling\": \"log\"}}], \"values\": {\"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05}}, \"start_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"end_order\": [\"00\", \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\"], \"run_times\": {\"00\": 1, \"01\": 1, \"02\": 1, \"03\": 1, \"04\": 1, \"05\": 1, \"06\": 1, \"07\": 1, \"08\": 1, \"09\": 1}, \"retry_queue\": [], \"seed\": 2999, \"seed_state\": 3049, \"tried_so_far\": [\"198210495baf2451b73e907a02b1546c\", \"c4be04584b5ae928a62b872b508505ad\", \"50113039fae8d6eed700e33ff1894756\", \"eedc9f1a30fcf368f6d691d2a4924552\", \"92fbfb13544f9424e707291e4b46ff92\", \"69e1b9791653b9499f6986d32cdc9cb0\", \"43c3fb8b908324dd46dcf557f1887c11\", \"7b84b94df32d8ccdb160bca166ab90b9\", \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"29c605d5e8a77f9296610df81f15c87c\"], \"id_to_hash\": {\"00\": \"29c605d5e8a77f9296610df81f15c87c\", \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\", \"02\": \"50113039fae8d6eed700e33ff1894756\", \"03\": \"198210495baf2451b73e907a02b1546c\", \"04\": \"92fbfb13544f9424e707291e4b46ff92\", \"05\": \"43c3fb8b908324dd46dcf557f1887c11\", \"06\": \"eedc9f1a30fcf368f6d691d2a4924552\", \"07\": \"7b84b94df32d8ccdb160bca166ab90b9\", \"08\": \"c4be04584b5ae928a62b872b508505ad\", \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"}, \"display\": {\"search_start\": \"2024-10-28T15:46:17.980894\", \"trial_start\": {\"00\": \"2024-10-28T15:46:17.980894\", \"01\": \"2024-10-28T16:51:12.209608\", \"02\": \"2024-10-28T17:56:08.716802\", \"03\": \"2024-10-28T19:04:18.575989\", \"04\": \"2024-10-28T20:07:40.362451\", \"05\": \"2024-10-28T21:45:06.150608\", \"06\": \"2024-10-28T22:49:03.029809\", \"07\": \"2024-10-28T23:54:05.650454\", \"08\": \"2024-10-29T00:26:59.364512\", \"09\": \"2024-10-29T01:39:46.488413\"}, \"trial_number\": {\"00\": 1, \"01\": 2, \"02\": 3, \"03\": 4, \"04\": 5, \"05\": 6, \"06\": 7, \"07\": 8, \"08\": 9, \"09\": 10}}}\n\n--- File: sentiment_transformer_tuning/trial_00/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_01/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_02/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_03/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_04/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_05/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_06/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_07/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_08/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n--- File: sentiment_transformer_tuning/trial_09/build_config.json ---\n{\"input_shape\": [null, 62]}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 6: Tuning Trial Configuration & State`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 6: Tuning Trial Configuration & State\n\nHello again! In the [previous chapter](05_keras_tuner_oracle.md), we met the **Keras Tuner Oracle**, the \"Head Chef\" or \"director\" that manages our hyperparameter tuning experiments. The Oracle decides *which* combinations of settings (hyperparameters) to try next and keeps track of the overall progress in its master logbook (`oracle.json`).\n\nBut what about the detailed records for each *individual* experiment? When our Head Chef decides to try baking a specific cake (say, Cake #5 with 3 eggs, 200g sugar, baked for 35 minutes), we need a dedicated place to write down *that specific recipe* and *exactly how that cake turned out*. Where do we store the notes for each single attempt?\n\nThat's where the concept of **Tuning Trial Configuration & State** comes in!\n\n## The Problem: Keeping Track of Each Experiment's Details\n\nThe [Keras Tuner Oracle](05_keras_tuner_oracle.md) gives us the big picture \u2013 which recipes have been tried (`tried_so_far` in `oracle.json`), the overall search space, and maybe the best score found so far. But if we want to know the *exact* settings used for Trial #3, or the specific accuracy achieved by Trial #7, we need to look at the records for those individual trials.\n\nWe need a system to store:\n*   The **configuration** used for each trial (the specific hyperparameter values).\n*   The **state** of each trial (Did it start? Did it finish? What was the result?).\n\n## What is a \"Trial\" in Hyperparameter Tuning?\n\nA **Trial** represents **one single experiment run** within the larger hyperparameter tuning process.\n\nThink back to our baking analogy:\n*   **Hyperparameter Tuning Process:** The entire project of baking dozens of cakes to find the best recipe.\n*   **Trial:** Baking *one specific cake* using *one specific recipe variation* chosen by the Head Chef (the Oracle).\n\nEach trial involves:\n1.  Getting a specific set of hyperparameter values from the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n2.  Building a machine learning model using *exactly* those hyperparameters.\n3.  Training the model on our data.\n4.  Evaluating its performance (e.g., measuring its accuracy).\n5.  Saving the details: which hyperparameters were used, and what the final score was.\n\n## The Trial's Notebook: `trial_XX/` Folders\n\nRemember the `sentiment_transformer_tuning/` folder we saw in [Chapter 4: Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)? Inside that main folder, we find several subfolders named `trial_00/`, `trial_01/`, `trial_02/`, and so on.\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 oracle.json                  # The Oracle's master logbook\n\u251c\u2500\u2500 ... (other top-level files)\n\u251c\u2500\u2500 trial_00/                    # <--- Notebook page for Trial 0\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_01/                    # <--- Notebook page for Trial 1\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n...\n\u2514\u2500\u2500 trial_09/                    # <--- Notebook page for Trial 9\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\nEach `trial_XX` folder is like a **dedicated page or section in our baking logbook** for one specific cake attempt. It contains all the important notes and results for that particular trial.\n\n## Inside the Trial Notebook: Key Files\n\nLet's open one of these trial folders, say `trial_05/`, and look at the most important files inside.\n\n### 1. `trial.json`: The Recipe Card and Result\n\nThis is the most crucial file within a trial folder. It stores the core information about this specific experiment: the exact configuration used and the outcome.\n\n--- File: `sentiment_transformer_tuning/trial_05/trial.json` (Conceptual Example Structure) ---\n```json\n{\n  \"trial_id\": \"05\", // Unique ID for this trial\n  \"hyperparameters\": {\n    \"space\": [ // Repeats the search space definition\n      // ... space definition ...\n    ],\n    \"values\": { // <-- THE IMPORTANT PART! Specific values used in THIS trial\n      \"num_heads\": 6,\n      \"dff\": 256,\n      \"num_layers\": 3,\n      \"dropout_rate\": 0.3,\n      \"learning_rate\": 0.00015 // Example values chosen by the Oracle for trial 05\n    }\n  },\n  \"score\": 0.885, // The performance metric (e.g., validation accuracy) achieved\n  \"best_step\": 4, // Which training step/epoch gave the best score\n  \"status\": \"COMPLETED\" // State of the trial (e.g., RUNNING, COMPLETED, FAILED)\n}\n```\n\n**Explanation:**\n\n*   `\"trial_id\"`: Matches the folder name (\"05\").\n*   `\"hyperparameters\"`:\n    *   `\"space\"`: Often repeats the overall search space definition (for completeness).\n    *   `\"values\"`: This is key! It shows the **exact hyperparameter values** that the [Keras Tuner Oracle](05_keras_tuner_oracle.md) selected *for this specific trial*. Think of this as the precise recipe used for Cake #5.\n*   `\"score\"`: The final evaluation score obtained by the model trained in this trial. This is the \"taste test\" result for Cake #5.\n*   `\"status\"`: Tells us if the trial finished successfully (`COMPLETED`), is still running (`RUNNING`), or encountered an error (`FAILED`).\n\nThis `trial.json` file gives you a complete snapshot of what happened in one specific experiment.\n\n### 2. `build_config.json`: Confirming the Setup\n\nThis file often stores configuration details that were fixed when the model for this trial was *built*, rather than settings that were tuned. A common example we've already seen is the [Model Input Shape](01_model_input_shape.md).\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n**Explanation:**\n\n*   This file confirms that for Trial #05, the model was built expecting the standard input shape `[null, 62]`, as discussed in [Chapter 1](01_model_input_shape.md).\n*   You'll notice this file is often identical across many `trial_XX` folders (like the snippets provided show for trials 00 through 09). This indicates that the input shape was a constant part of the setup for all these trials, not something being tuned.\n*   Analogy: This file is like a note confirming we used the standard 9x13 inch baking pan for Cake #5, just like we did for Cakes #0 through #9.\n\n### Other Files\n\nYou might sometimes find other files inside a `trial_XX` folder, such as:\n*   `checkpoints/`: Saved versions (weights) of the model trained during this trial.\n*   `logs/`: Detailed logs from the training process.\n\nFor understanding the **configuration and state**, `trial.json` and `build_config.json` are usually the most important.\n\n## Connecting the Oracle and the Trials\n\nHow do the Oracle's master logbook (`oracle.json`) and the individual trial notebooks (`trial_XX/trial.json`) relate to each other?\n\n1.  **Oracle Decides:** The [Keras Tuner Oracle](05_keras_tuner_oracle.md) picks a set of hyperparameters (a specific recipe) based on its strategy and history (`oracle.json`).\n2.  **Trial Starts:** A new trial folder (e.g., `trial_10/`) is created. The chosen hyperparameters are recorded in `trial_10/trial.json` with status `RUNNING`. The Oracle also notes the start time in `oracle.json`.\n3.  **Trial Runs:** The model is built (using fixed settings like `input_shape` recorded in `build_config.json`) and trained using the specific hyperparameters from `trial_10/trial.json`.\n4.  **Trial Ends:** The performance score is measured. This score, along with the final status (`COMPLETED`), is saved in `trial_10/trial.json`.\n5.  **Report Back:** The score is reported back to the Oracle, which updates its master logbook (`oracle.json`) to include this result, helping it decide future trials.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution Process\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    Oracle->>TrialRunner: Start Trial XX with specific Hyperparameters (HP_Set_XX)\n    TrialRunner->>TrialFolder: Create folder `trial_XX/`\n    TrialRunner->>TrialFolder: Write HP_Set_XX to `trial.json` (Status: RUNNING)\n    TrialRunner->>TrialFolder: Write build info (e.g., input shape) to `build_config.json`\n    Note right of TrialRunner: Build & Train Model using HP_Set_XX\n    TrialRunner->>TrialRunner: Evaluate Model, get Score_XX\n    TrialRunner->>TrialFolder: Update `trial.json` with Score_XX and Status: COMPLETED\n    TrialRunner-->>Oracle: Report Score_XX for Trial XX\n    Oracle->>Oracle: Update internal state (oracle.json) with Trial XX result\n\n```\n\n## How Are Trial Folders Created?\n\nYou don't need to create these `trial_XX` folders or the JSON files inside them manually! The Keras Tuner framework does this automatically for you when you run the tuning process (typically initiated by a `tuner.search(...)` command in your Python script).\n\nFor each trial the Oracle decides to run, the tuner framework handles:\n*   Creating the `trial_XX` directory.\n*   Populating `trial.json` with the chosen hyperparameters and initial status.\n*   Running your model-building function (which might save `build_config.json`).\n*   Executing the training and evaluation.\n*   Updating `trial.json` with the final score and status.\n\nIt's all part of the automated **Hyperparameter Tuning Setup** we discussed in [Chapter 4](04_hyperparameter_tuning_setup.md).\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual experiments within hyperparameter tuning: the **Trials**.\n\n*   A **Trial** is a single run testing one specific combination of hyperparameters chosen by the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   It's like baking **one specific cake** with **one specific recipe variation**.\n*   The details of each trial \u2013 its **configuration** (hyperparameters used) and **state** (status, score) \u2013 are stored in dedicated **`trial_XX` folders**.\n*   The key file inside is **`trial.json`**, acting as the \"recipe card and result note\" for that trial.\n*   **`build_config.json`** often stores fixed build-time settings like the [Model Input Shape](01_model_input_shape.md).\n*   These trial-specific records complement the Oracle's overall view and allow us to analyze each experiment individually.\n\nWe've now covered the core concepts of our tuning process: the data shape, the search space, constants, the overall setup, the Oracle director, and the individual trial records. Before you can run this project yourself, there's one last piece: knowing what software tools and libraries you need installed.\n\nLet's move on to the final chapter: [Project Dependencies](07_project_dependencies.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `SC4002-Group-Assignment` about the concept: \"Project Dependencies\". This is Chapter 7.\n\n        Concept Details:\n        - Description:\n        Lists all the external software libraries and their specific versions required for the project to run correctly. This ensures reproducibility and avoids conflicts between library versions.\nThink of this as the list of essential tools and appliances (like Python, TensorFlow, Keras Tuner, NLTK, etc.) needed in your kitchen. Without the correct tools listed in `requirements.txt`, you can't execute the code (bake the cake).\n\n\n        Complete Tutorial Structure:\n        1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n---\n# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).\n---\n# Chapter 4: Hyperparameter Tuning Setup\n\nHi there! In the [previous chapter](03_training_constants.md), we learned about **Training Constants** \u2013 the fixed settings like `BATCH_SIZE` that act as our standard tools. We now have:\n\n1.  The required data format: [Model Input Shape](01_model_input_shape.md) (`[null, 62]`).\n2.  The range of experimental settings: [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) (e.g., how many layers, learning rate).\n3.  The standard, fixed settings: [Training Constants](03_training_constants.md) (e.g., batch size).\n\nNow, how do we actually put all this together and *start the experiments* to find the best combination of hyperparameters? We need a system, a setup, to manage this process automatically. That's what this chapter is about!\n\n## The Problem: Running Many Experiments Systematically\n\nImagine you're determined to bake the absolute best chocolate cake. You have a basic recipe, but you know you can improve it by tweaking things like the amount of sugar, the number of eggs, or the baking time (these are like our hyperparameters).\n\nTrying just one variation isn't enough. You need to:\n*   Try *many* different combinations (recipes).\n*   Keep track of exactly what you did for each cake (which recipe variation).\n*   Record how well each cake turned out (taste test!).\n*   Compare the results to find the winning recipe.\n\nDoing this manually would be slow and messy. You need an organized *setup* for your baking experiments \u2013 maybe a dedicated corner of the kitchen, a notebook for results, and a clear plan.\n\nSimilarly, finding the best settings for our sentiment analysis model requires running many training experiments with different hyperparameter combinations. We need an organized way to manage these experiments, track their configurations, and store their results. This organized system is our **Hyperparameter Tuning Setup**.\n\n## What is the Hyperparameter Tuning Setup?\n\nThe **Hyperparameter Tuning Setup** is the overall **process and infrastructure** we use to automatically:\n\n1.  Define the experiments we want to run (using the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)).\n2.  Execute each experiment (called a \"trial\"). Each trial builds and trains a model with a specific set of hyperparameters.\n3.  Keep track of the hyperparameters used in each trial.\n4.  Record the performance (e.g., accuracy) of the model from each trial.\n5.  Organize all this information so we can later identify the best hyperparameters found.\n\nThink of it as setting up your **experimental kitchen** for finding the best cake recipe:\n\n*   **The Kitchen Space:** A dedicated place where all the experiments happen. In our project, this is the `sentiment_transformer_tuning` folder.\n*   **The Master Recipe Book:** A central place defining the range of ingredients to try ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)) and tracking the overall progress ([Keras Tuner Oracle](05_keras_tuner_oracle.md)).\n*   **Individual Recipe Cards & Notes:** For each specific cake baked (each trial), a record of the exact recipe used and how it turned out ([Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)).\n\n## Our Experimental Kitchen: The `sentiment_transformer_tuning` Folder\n\nIn our project, the main place where all the tuning experiments happen and their results are stored is a folder named `sentiment_transformer_tuning`.\n\nLet's peek at its structure and purpose:\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 README.md                    # A note about this folder\n\u251c\u2500\u2500 oracle.json                  # The 'master plan' / tuner's brain state\n\u251c\u2500\u2500 tuner0.json                  # Tuner configuration status (often empty or minimal)\n\u251c\u2500\u2500 trial_00/                    # Results & config for the 1st experiment\n\u2502   \u251c\u2500\u2500 build_config.json        # Input shape used for this trial\n\u2502   \u251c\u2500\u2500 trial.json               # Details of this specific trial (hyperparameters, score)\n\u2502   \u2514\u2500\u2500 ... (other files like model checkpoints)\n\u251c\u2500\u2500 trial_01/                    # Results & config for the 2nd experiment\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_02/\n...\n\u2514\u2500\u2500 trial_09/                    # Results & config for the 10th experiment\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\n**Explanation:**\n\n*   **`sentiment_transformer_tuning/`**: This is the main directory for our tuning process. Think of it as the logbook for our recipe experiments. The `README.md` inside just gives a brief description.\n    --- File: `sentiment_transformer_tuning/README.md` ---\n    ```\n    This folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n    ```\n*   **`oracle.json`**: This is a crucial file managed by the tuning tool (Keras Tuner). It stores the overall state of the tuning process, including the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined, which trials have been run, and maybe the best results found so far. We'll dive deeper into this in the next chapter: [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   **`tuner0.json`**: Stores some internal state for the tuner itself. For beginners, it's less important than `oracle.json`.\n*   **`trial_XX/` folders**: Each folder represents *one complete experiment* (one \"trial\") testing a specific combination of hyperparameters.\n    *   Inside each `trial_XX` folder, files like `trial.json` store the specific hyperparameter values used for *that trial* and the resulting performance score. Other files might store the trained model weights or configuration details like the input shape (`build_config.json`). We'll explore these in more detail in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\nThis folder structure is automatically created and managed by the hyperparameter tuning tool (like Keras Tuner) when we run the tuning script. It provides a clean and organized way to store the results of potentially hundreds of experiments.\n\n## How the Tuning Setup Works (The Big Picture)\n\nImagine you start the automatic tuning process (usually by running a Python script, which we won't detail here). Here\u2019s a simplified step-by-step flow:\n\n1.  **Start:** You initiate the tuning process.\n2.  **Ask Oracle:** The system asks the [Keras Tuner Oracle](05_keras_tuner_oracle.md) (using the info in `oracle.json`) for the next set of hyperparameters to try, based on the defined [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) and past results.\n3.  **Run Trial:** A new trial (e.g., `trial_10`) is started:\n    *   A model is built using the specific hyperparameters suggested by the Oracle.\n    *   The model is trained using the data, respecting the [Model Input Shape](01_model_input_shape.md) and using [Training Constants](03_training_constants.md) where needed.\n    *   The model's performance is evaluated (e.g., accuracy is measured).\n4.  **Save Results:** The hyperparameters used, the final performance score, and other relevant information for this trial are saved inside the corresponding `trial_XX` folder (e.g., in `sentiment_transformer_tuning/trial_10/trial.json`).\n5.  **Update Oracle:** The performance result is reported back to the Oracle, which updates its state in `oracle.json`. This helps it make better decisions about which hyperparameters to try next.\n6.  **Repeat:** Steps 2-5 are repeated for a predefined number of trials or until a certain condition is met.\n7.  **Finish:** Once done, you can examine the `oracle.json` file or the `trial_XX` folders (especially their `trial.json` files) to find the trial that achieved the best performance and the hyperparameters that led to it.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant TuningScript as Tuning Process\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    User->>TuningScript: Start Hyperparameter Tuning\n    loop For each trial\n        TuningScript->>Oracle: Get next Hyperparameters to test\n        Oracle-->>TuningScript: Provide Hyperparameter set\n        TuningScript->>TrialRunner: Run trial with these Hyperparameters\n        TrialRunner->>TrialRunner: Build Model\n        TrialRunner->>TrialRunner: Train Model\n        TrialRunner->>TrialRunner: Evaluate Performance (e.g., Accuracy)\n        TrialRunner->>TrialFolder: Save Trial Config & Results (trial.json)\n        TrialRunner-->>TuningScript: Return Performance Score\n        TuningScript->>Oracle: Report Score for these Hyperparameters\n        Oracle->>Oracle: Update internal state (oracle.json)\n    end\n    TuningScript-->>User: Tuning Finished (Results in folder)\n\n```\n\nThis whole setup allows us to systematically explore different model configurations without manual intervention, saving time and effort while increasing the chances of finding a high-performing model.\n\n## Conclusion\n\nYou've now learned about the **Hyperparameter Tuning Setup**:\n\n*   It's the **overall process and infrastructure** for automatically finding the best model settings (hyperparameters).\n*   It involves running multiple **trials** (experiments), each testing a different hyperparameter combination.\n*   It's like setting up an **experimental kitchen** to systematically test many cake recipes.\n*   In our project, the results and state are primarily stored in the **`sentiment_transformer_tuning` folder**.\n*   This folder contains the **`oracle.json`** file (the master plan) and individual **`trial_XX` folders** (results of each specific experiment).\n*   This setup automates the search, organizes results, and helps us find the best-performing model configuration.\n\nNow that we understand the overall structure, let's zoom in on the \"brain\" of this operation \u2013 the component that decides which hyperparameters to try next.\n\nReady to meet the mastermind? Let's explore the [Keras Tuner Oracle](05_keras_tuner_oracle.md) in the next chapter!\n---\n# Chapter 5: Keras Tuner Oracle\n\nWelcome back! In the [previous chapter](04_hyperparameter_tuning_setup.md), we learned about the overall **Hyperparameter Tuning Setup** \u2013 our organized \"experimental kitchen\" (`sentiment_transformer_tuning/` folder) for automatically running many model training experiments (trials) to find the best settings. We saw that this setup involves asking for hyperparameter combinations, running trials, and recording results.\n\nBut who decides *which* combination of hyperparameters to try next? And who keeps track of all the experiments and their outcomes? That's the job of the **Keras Tuner Oracle**!\n\n## The Problem: Who's Directing the Experiments?\n\nImagine you're in your experimental kitchen, ready to bake dozens of cakes to find the perfect recipe. You've defined the possible ranges for your ingredients ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)): 2-8 eggs, 100g-500g sugar, etc.\n\nYou *could* just randomly pick values from these ranges for each cake. But maybe there's a smarter way? Perhaps after baking a few cakes, you notice that cakes with more sugar tend to taste better. Shouldn't your next experiments focus more on the higher end of the sugar range?\n\nYou need a **director** or a **head chef** for your experiments. Someone (or something!) that:\n\n1.  Knows the allowed ingredient ranges (the hyperparameter space).\n2.  Decides the *exact* recipe (hyperparameter combination) for the *next* cake (trial).\n3.  Keeps a detailed logbook of every recipe tried and how well the cake turned out (the performance score).\n4.  Uses this information (and possibly a clever strategy) to guide future experiments towards better recipes.\n\nThis \"director\" role in Keras Tuner is filled by the **Oracle**.\n\n## What is the Keras Tuner Oracle?\n\nThe **Keras Tuner Oracle** is the central component that **manages the hyperparameter search process**. It's the \"brain\" behind the tuning operation within the Keras Tuner library.\n\nThink of it as the **Head Chef** in our experimental kitchen:\n\n*   **Knows the Recipe Boundaries:** It holds the definition of the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n*   **Keeps the Master Logbook:** It tracks which hyperparameter combinations have been tried (`tried_so_far`), the results (scores) of those trials, and the overall state of the search.\n*   **Plans the Next Experiment:** Based on a defined strategy (like random search, or more advanced methods), it decides which specific combination of hyperparameters should be tested in the next trial.\n\nThe Oracle ensures that the search is conducted systematically and efficiently, guiding the process towards finding the best possible hyperparameters within the defined space.\n\n## The Oracle's Logbook: Peeking Inside `oracle.json`\n\nThe Oracle needs to store its knowledge and the history of the search somewhere. In our project setup ([Chapter 4](04_hyperparameter_tuning_setup.md)), this information is primarily stored in the `sentiment_transformer_tuning/oracle.json` file. This file acts as the Oracle's master logbook and state tracker.\n\nLet's look at some key parts of this file (we saw the full file in Chapter 4, here are the highlights):\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Simplified Snippet) ---\n```json\n{\n  \"hyperparameters\": { // Section about the settings being tuned\n    \"space\": [ // The allowed ranges (defined in Chapter 2)\n      // ... definitions for num_heads, dff, num_layers, etc. ...\n    ]\n  },\n  \"tried_so_far\": [ // A list representing unique recipes already tried\n    \"198210495baf2451b73e907a02b1546c\", // Hash representing one combo\n    \"c4be04584b5ae928a62b872b508505ad\", // Another unique combo\n    // ... more hashes ...\n    \"29c605d5e8a77f9296610df81f15c87c\"\n  ],\n  \"id_to_hash\": { // Maps trial IDs (like folder names) to the recipe hash\n    \"00\": \"29c605d5e8a77f9296610df81f15c87c\",\n    \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\",\n    // ... mapping for all trials (00 to 09) ...\n    \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"\n  },\n  \"seed\": 2999, // Starting point for random number generation\n  \"seed_state\": 3049, // Current state of random generation\n  // ... other details like trial start/end times ...\n}\n```\n\n**What this tells us about the Oracle:**\n\n*   **It knows the rules:** The `\"hyperparameters\"` section shows the Oracle has stored the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined \u2013 the allowed ranges for each setting.\n*   **It remembers the past:** The `\"tried_so_far\"` list contains unique identifiers (hashes) for each hyperparameter combination that has already been tested. This prevents the Oracle from trying the exact same recipe twice (unless intended). Like the chef crossing off recipes in the logbook.\n*   **It connects trials to recipes:** The `\"id_to_hash\"` dictionary links the trial number (like `00`, `01`) to the specific recipe (hash) used in that trial. This helps organize the results.\n*   **It manages randomness:** The `\"seed\"` and `\"seed_state\"` relate to how the Oracle uses random numbers (important for strategies like Random Search or breaking ties). Using seeds helps make experiments reproducible.\n\nThis `oracle.json` file essentially captures the Oracle's memory and current understanding of the search process.\n\n## How the Oracle Decides: The Strategy\n\nOkay, the Oracle remembers the past, but how does it decide what to try *next*? This depends on the **type** of Oracle being used, which corresponds to a specific search strategy.\n\nCommon strategies (types of Oracles) include:\n\n1.  **Random Search:** The Oracle simply picks the next hyperparameter combination randomly from the allowed space. It's like the chef randomly choosing ingredient amounts within the defined ranges for each cake. This is simple and surprisingly effective sometimes. The `oracle.json` we looked at likely came from a Random Search, as it explores different combinations without a clear pattern based only on the hashes shown.\n2.  **Bayesian Optimization:** A more advanced strategy. The Oracle builds a statistical model based on the results of previous trials. It uses this model to predict which *untried* combinations are most likely to yield good results. It tries to balance exploring new areas of the search space with exploiting areas that have already shown promise. Like the chef noticing that 3-egg cakes were good, and deciding to try 4 eggs next, based on that trend.\n3.  **Hyperband:** Another clever strategy that tries many combinations quickly with limited resources (e.g., training for fewer epochs initially) and then dedicates more resources to the most promising ones.\n\nThe choice of which Oracle (and thus strategy) to use is made when you set up the tuner in your Python code. The Oracle then implements that strategy, using the information in `oracle.json` to guide its decisions.\n\n## How it Works Under the Hood\n\nLet's visualize the Oracle's role in the tuning loop:\n\n1.  The main tuning process needs parameters for a new trial.\n2.  It asks the Oracle.\n3.  The Oracle consults its internal state (strategy, history from `oracle.json`, defined space).\n4.  It decides on a new set of hyperparameters.\n5.  It gives these back to the tuning process.\n6.  The trial runs with these parameters.\n7.  The tuning process reports the final score (e.g., accuracy) back to the Oracle.\n8.  The Oracle updates its state (`oracle.json`) with the results of this trial.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant Tuner as Tuning Process\n    participant Oracle as Keras Tuner Oracle (manages oracle.json)\n    participant Trial as Trial Execution\n\n    Tuner->>Oracle: Request hyperparameters for next trial\n    Oracle->>Oracle: Consult space, history, strategy\n    Oracle->>Oracle: Decide next hyperparameters (e.g., num_heads=4, lr=0.0005)\n    Oracle-->>Tuner: Provide chosen hyperparameters\n    Tuner->>Trial: Run trial with these parameters\n    Trial->>Trial: Build, Train, Evaluate Model\n    Trial-->>Tuner: Return performance score (e.g., 0.85 accuracy)\n    Tuner->>Oracle: Report score for the tried hyperparameters\n    Oracle->>Oracle: Update history & state (in oracle.json)\n```\n\n## Using the Oracle (Implicitly)\n\nAs a beginner, you usually don't interact with the Oracle object directly *while* the tuning is running. You interact with it primarily during the initial **setup**:\n\n```python\n# Conceptual Python code for setting up a Tuner with an Oracle\n\nimport keras_tuner as kt\nimport tensorflow as tf # Just for context\n\n# Assume build_model is a function that defines your model\n# and uses hp.Int, hp.Float etc. to define the search space\n# (as shown in Chapter 2)\n# def build_model(hp):\n#   ... define model using hp ...\n#   return model\n\n# 1. CHOOSE the Oracle/Strategy: Here we choose RandomSearch\ntuner = kt.RandomSearch(\n    hypermodel=build_model,           # Function that builds the model\n    objective=\"val_accuracy\",        # What score to maximize\n    max_trials=10,                   # How many different combinations to try\n    executions_per_trial=1,          # How many times to train each combination\n    directory=\"sentiment_transformer_tuning\", # Where to store results (incl. oracle.json)\n    project_name=\"my_first_tuning\"   # Subfolder within the directory\n)\n\n# 2. START the search (The Tuner uses the Oracle internally)\n# tuner.search(train_data, epochs=5, validation_data=val_data)\n# ... The Oracle manages things behind the scenes during search ...\n\nprint(\"Tuning setup complete. The Oracle will manage the search.\")\n# When tuner.search() runs, the RandomSearch Oracle inside the 'tuner'\n# object will be consulted repeatedly.\n```\n\n**Explanation:**\n\n1.  We choose `kt.RandomSearch`. This creates a Tuner that *contains* a `RandomSearchOracle`. If we wanted Bayesian Optimization, we'd use `kt.BayesianOptimization(...)`.\n2.  We configure the Tuner, telling it which function defines our model and search space (`build_model`), what metric to optimize (`val_accuracy`), how many trials to run (`max_trials`), and where to save the results (`directory`).\n3.  When we would call `tuner.search(...)` (commented out here), the Tuner object takes over. It repeatedly asks its internal Oracle for hyperparameters, runs the trial, and reports back the score, all automatically. The `oracle.json` file is created and updated within the specified `directory`.\n\nSo, you choose the *type* of Oracle by choosing the type of Tuner, and the framework handles the rest!\n\n## Conclusion\n\nYou've now met the **Keras Tuner Oracle**, the mastermind behind hyperparameter tuning!\n\n*   It's the **central component managing the search**, like the head chef directing recipe experiments.\n*   It **knows the hyperparameter space**, **tracks tried combinations** and their **results**, and **decides which parameters to try next**.\n*   Its state and history are stored in the **`oracle.json`** file.\n*   The **strategy** it uses (e.g., Random Search, Bayesian Optimization) depends on the type of Oracle/Tuner you choose during setup.\n*   It works behind the scenes during the `tuner.search()` process to guide the experiments intelligently.\n\nThe Oracle manages the overall search, but what information is stored for each *individual* experiment or \"trial\" that it launches?\n\nLet's dive into the details of each specific experiment in the next chapter: [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n---\n# Chapter 6: Tuning Trial Configuration & State\n\nHello again! In the [previous chapter](05_keras_tuner_oracle.md), we met the **Keras Tuner Oracle**, the \"Head Chef\" or \"director\" that manages our hyperparameter tuning experiments. The Oracle decides *which* combinations of settings (hyperparameters) to try next and keeps track of the overall progress in its master logbook (`oracle.json`).\n\nBut what about the detailed records for each *individual* experiment? When our Head Chef decides to try baking a specific cake (say, Cake #5 with 3 eggs, 200g sugar, baked for 35 minutes), we need a dedicated place to write down *that specific recipe* and *exactly how that cake turned out*. Where do we store the notes for each single attempt?\n\nThat's where the concept of **Tuning Trial Configuration & State** comes in!\n\n## The Problem: Keeping Track of Each Experiment's Details\n\nThe [Keras Tuner Oracle](05_keras_tuner_oracle.md) gives us the big picture \u2013 which recipes have been tried (`tried_so_far` in `oracle.json`), the overall search space, and maybe the best score found so far. But if we want to know the *exact* settings used for Trial #3, or the specific accuracy achieved by Trial #7, we need to look at the records for those individual trials.\n\nWe need a system to store:\n*   The **configuration** used for each trial (the specific hyperparameter values).\n*   The **state** of each trial (Did it start? Did it finish? What was the result?).\n\n## What is a \"Trial\" in Hyperparameter Tuning?\n\nA **Trial** represents **one single experiment run** within the larger hyperparameter tuning process.\n\nThink back to our baking analogy:\n*   **Hyperparameter Tuning Process:** The entire project of baking dozens of cakes to find the best recipe.\n*   **Trial:** Baking *one specific cake* using *one specific recipe variation* chosen by the Head Chef (the Oracle).\n\nEach trial involves:\n1.  Getting a specific set of hyperparameter values from the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n2.  Building a machine learning model using *exactly* those hyperparameters.\n3.  Training the model on our data.\n4.  Evaluating its performance (e.g., measuring its accuracy).\n5.  Saving the details: which hyperparameters were used, and what the final score was.\n\n## The Trial's Notebook: `trial_XX/` Folders\n\nRemember the `sentiment_transformer_tuning/` folder we saw in [Chapter 4: Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)? Inside that main folder, we find several subfolders named `trial_00/`, `trial_01/`, `trial_02/`, and so on.\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 oracle.json                  # The Oracle's master logbook\n\u251c\u2500\u2500 ... (other top-level files)\n\u251c\u2500\u2500 trial_00/                    # <--- Notebook page for Trial 0\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_01/                    # <--- Notebook page for Trial 1\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n...\n\u2514\u2500\u2500 trial_09/                    # <--- Notebook page for Trial 9\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\nEach `trial_XX` folder is like a **dedicated page or section in our baking logbook** for one specific cake attempt. It contains all the important notes and results for that particular trial.\n\n## Inside the Trial Notebook: Key Files\n\nLet's open one of these trial folders, say `trial_05/`, and look at the most important files inside.\n\n### 1. `trial.json`: The Recipe Card and Result\n\nThis is the most crucial file within a trial folder. It stores the core information about this specific experiment: the exact configuration used and the outcome.\n\n--- File: `sentiment_transformer_tuning/trial_05/trial.json` (Conceptual Example Structure) ---\n```json\n{\n  \"trial_id\": \"05\", // Unique ID for this trial\n  \"hyperparameters\": {\n    \"space\": [ // Repeats the search space definition\n      // ... space definition ...\n    ],\n    \"values\": { // <-- THE IMPORTANT PART! Specific values used in THIS trial\n      \"num_heads\": 6,\n      \"dff\": 256,\n      \"num_layers\": 3,\n      \"dropout_rate\": 0.3,\n      \"learning_rate\": 0.00015 // Example values chosen by the Oracle for trial 05\n    }\n  },\n  \"score\": 0.885, // The performance metric (e.g., validation accuracy) achieved\n  \"best_step\": 4, // Which training step/epoch gave the best score\n  \"status\": \"COMPLETED\" // State of the trial (e.g., RUNNING, COMPLETED, FAILED)\n}\n```\n\n**Explanation:**\n\n*   `\"trial_id\"`: Matches the folder name (\"05\").\n*   `\"hyperparameters\"`:\n    *   `\"space\"`: Often repeats the overall search space definition (for completeness).\n    *   `\"values\"`: This is key! It shows the **exact hyperparameter values** that the [Keras Tuner Oracle](05_keras_tuner_oracle.md) selected *for this specific trial*. Think of this as the precise recipe used for Cake #5.\n*   `\"score\"`: The final evaluation score obtained by the model trained in this trial. This is the \"taste test\" result for Cake #5.\n*   `\"status\"`: Tells us if the trial finished successfully (`COMPLETED`), is still running (`RUNNING`), or encountered an error (`FAILED`).\n\nThis `trial.json` file gives you a complete snapshot of what happened in one specific experiment.\n\n### 2. `build_config.json`: Confirming the Setup\n\nThis file often stores configuration details that were fixed when the model for this trial was *built*, rather than settings that were tuned. A common example we've already seen is the [Model Input Shape](01_model_input_shape.md).\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n**Explanation:**\n\n*   This file confirms that for Trial #05, the model was built expecting the standard input shape `[null, 62]`, as discussed in [Chapter 1](01_model_input_shape.md).\n*   You'll notice this file is often identical across many `trial_XX` folders (like the snippets provided show for trials 00 through 09). This indicates that the input shape was a constant part of the setup for all these trials, not something being tuned.\n*   Analogy: This file is like a note confirming we used the standard 9x13 inch baking pan for Cake #5, just like we did for Cakes #0 through #9.\n\n### Other Files\n\nYou might sometimes find other files inside a `trial_XX` folder, such as:\n*   `checkpoints/`: Saved versions (weights) of the model trained during this trial.\n*   `logs/`: Detailed logs from the training process.\n\nFor understanding the **configuration and state**, `trial.json` and `build_config.json` are usually the most important.\n\n## Connecting the Oracle and the Trials\n\nHow do the Oracle's master logbook (`oracle.json`) and the individual trial notebooks (`trial_XX/trial.json`) relate to each other?\n\n1.  **Oracle Decides:** The [Keras Tuner Oracle](05_keras_tuner_oracle.md) picks a set of hyperparameters (a specific recipe) based on its strategy and history (`oracle.json`).\n2.  **Trial Starts:** A new trial folder (e.g., `trial_10/`) is created. The chosen hyperparameters are recorded in `trial_10/trial.json` with status `RUNNING`. The Oracle also notes the start time in `oracle.json`.\n3.  **Trial Runs:** The model is built (using fixed settings like `input_shape` recorded in `build_config.json`) and trained using the specific hyperparameters from `trial_10/trial.json`.\n4.  **Trial Ends:** The performance score is measured. This score, along with the final status (`COMPLETED`), is saved in `trial_10/trial.json`.\n5.  **Report Back:** The score is reported back to the Oracle, which updates its master logbook (`oracle.json`) to include this result, helping it decide future trials.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution Process\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    Oracle->>TrialRunner: Start Trial XX with specific Hyperparameters (HP_Set_XX)\n    TrialRunner->>TrialFolder: Create folder `trial_XX/`\n    TrialRunner->>TrialFolder: Write HP_Set_XX to `trial.json` (Status: RUNNING)\n    TrialRunner->>TrialFolder: Write build info (e.g., input shape) to `build_config.json`\n    Note right of TrialRunner: Build & Train Model using HP_Set_XX\n    TrialRunner->>TrialRunner: Evaluate Model, get Score_XX\n    TrialRunner->>TrialFolder: Update `trial.json` with Score_XX and Status: COMPLETED\n    TrialRunner-->>Oracle: Report Score_XX for Trial XX\n    Oracle->>Oracle: Update internal state (oracle.json) with Trial XX result\n\n```\n\n## How Are Trial Folders Created?\n\nYou don't need to create these `trial_XX` folders or the JSON files inside them manually! The Keras Tuner framework does this automatically for you when you run the tuning process (typically initiated by a `tuner.search(...)` command in your Python script).\n\nFor each trial the Oracle decides to run, the tuner framework handles:\n*   Creating the `trial_XX` directory.\n*   Populating `trial.json` with the chosen hyperparameters and initial status.\n*   Running your model-building function (which might save `build_config.json`).\n*   Executing the training and evaluation.\n*   Updating `trial.json` with the final score and status.\n\nIt's all part of the automated **Hyperparameter Tuning Setup** we discussed in [Chapter 4](04_hyperparameter_tuning_setup.md).\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual experiments within hyperparameter tuning: the **Trials**.\n\n*   A **Trial** is a single run testing one specific combination of hyperparameters chosen by the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   It's like baking **one specific cake** with **one specific recipe variation**.\n*   The details of each trial \u2013 its **configuration** (hyperparameters used) and **state** (status, score) \u2013 are stored in dedicated **`trial_XX` folders**.\n*   The key file inside is **`trial.json`**, acting as the \"recipe card and result note\" for that trial.\n*   **`build_config.json`** often stores fixed build-time settings like the [Model Input Shape](01_model_input_shape.md).\n*   These trial-specific records complement the Oracle's overall view and allow us to analyze each experiment individually.\n\nWe've now covered the core concepts of our tuning process: the data shape, the search space, constants, the overall setup, the Oracle director, and the individual trial records. Before you can run this project yourself, there's one last piece: knowing what software tools and libraries you need installed.\n\nLet's move on to the final chapter: [Project Dependencies](07_project_dependencies.md).\n\n        Relevant Code Snippets:\n        --- File: requirements.txt ---\nabsl-py==2.1.0\naiohappyeyeballs==2.4.3\naiohttp==3.10.10\naiosignal==1.3.1\nasttokens==2.4.1\nastunparse==1.6.3\nattrs==24.2.0\ncertifi==2024.8.30\ncharset-normalizer==3.4.0\nclick==8.1.7\ncolorama==0.4.6\ncomm==0.2.2\ncontourpy==1.3.0\ncycler==0.12.1\ndatasets==3.1.0\ndebugpy==1.8.7\ndecorator==5.1.1\ndill==0.3.8\nexecuting==2.1.0\nfilelock==3.16.1\nflatbuffers==24.3.25\nfonttools==4.54.1\nfrozenlist==1.5.0\nfsspec==2024.9.0\ngast==0.6.0\ngensim==4.3.3\ngoogle-pasta==0.2.0\ngrpcio==1.67.1\nh5py==3.12.1\nhuggingface-hub==0.26.2\nidna==3.10\nipykernel==6.29.5\nipython==8.29.0\njedi==0.19.1\njoblib==1.4.2\njupyter_client==8.6.3\njupyter_core==5.7.2\nkeras==3.6.0\nkeras-tuner==1.4.7\nkiwisolver==1.4.7\nkt-legacy==1.0.5\nlibclang==18.1.1\nMarkdown==3.7\nmarkdown-it-py==3.0.0\nMarkupSafe==3.0.2\nmatplotlib==3.9.2\nmatplotlib-inline==0.1.7\nmdurl==0.1.2\nml-dtypes==0.4.1\nmultidict==6.1.0\nmultiprocess==0.70.16\nnamex==0.0.8\nnest-asyncio==1.6.0\nnltk==3.9.1\nnumpy==1.26.4\nopt_einsum==3.4.0\noptree==0.13.0\npackaging==24.1\npandas==2.2.3\nparso==0.8.4\npillow==11.0.0\nplatformdirs==4.3.6\nprompt_toolkit==3.0.48\npropcache==0.2.0\nprotobuf==5.28.3\npsutil==6.1.0\npure_eval==0.2.3\npyarrow==18.0.0\nPygments==2.18.0\npyparsing==3.2.0\npython-dateutil==2.9.0.post0\npytz==2024.2\npywin32==308\nPyYAML==6.0.2\npyzmq==26.2.0\nregex==2024.11.6\nrequests==2.32.3\nrich==13.9.4\nscipy==1.13.1\nsentencepiece==0.2.0\nsetuptools==75.3.0\nsix==1.16.0\nsmart-open==7.0.5\nstack-data==0.6.3\ntransformers==4.44.2\ntensorboard==2.18.0\ntensorboard-data-server==0.7.2\ntensorflow==2.18.0\ntensorflow_intel==2.18.0\ntermcolor==2.5.0\ntornado==6.4.1\ntorch==2.5.0\ntqdm==4.67.0\ntraitlets==5.14.3\ntyping_extensions==4.12.2\ntzdata==2024.2\nurllib3==2.2.3\nwcwidth==0.2.13\nWerkzeug==3.1.2\nwheel==0.44.0\nwrapt==1.16.0\nxxhash==3.5.0\nyarl==1.17.1\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 7: Project Dependencies`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 7: Project Dependencies\n\nWelcome to the final chapter of our `SC4002-Group-Assignment` tutorial! In the [previous chapter](06_tuning_trial_configuration___state.md), we looked closely at individual **Tuning Trials**, understanding how the configuration and results of each specific experiment are stored in the `trial_XX` folders. We've covered how data is shaped, how hyperparameters are tuned, and how results are tracked.\n\nNow, imagine you've just downloaded this project onto your computer, eager to run it. You try to execute the code, but... errors pop up everywhere! It complains about missing tools like \"TensorFlow\" or \"NLTK\". Why is this happening? It's because your computer doesn't yet have all the necessary \"appliances\" and \"ingredients\" (software libraries) required to run the project's \"recipe\" (the code). How do we make sure everyone has the exact same set of tools? That's where **Project Dependencies** come in.\n\n## The Problem: Missing Tools in Your Kitchen\n\nThink of our project's code as a complex recipe for baking a cake (analyzing sentiment). This recipe relies on specific kitchen tools and appliances:\n\n*   A powerful mixer (like **TensorFlow** for machine learning calculations)\n*   A specific brand of food processor (like **Keras Tuner** for hyperparameter optimization)\n*   A set of specialized measuring spoons (like **NLTK** for text processing)\n*   Even specific brands of flour or sugar (particular versions of these libraries)\n\nIf you try to follow the recipe, but your kitchen is missing the mixer, or you have a different brand of food processor that works slightly differently, the cake might not turn out right, or you might not be able to bake it at all!\n\nSimilarly, our Python code relies on many external software packages (libraries) written by other developers. To run our project, you need to have these libraries installed on your computer, and often, you need *specific versions* of them to ensure everything works together correctly. If you have the wrong version, or a library is missing, the code will fail.\n\nHow can we easily list all the required tools and their exact models (versions) so that anyone can set up their \"kitchen\" correctly?\n\n## What are Project Dependencies?\n\n**Project Dependencies** are the collection of external software libraries and tools that our project's code needs to function. They are \"dependencies\" because our code *depends* on them \u2013 it can't run without them.\n\nTo manage these dependencies, we typically use a simple text file, often named `requirements.txt`. This file acts like a shopping list for your kitchen setup:\n\n*   It lists every essential \"tool\" (library) needed.\n*   It specifies the exact \"model number\" (version) for each tool.\n\nWhy specific versions? Sometimes, newer versions of a library might change how things work, potentially breaking our code (like a new mixer model having different speed settings). Listing exact versions ensures that everyone running the project uses the *exact same set* of compatible tools, making the results reproducible and avoiding unexpected errors.\n\n## Our Project's Shopping List: `requirements.txt`\n\nIn our `SC4002-Group-Assignment` project, you'll find a file named `requirements.txt`. This file lists all the external Python packages required. Let's look at a small part of it:\n\n--- File: `requirements.txt` (Snippet) ---\n```txt\n# Core ML libraries\ntensorflow==2.18.0\nkeras==3.6.0\nkeras-tuner==1.4.7\n\n# Data handling and text processing\nnltk==3.9.1\npandas==2.2.3\ndatasets==3.1.0\nnumpy==1.26.4\n\n# Utilities and support libraries\nhuggingface-hub==0.26.2\ntqdm==4.67.0\nrequests==2.32.3\n# ... many more lines ...\n```\n\n**Explanation:**\n\n*   Each line specifies one required library.\n*   The format is typically `library_name==version_number`.\n    *   `tensorflow==2.18.0`: This means our project requires the TensorFlow library, specifically version 2.18.0.\n    *   `nltk==3.9.1`: Requires the Natural Language Toolkit (NLTK) version 3.9.1.\n    *   `keras-tuner==1.4.7`: Requires the Keras Tuner library (which manages the [Keras Tuner Oracle](05_keras_tuner_oracle.md) and [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)) version 1.4.7.\n*   Lines starting with `#` are comments and are ignored.\n*   Sometimes you might see `>=` (greater than or equal to) or `<` (less than), but using `==` (exactly equal to) provides the highest level of reproducibility.\n\nThis file is the definitive list of everything you need to install *after* you have Python itself set up.\n\n## How to Install the Dependencies\n\nOkay, you have the shopping list (`requirements.txt`). How do you actually get all these tools into your kitchen (install the libraries on your computer)? Python comes with a package manager called `pip` that can read this file and install everything automatically.\n\nYou typically run this command in your terminal or command prompt, making sure you are in the main directory of the project where `requirements.txt` is located:\n\n```bash\npip install -r requirements.txt\n```\n\n**Explanation:**\n\n*   `pip`: This is the command-line tool for installing Python packages.\n*   `install`: Tells `pip` that you want to install packages.\n*   `-r requirements.txt`: This is the crucial part. The `-r` flag tells `pip` to read the package names and versions from the specified file (`requirements.txt`) and install them.\n\n**What Happens Next?**\n\nWhen you run this command, `pip` will:\n1.  Read each line in `requirements.txt`.\n2.  Connect to the Python Package Index (PyPI) \u2013 a huge online repository of Python software.\n3.  Find the specified version of each library (e.g., TensorFlow 2.18.0).\n4.  Download the library and any additional libraries *it* might depend on.\n5.  Install them onto your system in a place where your Python interpreter can find them.\n\nYou'll see a lot of messages in your terminal showing the download and installation progress. Once it finishes without errors, you have successfully set up your \"kitchen\" with all the necessary tools!\n\n## Under the Hood: How `pip` Uses the List\n\nLet's visualize the process with a simple diagram:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Terminal as Command Line\n    participant Pip as pip command\n    participant PyPI as Python Package Index (Online Repository)\n    participant System as Your Computer\n\n    User->>Terminal: Enters `pip install -r requirements.txt`\n    Terminal->>Pip: Executes the pip command with the file argument\n    Pip->>Pip: Reads `requirements.txt` line by line (e.g., \"tensorflow==2.18.0\")\n    Pip->>PyPI: Asks for \"tensorflow\" version \"2.18.0\"\n    PyPI-->>Pip: Provides download link for the package\n    Pip->>System: Downloads the package file\n    Pip->>System: Installs the package\n    Note over Pip, System: Repeats for all lines in requirements.txt\n    System-->>User: Installation complete (or shows errors)\n```\n\nThis automated process ensures that you get the exact versions needed, saving you the hassle of finding and installing each library manually.\n\n## Why This Matters for Our Project\n\nHaving the correct dependencies installed is crucial for running the `SC4002-Group-Assignment` project:\n\n*   **Running the Code:** Without TensorFlow, Keras, and Keras Tuner, you can't build, train, or tune the sentiment analysis model.\n*   **Data Processing:** Libraries like NLTK, Pandas, and Datasets are essential for loading and preparing the text data before it fits the [Model Input Shape](01_model_input_shape.md).\n*   **Reproducibility:** Using the exact versions specified in `requirements.txt` ensures that if you run the code, you should get similar results to what the original developers got (assuming the same data and code). This is vital for scientific and group work.\n*   **Avoiding Conflicts:** Different libraries sometimes depend on *other* libraries. Using specific versions helps prevent situations where Library A needs Tool X version 1.0, but Library B needs Tool X version 2.0, causing a conflict. The `requirements.txt` represents a set of versions known to work together.\n\n## Conclusion\n\nCongratulations on reaching the end of the tutorial! In this chapter, we learned about **Project Dependencies**:\n\n*   They are the **external software libraries** our project needs to run (like TensorFlow, Keras Tuner, NLTK).\n*   They are listed, along with their specific **versions**, in the **`requirements.txt`** file \u2013 our project's \"shopping list\".\n*   Specifying exact versions ensures **reproducibility** and avoids **conflicts**.\n*   We use the command **`pip install -r requirements.txt`** to automatically download and install all listed dependencies.\n*   Having the correct dependencies is the final step needed to ensure you can successfully run the project code and reproduce the experiments we've discussed throughout these chapters.\n\nFrom understanding the [Model Input Shape](01_model_input_shape.md) to exploring the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md), defining [Training Constants](03_training_constants.md), setting up the [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md), understanding the [Keras Tuner Oracle](05_keras_tuner_oracle.md), examining [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md), and now managing [Project Dependencies](07_project_dependencies.md), you've gained a solid overview of the key concepts involved in this `SC4002-Group-Assignment` project.\n\nWe hope this tutorial has been helpful. Happy coding!", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"mcp server for python\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"mcp\", \"server\", \"python\"]\n```", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    .. This file contains a series of comments that are used to include sections of this README in other files. Do not modify these comments unless you know what you are doing. tag:intro-begin\n\n|build-status|\n\n.. |build-status| image:: https://img.shields.io/azure-devops/build/certbot/ba534f81-a483-4b9b-9b4e-a60bec8fee72/5/main\n   :target: https://dev.azure.com/certbot/certbot/_build?definitionId=5\n   :alt: Azure Pipelines CI status\n\n.. image:: https://raw.githubusercontent.com/EFForg/design/master/logos/certbot/eff-certbot-lockup.png\n  :width: 200\n  :alt: EFF Certbot Logo\n\nCertbot is part of EFF\u2019s effort to encrypt the entire Internet. Secure communication over the Web relies on HTTPS, which requires the use of a digital certificate that lets browsers verify the identity of web servers (e.g., is that really google.com?). Web servers obtain their certificates from trusted third parties called certificate authorities (CAs). Certbot is an easy-to-use client that fetches a certificate from Let\u2019s Encrypt\u2014an open certificate authority launched by the EFF, Mozilla, and others\u2014and deploys it to a web server.\n\nAnyone who has gone through the trouble of setting up a secure website knows what a hassle getting and maintaining a certificate is. Certbot and Let\u2019s Encrypt can automate away the pain and let you turn on and manage HTTPS with simple commands. Using Certbot and Let's Encrypt is free.\n\n.. _installation:\n\nGetting Started\n---------------\nThe best way to get started is to use our `interactive guide <https://certbot.eff.org>`_. It generates instructions based on your configuration settings. In most cases, you\u2019ll need `root or administrator access <https://certbot.eff.org/faq/#does-certbot-require-root-administrator-privileges>`_ to your web server to run Certbot.\n\nCertbot is meant to be run directly on your web server on the command line, not on your personal computer. If you\u2019re using a hosted service and don\u2019t have direct access to your web server, you might not be able to use Certbot. Check with your hosting provider for documentation about uploading certificates or using certificates issued by Let\u2019s Encrypt.\n\nContributing\n------------\n\nIf you'd like to contribute to this project please read `Developer Guide\n<https://certbot.eff.org/docs/contributing.html>`_.\n\nThis project is governed by `EFF's Public Projects Code of Conduct <https://www.eff.org/pages/eppcode>`_.\n\nLinks\n=====\n\n.. Do not modify this comment unless you know what you're doing. tag:links-begin\n\nDocumentation: https://certbot.eff.org/docs\n\nSoftware project: https://github.com/certbot/certbot\n\nChangelog: https://github.com/certbot/certbot/blob/main/certbot/CHANGELOG.md\n\nFor Contributors: https://certbot.eff.org/docs/contributing.html\n\nFor Users: https://certbot.eff.org/docs/using.html\n\nMain Website: https://certbot.eff.org\n\nLet's Encrypt Website: https://letsencrypt.org\n\nCommunity: https://community.letsencrypt.org\n\nACME spec: `RFC 8555 <https://tools.ietf.org/html/rfc8555>`_\n\nACME working area in github (archived): https://github.com/ietf-wg-acme/acme\n\n.. Do not modify this comment unless you know what you're doing. tag:links-end\n\n.. Do not modify this comment unless you know what you're doing. tag:intro-end\n\n.. Do not modify this comment unless you know what you're doing. tag:features-begin\n\nCurrent Features\n=====================\n\n* Supports multiple web servers:\n\n  - Apache 2.4+\n  - nginx/0.8.48+\n  - webroot (adds files to webroot directories in order to prove control of\n    domains and obtain certificates)\n  - standalone (runs its own simple webserver to prove you control a domain)\n  - other server software via `third party plugins <https://certbot.eff.org/docs/using.html#third-party-plugins>`_\n\n* The private key is generated locally on your system.\n* Can talk to the Let's Encrypt CA or optionally to other ACME\n  compliant services.\n* Can get domain-validated (DV) certificates.\n* Can revoke certificates.\n* Supports ECDSA (default) and RSA certificate private keys.\n* Can optionally install a http -> https redirect, so your site effectively\n  runs https only.\n* Fully automated.\n* Configuration changes are logged and can be reverted.\n\n.. Do not modify this comment unless you know what you're doing. tag:features-end\n\n    \n    YOUR SUMMARY:\n    ": "This repository contains Certbot, a free tool by EFF designed to simplify enabling HTTPS on websites. It automatically fetches free certificates from Let's Encrypt and deploys them on various web servers like Apache and Nginx, automating the setup process. The purpose is to make encrypting the web easy and accessible for everyone.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # Zulip overview\n\n[Zulip](https://zulip.com) is an open-source team collaboration tool with unique\n[topic-based threading][why-zulip] that combines the best of email and chat to\nmake remote work productive and delightful. Fortune 500 companies, [leading open\nsource projects][rust-case-study], and thousands of other organizations use\nZulip every day. Zulip is the only [modern team chat app][features] that is\ndesigned for both live and asynchronous conversations.\n\nZulip is built by a distributed community of developers from all around the\nworld, with 74+ people who have each contributed 100+ commits. With\nover 1000 contributors merging over 500 commits a month, Zulip is the\nlargest and fastest growing open source team chat project.\n\nCome find us on the [development community chat](https://zulip.com/development-community/)!\n\n[![GitHub Actions build status](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml/badge.svg)](https://github.com/zulip/zulip/actions/workflows/zulip-ci.yml?query=branch%3Amain)\n[![coverage status](https://img.shields.io/codecov/c/github/zulip/zulip/main.svg)](https://codecov.io/gh/zulip/zulip)\n[![Mypy coverage](https://img.shields.io/badge/mypy-100%25-green.svg)][mypy-coverage]\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![code style: prettier](https://img.shields.io/badge/code_style-prettier-ff69b4.svg)](https://github.com/prettier/prettier)\n[![GitHub release](https://img.shields.io/github/release/zulip/zulip.svg)](https://github.com/zulip/zulip/releases/latest)\n[![docs](https://readthedocs.org/projects/zulip/badge/?version=latest)](https://zulip.readthedocs.io/en/latest/)\n[![Zulip chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://chat.zulip.org)\n[![Twitter](https://img.shields.io/badge/twitter-@zulip-blue.svg?style=flat)](https://twitter.com/zulip)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/zulip)](https://github.com/sponsors/zulip)\n\n[mypy-coverage]: https://blog.zulip.org/2016/10/13/static-types-in-python-oh-mypy/\n[why-zulip]: https://zulip.com/why-zulip/\n[rust-case-study]: https://zulip.com/case-studies/rust/\n[features]: https://zulip.com/features/\n\n## Getting started\n\n- **Contributing code**. Check out our [guide for new\n  contributors](https://zulip.readthedocs.io/en/latest/contributing/contributing.html)\n  to get started. We have invested in making Zulip\u2019s code highly\n  readable, thoughtfully tested, and easy to modify. Beyond that, we\n  have written an extraordinary 150K words of documentation for Zulip\n  contributors.\n\n- **Contributing non-code**. [Report an\n  issue](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#reporting-issues),\n  [translate](https://zulip.readthedocs.io/en/latest/translating/translating.html)\n  Zulip into your language, or [give us\n  feedback](https://zulip.readthedocs.io/en/latest/contributing/suggesting-features.html).\n  We'd love to hear from you, whether you've been using Zulip for years, or are just\n  trying it out for the first time.\n\n- **Checking Zulip out**. The best way to see Zulip in action is to drop by the\n  [Zulip community server](https://zulip.com/development-community/). We also\n  recommend reading about Zulip's [unique\n  approach](https://zulip.com/why-zulip/) to organizing conversations.\n\n- **Running a Zulip server**. Self-host Zulip directly on Ubuntu or Debian\n  Linux, in [Docker](https://github.com/zulip/docker-zulip), or with prebuilt\n  images for [Digital Ocean](https://marketplace.digitalocean.com/apps/zulip) and\n  [Render](https://render.com/docs/deploy-zulip).\n  Learn more about [self-hosting Zulip](https://zulip.com/self-hosting/).\n\n- **Using Zulip without setting up a server**. Learn about [Zulip\n  Cloud](https://zulip.com/plans/) hosting options. Zulip sponsors free [Zulip\n  Cloud Standard](https://zulip.com/plans/) for hundreds of worthy\n  organizations, including [fellow open-source\n  projects](https://zulip.com/for/open-source/).\n\n- **Participating in [outreach\n  programs](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#outreach-programs)**\n  like [Google Summer of Code](https://developers.google.com/open-source/gsoc/)\n  and [Outreachy](https://www.outreachy.org/).\n\n- **Supporting Zulip**. Advocate for your organization to use Zulip, become a\n  [sponsor](https://github.com/sponsors/zulip), write a review in the mobile app\n  stores, or [help others find\n  Zulip](https://zulip.readthedocs.io/en/latest/contributing/contributing.html#help-others-find-zulip).\n\nYou may also be interested in reading our [blog](https://blog.zulip.org/), and\nfollowing us on [Twitter](https://twitter.com/zulip) and\n[LinkedIn](https://www.linkedin.com/company/zulip-project/).\n\nZulip is distributed under the\n[Apache 2.0](https://github.com/zulip/zulip/blob/main/LICENSE) license.\n\n    \n    YOUR SUMMARY:\n    ": "This repository contains the code for Zulip, an open-source team collaboration tool designed for productive remote work. Its key feature is unique topic-based threading, combining aspects of email and chat for both live and asynchronous conversations. Zulip is used by diverse organizations and maintained by a large, active open-source community.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <h1 align=\"center\">\n        \ud83d\ude85 LiteLLM\n    </h1>\n    <p align=\"center\">\n        <p align=\"center\">\n        <a href=\"https://render.com/deploy?repo=https://github.com/BerriAI/litellm\" target=\"_blank\" rel=\"nofollow\"><img src=\"https://render.com/images/deploy-to-render-button.svg\" alt=\"Deploy to Render\"></a>\n        <a href=\"https://railway.app/template/HLP0Ub?referralCode=jch2ME\">\n          <img src=\"https://railway.app/button.svg\" alt=\"Deploy on Railway\">\n        </a>\n        </p>\n        <p align=\"center\">Call all LLM APIs using the OpenAI format [Bedrock, Huggingface, VertexAI, TogetherAI, Azure, OpenAI, Groq etc.]\n        <br>\n    </p>\n<h4 align=\"center\"><a href=\"https://docs.litellm.ai/docs/simple_proxy\" target=\"_blank\">LiteLLM Proxy Server (LLM Gateway)</a> | <a href=\"https://docs.litellm.ai/docs/hosted\" target=\"_blank\"> Hosted Proxy (Preview)</a> | <a href=\"https://docs.litellm.ai/docs/enterprise\"target=\"_blank\">Enterprise Tier</a></h4>\n<h4 align=\"center\">\n    <a href=\"https://pypi.org/project/litellm/\" target=\"_blank\">\n        <img src=\"https://img.shields.io/pypi/v/litellm.svg\" alt=\"PyPI Version\">\n    </a>\n    <a href=\"https://www.ycombinator.com/companies/berriai\">\n        <img src=\"https://img.shields.io/badge/Y%20Combinator-W23-orange?style=flat-square\" alt=\"Y Combinator W23\">\n    </a>\n    <a href=\"https://wa.link/huol9n\">\n        <img src=\"https://img.shields.io/static/v1?label=Chat%20on&message=WhatsApp&color=success&logo=WhatsApp&style=flat-square\" alt=\"Whatsapp\">\n    </a>\n    <a href=\"https://discord.gg/wuPM9dRgDw\">\n        <img src=\"https://img.shields.io/static/v1?label=Chat%20on&message=Discord&color=blue&logo=Discord&style=flat-square\" alt=\"Discord\">\n    </a>\n</h4>\n\nLiteLLM manages:\n\n- Translate inputs to provider's `completion`, `embedding`, and `image_generation` endpoints\n- [Consistent output](https://docs.litellm.ai/docs/completion/output), text responses will always be available at `['choices'][0]['message']['content']`\n- Retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - [Router](https://docs.litellm.ai/docs/routing)\n- Set Budgets & Rate limits per project, api key, model [LiteLLM Proxy Server (LLM Gateway)](https://docs.litellm.ai/docs/simple_proxy)\n\n[**Jump to LiteLLM Proxy (LLM Gateway) Docs**](https://github.com/BerriAI/litellm?tab=readme-ov-file#openai-proxy---docs) <br>\n[**Jump to Supported LLM Providers**](https://github.com/BerriAI/litellm?tab=readme-ov-file#supported-providers-docs)\n\n\ud83d\udea8 **Stable Release:** Use docker images with the `-stable` tag. These have undergone 12 hour load tests, before being published. [More information about the release cycle here](https://docs.litellm.ai/docs/proxy/release_cycle)\n\nSupport for more providers. Missing a provider or LLM Platform, raise a [feature request](https://github.com/BerriAI/litellm/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.yml&title=%5BFeature%5D%3A+).\n\n# Usage ([**Docs**](https://docs.litellm.ai/docs/))\n\n> [!IMPORTANT]\n> LiteLLM v1.0.0 now requires `openai>=1.0.0`. Migration guide [here](https://docs.litellm.ai/docs/migration)  \n> LiteLLM v1.40.14+ now requires `pydantic>=2.0.0`. No changes required.\n\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/BerriAI/litellm/blob/main/cookbook/liteLLM_Getting_Started.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n```shell\npip install litellm\n```\n\n```python\nfrom litellm import completion\nimport os\n\n## set ENV variables\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n\nmessages = [{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n\n# openai call\nresponse = completion(model=\"openai/gpt-4o\", messages=messages)\n\n# anthropic call\nresponse = completion(model=\"anthropic/claude-3-sonnet-20240229\", messages=messages)\nprint(response)\n```\n\n### Response (OpenAI Format)\n\n```json\n{\n    \"id\": \"chatcmpl-565d891b-a42e-4c39-8d14-82a1f5208885\",\n    \"created\": 1734366691,\n    \"model\": \"claude-3-sonnet-20240229\",\n    \"object\": \"chat.completion\",\n    \"system_fingerprint\": null,\n    \"choices\": [\n        {\n            \"finish_reason\": \"stop\",\n            \"index\": 0,\n            \"message\": {\n                \"content\": \"Hello! As an AI language model, I don't have feelings, but I'm operating properly and ready to assist you with any questions or tasks you may have. How can I help you today?\",\n                \"role\": \"assistant\",\n                \"tool_calls\": null,\n                \"function_call\": null\n            }\n        }\n    ],\n    \"usage\": {\n        \"completion_tokens\": 43,\n        \"prompt_tokens\": 13,\n        \"total_tokens\": 56,\n        \"completion_tokens_details\": null,\n        \"prompt_tokens_details\": {\n            \"audio_tokens\": null,\n            \"cached_tokens\": 0\n        },\n        \"cache_creation_input_tokens\": 0,\n        \"cache_read_input_tokens\": 0\n    }\n}\n```\n\nCall any model supported by a provider, with `model=<provider_name>/<model_name>`. There might be provider-specific details here, so refer to [provider docs for more information](https://docs.litellm.ai/docs/providers)\n\n## Async ([Docs](https://docs.litellm.ai/docs/completion/stream#async-completion))\n\n```python\nfrom litellm import acompletion\nimport asyncio\n\nasync def test_get_response():\n    user_message = \"Hello, how are you?\"\n    messages = [{\"content\": user_message, \"role\": \"user\"}]\n    response = await acompletion(model=\"openai/gpt-4o\", messages=messages)\n    return response\n\nresponse = asyncio.run(test_get_response())\nprint(response)\n```\n\n## Streaming ([Docs](https://docs.litellm.ai/docs/completion/stream))\n\nliteLLM supports streaming the model response back, pass `stream=True` to get a streaming iterator in response.  \nStreaming is supported for all models (Bedrock, Huggingface, TogetherAI, Azure, OpenAI, etc....\n    \n    YOUR SUMMARY:\n    ": "LiteLLM enables calling various LLM APIs (like Bedrock, Azure, OpenAI, VertexAI) using the consistent OpenAI input/output format. Key features include translating requests, standardizing responses, managing retries/fallbacks, and providing a proxy server for budget and rate limiting. Its purpose is to simplify integrating and managing multiple LLM providers within applications.", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"Find me a github on SC4001NN projects\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"SC4001NN\", \"projects\"]\n```", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"Find me a github on SC4001 NN projects\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"SC4001\", \"NN\", \"projects\", \"Neural Networks\"]\n```", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"Find me a github on llm projects\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"llm\", \"projects\", \"github\"]\n```", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # Warning: This is the legacy version, find the project here: https://github.com/gyroflow/gyroflow\r\n\r\n# Gyroflow - Video stabilization using gyroscope data targeting drone footage\r\n\r\nJoin the [Gyroflow Discord server](https://discord.gg/BBJ2UVAr2D) for discussion and support.\r\n\r\n## [Website with more info](http://gyroflow.xyz/)\r\n\r\n## [In-depth video guide for 0.3.x-beta](https://youtu.be/f4YD5pGmnxM)\r\n\r\nA program built around Python, OpenCV, and PySide2 for video stabilization using gyroscope data.\r\n\r\nThe project consists of three core parts: A utility for the generation of lens undistortion preset, a utility for stabilizing footage using gyro data, and a utility for stretching 4:3 video to 16:9 using non-linear horizontal stretching (similar to GoPro's superview).\r\n\r\nThe main goal of creating something that can stabilize footage based on blackbox gyro data has mostly been achieved. Moving forwards, more work will be done to improve the quality of the result.\r\n\r\nThe launcher containing all the utilities is available by executing `gyroflow.py` if all the dependencies are met. Otherwise a binary can be found over in [releases](https://github.com/ElvinC/gyroflow/releases). Also, check out the wiki where there's some more information about the camera calibration and video stabilization process.\r\n\r\n## Run using python and Poetry:\r\nNote: Try the dev branch for the newest features.\r\n\r\n* Install [poetry](https://python-poetry.org/docs/#installation)\r\n* Clone or download the files from this repo\r\n* Navigate to the folder using a commandline and install dependencies using `poetry install`\r\n* Run the application using `poetry run python gyroflow.py`\r\n\r\n## Other things to check out:\r\n* [BlackboxToGPMF](https://github.com/Cleric-K/BlackboxToGPMF/tree/gui) by Cleric-K and Attilafustos. Tool for adding GoPro metadata and blackbox data to non-GoPro cameras for use with Reelsteady GO. Initial discussion [here](https://github.com/ElvinC/gyroflow/issues/1).\r\n* [blackbox2gpmf](https://github.com/jaromeyer/blackbox2gpmf) by Jaromeyer. Tool for adding blackbox gyro data to Hero 7 files for Reelsteady Go.\r\n* [Gyroflow Discord server](https://discord.gg/BBJ2UVAr2D)\r\n* [RSGoBlackbox Discord server](https://discord.gg/Rs4GBPm) maintained by [Nicecrash](https://www.youtube.com/channel/UCl3M972T7GbxnEucYHzZ05g) for discussion about gyroflow, BlackboxToGPMF, blackbox2gpmf and other related projects.\r\n* [FPV Stabilization Tools Facebook group](https://www.facebook.com/groups/fpvtools) maintained by Attilafustos.\r\n\r\n\r\n## General recording tips\r\n* Use the settings that give the widest possible field of view (more data to work with). For a lot of cameras, this is in the 4:3 aspect ratio.\r\n* If using the main drone flight controller for logging, the camera should be hardmounted.\r\n* If using a secondary logger on the camera or internal camera logging, some soft mounting is preferred to isolate vibrations.\r\n\r\n\r\n## Status\r\n\r\n**Sample clips:**\r\n* [0.2.x-alpha results](https://youtu.be/xkVtbYQnH04)\r\n* [In-depth video guide](https://youtu.be/f4YD5pGmnxM)\r\n\r\n\r\n**Working:**\r\n* Videoplayer based on OpenCV and Pyside2\r\n* Gyro integration using quaternions\r\n* Non-linear stretch utility\r\n* Basic video import/export\r\n* Camera calibration utility with preset import/export\r\n* GoPro metadata import\r\n* Symmetrical slerp-based quaternion low-pass filter\r\n* Blackbox data import\r\n* Undistort and rotation perspective transform\r\n* Semi-automatic temporal gyro/video sync.\r\n* Blackbox orientation handling and wide interval sync.\r\n* Stabilization UI without video player\r\n* (Basic) gyro orientation presets for GoPro cameras\r\n* Frame blending timelapse/hyperlapse\r\n* Automatic determination of required crop\r\n* Native support for insta360 gyro\r\n* Support for high speed video\r\n* GoPro/Insta360 as gyro logger for cinema cameras.\r\n* Basic prores import/export through ffmpeg\r\n* Modular/improved smoothing methods\r\n* Support for non-square pixel aspect ratios\r\n* Incorporate acceleration data in orientation estimation for horizon lock\r\n* Audio handling\r\n* .gyroflow file with synced data\r\n\n    \n    YOUR SUMMARY:\n    ": "This repository contains a legacy version of Gyroflow, a tool designed for video stabilization using gyroscope data, primarily aimed at drone footage. It features utilities for lens undistortion, gyro-based stabilization, and non-linear stretching of 4:3 video to 16:9. The project's main goal was to stabilize footage based on blackbox gyro data, though development has moved to a new repository.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # lastversion\n\n[![Python package](https://github.com/dvershinin/lastversion/actions/workflows/pythonpackage.yml/badge.svg)](https://github.com/dvershinin/lastversion/actions/workflows/pythonpackage.yml)\n[![PyPI version](https://badge.fury.io/py/lastversion.svg)](https://badge.fury.io/py/lastversion)\n[![GitHub Release](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Flastversion-api.getpagespeed.com%2Fdvershinin%2Flastversion&query=version&label=Release)](https://github.com/dvershinin/lastversion/releases)\n[![Documentation Status](https://readthedocs.org/projects/lastversion/badge/?version=latest)](https://lastversion.getpagespeed.com/en/latest/?badge=latest)\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/380e3a38dc524112b4dcfc0492d5b816)](https://www.codacy.com/manual/GetPageSpeed/lastversion?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=dvershinin/lastversion&amp;utm_campaign=Badge_Grade)\n[![Codacy Badge](https://app.codacy.com/project/badge/Coverage/380e3a38dc524112b4dcfc0492d5b816)](https://www.codacy.com/gh/dvershinin/lastversion/dashboard?utm_source=github.com&utm_medium=referral&utm_content=dvershinin/lastversion&utm_campaign=Badge_Coverage)\n[![Buy Me a Coffee](https://img.shields.io/badge/dynamic/json?color=blue&label=Buy%20me%20a%20Coffee&prefix=%23&query=next_time_total&url=https%3A%2F%2Fwww.getpagespeed.com%2Fbuymeacoffee.json&logo=buymeacoffee)](https://www.buymeacoffee.com/dvershinin)\n\n![Using lastversion in terminal](https://www.getpagespeed.com/img/lastversion.gif)\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](README-ZH-CN.md)\n\nA tiny command-line utility that helps to answer a simple question:\n\n> What is the latest *stable* version for a project?\n\n... and, optionally, download/install it.\n\n`lastversion` allows finding well-formatted, the latest release version of a project from these \n supported locations:\n\n*   [GitHub](https://github.com/dvershinin/lastversion/wiki/GitHub-specifics)\n*   GitLab\n*   BitBucket\n*   PyPI\n*   Mercurial\n*   SourceForge\n*   Wikipedia\n*   WordPress plugin directory\n*   Arbitrary software sites which publish releases in RSS/ATOM feeds\n\n## Why you need `lastversion`\n\nIn general, quite many project authors complicate finding the latest version by:\n\n*   Creating a formal release that is clearly a Release Candidate (`rc` in tag), but forgetting to\n    mark it as a pre-release\n\n*   Putting extraneous text in release tag e.g. `release-1.2.3` or `name-1.2.3-2019` anything fancy \n    like that\n\n*   Putting or not putting the `v` prefix inside release tags. Today yes, tomorrow not. I'm not\n    consistent about it myself :)\n\n*   Switching from one version format to another, e.g. `v20150121` to `v2.0.1`\n\nThere is no consistency in human beings.\n\nTo deal with all this mess and get a well-formatted, last *stable* version (or download\n URL!) on the command line, you can use `lastversion`.\n\n`lastversion` is particularly useful in automated build systems where tracking the latest release versions is essential.\n\n`lastversion` does a little bit of AI to detect if releasers mistakenly filed a beta version as a \nstable release.\nIt incorporates logic for cleaning up human inconsistency from \nversion information.\n\n## Synopsis\n\n```bash\nlastversion apache/incubator-pagespeed-ngx \n#> 1.13.35.2\n\nlastversion download apache/incubator-pagespeed-ngx \n#> downloaded incubator-pagespeed-ngx-v1.13.35.2-stable.tar.gz\n\nlastversion download apache/incubator-pagespeed-ngx -o pagespeed.tar.gz \n#> downloads with chosen filename\n\nlastversion https://transmissionbt.com/\n#> 3.0\n\nlastversion format \"mysqld  Ver 5.6.51-91.0 for Linux\"\n#> 5.6.51\n```\n\n## Installation for RPM-based systems\n\nSupported:\n\n* CentOS/RHEL 7, 8, 9 including clones like AlmaLinux and Rocky Linux\n* Amazon Linux 2, Amazon Linux 2023\n* Fedora Linux\n\n```bash\nsudo yum -y install https://extras.getpagespeed.com/release-latest.rpm\nsudo yum -y install lastversion\n```\n   \n## Installation for other systems\n\nInstalling with `pip` is easiest:\n\n```bash\npip install lastversion\n```\n    \n## Usage\n\nTypically, you would just pass a repository URL (or repo owner/name to it) as the only argument, \ne.g.:\n\n```bash\nlastversion https://github.com/gperftools/gperftools\n```\n\nEquivalently accepted invocation with the same output is:\n\n```bash\nlastversion gperftools/gperftools\n```    \n\nIf you're lazy to even copy-paste a project's URL, you can just type its name as argument, which \nwill use repository search API (slower).\nHelps to answer what is the latest Linux version:\n\n```bash\nlastversion linux\n```\n\nOr wondering what is the latest version of WordPress? \n\n```bash\nlastversion wordpress\n```\n   \nA special value of `self` for the main argument, will look up the last release of `lastversion` \nitself.\n\nFor more options to control output or behavior, see `--help` output:    \n\n```text\nusage: lastversion [-h] [--pre] [--sem {major,minor,patch,any}] [-v]\n                   [-d [FILENAME]] [--format {version,assets,source,json,tag}]\n                   [--assets] [--source] [-gt VER] [-b MAJOR] [--only REGEX]\n                   [--exclude REGEX] [--filter REGEX] [--having-asset [ASSET]]\n                   [-su] [--even]\n                   [--at {github,gitlab,bitbucket,pip,hg,sf,website-feed,local,helm_chart,wiki,system,wp,gitea}]\n                   [-y] [--version]\n                   [{get,download,extract,unzip,test,format,install,update-spec}]\n                   <repo URL or string>\n\nFind the latest software release.\n\npositional arguments:\n  {get,download,extract,unzip,test,format,install,update-spec}\n                        Action to run....\n    \n    YOUR SUMMARY:\n    ": "`lastversion` is a command-line utility designed to find the latest *stable* version of software projects from various sources like GitHub, GitLab, PyPI, and websites. It intelligently handles inconsistent version tagging (like pre-releases or `v` prefixes) and can optionally download or install the found version. Its main purpose is to reliably track the latest releases, which is particularly useful for automated build systems.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    ![alt tag](https://raw.githubusercontent.com/waseem18/gitpromote/master/static/images/dp.PNG)\n\n<a href=\"https://gitpromote.appspot.com/\"><h2>gitpromote</h2></a>\n\nNote : This repo contains legacy code which I wrote in the initial days of my college.\n\nPromote your open source projects to many other people like you!\n\n    \n    YOUR SUMMARY:\n    ": "This repository contains legacy code for Gitpromote, a platform designed to help developers promote their open source projects. Its main purpose is to increase project visibility and connect developers within the open-source community.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # UAV-Target-Recognition-And-Tracking\nThis project aims to build an UAV that can realize automatic follow-up shooting. It can free your hands, reduce the burden of shooting, and allow more people to record life easily. The UAV will maintain stability and smoothness during shooting,  which will be really useful for extreme sports follow-up shooting. The project is based on deep learning algorithm training target recognition model, using NVIDIA Jetson TX2 development and Linux system ROS and pixhawk interaction, PID algorithm for flight control.\n\n    \n    YOUR SUMMARY:\n    ": "This repository describes a project to create a UAV capable of automatic target recognition and follow-up shooting, designed to simplify recording, particularly for activities like extreme sports. It leverages deep learning for object detection, runs on an NVIDIA Jetson TX2 with ROS and Pixhawk integration, and uses PID control for stable flight and smooth tracking.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # Git-Influencer\n\nA platform for you to make most of the github by discovering social influencers in github network regarding to your interest.\n\n## Project Idea\nStarted from 2008, Github is now one of the most popular open source community in tech world. As of 2018, there are 28 million users and 57 million repositories, making it the largest host of source code in the world.\n\nGithub is also one of the best sources for learning coding, we share code, publish new project, follow and learn from other users. But github do not have a system to help you find the right person and resources on specific area.\n\n**Everyone wants to learn from the best, this project aims to creat a platform which can help you on finding the social influencers from the github network.**\n\n## Data Source\n* [Github archive](https://www.gharchive.org/) : GH Archive is a project to record the public GitHub timeline, which stores all the event based github. Weighing in over **3TB** total, this is the largest Bigquery dataset available on kaggle.\n* Data size: 80~100G/month since 2011\n* Update frenquency: Update every 1 hour\n\n## Tech Stack\n\n![Tech Stack](https://raw.githubusercontent.com/catherinesdataanalytics/LearnFromBest/master/pics/tech_flow_V4.png)\n\n* **Data Ingestion**\n   - Historical data: Raw data stored in github Archive, it is scheduled to be downloaded with python and Airflow and saved to HDFS.\n   - New coming data: use github Archive api and airflow scheduler to clean the updated new coming data and append to historical data in HDFS.\n   - HDFS: All data in HDFS will be cleaned with spark and saved for data processing in spark.\n\n* **Data Processing**\n   - use spark for batch processing data on HDFS\n   - PageRank and other network analysis algorithms in graphX\n\n* **Database**\n   - MySQL, with 10 table corresponding to 10 languages user rank scores and user details.\n\n* **User Interface**\n   - Dash, user can select the language and they will get recommended users ranking by the social influencer score.\n\n## Engineering challenge\n* Data modeling: find clues from raw json event data for mapping users with languages.\n* Data size and update: Processing and cleaning 2.9TB github event data, combining both historical data cleaning and new coming data cleaning: 80~100 G per month, and update every hour. Use airflow auto the whole processing.\n\n## Alogorithms\n* Centrality Measures: Pagerank\n* only people who used this language before has been included in the pagerank algorithms\n* Community Detection: Strongly Connected Components\n* GraphX and more analysis in the future.\n\n## Business Value\n1. If you want to learn \"Golang\" or other languages, this platform will recommend you the most valueble github user to follow and learn from based on network analysis results.\n2. For example, Show N people to learn from based on network analysis result.\nRecommend community for colaboration.\n![Dashboard](https://raw.githubusercontent.com/catherinesdataanalytics/LearnFromBest/master/pics/dashboard_screenshot.png)\n\n\n## Further Improvement\n* Explore HDFS data storage efficiency - Parquet\n* Try different classification metric for discovering more user topics\n* Use more Graph analysis algorithms in GraphX\n\n## Dashboard and PPT\n* [Git Influencer dashboard](http://bit.ly/Git-Influencer)\n* [Project Slides](https://www.slideshare.net/CatherineShen10/git-influencer-catherine-shen)\n* [Demo video](https://youtu.be/bOVR8one7pY)\n\n    \n    YOUR SUMMARY:\n    ": "Git-Influencer is a platform designed to help users discover influential GitHub users within specific programming language communities. It analyzes vast amounts of GitHub event data using Spark and network analysis algorithms like PageRank. The ultimate goal is to recommend key figures users can follow and learn from, facilitating better knowledge discovery on GitHub.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    \n## Project: Data Pipelines with Airflow\n**Project Description**: A music streaming company wants to introduce more automation and monitoring to their data warehouse ETL pipelines and they have come to the conclusion that the best tool to achieve this is **Apache Airflow**. As their Data Engineer, I was tasked to create a reusable production-grade data pipeline that incorporates data quality checks and allows for easy backfills. Several analysts and Data Scientists rely on the output generated by this pipeline and it is expected that the pipeline runs daily on a schedule by pulling new data from the source and store the results to the destination.\n\n**Data Description**: The source data resides in S3 and needs to be processed in a data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.\n\n**Data Pipeline design**:\nAt a high-level the pipeline does the following tasks.\n1. Extract data from multiple S3 locations.\n2. Load the data into Redshift cluster.\n3. Transform the data into a star schema.\n4. Perform data validation and data quality checks.\n5. Calculate the most played songs for the specified time interval.\n6. Load the result back into S3.\n\n![dag](images/dag.png)\n> Structure of the Airflow DAG\n\n\n\n\n\n\n**Design Goals**:\nBased on the requirements of our data consumers, our pipeline is required to adhere to the following guidelines:\n* The DAG should not have any dependencies on past runs.\n* On failure, the task is retried for 3 times.\n* Retries happen every 5 minutes.\n* Catchup is turned off.\n* Do not email on retry. \n\n**Pipeline Implementation**:\n\nApache Airflow is a Python framework for programmatically creating workflows in DAGs, e.g. ETL processes, generating reports, and retraining models on a daily basis. The Airflow UI automatically parses our DAG and creates a natural representation for the movement and transformation of data. A DAG simply is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies. A **DAG** describes *how* you want to carry out your workflow, and **Operators** determine *what* actually gets done. \n\nBy default, airflow comes with some simple built-in operators like `PythonOperator`, `BashOperator`, `DummyOperator` etc., however, airflow lets you extend the features of a `BaseOperator` and create custom operators. For this project, I developed several custom operators. \n\n![operators](images/operators.png)\n\nThe description of each of these operators follows:\n- **StageToRedshiftOperator**: Stages data to a specific redshift cluster from a specified S3 location. Operator uses templated fields to handle partitioned S3 locations.\n- **LoadFactOperator**: Loads data to the given fact table by running the provided sql statement. Supports delete-insert and append style loads.\n- **LoadDimensionOperator**: Loads data to the given dimension table by running the provided sql statement. Supports delete-insert and append style loads.\n- **SubDagOperator**: Two or more operators can be grouped into one task using the SubDagOperator. Here, I am grouping the tasks of checking if the given table has rows and then run a series of data quality sql commands.\n    - **HasRowsOperator**: Data quality check to ensure that the specified table has rows.\n    - **DataQualityOperator**: Performs data quality checks by running sql statements to validate the data.\n- **SongPopularityOperator**: Calculates the top ten most popular songs for a given interval. The interval is dictated by the DAG schedule.\n- **UnloadToS3Operator**: Stores the analysis result back to the given S3 location.\n\n> Code for each of these operators is located in the **plugins/operators** directory.\n\n**Pipeline Schedule and Data Partitioning**: \nThe events data residing on S3 is partitioned by *year* (2018) and *month* (11). Our task is to incrementally load the event json files, and run it through the entire pipeline to calculate song popularity and store the result back into S3. In this manner, we can obtain the top songs per day in an automated fashion using the pipeline. Please note, this is a trivial analyis, but you can imagine other complex queries that follow similar structure.\n\n*S3 Input events data*:\n```bash\ns3://<bucket>/log_data/2018/11/\n2018-11-01-events.json\n2018-11-02-events.json\n2018-11-03-events.json\n..\n2018-11-28-events.json\n2018-11-29-events.json\n2018-11-30-events.json\n```\n\n*S3 Output song popularity data*:\n```bash\ns3://skuchkula-topsongs/\nsongpopularity_2018-11-01\nsongpopularity_2018-11-02\nsongpopularity_2018-11-03\n...\nsongpopularity_2018-11-28\nsongpopularity_2018-11-29\nsongpopularity_2018-11-30\n```\n\nThe DAG can be configured by giving it some default_args which specify the `start_date`, `end_date` and other design choices which I have mentioned above.\n\n```python\ndefault_args = {\n    'owner': 'shravan',\n    'start_date': datetime(2018, 11, 1),\n    'end_date': datetime(2018, 11, 30),\n    'depends_on_past': False,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'catchup_by_default': False,\n    'provide_context': True,\n}\n```\n\n## How to run this project?\n***Step 1: Create AWS Redshift Cluster using either the console or through the notebook provided in `create-redshift-cluster`***\n\nRun the notebook to create AWS Redshift Cluster. Make a note of:\n- DWN_ENDPOINT ::  dwhcluster.c4m4dhrmsdov.us-west-2.redshift.amazonaws.com\n- DWH_ROLE_ARN ::  arn:aws:iam::506140549518:role/dwhRole\n\n***Step 2: Start Apache Airflow***\n\nRun `docker-compose up` from the directory containing `docker-compose.yml`. Ensure that you have mapped the volume to point to the location where you have your DAGs.\n\n> **NOTE: You can find details of how to manage Apache Airflow on mac here:** https://gist.github.com/shravan-kuchkula/a3f357ff34cf5e3b862f3132fb599cf3\n\n![start_airflow](images/start_airflow....\n    \n    YOUR SUMMARY:\n    ": "This repository details an Apache Airflow project creating an automated ETL pipeline for music streaming data. It extracts JSON data from S3, transforms it into a star schema in Redshift, performs data quality checks, and calculates daily song popularity, saving results back to S3. The project emphasizes building a reusable, production-grade pipeline using custom Airflow operators and incorporating features like automated scheduling and task retries.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    \n# Learning Django with myapp \ud83d\udc0d\n\nWelcome to the Learning Django project with \"myapp\"! This repository is my journey in learning Django, a powerful Python web framework. \ud83d\ude80\n\n![Alt Text](https://imgs.search.brave.com/vM9eQKwHOveCj0C61bw6jPmmXs3dJustYYQyiuDi1bQ/rs:fit:860:0:0/g:ce/aHR0cHM6Ly9taXJv/Lm1lZGl1bS5jb20v/djIvMSp3MWVVWFpZ/amZTdVJzdzI0V0E4/RkxBLmpwZWc)\n\n\n## Project Structure \ud83d\udcc2\n\n- **myapp**: This directory contains your Django application. Inside this directory, you'll find files related to your Django models, views, and templates.\n\n- **templates**: This directory stores HTML templates used to render web pages in your Django application.\n\n- **website**: This directory may contain additional components, apps, or configurations specific to your project's website.\n\n- **db.sqlite3**: This is the SQLite database file for your Django project. Django uses this database by default for local development. \ud83d\uddc3\ufe0f\n\n- **manage.py**: This is the Django management script. You use it to perform various tasks, such as running the development server, creating database tables, and more. \ud83d\udee0\ufe0f\n\n## Tools Used \ud83d\udee0\ufe0f\n\n<table align=\"center\">\n  <tr>\n    <td align=\"center\" width=\"170\">\n      <img src=\"https://github.com/Shubh2-0/Shubh2-0/blob/main/Images/django.png\" width=\"48\" height=\"48\" alt=\"Python\" />\n      <br>django \n    </td>\n     <td align=\"center\" width=\"170\">\n      <img src=\"https://github.com/Shubh2-0/Shubh2-0/blob/main/Images/python.png\" width=\"45\" height=\"45\" alt=\"Github\" />\n      <br>Python\n    </td>\n     <td align=\"center\" width=\"170\">\n      <img src=\"https://github.com/Shubh2-0/EchoCharm/blob/main/images/PyCharm.png\" width=\"45\" height=\"45\" alt=\"PyCharm\" />\n      <br>PyCharm\n    </td>\n    <td align=\"center\" width=\"170\">\n      <img src=\"https://github.com/Shubh2-0/Shubh2-0/blob/main/Images/github.png\" width=\"45\" height=\"45\" alt=\"Github\" />\n      <br>Github\n    </td>\n  </tr>\n \n</table>\n\n## Getting Started \ud83c\udfc1\n\nTo get started with this project, follow these steps:\n\n1. Clone this repository to your local machine:    ``git clone https://github.com/Shubh2-0/Learning_django.git``\n\n2. Navigate to the project directory:    ``cd myapp``\n\n3. Install the project's dependencies:    ``pip install -r requirements.txt``\n\n4. Run the Django development server:    ``python manage.py runserver``\n\n\ud83c\udf10 Access the project in your web browser at http://localhost:8000. \ud83c\udf10\n\n# Usage \ud83d\udea7\nFeel free to explore the project's code and structure to learn more about Django development. You can also contribute to this project by opening issues or submitting pull requests. \ud83e\udd1d\n\n# Contributing \ud83d\ude4c\nContributions are welcome! If you have ideas for improvements or would like to report issues, please create a GitHub issue or submit a pull request following our contribution guidelines. \ud83c\udf89\n\n## \ud83d\udcec Contact\n\nIf you want to contact me, you can reach me through below handles.\n\n <p align=\"left\">\n  <a href=\"https://www.linkedin.com/in/shubham-bhati-787319213/\" target=\"_blank\"><img align=\"center\" src=\"https://skillicons.dev/icons?i=linkedin\" width=\"40px\" alt=\"linkedin\" /></a>&emsp;\n  <a title=\"shubhambhati226@gmail.com\" href=\"mailto:shubhambhati226@gmail.com\" target=\"_blank\"><img align=\"center\"  src=\"https://cdn-icons-png.flaticon.com/128/888/888853.png\"  width=\"40px\"   alt=\"mail-me\" /></a>&emsp;\n  <a href=\"https://wa.me/+916232133187\" target=\"blank\"><img align=\"center\" src=\"https://media2.giphy.com/media/Q8I2fYA773h5wmQQcR/giphy.gif\" width=\"40px\"  alt=\"whatsapp-me\" /></a>&emsp;\t\n </p>\n\n\n\n    \n    YOUR SUMMARY:\n    ": "This repository documents the author's journey learning the Django web framework through a sample project named \"myapp\". It includes the core Django application, HTML templates, and utilizes standard tools like SQLite and `manage.py`. The primary purpose is to serve as a personal learning exercise and demonstrate a basic Django project structure.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <h1 align=\"center\">GitGram</h1>\n<h3 align=\"center\">Applied Social Network Analysis<h3>\n\n#### Hosted App has a bug due to which results never shows up for a user with high number of following or follow ups, Fixing will take time because of other projects rush work. Till then, hit it locally! or maybe try your luck!\n\n\n## Documentation\n- You may read from [here](https://gitgram.herokuapp.com/documentation)\n- Also suggested to read code inside utils folder where file name does'nt start with util\n  - I may be writing docs too quickly and very briefly ignoring code\n  \n## Running web app locally\n- clone the repo locally\n- `pip install -r requirements.txt`\n- `uvicorn server:app --reload`\n\n\n## WebApp ScreenShots\n![image](https://user-images.githubusercontent.com/41824020/97091109-e9c60780-1656-11eb-892a-658770664f8a.png)\n![image](https://user-images.githubusercontent.com/41824020/97091074-a4093f00-1656-11eb-8314-2364296fc729.png)\n![image](https://user-images.githubusercontent.com/41824020/97091083-b4211e80-1656-11eb-9bc0-055d9779a4a3.png)\n![image](https://user-images.githubusercontent.com/41824020/97091282-73c2a000-1658-11eb-87dc-c3ef2792ce14.png)\n![image](https://user-images.githubusercontent.com/41824020/97091556-4971e200-165a-11eb-9791-a30828225bce.png)\n![image](https://user-images.githubusercontent.com/41824020/97091671-1a0fa500-165b-11eb-9465-e9aae2eea97d.png)\n![image](https://user-images.githubusercontent.com/41824020/97091737-b9cd3300-165b-11eb-9806-f0b8507fd4ff.png)\n\n## Some Graphs SS\n![image](https://user-images.githubusercontent.com/41824020/97091015-3e1cb780-1656-11eb-850b-db7eb22709dd.png)\n![image](https://user-images.githubusercontent.com/41824020/97091020-4e349700-1656-11eb-99a4-18553482dcff.png)\n![image](https://user-images.githubusercontent.com/41824020/97091048-7f14cc00-1656-11eb-873a-e3ad514a3cbb.png)\n![image](https://user-images.githubusercontent.com/41824020/97091056-8cca5180-1656-11eb-81c2-63e946e59d09.png)\n\n## PRs Welcomed for\n- [ ] Shift code to use GitHub API\n- [ ] Parallelize the code\n- [ ] Data cruching only once for complete analysis\n- [ ] Add/Fix link to repositories in suggessted repositories\n\n## Note:\n- Be patient because their is hell lot of scraping happening in behind\n  - GitHub api crashes when sending data of someone with above 5k followers, and some bots are their trying to does the same\n  - I can easily control that part with this approach, although later I will shift it to api usage and make it multi threaded\n  - Time Complexity / Scraping for each analysis: `((followers + following) of followers) * ((followers + following) of following)`\n- It's Suggested to run locally\n- Possible reason for heroku bug is request getting timeout, only effective way to handle this is to revamp the web app server code\n\n    \n    YOUR SUMMARY:\n    ": "GitGram performs social network analysis on GitHub users, visualizing their follower and following networks and suggesting repositories. It scrapes GitHub profiles to generate these insights. Due to potential issues with the hosted version for users with many connections, running the application locally is suggested for better performance.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # GitHub unmerged forks finder\n\nLists forks of a repository containing changes not present in the parent repo. Sorts the results according to the amount of stars earned. Can be used to locate prospective forks for dying projects.\n\n    \n    YOUR SUMMARY:\n    ": "This tool finds GitHub forks that contain unique changes not merged into the original repository. It lists these forks and sorts them by star count, helping users discover potentially valuable alternative versions, especially for inactive or abandoned projects.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # Kindle Custom Dictionary Scripts\n\nHere you can find scripts to automate the preparation of the dictionary data. This is a sub-project of [English-Persian Kindle Custom Dictionary](https://github.com/hossein1376/English-Persian-Kindle-Custom-Dictionary).  \nScripts are written in Python and you can modify them based on your needs and database.\n\n## content.py\n\nThe data will be transformed into multiple HTML files based on the given template of [official Amazon guide](https://kdp.amazon.com/en_US/help/topic/G2HXJS944GL88DNV).\n\nWhen you run the script, it will ask for the source file. Both the script and the source file need to be in the same folder.  \nI had a .txt file so I proceeded with that, you may need some modifications based on your situation. As long as the source file is iterable and each entry is on its own seprate line, it should work.\n\nIn the script body, you can change the number of entries per each HTML file (`counter` variable value). I suggest to choose a number that keeps the size of each file under 5mb.  \nAlso, you can hardcode the input and output files path.\n\nWhat the code does is essentially, it reads each line of the source file, detects the main term (entry) and all of its inflections which all are separated with a `|` in my case (ofc you can change it). The term and the definitions are seprated by a tab character, so I introduced the `break_point` variable to let the script know when to stop looking for inflections.  \nThe code calls multiple functions that are defined at the top, most of these are the HTML code that the data is being parsed into.\n\n## content_rtl.py\n\nIt's pretty much same as the content.py file, with the exception that it is intended for RTL language.  \nSince .mobi format has issues with RTL, as in the words are displayed in reverse order, so I preemptively reversed the words' order.\n\nExample:\nUsual text: `\u0633\u0644\u0627\u0645 \u0645\u0646 \u062d\u0633\u06cc\u0646 \u0647\u0633\u062a\u0645`  \nWhat will script will do to it: `\u0647\u0633\u062a\u0645 \u062d\u0633\u06cc\u0646 \u0645\u0646 \u0633\u0644\u0627\u0645`  \nWhat will kindle show: `\u0633\u0644\u0627\u0645 \u0645\u0646 \u062d\u0633\u06cc\u0646 \u0647\u0633\u062a\u0645`\n\nThere is the possibility of issues with this method, so you may need to consider beforehand or modify the script (or your data).\n\n## opf.py\n\nYou will need an .opf file to create your dictionary. Run this script and provide the requested information. You can find the ISO 639-1 codes [from here](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes).  \nThat's pretty much all to it. Put this file in the same folder of HTML file(s) and open it with the Kindle Previewer.  \nCongratulations!\n\n    \n    YOUR SUMMARY:\n    ": "This repository provides Python scripts to automate the creation of custom Kindle dictionaries. The scripts process source data (like .txt files) into Kindle-compatible HTML files, with specific handling for Right-to-Left (RTL) languages. It also includes a script to generate the necessary OPF file for compiling the dictionary using Kindle tools, simplifying the overall process.", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"LLAMA LLM APP\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"llama\", \"llm\", \"app\"]\n```", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <div align=\"center\">\n\u00a0 <a href=\"https://ollama.com\">\n    <img alt=\"ollama\" height=\"200px\" src=\"https://github.com/ollama/ollama/assets/3325447/0d0b44e2-8f4a-4e99-9b52-a5c1c741c8f7\">\n  </a>\n</div>\n\n# Ollama\n\nGet up and running with large language models.\n\n### macOS\n\n[Download](https://ollama.com/download/Ollama-darwin.zip)\n\n### Windows\n\n[Download](https://ollama.com/download/OllamaSetup.exe)\n\n### Linux\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n[Manual install instructions](https://github.com/ollama/ollama/blob/main/docs/linux.md)\n\n### Docker\n\nThe official [Ollama Docker image](https://hub.docker.com/r/ollama/ollama) `ollama/ollama` is available on Docker Hub.\n\n### Libraries\n\n- [ollama-python](https://github.com/ollama/ollama-python)\n- [ollama-js](https://github.com/ollama/ollama-js)\n\n### Community\n\n- [Discord](https://discord.gg/ollama)\n- [Reddit](https://reddit.com/r/ollama)\n\n## Quickstart\n\nTo run and chat with [Llama 3.2](https://ollama.com/library/llama3.2):\n\n```shell\nollama run llama3.2\n```\n\n## Model library\n\nOllama supports a list of models available on [ollama.com/library](https://ollama.com/library 'ollama model library')\n\nHere are some example models that can be downloaded:\n\n| Model              | Parameters | Size  | Download                         |\n| ------------------ | ---------- | ----- | -------------------------------- |\n| Gemma 3            | 1B         | 815MB | `ollama run gemma3:1b`           |\n| Gemma 3            | 4B         | 3.3GB | `ollama run gemma3`              |\n| Gemma 3            | 12B        | 8.1GB | `ollama run gemma3:12b`          |\n| Gemma 3            | 27B        | 17GB  | `ollama run gemma3:27b`          |\n| QwQ                | 32B        | 20GB  | `ollama run qwq`                 |\n| DeepSeek-R1        | 7B         | 4.7GB | `ollama run deepseek-r1`         |\n| DeepSeek-R1        | 671B       | 404GB | `ollama run deepseek-r1:671b`    |\n| Llama 3.3          | 70B        | 43GB  | `ollama run llama3.3`            |\n| Llama 3.2          | 3B         | 2.0GB | `ollama run llama3.2`            |\n| Llama 3.2          | 1B         | 1.3GB | `ollama run llama3.2:1b`         |\n| Llama 3.2 Vision   | 11B        | 7.9GB | `ollama run llama3.2-vision`     |\n| Llama 3.2 Vision   | 90B        | 55GB  | `ollama run llama3.2-vision:90b` |\n| Llama 3.1          | 8B         | 4.7GB | `ollama run llama3.1`            |\n| Llama 3.1          | 405B       | 231GB | `ollama run llama3.1:405b`       |\n| Phi 4              | 14B        | 9.1GB | `ollama run phi4`                |\n| Phi 4 Mini         | 3.8B       | 2.5GB | `ollama run phi4-mini`           |\n| Mistral            | 7B         | 4.1GB | `ollama run mistral`             |\n| Moondream 2        | 1.4B       | 829MB | `ollama run moondream`           |\n| Neural Chat        | 7B         | 4.1GB | `ollama run neural-chat`         |\n| Starling           | 7B         | 4.1GB | `ollama run starling-lm`         |\n| Code Llama         | 7B         | 3.8GB | `ollama run codellama`           |\n| Llama 2 Uncensored | 7B         | 3.8GB | `ollama run llama2-uncensored`   |\n| LLaVA              | 7B         | 4.5GB | `ollama run llava`               |\n| Granite-3.2         | 8B         | 4.9GB | `ollama run granite3.2`          |\n\n> [!NOTE]\n> You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n\n## Customize a model\n\n### Import from GGUF\n\nOllama supports importing GGUF models in the Modelfile:\n\n1. Create a file named `Modelfile`, with a `FROM` instruction with the local filepath to the model you want to import.\n\n   ```\n   FROM ./vicuna-33b.Q4_0.gguf\n   ```\n\n2. Create the model in Ollama\n\n   ```shell\n   ollama create example -f Modelfile\n   ```\n\n3. Run the model\n\n   ```shell\n   ollama run example\n   ```\n\n### Import from Safetensors\n\nSee the [guide](docs/import.md) on importing models for more information.\n\n### Customize a prompt\n\nModels from the Ollama library can be customized with a prompt. For example, to customize the `llama3.2` model:\n\n```shell\nollama pull llama3.2\n```\n\nCreate a `Modelfile`:\n\n```\nFROM llama3.2\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n\n# set the system message\nSYSTEM \"\"\"\nYou are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n\"\"\"\n```\n\nNext, create and run the model:\n\n```\nollama create mario -f ./Modelfile\nollama run mario\n>>> hi\nHello! It's your friend Mario.\n```\n\nFor more information on working with a Modelfile, see the [Modelfile](docs/modelfile.md) documentation.\n\n## CLI Reference\n\n### Create a model\n\n`ollama create` is used to create a model from a Modelfile.\n\n```shell\nollama create mymodel -f ./Modelfile\n```\n\n### Pull a model\n\n```shell\nollama pull llama3.2\n```\n\n> This command can also be used to update a local model. Only the diff will be pulled.\n\n### Remove a model\n\n```shell\nollama rm llama3.2\n```\n\n### Copy a model\n\n```shell\nollama cp llama3.2 my-model\n```\n\n### Multiline input\n\nFor multiline input, you can wrap text with `\"\"\"`:\n\n```\n>>> \"\"\"Hello,\n... world!\n... \"\"\"\nI'm a basic program that prints the famous \"Hello, world!\" message to the console.\n```\n\n### Multimodal models\n\n```\nollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n```\n\n> **Output**: The image features a yellow smiley face, which is likely the central focus of the picture.\n\n### Pass the prompt as an argument\n\n```shell\nollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n```\n\n> **Output**: Ollama is a lightweight, extensible framework for building and running language models on the local machine. It provides a simple API for creating, running, and managing models, as well as a library of pre-built models that can be easily used in a variety of applications.\n\n### Show model information\n\n```shell\nollama show llama3....\n    \n    YOUR SUMMARY:\n    ": "Ollama is a tool designed to easily run large language models (LLMs) like Llama 3, Gemma, and others directly on your local machine (macOS, Windows, Linux, Docker). It provides a simple command-line interface to download, manage, run, and customize these models. The primary purpose is to simplify the process of setting up and interacting with LLMs locally.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # llama.cpp\n\n![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)\n\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Server](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg)](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml)\n\n[Roadmap](https://github.com/users/ggerganov/projects/7) / [Project status](https://github.com/ggml-org/llama.cpp/discussions/3471) / [Manifesto](https://github.com/ggml-org/llama.cpp/discussions/205) / [ggml](https://github.com/ggml-org/ggml)\n\nInference of Meta's [LLaMA](https://arxiv.org/abs/2302.13971) model (and others) in pure C/C++\n\n## Recent API changes\n\n- [Changelog for `libllama` API](https://github.com/ggml-org/llama.cpp/issues/9289)\n- [Changelog for `llama-server` REST API](https://github.com/ggml-org/llama.cpp/issues/9291)\n\n## Hot topics\n\n- A new binary `llama-mtmd-cli` is introduced to replace `llava-cli`, `minicpmv-cli` and `gemma3-cli` https://github.com/ggml-org/llama.cpp/pull/13012, `libllava` will be deprecated\n- **How to use [MTLResidencySet](https://developer.apple.com/documentation/metal/mtlresidencyset?language=objc) to keep the GPU memory active?** https://github.com/ggml-org/llama.cpp/pull/11427\n- **VS Code extension for FIM completions:** https://github.com/ggml-org/llama.vscode\n- Universal [tool call support](./docs/function-calling.md) in `llama-server` https://github.com/ggml-org/llama.cpp/pull/9639\n- Vim/Neovim plugin for FIM completions: https://github.com/ggml-org/llama.vim\n- Introducing GGUF-my-LoRA https://github.com/ggml-org/llama.cpp/discussions/10123\n- Hugging Face Inference Endpoints now support GGUF out of the box! https://github.com/ggml-org/llama.cpp/discussions/9669\n- Hugging Face GGUF editor: [discussion](https://github.com/ggml-org/llama.cpp/discussions/9268) | [tool](https://huggingface.co/spaces/CISCai/gguf-editor)\n\n----\n\n## Description\n\nThe main goal of `llama.cpp` is to enable LLM inference with minimal setup and state-of-the-art performance on a wide\nrange of hardware - locally and in the cloud.\n\n- Plain C/C++ implementation without any dependencies\n- Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n- AVX, AVX2, AVX512 and AMX support for x86 architectures\n- 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\n- Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)\n- Vulkan and SYCL backend support\n- CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\n\nThe `llama.cpp` project is the main playground for developing new features for the [ggml](https://github.com/ggml-org/ggml) library.\n\n<details>\n<summary>Models</summary>\n\nTypically finetunes of the base models below are supported as well.\n\nInstructions for adding support for new models: [HOWTO-add-model.md](docs/development/HOWTO-add-model.md)\n\n#### Text-only\n\n- [X] LLaMA \ud83e\udd99\n- [x] LLaMA 2 \ud83e\udd99\ud83e\udd99\n- [x] LLaMA 3 \ud83e\udd99\ud83e\udd99\ud83e\udd99\n- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)\n- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)\n- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)\n- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)\n- [X] [BERT](https://github.com/ggml-org/llama.cpp/pull/5423)\n- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)\n- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)\n- [X] [Starcoder models](https://github.com/ggml-org/llama.cpp/pull/3187)\n- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)\n- [X] [MPT](https://github.com/ggml-org/llama.cpp/pull/3417)\n- [X] [Bloom](https://github.com/ggml-org/llama.cpp/pull/3553)\n- [x] [Yi models](https://huggingface.co/models?search=01-ai/Yi)\n- [X] [StableLM models](https://huggingface.co/stabilityai)\n- [x] [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)\n- [x] [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)\n- [x] [PLaMo-13B](https://github.com/ggml-org/llama.cpp/pull/3557)\n- [x] [Phi models](https://huggingface.co/models?search=microsoft/phi)\n- [x] [PhiMoE](https://github.com/ggml-org/llama.cpp/pull/11003)\n- [x] [GPT-2](https://huggingface.co/gpt2)\n- [x] [Orion 14B](https://github.com/ggml-org/llama.cpp/pull/5118)\n- [x] [InternLM2](https://huggingface.co/models?search=internlm2)\n- [x] [CodeShell](https://github.com/WisdomShell/codeshell)\n- [x] [Gemma](https://ai.google.dev/gemma)\n- [x] [Mamba](https://github.com/state-spaces/mamba)\n- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)\n- [x] [Xverse](https://huggingface.co/models?search=xverse)\n- [x] [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)\n- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)\n- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)\n- [x] [OLMo](https://allenai.org/olmo)\n- [x] [OLMo 2](https://allenai.org/olmo)\n- [x] [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)\n- [x] [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)\n- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)\n- [x] [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)\n- [x] [Smaug](https://huggingface.co/models?...\n    \n    YOUR SUMMARY:\n    ": "`llama.cpp` enables inference of Meta's LLaMA and other large language models using pure C/C++. Its purpose is to provide high-performance LLM execution with minimal setup across a wide range of hardware, including CPUs (Apple Silicon, x86) and GPUs (NVIDIA, AMD, Apple Metal). Key features include various quantization methods for speed and memory efficiency, platform-specific optimizations (like ARM NEON, AVX, Metal, CUDA), and broad model compatibility.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    ## **Note of deprecation**\n\nThank you for developing with Llama models. As part of the Llama 3.1 release, we\u2019ve consolidated GitHub repos and added some additional repos as we\u2019ve expanded Llama\u2019s functionality into being an e2e Llama Stack. Please use the following repos going forward:\n- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies\n- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations \n- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations\n- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications\n- [llama-cookbook](https://github.com/meta-llama/llama-recipes) - Community driven scripts and integrations\n\nIf you have any questions, please feel free to file an issue on any of the above repos and we will do our best to respond in a timely manner. \n\nThank you!\n\n\n# (Deprecated) Llama 2\n\nWe are unlocking the power of large language models. Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly. \n\nThis release includes model weights and starting code for pre-trained and fine-tuned Llama language models \u2014 ranging from 7B to 70B parameters.\n\nThis repository is intended as a minimal example to load [Llama 2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) models and run inference. For more detailed examples leveraging Hugging Face, see [llama-cookbook](https://github.com/facebookresearch/llama-recipes/).\n\n## Updates post-launch\n\nSee [UPDATES.md](UPDATES.md). Also for a running list of frequently asked questions, see [here](https://ai.meta.com/llama/faq/).\n\n## Download\n\nIn order to download the model weights and tokenizer, please visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download.\n\nPre-requisites: Make sure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n\nKeep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n\n### Access to Hugging Face\n\nWe are also providing downloads on [Hugging Face](https://huggingface.co/meta-llama). You can request access to the models by acknowledging the license and filling the form in the model card of a repo. After doing so, you should get access to all the Llama models of a version (Code Llama, Llama 2, or Llama Guard) within 1 hour.\n\n## Quick Start\n\nYou can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the [Llama 2 cookbook repository](https://github.com/facebookresearch/llama-recipes). \n\n1. In a conda env with PyTorch / CUDA available clone and download this repository.\n\n2. In the top-level directory run:\n    ```bash\n    pip install -e .\n    ```\n3. Visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and register to download the model/s.\n\n4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n\n5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script. \n    - Make sure to grant execution permissions to the download.sh script\n    - During this process, you will be prompted to enter the URL from the email. \n    - Do not use the \u201cCopy Link\u201d option but rather make sure to manually copy the link from the email.\n\n6. Once the model/s you want have been downloaded, you can run the model locally using the command below:\n```bash\ntorchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir llama-2-7b-chat/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\n```\n**Note**\n- Replace  `llama-2-7b-chat/` with the path to your checkpoint directory and `tokenizer.model` with the path to your tokenizer model.\n- The `\u2013nproc_per_node` should be set to the [MP](#inference) value for the model you are using.\n- Adjust the `max_seq_len` and `max_batch_size` parameters as needed.\n- This example runs the [example_chat_completion.py](example_chat_completion.py) found in this repository but you can change that to a different .py file.\n\n## Inference\n\nDifferent models require different model-parallel (MP) values:\n\n|  Model | MP |\n|--------|----|\n| 7B     | 1  |\n| 13B    | 2  |\n| 70B    | 8  |\n\nAll models support sequence length up to 4096 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware.\n\n### Pretrained Models\n\nThese models are not finetuned for chat or Q&A. They should be prompted so that the expected answer is the natural continuation of the prompt.\n\nSee `example_text_completion.py` for some examples. To illustrate, see the command below to run it with the llama-2-7b model (`nproc_per_node` needs to be set to the `MP` value):\n\n```\ntorchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir llama-2-7b/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n```\n\n### Fine-tuned Chat Models\n\nThe fine-tuned models were trained for dialogue applications....\n    \n    YOUR SUMMARY:\n    ": "This repository, now **deprecated**, originally provided minimal code examples for downloading and running inference with Llama 2 models (7B to 70B). Following the Llama 3.1 release, its functionality is superseded by newer, consolidated repositories like `llama-models` and `llama-toolchain`. Users should now utilize these updated repositories for Llama models and development tools.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    ![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-429-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)](https://github.com/hiyouga/LLaMA-Factory/pulls)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](https://dcbadge.vercel.app/api/server/rKfvV9r9FK?compact=true&style=flat)](https://discord.gg/rKfvV9r9FK)\n[![GitCode](https://gitcode.com/zhengyaowei/LLaMA-Factory/star/badge.svg)](https://gitcode.com/zhengyaowei/LLaMA-Factory)\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](https://gallery.pai-ml.com/assets/open-in-dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Spaces](https://img.shields.io/badge/\ud83e\udd17-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![SageMaker](https://img.shields.io/badge/SageMaker-Open%20in%20AWS-blue)](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n\n<h3 align=\"center\">\n    Easily fine-tune 100+ large language models with zero-code <a href=\"#quickstart\">CLI</a> and <a href=\"#fine-tuning-with-llama-board-gui-powered-by-gradio\">Web UI</a>\n</h3>\n<p align=\"center\">\n    <picture>\n        <img alt=\"Github trend\" src=\"https://trendshift.io/api/badge/repositories/4535\">\n    </picture>\n</p>\n\n\ud83d\udc4b Join our [WeChat](assets/wechat.jpg) or [NPU user group](assets/wechat_npu.jpg).\n\n\\[ English | [\u4e2d\u6587](README_zh.md) \\]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nChoose your path:\n\n- **Documentation**: https://llamafactory.readthedocs.io/en/latest/\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **Local machine**: Please refer to [usage](#getting-started)\n- **PAI-DSW (free trial)**: [Llama3 Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) | [Qwen2-VL Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) | [DeepSeek-R1-Distill Example](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b)\n- **Amazon SageMaker**: [Blog](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/)\n- **Easy Dataset**: [Fine-tune on Synthetic Data](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g)\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Benchmark](#benchmark)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc....\n    \n    YOUR SUMMARY:\n    ": "LLaMA Factory is a user-friendly platform designed for easily fine-tuning over 100 large language models like LLaMA, Mistral, and Qwen with zero code. It supports various training methods (SFT, PPO, DPO, etc.), efficient techniques like LoRA and QLoRA, and advanced algorithms. The repository offers both a command-line interface (CLI) and a web UI (LLaMA Board) for convenient model customization.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://blog.vllm.ai/\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://x.com/vllm_project\"><b>Twitter/X</b></a> | <a href=\"https://discuss.vllm.ai\"><b>User Forum</b></a> | <a href=\"https://slack.vllm.ai\"><b>Developer Slack</b></a> |\n</p>\n\n---\n\n*Latest News* \ud83d\udd25\n- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).\n- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).\n- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).\n- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).\n- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.\n- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).\n- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n\n<details>\n<summary>Previous News</summary>\n\n- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/06] We officially released vLLM!...\n    \n    YOUR SUMMARY:\n    ": "vLLM is a library designed for easy, fast, and cost-effective serving of Large Language Models (LLMs). Its main purpose is to optimize LLM inference, providing high throughput and efficiency. Key features include state-of-the-art serving performance and seamless integration with popular Hugging Face models.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # \ud83d\uddc2\ufe0f LlamaIndex \ud83e\udd99\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-index)](https://pypi.org/project/llama-index/)\n[![GitHub contributors](https://img.shields.io/github/contributors/jerryjliu/llama_index)](https://github.com/jerryjliu/llama_index/graphs/contributors)\n[![Discord](https://img.shields.io/discord/1059199217496772688)](https://discord.gg/dGcwcsnxhU)\n[![Reddit](https://img.shields.io/reddit/subreddit-subscribers/LlamaIndex?style=plastic&logo=reddit&label=r%2FLlamaIndex&labelColor=white)](https://www.reddit.com/r/LlamaIndex/)\n[![Ask AI](https://img.shields.io/badge/Phorm-Ask_AI-%23F2777A.svg?&logo=data:image/svg+xml;base64,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)](https://www.phorm.ai/query?projectId=c5863b56-6703-4a5d-87b6-7e6031bf16b6)\n\nLlamaIndex (GPT Index) is a data framework for your LLM application. Building with LlamaIndex typically involves working with LlamaIndex core and a chosen set of integrations (or plugins). There are two ways to start building with LlamaIndex in\nPython:\n\n1. **Starter**: [`llama-index`](https://pypi.org/project/llama-index/). A starter Python package that includes core LlamaIndex as well as a selection of integrations.\n\n2. **Customized**: [`llama-index-core`](https://pypi.org/project/llama-index-core/). Install core LlamaIndex and add your chosen LlamaIndex integration packages on [LlamaHub](https://llamahub.ai/)\n   that are required for your application. There are over 300 LlamaIndex integration\n   packages that work seamlessly with core, allowing you to build with your preferred\n   LLM, embedding, and vector store providers.\n\nThe LlamaIndex Python library is namespaced such that import statements which\ninclude `core` imply that the core package is being used. In contrast, those\nstatements without `core` imply that an integration package is being used.\n\n```python\n# typical pattern\nfrom llama_index.core.xxx import ClassABC  # core submodule xxx\nfrom llama_index.xxx.yyy import (\n    SubclassABC,\n)  # integration yyy for submodule xxx\n\n# concrete example\nfrom llama_index.core.llms import LLM\nfrom llama_index.llms.openai import OpenAI\n```\n\n### Important Links\n\nLlamaIndex.TS [(Typescript/Javascript)](https://github.com/run-llama/LlamaIndexTS)\n\n[Documentation](https://docs.llamaindex.ai/en/stable/)\n\n[X (formerly Twitter)](https://x.com/llama_index)\n\n[LinkedIn](https://www.linkedin.com/company/llamaindex/)\n\n[Reddit](https://www.reddit.com/r/LlamaIndex/)\n\n[Discord](https://discord.gg/dGcwcsnxhU)\n\n### Ecosystem\n\n- LlamaHub [(community library of data loaders)](https://llamahub.ai)\n- LlamaLab [(cutting-edge AGI projects using LlamaIndex)](https://github.com/run-llama/llama-lab)\n\n## \ud83d\ude80 Overview\n\n**NOTE**: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!\n\n### Context\n\n- LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data.\n- How do we best augment LLMs with our own private data?...\n    \n    YOUR SUMMARY:\n    ": "LlamaIndex is a data framework for building LLM applications, specifically designed to help connect Large Language Models with private data sources. It offers a core library and numerous integrations (plugins) available via LlamaHub, allowing developers to customize connections to different LLMs, embedding models, and vector stores. Users can start with a basic package or build a tailored setup using `llama-index-core` and specific integrations.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    # Quivr - Your Second Brain, Empowered by Generative AI\n\n<div align=\"center\">\n    <img src=\"./logo.png\" alt=\"Quivr-logo\" width=\"31%\"  style=\"border-radius: 50%; padding-bottom: 20px\"/>\n</div>\n\n[![Discord Follow](https://dcbadge.vercel.app/api/server/HUpRgp2HG8?style=flat)](https://discord.gg/HUpRgp2HG8)\n[![GitHub Repo stars](https://img.shields.io/github/stars/quivrhq/quivr?style=social)](https://github.com/quivrhq/quivr)\n[![Twitter Follow](https://img.shields.io/twitter/follow/StanGirard?style=social)](https://twitter.com/_StanGirard)\n\nQuivr, helps you build your second brain, utilizes the power of GenerativeAI to be your personal assistant !\n\n## Key Features \ud83c\udfaf\n\n- **Opiniated RAG**: We created a RAG that is opinionated, fast and efficient so you can focus on your product\n- **LLMs**: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc.\n- **Any File**: Quivr works with any file, you can use it with PDF, TXT, Markdown, etc and even add your own parsers.\n- **Customize your RAG**: Quivr allows you to customize your RAG, add internet search, add tools, etc.\n- **Integrations with Megaparse**: Quivr works with [Megaparse](https://github.com/quivrhq/megaparse), so you can ingest your files with Megaparse and use the RAG with Quivr.\n\n>We take care of the RAG so you can focus on your product. Simply install quivr-core and add it to your project. You can now ingest your files and ask questions.*\n\n**We will be improving the RAG and adding more features, stay tuned!**\n\n\nThis is the core of Quivr, the brain of Quivr.com.\n\n<!-- ## Demo Highlight \ud83c\udfa5\n\nhttps://github.com/quivrhq/quivr/assets/19614572/a6463b73-76c7-4bc0-978d-70562dca71f5 -->\n\n## Getting Started \ud83d\ude80\n\nYou can find everything on the [documentation](https://core.quivr.com/).\n\n### Prerequisites \ud83d\udccb\n\nEnsure you have the following installed:\n\n- Python 3.10 or newer\n\n### 30 seconds Installation \ud83d\udcbd\n\n\n- **Step 1**: Install the package\n\n  \n\n  ```bash\n  pip install quivr-core # Check that the installation worked\n  ```\n\n\n- **Step 2**: Create a RAG with 5 lines of code\n\n  ```python\n  import tempfile\n\n  from quivr_core import Brain\n\n  if __name__ == \"__main__\":\n      with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\") as temp_file:\n          temp_file.write(\"Gold is a liquid of blue-like colour.\")\n          temp_file.flush()\n\n          brain = Brain.from_files(\n              name=\"test_brain\",\n              file_paths=[temp_file.name],\n          )\n\n          answer = brain.ask(\n              \"what is gold? asnwer in french\"\n          )\n          print(\"answer:\", answer)\n  ```\n## Configuration\n\n### Workflows\n\n#### Basic RAG\n\n![](docs/docs/workflows/examples/basic_rag.excalidraw.png)\n\n\nCreating a basic RAG workflow like the one above is simple, here are the steps:\n\n\n1. Add your API Keys to your environment variables\n```python\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"myopenai_apikey\"\n\n```\nQuivr supports APIs from Anthropic, OpenAI, and Mistral. It also supports local models using Ollama.\n\n1. Create the YAML file ``basic_rag_workflow.yaml`` and copy the following content in it\n```yaml\nworkflow_config:\n  name: \"standard RAG\"\n  nodes:\n    - name: \"START\"\n      edges: [\"filter_history\"]\n\n    - name: \"filter_history\"\n      edges: [\"rewrite\"]\n\n    - name: \"rewrite\"\n      edges: [\"retrieve\"]\n\n    - name: \"retrieve\"\n      edges: [\"generate_rag\"]\n\n    - name: \"generate_rag\" # the name of the last node, from which we want to stream the answer to the user\n      edges: [\"END\"]\n\n# Maximum number of previous conversation iterations\n# to include in the context of the answer\nmax_history: 10\n\n# Reranker configuration\nreranker_config:\n  # The reranker supplier to use\n  supplier: \"cohere\"\n\n  # The model to use for the reranker for the given supplier\n  model: \"rerank-multilingual-v3.0\"\n\n  # Number of chunks returned by the reranker\n  top_n: 5\n\n# Configuration for the LLM\nllm_config:\n\n  # maximum number of tokens passed to the LLM to generate the answer\n  max_input_tokens: 4000\n\n  # temperature for the LLM\n  temperature: 0.7\n```\n\n3. Create a Brain with the default configuration\n```python\nfrom quivr_core import Brain\n\nbrain = Brain.from_files(name = \"my smart brain\",\n                        file_paths = [\"./my_first_doc.pdf\", \"./my_second_doc.txt\"],\n                        )\n\n```\n\n4. Launch a Chat\n```python\nbrain.print_info()\n\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\nfrom quivr_core.config import RetrievalConfig\n\nconfig_file_name = \"./basic_rag_workflow.yaml\"\n\nretrieval_config = RetrievalConfig.from_yaml(config_file_name)\n\nconsole = Console()\nconsole.print(Panel.fit(\"Ask your brain !\", style=\"bold magenta\"))\n\nwhile True:\n    # Get user input\n    question = Prompt.ask(\"[bold cyan]Question[/bold cyan]\")\n\n    # Check if user wants to exit\n    if question.lower() == \"exit\":\n        console.print(Panel(\"Goodbye!\", style=\"bold yellow\"))\n        break\n\n    answer = brain.ask(question, retrieval_config=retrieval_config)\n    # Print the answer with typing effect\n    console.print(f\"[bold green]Quivr Assistant[/bold green]: {answer.answer}\")\n\n    console.print(\"-\" * console.width)\n\nbrain.print_info()\n```\n\n5. You are now all set up to talk with your brain and test different retrieval strategies by simply changing the configuration file!\n\n## Go further\n\nYou can go further with Quivr by adding internet search, adding tools, etc. Check the [documentation](https://core.quivr.com/) for more information.\n\n\n## Contributors \u2728\n\nThanks go to these wonderful people:\n<a href=\"https://github.com/quivrhq/quivr/graphs/contributors\">\n<img src=\"https://contrib.rocks/image?repo=quivrhq/quivr\" />\n</a>\n\n## Contribute \ud83e\udd1d\n\nDid you get a pull request? Open it, and we'll review it as soon as possible. Check out our project board [here](https://github.com/users/StanGirard/projects/5) to see what we're currently focused on, and feel free to bring your fresh ideas to the table!\n\n- [Open Issues](https://github....\n    \n    YOUR SUMMARY:\n    ": "This repository contains Quivr-core, the engine for the Quivr \"second brain\" application, designed as a personal AI assistant. It provides an opinionated, customizable Retrieval-Augmented Generation (RAG) system compatible with various LLMs (like OpenAI, Mistral) and file types (PDF, TXT). Its purpose is to offer developers a fast, easy-to-integrate RAG core for building AI-powered question-answering features into their own applications.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <div align=\"center\">\n\n  <a href=\"https://unsloth.ai\"><picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20white%20text.png\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\">\n    <img alt=\"unsloth logo\" src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20logo%20black%20text.png\" height=\"110\" style=\"max-width: 100%;\">\n  </picture></a>\n  \n<a href=\"https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/start free finetune button.png\" height=\"48\"></a>\n<a href=\"https://discord.com/invite/unsloth\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord button.png\" height=\"48\"></a>\n<a href=\"https://docs.unsloth.ai\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/Documentation%20Button.png\" height=\"48\"></a>\n\n### Finetune Llama 4, Gemma 3, Phi-4, Qwen 2.5 & Mistral 2x faster with 80% less VRAM!\n\n![](https://i.ibb.co/sJ7RhGG/image-41.png)\n\n</div>\n\n## \u2728 Finetune for Free\n\nNotebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add your dataset, click \"Run All\", and export your finetuned model to GGUF, Ollama, vLLM or Hugging Face.\n\n| Unsloth supports | Free Notebooks | Performance | Memory use |\n|-----------|---------|--------|----------|\n| **GRPO (R1 reasoning)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Gemma 3 (4B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb)               | 1.6x faster | 60% less |\n| **Llama 3.2 (3B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2x faster | 70% less |\n| **Phi-4 (14B)** | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4-Conversational.ipynb)               | 2x faster | 70% less |\n| **Llama 3.2 Vision (11B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 50% less |\n| **Llama 3.1 (8B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2x faster | 70% less |\n| **Qwen 2.5 (7B)**      | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 70% less |\n| **Mistral v0.3 (7B)**    | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 75% less |\n| **Ollama**     | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)               | 1.9x faster | 60% less |\n| **DPO Zephyr**     | [\u25b6\ufe0f Start for free](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Zephyr_(7B)-DPO.ipynb)               | 1.9x faster | 50% less |\n\n- See [all our notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks) and [all our models](https://docs.unsloth.ai/get-started/all-our-models)\n- **Kaggle Notebooks** for [Llama 3.2 Kaggle notebook](https://www.kaggle.com/danielhanchen/kaggle-llama-3-2-1b-3b-unsloth-notebook), [Llama 3.1 (8B)](https://www.kaggle.com/danielhanchen/kaggle-llama-3-1-8b-unsloth-notebook), [Phi-4 (14B)](https://www.kaggle.com/code/danielhanchen/phi-4-finetuning-unsloth-notebook), [Mistral (7B)](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n- See detailed documentation for Unsloth [here](https://docs.unsloth.ai/).\n\n## \u26a1 Quickstart\n\n- **Install with pip (recommended)** for Linux devices:\n```\npip install unsloth\n```\nFor Windows install instructions, see [here](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).\n\n## \ud83e\udda5 Unsloth.ai News\n- \ud83d\udce3 NEW! **[Llama 4](https://unsloth.ai/blog/llama4)**, Meta's latest models including Scout & Maverick are now supported.\n- \ud83d\udce3 NEW! [**EVERYTHING** is now supported](https://unsloth.ai/blog/gemma3#everything) incuding: FFT, ALL models (Mixtral, MOE, Cohere, Mamba) and all training algorithms (KTO, DoRA) etc. MultiGPU support coming very soon.\n  To enable full-finetuning, set ```full_finetuning = True``` and for 8-bit finetuning, set ```load_in_8bit = True```\n- \ud83d\udce3 NEW! **Gemma 3** by Google: [Read Blog](https://unsloth.ai/blog/gemma3). We [uploaded GGUFs, 4-bit models](https://huggingface.co/collections/unsloth/gemma-3-67d12b7e8816ec6efa7e4e5b).\n- \ud83d\udce3 NEW! Introducing Long-context [Reasoning (GRPO)](https://unsloth.ai/blog/grpo) in Unsloth. Train your own reasoning model with just 5GB VRAM. Transform Llama, Phi, Mistral etc. into reasoning LLMs!\n- \ud83d\udce3 NEW! [DeepSeek-R1](https://unsloth.ai/blog/deepseek-r1) - the most powerful open reasoning models with Llama & Qwen distillations. Run or fine-tune them now [with our guide](https://unsloth.ai/blog/deepseek-r1). All model uploads: [here](https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5).\n- \ud83d\udce3 NEW! [Phi-4](https://unsloth.ai/blog/phi4) by Microsoft: We also [fixed bugs](https://unsloth.ai/blog/phi4) in Phi-4 and [uploaded GGUFs, 4-bit](https://huggingface.co/collections/unsloth/phi-4-all-versions-677eecf93784e61afe762afa).\n- \ud83d\udce3 Introducing Unsloth [Dynamic 4-bit Quantization](https://unsloth.ai/blog/dynamic-4bit)!...\n    \n    YOUR SUMMARY:\n    ": "This repository, Unsloth, provides tools to finetune large language models like Llama, Gemma, Phi, and Mistral significantly faster (up to 2x) and with drastically reduced VRAM usage (up to 80%). Its primary purpose is to make LLM finetuning more accessible and efficient, offering beginner-friendly notebooks and supporting export to various formats like GGUF and Ollama.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    ![](docs/img/logo-long-chatchat-trans-v2.png)\n<a href=\"https://trendshift.io/repositories/329\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/329\" alt=\"chatchat-space%2FLangchain-Chatchat | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n\n[![pypi badge](https://img.shields.io/pypi/v/langchain-chatchat.svg)](https://shields.io/)\n[![Generic badge](https://img.shields.io/badge/python-3.8%7C3.9%7C3.10%7C3.11-blue.svg)](https://pypi.org/project/pypiserver/)\n\n\ud83c\udf0d [READ THIS IN ENGLISH](README_en.md)\n\n\ud83d\udcc3 **LangChain-Chatchat** (\u539f Langchain-ChatGLM)\n\n\u57fa\u4e8e ChatGLM \u7b49\u5927\u8bed\u8a00\u6a21\u578b\u4e0e Langchain \u7b49\u5e94\u7528\u6846\u67b6\u5b9e\u73b0\uff0c\u5f00\u6e90\u3001\u53ef\u79bb\u7ebf\u90e8\u7f72\u7684 RAG \u4e0e Agent \u5e94\u7528\u9879\u76ee\u3002\n\n---\n\n## \u76ee\u5f55\n\n* [\u6982\u8ff0](README.md#\u6982\u8ff0)\n* [\u529f\u80fd\u4ecb\u7ecd](README.md#\u529f\u80fd\u4ecb\u7ecd)\n    * [0.3.x \u529f\u80fd\u4e00\u89c8](README.md#03x-\u7248\u672c\u529f\u80fd\u4e00\u89c8)\n    * [\u5df2\u652f\u6301\u7684\u6a21\u578b\u63a8\u7406\u6846\u67b6\u4e0e\u6a21\u578b](README.md#\u5df2\u652f\u6301\u7684\u6a21\u578b\u90e8\u7f72\u6846\u67b6\u4e0e\u6a21\u578b)\n* [\u5feb\u901f\u4e0a\u624b](README.md#\u5feb\u901f\u4e0a\u624b)\n    * [pip \u5b89\u88c5\u90e8\u7f72](README.md#pip-\u5b89\u88c5\u90e8\u7f72)\n    * [\u6e90\u7801\u5b89\u88c5\u90e8\u7f72/\u5f00\u53d1\u90e8\u7f72](README.md#\u6e90\u7801\u5b89\u88c5\u90e8\u7f72\u5f00\u53d1\u90e8\u7f72)\n    * [Docker \u90e8\u7f72](README.md#docker-\u90e8\u7f72)\n* [\u9879\u76ee\u91cc\u7a0b\u7891](README.md#\u9879\u76ee\u91cc\u7a0b\u7891)\n* [\u8054\u7cfb\u6211\u4eec](README.md#\u8054\u7cfb\u6211\u4eec)\n\n## \u6982\u8ff0\n\n\ud83e\udd16\ufe0f \u4e00\u79cd\u5229\u7528 [langchain](https://github.com/langchain-ai/langchain)\n\u601d\u60f3\u5b9e\u73b0\u7684\u57fa\u4e8e\u672c\u5730\u77e5\u8bc6\u5e93\u7684\u95ee\u7b54\u5e94\u7528\uff0c\u76ee\u6807\u671f\u671b\u5efa\u7acb\u4e00\u5957\u5bf9\u4e2d\u6587\u573a\u666f\u4e0e\u5f00\u6e90\u6a21\u578b\u652f\u6301\u53cb\u597d\u3001\u53ef\u79bb\u7ebf\u8fd0\u884c\u7684\u77e5\u8bc6\u5e93\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\u3002\n\n\ud83d\udca1 \u53d7 [GanymedeNil](https://github.com/GanymedeNil) \u7684\u9879\u76ee [document.ai](https://github.com/GanymedeNil/document.ai)\n\u548c [AlexZhangji](https://github.com/AlexZhangji)\n\u521b\u5efa\u7684 [ChatGLM-6B Pull Request](https://github.com/THUDM/ChatGLM-6B/pull/216)\n\u542f\u53d1\uff0c\u5efa\u7acb\u4e86\u5168\u6d41\u7a0b\u53ef\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u5b9e\u73b0\u7684\u672c\u5730\u77e5\u8bc6\u5e93\u95ee\u7b54\u5e94\u7528\u3002\u672c\u9879\u76ee\u7684\u6700\u65b0\u7248\u672c\u4e2d\u53ef\u4f7f\u7528 [Xinference](https://github.com/xorbitsai/inference)\u3001[Ollama](https://github.com/ollama/ollama)\n\u7b49\u6846\u67b6\u63a5\u5165 [GLM-4-Chat](https://github.com/THUDM/GLM-4)\u3001 [Qwen2-Instruct](https://github.com/QwenLM/Qwen2)\u3001 [Llama3](https://github.com/meta-llama/llama3)\n\u7b49\u6a21\u578b\uff0c\u4f9d\u6258\u4e8e [langchain](https://github.com/langchain-ai/langchain)\n\u6846\u67b6\u652f\u6301\u901a\u8fc7\u57fa\u4e8e [FastAPI](https://github.com/tiangolo/fastapi) \u63d0\u4f9b\u7684 API\n\u8c03\u7528\u670d\u52a1\uff0c\u6216\u4f7f\u7528\u57fa\u4e8e [Streamlit](https://github.com/streamlit/streamlit) \u7684 WebUI \u8fdb\u884c\u64cd\u4f5c\u3002\n\n![](docs/img/langchain_chatchat_0.3.0.png)\n\n\u2705 \u672c\u9879\u76ee\u652f\u6301\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u5f00\u6e90 LLM\u3001 Embedding \u6a21\u578b\u4e0e\u5411\u91cf\u6570\u636e\u5e93\uff0c\u53ef\u5b9e\u73b0\u5168\u90e8\u4f7f\u7528**\u5f00\u6e90**\u6a21\u578b**\u79bb\u7ebf\u79c1\u6709\u90e8\u7f72**\u3002\u4e0e\u6b64\u540c\u65f6\uff0c\u672c\u9879\u76ee\u4e5f\u652f\u6301\nOpenAI GPT API \u7684\u8c03\u7528\uff0c\u5e76\u5c06\u5728\u540e\u7eed\u6301\u7eed\u6269\u5145\u5bf9\u5404\u7c7b\u6a21\u578b\u53ca\u6a21\u578b API \u7684\u63a5\u5165\u3002\n\n\u26d3\ufe0f \u672c\u9879\u76ee\u5b9e\u73b0\u539f\u7406\u5982\u4e0b\u56fe\u6240\u793a\uff0c\u8fc7\u7a0b\u5305\u62ec\u52a0\u8f7d\u6587\u4ef6 -> \u8bfb\u53d6\u6587\u672c -> \u6587\u672c\u5206\u5272 -> \u6587\u672c\u5411\u91cf\u5316 -> \u95ee\u53e5\u5411\u91cf\u5316 ->\n\u5728\u6587\u672c\u5411\u91cf\u4e2d\u5339\u914d\u51fa\u4e0e\u95ee\u53e5\u5411\u91cf\u6700\u76f8\u4f3c\u7684 `top k`\u4e2a -> \u5339\u914d\u51fa\u7684\u6587\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u548c\u95ee\u9898\u4e00\u8d77\u6dfb\u52a0\u5230 `prompt`\u4e2d -> \u63d0\u4ea4\u7ed9 `LLM`\u751f\u6210\u56de\u7b54\u3002\n\n\ud83d\udcfa [\u539f\u7406\u4ecb\u7ecd\u89c6\u9891](https://www.bilibili.com/video/BV13M4y1e7cN/?share_source=copy_web&vd_source=e6c5aafe684f30fbe41925d61ca6d514)\n\n![\u5b9e\u73b0\u539f\u7406\u56fe](docs/img/langchain+chatglm.png)\n\n\u4ece\u6587\u6863\u5904\u7406\u89d2\u5ea6\u6765\u770b\uff0c\u5b9e\u73b0\u6d41\u7a0b\u5982\u4e0b\uff1a\n\n![\u5b9e\u73b0\u539f\u7406\u56fe2](docs/img/langchain+chatglm2.png)\n\n\ud83d\udea9 \u672c\u9879\u76ee\u672a\u6d89\u53ca\u5fae\u8c03\u3001\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4f46\u53ef\u5229\u7528\u5fae\u8c03\u6216\u8bad\u7ec3\u5bf9\u672c\u9879\u76ee\u6548\u679c\u8fdb\u884c\u4f18\u5316\u3002\n\n\ud83c\udf10 [AutoDL \u955c\u50cf](https://www.codewithgpu.com/i/chatchat-space/Langchain-Chatchat/Langchain-Chatchat) \u4e2d `0.3.0`\n\u7248\u672c\u6240\u4f7f\u7528\u4ee3\u7801\u5df2\u66f4\u65b0\u81f3\u672c\u9879\u76ee `v0.3.0` \u7248\u672c\u3002\n\n\ud83d\udc33 Docker \u955c\u50cf\u5c06\u4f1a\u5728\u8fd1\u671f\u66f4\u65b0\u3002\n\n\ud83e\uddd1\u200d\ud83d\udcbb \u5982\u679c\u4f60\u60f3\u5bf9\u672c\u9879\u76ee\u505a\u51fa\u8d21\u732e\uff0c\u6b22\u8fce\u79fb\u6b65[\u5f00\u53d1\u6307\u5357](docs/contributing/README_dev.md) \u83b7\u53d6\u66f4\u591a\u5f00\u53d1\u90e8\u7f72\u76f8\u5173\u4fe1\u606f\u3002\n\n## \u529f\u80fd\u4ecb\u7ecd\n\n### 0.3.x \u7248\u672c\u529f\u80fd\u4e00\u89c8\n\n| \u529f\u80fd        | 0.2.x                            | 0.3.x                                                               |\n|-----------|----------------------------------|---------------------------------------------------------------------|\n| \u6a21\u578b\u63a5\u5165      | \u672c\u5730\uff1afastchat<br>\u5728\u7ebf\uff1aXXXModelWorker | \u672c\u5730\uff1amodel_provider,\u652f\u6301\u5927\u90e8\u5206\u4e3b\u6d41\u6a21\u578b\u52a0\u8f7d\u6846\u67b6<br>\u5728\u7ebf\uff1aoneapi<br>\u6240\u6709\u6a21\u578b\u63a5\u5165\u5747\u517c\u5bb9openai sdk |\n| Agent     | \u274c\u4e0d\u7a33\u5b9a                             | \u2705\u9488\u5bf9ChatGLM3\u548cQwen\u8fdb\u884c\u4f18\u5316,Agent\u80fd\u529b\u663e\u8457\u63d0\u5347                                    ||\n| LLM\u5bf9\u8bdd     | \u2705                                | \u2705                                                                   ||\n| \u77e5\u8bc6\u5e93\u5bf9\u8bdd     | \u2705                                | \u2705                                                                   ||\n| \u641c\u7d22\u5f15\u64ce\u5bf9\u8bdd    | \u2705                                | \u2705                                                                   ||\n| \u6587\u4ef6\u5bf9\u8bdd      | \u2705\u4ec5\u5411\u91cf\u68c0\u7d22                           | \u2705\u7edf\u4e00\u4e3aFile RAG\u529f\u80fd,\u652f\u6301BM25+KNN\u7b49\u591a\u79cd\u68c0\u7d22\u65b9\u5f0f                                    ||\n| \u6570\u636e\u5e93\u5bf9\u8bdd     | \u274c                                | \u2705                                                                   ||\n| \u591a\u6a21\u6001\u56fe\u7247\u5bf9\u8bdd     | \u274c                                | \u2705  \u63a8\u8350\u4f7f\u7528 qwen-vl-chat                   ||\n| ARXIV\u6587\u732e\u5bf9\u8bdd | \u274c                                | \u2705                                                                   ||\n| Wolfram\u5bf9\u8bdd | \u274c                                | \u2705                                                                   ||\n| \u6587\u751f\u56fe       | \u274c                                | \u2705                                                                   ||\n| \u672c\u5730\u77e5\u8bc6\u5e93\u7ba1\u7406   | \u2705                                | \u2705                                                                   ||\n| WEBUI     | \u2705                                | \u2705\u66f4\u597d\u7684\u591a\u4f1a\u8bdd\u652f\u6301,\u81ea\u5b9a\u4e49\u7cfb\u7edf\u63d0\u793a\u8bcd...                                               |\n\n0.3.x \u7248\u672c\u7684\u6838\u5fc3\u529f\u80fd\u7531 Agent \u5b9e\u73b0,\u4f46\u7528\u6237\u4e5f\u53ef\u4ee5\u624b\u52a8\u5b9e\u73b0\u5de5\u5177\u8c03\u7528:\n\n|\u64cd\u4f5c\u65b9\u5f0f|\u5b9e\u73b0\u7684\u529f\u80fd|\u9002\u7528\u573a\u666f|\n|-------|---------|-------|\n|\u9009\u4e2d\"\u542f\u7528Agent\",\u9009\u62e9\u591a\u4e2a\u5de5\u5177|\u7531LLM\u81ea\u52a8\u8fdb\u884c\u5de5\u5177\u8c03\u7528|\u4f7f\u7528ChatGLM3/Qwen\u6216\u5728\u7ebfAPI\u7b49\u5177\u5907Agent\u80fd\u529b\u7684\u6a21\u578b|\n|\u9009\u4e2d\"\u542f\u7528Agent\",\u9009\u62e9\u5355\u4e2a\u5de5\u5177|LLM\u4ec5\u89e3\u6790\u5de5\u5177\u53c2\u6570|\u4f7f\u7528\u7684\u6a21\u578bAgent\u80fd\u529b\u4e00\u822c,\u4e0d\u80fd\u5f88\u597d\u7684\u9009\u62e9\u5de5\u5177<br>\u60f3\u624b\u52a8\u9009\u62e9\u529f\u80fd|\n|\u4e0d\u9009\u4e2d\"\u542f\u7528Agent\",\u9009\u62e9\u5355\u4e2a\u5de5\u5177|\u4e0d\u4f7f\u7528Agent\u529f\u80fd\u7684\u60c5\u51b5\u4e0b,\u624b\u52a8\u586b\u5165\u53c2\u6570\u8fdb\u884c\u5de5\u5177\u8c03\u7528|\u4f7f\u7528\u7684\u6a21\u578b\u4e0d\u5177\u5907Agent\u80fd\u529b|\n|\u4e0d\u9009\u4e2d\u4efb\u4f55\u5de5\u5177\uff0c\u4e0a\u4f20\u4e00\u4e2a\u56fe\u7247|\u56fe\u7247\u5bf9\u8bdd|\u4f7f\u7528 qwen-vl-chat \u7b49\u591a\u6a21\u6001\u6a21\u578b|\n\n\u66f4\u591a\u529f\u80fd\u548c\u66f4\u65b0\u8bf7\u5b9e\u9645\u90e8\u7f72\u4f53\u9a8c.\n\n### \u5df2\u652f\u6301\u7684\u6a21\u578b\u90e8\u7f72\u6846\u67b6\u4e0e\u6a21\u578b\n\n\u672c\u9879\u76ee\u4e2d\u5df2\u7ecf\u652f\u6301\u5e02\u9762\u4e0a\u4e3b\u6d41\u7684\u5982 [GLM-4-Chat](https://github.com/THUDM/GLM-4)\n\u4e0e [Qwen2-Instruct](https://github....\n    \n    YOUR SUMMARY:\n    ": "Langchain-Chatchat is an open-source application using LLMs (like ChatGLM) and Langchain for Retrieval-Augmented Generation (RAG) and Agent tasks. It enables offline, private deployment for local knowledge base question-answering, supporting various models and frameworks (e.g., Xinference, Ollama). Key features include knowledge base interaction, Agent capabilities, file dialogue, and access via API or WebUI.", "\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    <h1 align=\"center\">\n  <br>\n  <img height=\"300\" src=\"./core/http/static/logo.png\"> <br>\n<br>\n</h1>\n\n<p align=\"center\">\n<a href=\"https://github.com/go-skynet/LocalAI/fork\" target=\"blank\">\n<img src=\"https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI forks\"/>\n</a>\n<a href=\"https://github.com/go-skynet/LocalAI/stargazers\" target=\"blank\">\n<img src=\"https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI stars\"/>\n</a>\n<a href=\"https://github.com/go-skynet/LocalAI/pulls\" target=\"blank\">\n<img src=\"https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge\" alt=\"LocalAI pull-requests\"/>\n</a>\n<a href='https://github.com/go-skynet/LocalAI/releases'>\n<img src='https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge'>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://hub.docker.com/r/localai/localai\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker\" alt=\"LocalAI Docker hub\"/>\n</a>\n<a href=\"https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest\" target=\"blank\">\n<img src=\"https://img.shields.io/badge/quay.io-images-important.svg?\" alt=\"LocalAI Quay.io\"/>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://twitter.com/LocalAI_API\" target=\"blank\">\n<img src=\"https://img.shields.io/twitter/follow/LocalAI_API?label=Follow: LocalAI_API&style=social\" alt=\"Follow LocalAI_API\"/>\n</a>\n<a href=\"https://discord.gg/uJAeKSAGDy\" target=\"blank\">\n<img src=\"https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&theme=default-inverted\" alt=\"Join LocalAI Discord Community\"/>\n</a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/5539\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/5539\" alt=\"mudler%2FLocalAI | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n> :bulb: Get help - [\u2753FAQ](https://localai.io/faq/) [\ud83d\udcadDiscussions](https://github.com/go-skynet/LocalAI/discussions) [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy) [:book: Documentation website](https://localai.io/)\n>\n> [\ud83d\udcbb Quickstart](https://localai.io/basics/getting_started/) [\ud83d\uddbc\ufe0f Models](https://models.localai.io/) [\ud83d\ude80 Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) [\ud83e\udd7d Demo](https://demo.localai.io) [\ud83c\udf0d Explorer](https://explorer.localai.io) [\ud83d\udeeb Examples](https://github.com/mudler/LocalAI-examples) \n\n[![tests](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)[![Build and Release](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)[![build container images](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)[![Bump dependencies](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai)](https://artifacthub.io/packages/search?repo=localai)\n\n**LocalAI** is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that's compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by [Ettore Di Giacinto](https://github.com/mudler).\n\n\n## \ud83d\udcda\ud83c\udd95 Local Stack Family\n\n\ud83c\udd95 LocalAI is now part of a comprehensive suite of AI tools designed to work together:\n\n<table>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <a href=\"https://github.com/mudler/LocalAGI\">\n        <img src=\"https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png\" width=\"300\" alt=\"LocalAGI Logo\">\n      </a>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <h3><a href=\"https://github.com/mudler/LocalAGI\">LocalAGI</a></h3>\n      <p>A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI's Responses API, enhanced with advanced agentic capabilities.</p>\n    </td>\n  </tr>\n  <tr>\n    <td width=\"50%\" valign=\"top\">\n      <a href=\"https://github.com/mudler/LocalRecall\">\n        <img src=\"https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png\" width=\"300\" alt=\"LocalRecall Logo\">\n      </a>\n    </td>\n    <td width=\"50%\" valign=\"top\">\n      <h3><a href=\"https://github.com/mudler/LocalRecall\">LocalRecall</a></h3>\n      <p>A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.</p>\n    </td>\n  </tr>\n</table>\n\n## Screenshots\n\n\n| Talk Interface | Generate Audio |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk](./docs/assets/images/screenshots/screenshot_tts.png) | ![Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low](./docs/assets/images/screenshots/screenshot_tts.png) |\n\n| Models Overview | Generate Images |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-20 LocalAI - Models](./docs/assets/images/screenshots/screenshot_gallery.png) | ![Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev](./docs/assets/images/screenshots/screenshot_image.png) |\n\n| Chat Interface | Home |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5](./docs/assets/images/screenshots/screenshot_chat.png) | ![Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)](./docs/assets/images/screenshots/screenshot_home.png) |\n\n| Login | Swarm |\n| --- | --- |\n|!...\n    \n    YOUR SUMMARY:\n    ": "LocalAI is a free, open-source alternative to the OpenAI API, designed for local AI inferencing. It acts as a drop-in replacement, allowing users to run LLMs, generate images, and audio locally on consumer-grade hardware, even without a GPU. Its purpose is to provide a self-hosted, private AI solution compatible with OpenAI's API specifications.", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"LLM natural query text to SQL converter\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"LLM\", \"text-to-SQL\", \"natural language\", \"SQL\", \"converter\", \"NL2SQL\"]\n```", "\n    Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"project idea on Natural Query to SQL\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    ": "```json\n[\"Natural Language Query\", \"SQL\", \"NLQ\", \"NL2SQL\", \"Text to SQL\"]\n```", "\n        For the project `LLM -CODEBASE-DOCUMENTOR`:\n\n        Codebase Context:\n        --- File Index 0: LLM-CODEBASE-DOCUMENTOR-main\\flow.py ---\nfrom pocketflow import Flow\n\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial,\n    # New nodes for Streamlit UI\n    SmartSearchRepo,\n    FilterRepos,\n    SelectRepository,\n    RenderAndDownload\n)\n\n\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow, fetch_repo, combine_tutorial\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n    \n    # Create the original tutorial flow and get the first and last nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n    \n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n    \n    # Connect the UI nodes\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n    \n    # Connect the select_repo node to the FetchRepo node\n    select_repo >> fetch_repo\n    \n    # Add the render and download node after CombineTutorial\n    combine_tutorial >> render_download\n    \n    # Create the full UI flow\n    ui_flow = Flow(start=smart_search)\n    \n    return ui_flow\n\n--- File Index 1: LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File Index 2: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File Index 3: LLM-CODEBASE-DOCUMENTOR-main\\README.md ---\n# LLM Codebase Documentor\n\nA tool that helps developers understand codebases by generating detailed tutorials explaining core abstractions, relationships, and code organization.\n\n## Features\n\n1. **CLI Interface**: Generate tutorials from GitHub URLs or local directories using command-line arguments\n2. **Web Interface**: User-friendly Streamlit UI for repository search and tutorial generation\n3. **Smart GitHub Search**: Natural language query processing to find relevant repositories\n4. **Advanced Filtering**: Sort and filter repositories by stars, forks, language, and more\n5. **Customizable LLM Providers**: Support for OpenAI, Anthropic Claude, and Google Gemini\n\n## Installation\n\n1. Clone the repository:\n   ```bash\n   git clone https://github.com/yourusername/LLM-CODEBASE-DOCUMENTOR.git\n   cd LLM-CODEBASE-DOCUMENTOR\n   ```\n\n2. Install dependencies:\n   ```bash\n   pip install -r requirements.txt\n   ```\n\n3. Set up environment variables by creating a `.env` file (copy from `.env.example`):\n   ```bash\n   cp .env.example .env\n   ```\n   \n4. Edit the `.env` file to add your API keys:\n   ```\n   GITHUB_TOKEN=your_github_token_here\n   OPENAI_API_KEY=your_openai_key_here\n   ANTHROPIC_API_KEY=your_anthropic_key_here\n   GEMINI_API_KEY=your_gemini_key_here\n   ```\n\n## Usage\n\n### Command Line Interface\n\nGenerate a tutorial for a GitHub repository:\n```bash\npython main.py --repo https://github.com/username/repository\n```\n\nGenerate a tutorial for a local directory:\n```bash\npython main.py --dir /path/to/local/codebase\n```\n\nAdditional options:\n```\n--model [anthropic-claude|google-gemini|openai-gpt]  # Choose LLM provider\n--name PROJECT_NAME                                  # Custom project name\n--token GITHUB_TOKEN                                 # GitHub token (if not in .env)\n--output OUTPUT_DIR                                  # Output directory\n--include \"*.py\" \"*.js\"                              # File patterns to include\n--exclude \"tests/*\" \"docs/*\"                         # File patterns to exclude\n--max-size SIZE_BYTES                                # Maximum file size\n```\n\n### Web Interface\n\nStart the Streamlit web app:\n```bash\nstreamlit run streamlit_app.py\n```\n\nThen open your browser at http://localhost:8501 to access the web interface.\n\n## How It Works\n\n1. **Repository Crawling**: Fetches code from GitHub or local directory\n2. **Abstraction Identification**: Analyzes code to identify key abstractions\n3. **Relationship Analysis**: Determines how abstractions interact\n4. **Chapter Generation**: Creates beginner-friendly tutorials for each abstraction\n5. **Tutorial Assembly**: Combines chapters with a diagram in structured Markdown\n\n## LLM Providers\n\nThe tool supports three LLM providers:\n- **Google Gemini** (default): Uses Google's Gemini 2.5 Pro model or any other Gemini models supported via API calls\n- **Anthropic Claude**: Uses Claude 3.7 Sonnet model or any other Anthropic models supported via API calls\n- **OpenAI GPT**: Uses GPT-4o-mini model or any other OpenAI models supported via API calls\n\n## Credits\n\nThis project uses PocketFlow, a minimalist LLM framework for agent-based workflows. \n\n--- File Index 4: LLM-CODEBASE-DOCUMENTOR-main\\requirements.txt ---\npocketflow>=0.0.1\npyyaml>=6.0\nrequests>=2.28.0\ngitpython>=3.1.0\ngoogle-cloud-aiplatform>=1.25.0\ngoogle-genai>=1.9.0\npython-dotenv>=1.0.0\nstreamlit>=1.30.0\nanthropic>=0.22.0\nopenai>=1.8.0\nzipfile36>=0.1.3\n\n\n--- File Index 5: LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py ---\nimport streamlit as st\nimport os\nimport time\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\n\n# Import utilities\nfrom utils.env_loader import load_env_vars, get_api_keys, get_model_names, create_env_example\nfrom utils.call_llm import LLMProvider_enum\nfrom utils.github_api import RepoMetadata\n\n# Import the flow\nfrom flow import create_streamlit_flow\nfrom nodes import SmartSearchRepo, FilterRepos, SelectRepository, RenderAndDownload\n\n# Load environment variables\nload_env_vars()\n\n# 1. Page configuration (with a custom About menu item)\nst.set_page_config(\n    page_title=\"\ud83d\udcd6\ud83e\udd13 LLM Codebase Finder & Documentor\",\n    page_icon=\"\ud83e\uddd0\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'About': (\n            \"### LLM Codebase Documentor\\n\"\n            \"A&nbsp;Streamlit app to auto-generate beginner-friendly tutorials from any GitHub repo.\\n\\n\"\n            \"Built by **TEG SINGH TIWANA** for Cloud Assignment 2:\\n\"\n            \"[GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n        )\n    }\n)\n# Initialize session state if not already done\nif \"initialized\" not in st.session_state:\n    st.session_state.initialized = True\n    st.session_state.search_results = []\n    st.session_state.selected_repo = None\n    st.session_state.filter_params = {\n        \"min_stars\": 0,\n        \"min_forks\": 0,\n        \"language\": \"\",\n        \"sort_by\": \"stars\",\n        \"updated_since\": \"\"\n    }\n    st.session_state.shared_store = {}\n    st.session_state.markdown_files = {}\n    st.session_state.zip_path = None\n    st.session_state.task_completed = False\n    st.session_state.ui_view = \"search\"  # Possible values: search, results, tutorial\n    \n    # Get API keys from environment\n    api_keys = get_api_keys()\n    for key, value in api_keys.items():\n        st.session_state[key] = value or \"\"\n    \n    # Get model names from environment\n    model_names = get_model_names()\n    for key, value in model_names.items():\n        st.session_state[key] = value\n\n# --- Sidebar ---\nwith st.sidebar:\n    st.title(\"LLM Codebase Documentor\")\n    \n    # Create tabs for different sidebar sections\n    tab1, tab2, tab3 = st.tabs([\"Search\", \"API Keys\", \"Model Settings\"])\n    \n    with tab1:\n        # Search input and type\n        st.subheader(\"Repository Search\")\n        query_input = st.text_input(\n            \"Enter GitHub URL, local path, or natural language query:\",\n            placeholder=\"e.g., https://github.com/user/repo OR frameworks for UI prototyping\"\n        )\n        \n        # Filters (only shown for natural language search)\n        st.subheader(\"Filters\")\n        min_stars = st.number_input(\"Minimum Stars\", min_value=0, value=st.session_state.filter_params[\"min_stars\"])\n        min_forks = st.number_input(\"Minimum Forks\", min_value=0, value=st.session_state.filter_params[\"min_forks\"])\n        language = st.selectbox(\n            \"Language\", \n            [\"\", \"Python\", \"JavaScript\", \"TypeScript\", \"Java\", \"Go\", \"Rust\", \"C++\", \"Ruby\", \"PHP\"],\n            index=0\n        )\n        sort_by = st.selectbox(\n            \"Sort By\", \n            [\"stars\", \"forks\", \"updated\", \"help-wanted-issues\"],\n            index=0\n        )\n        updated_since = st.date_input(\n            \"Updated Since\", \n            value=None\n        )\n        \n        # Update filter params in session state\n        if st.button(\"Search\", key=\"search_button\"):\n            # Update filter params\n            st.session_state.filter_params = {\n                \"min_stars\": min_stars,\n                \"min_forks\": min_forks,\n                \"language\": language,\n                \"sort_by\": sort_by,\n                \"updated_since\": updated_since.isoformat() if updated_since else \"\"\n            }\n            \n            # Initialize shared store for the flow\n            st.session_state.shared_store = {\n                \"query_input\": query_input,\n                \"filter_params\": st.session_state.filter_params,\n                \"github_token\": st.session_state.github_token,\n                \"llm_provider\": LLMProvider_enum(st.session_state.provider_selection),\n                \n                # Default parameters from original flow\n                \"include_patterns\": {\n                    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n                    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n                    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n                    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n                    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n                    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \"*.md\", \"*.pdf\"\n                },\n                \"exclude_patterns\": {\n                    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n                    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n                    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n                },\n                \"max_file_size\": 300000,\n                \"output_dir\": \"output\"\n            }\n            \n            # Create the flow\n            flow = create_streamlit_flow()\n            \n            # Run the SmartSearchRepo and FilterRepos nodes\n            with st.spinner(\"Processing query...\"):\n                # Set state to indicate we're waiting for results\n                st.session_state.ui_view = \"results\"\n                \n                # Run the SmartSearch and FilterRepos nodes\n                smart_search = SmartSearchRepo()\n                filter_repos = FilterRepos()\n                \n                # Run nodes sequentially\n                smart_search.run(st.session_state.shared_store)\n                filter_repos.run(st.session_state.shared_store)\n                \n                # Update session state with search results\n                if \"search_results\" in st.session_state.shared_store:\n                    st.session_state.search_results = st.session_state.shared_store[\"search_results\"]\n                \n                # If we have a selected_repo, run the full pipeline\n                if \"selected_repo\" in st.session_state.shared_store:\n                    st.session_state.selected_repo = st.session_state.shared_store[\"selected_repo\"]\n                    with st.spinner(\"Generating tutorial...\"):\n                        st.session_state.ui_view = \"tutorial\"\n                        # Run the full flow\n                        flow.run(st.session_state.shared_store)\n                        \n                        # Get the rendered Markdown files and ZIP path if available\n                        if \"markdown_files\" in st.session_state.shared_store:\n                            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n                        if \"zip_path\" in st.session_state.shared_store:\n                            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n                        \n                        st.session_state.task_completed = True\n    \n    with tab2:\n        st.subheader(\"API Keys\")\n        \n        # GitHub token\n        github_token = st.text_input(\n            \"GitHub Token\",\n            value=st.session_state.github_token,\n            type=\"password\",\n            help=\"Required for private repos or to avoid rate limits\"\n        )\n        st.session_state.github_token = github_token\n        \n        # LLM API keys\n        openai_api_key = st.text_input(\n            \"OpenAI API Key\",\n            value=st.session_state.openai_api_key,\n            type=\"password\"\n        )\n        st.session_state.openai_api_key = openai_api_key\n        \n        anthropic_api_key = st.text_input(\n            \"Anthropic API Key\",\n            value=st.session_state.anthropic_api_key,\n            type=\"password\"\n        )\n        st.session_state.anthropic_api_key = anthropic_api_key\n        \n        gemini_api_key = st.text_input(\n            \"Google Gemini API Key\",\n            value=st.session_state.gemini_api_key,\n            type=\"password\"\n        )\n        st.session_state.gemini_api_key = gemini_api_key\n        \n        # Save API keys to environment\n        if st.button(\"Save API Keys\"):\n            os.environ[\"GITHUB_TOKEN\"] = github_token\n            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n            os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n            os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n            st.success(\"API keys saved to environment!\")\n    \n    with tab3:\n        st.subheader(\"Model Settings\")\n        \n        # LLM Provider selection\n        provider_selection = st.selectbox(\n            \"LLM Provider\",\n            options=[provider.value for provider in LLMProvider_enum],\n            index=0\n        )\n        st.session_state.provider_selection = provider_selection\n        \n        # Model name based on provider\n        if provider_selection == LLMProvider_enum.OPENAI.value:\n            model_name = st.text_input(\n                \"OpenAI Model\",\n                value=st.session_state.openai_model\n            )\n            st.session_state.openai_model = model_name\n            os.environ[\"OPENAI_MODEL\"] = model_name\n        \n        elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n            model_name = st.text_input(\n                \"Anthropic Model\",\n                value=st.session_state.anthropic_model\n            )\n            st.session_state.anthropic_model = model_name\n            os.environ[\"ANTHROPIC_MODEL\"] = model_name\n        \n        else:  # Gemini\n            model_name = st.text_input(\n                \"Google Gemini Model\",\n                value=st.session_state.gemini_model\n            )\n            st.session_state.gemini_model = model_name\n            os.environ[\"GEMINI_MODEL\"] = model_name\n            \n        # Add save button for model settings\n        if st.button(\"Save Model Settings\"):\n            # Save provider selection to session state\n            st.session_state.provider_selection = provider_selection\n            st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(provider_selection)\n            \n            # Save model name based on provider\n            if provider_selection == LLMProvider_enum.OPENAI.value:\n                os.environ[\"OPENAI_MODEL\"] = model_name\n                st.session_state.openai_model = model_name\n            elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n                os.environ[\"ANTHROPIC_MODEL\"] = model_name\n                st.session_state.anthropic_model = model_name\n            else:  # Gemini\n                os.environ[\"GEMINI_MODEL\"] = model_name\n                st.session_state.gemini_model = model_name\n                \n            st.success(f\"Model settings saved! Provider: {provider_selection}, Model: {model_name}\")\n\n# --- Main Content ---\ndef display_repo_card(repo: RepoMetadata, index: int):\n    \"\"\"Display a repository card with details and select button.\"\"\"\n    col1, col2 = st.columns([1, 4])\n    \n    with col1:\n        st.image(\"https://github.githubassets.com/favicons/favicon.png\", width=50)\n        st.button(f\"Select\", key=f\"select_{index}\", on_click=select_repository, args=(repo.url,))\n    \n    with col2:\n        st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n        st.markdown(f\"**Description:** {repo.description}\")\n        st.markdown(f\"**Summary:** {repo.readme_summary}\")\n        st.markdown(f\"**Language:** {repo.language} | **Stars:** {repo.stars} | **Forks:** {repo.forks}\")\n\ndef select_repository(repo_url: str):\n    \"\"\"Handle repository selection from search results.\"\"\"\n    st.session_state.selected_repo = repo_url\n    \n    # Make sure we have the shared store initialized\n    if not st.session_state.shared_store:\n        st.session_state.shared_store = {}\n    \n    # Set the selected repository URL in the shared store\n    st.session_state.shared_store[\"selected_repo\"] = repo_url\n    st.session_state.shared_store[\"repo_url\"] = repo_url\n    st.session_state.shared_store[\"local_dir\"] = None\n    \n    # Make sure we have other necessary parameters in the shared store\n    if \"include_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"include_patterns\"] = {\n            \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n            \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n            \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n            \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n            \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n            \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n        }\n    \n    if \"exclude_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"exclude_patterns\"] = {\n            \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n            \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n            \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n        }\n        \n    if \"max_file_size\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"max_file_size\"] = 300000\n        \n    if \"output_dir\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"output_dir\"] = \"output\"\n        \n    if \"github_token\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"github_token\"] = st.session_state.github_token\n        \n    if \"llm_provider\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(st.session_state.provider_selection)\n    \n    st.session_state.ui_view = \"tutorial\"\n    \n    # Create and run the flow\n    flow = create_streamlit_flow()\n    with st.spinner(\"Generating tutorial...\"):\n        # Run the full flow\n        flow.run(st.session_state.shared_store)\n        \n        # Get the rendered Markdown files and ZIP path if available\n        if \"markdown_files\" in st.session_state.shared_store:\n            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n        if \"zip_path\" in st.session_state.shared_store:\n            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n        \n        st.session_state.task_completed = True\n\n# Conditional display based on current view\nif st.session_state.ui_view == \"search\":\n    # Show welcome message and instructions\n    st.title(\"Welcome to **GITHUB** Finder & Documentor\ud83d\udcd6\ud83e\udd13\")\n    st.markdown(\"\"\"\n    **Built by [TEG SINGH TIWANA](https://github.com/tej172)** for _Cloud Assignment 2_:  \n    [GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n    #### Welcome to **LLM Codebase Documentor**, your friendly assistant for auto-generating beginner-friendly tutorials from any codebase! \n    ###### This tool helps you understand codebases by generating detailed tutorials explaining core abstractions, relationships, and code organization.\n    \n    ### Getting Started\n    1. In the **sidebar**, enter one of:\n       - A **GitHub URL** (e.g. `https://github.com/user/repo`)\n       - A **local folder path** on your machine\n       - A **natural language** query (e.g. \"frameworks for UI prototyping\")\n    2. If you chose natural language, adjust **advanced filters** (stars, forks, language, updated date).\n    3. Click **Search** to fetch repositories or **Generate** to build the tutorial.\n    \n    ### Features\n    - \ud83d\udd0d **Smart Search** powered by GPT / Gemini / Claude for effortless keyword extraction  \n    - \ud83d\udcc2 **Local Path** support\u2014point to any folder you've already cloned  \n    - \u2b50 **Advanced Filters**\u2014stars, forks, issues, language, last\u2010updated  \n    - \ud83d\udcc4 **Markdown Preview**\u2014click through `index.md` and each chapter in-app  \n    - \ud83d\udce5 **Downloadable ZIP**\u2014grab your entire tutorial with one click  \n\n    > _Tip_: Make sure your **API keys** (GITHUB_TOKEN, OPENAI_API_KEY, GEMINI_API_KEY, etc.) are set in the sidebar or in your `.env` file.\n    \"\"\")\n    \nelif st.session_state.ui_view == \"results\":\n    # Show search results\n    st.title(\"Search Results\")\n    \n    if st.session_state.search_results:\n        st.markdown(f\"Found {len(st.session_state.search_results)} repositories matching your query.\")\n        for i, repo in enumerate(st.session_state.search_results):\n            with st.container():\n                display_repo_card(repo, i)\n                st.markdown(\"---\")\n    else:\n        st.info(\"No results found. Try different keywords or filters.\")\n        \nelif st.session_state.ui_view == \"tutorial\":\n    # Show the generated tutorial\n    project_name = st.session_state.shared_store.get(\"project_name\", \"Repository\")\n    st.title(f\"Tutorial for {project_name}\")\n    \n    # Download button for ZIP if available\n    if st.session_state.zip_path:\n        with open(st.session_state.zip_path, \"rb\") as f:\n            st.download_button(\n                label=\"Download Tutorial ZIP\",\n                data=f,\n                file_name=f\"{project_name}_tutorial.zip\",\n                mime=\"application/zip\"\n            )\n    \n    # Display the Markdown files\n    if st.session_state.markdown_files:\n        # First show index.md if it exists\n        if \"index.md\" in st.session_state.markdown_files:\n            st.markdown(st.session_state.markdown_files[\"index.md\"])\n            st.markdown(\"---\")\n        \n        # Then show all other files in order\n        for filename in sorted([f for f in st.session_state.markdown_files if f != \"index.md\"]):\n            with st.expander(f\"{filename}\", expanded=True):\n                st.markdown(st.session_state.markdown_files[filename])\n    else:\n        st.info(\"Generating tutorial content... Please wait.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\n    \"LLM Codebase Documentor NTU \u00a9 2025 | \"\n    \"[View on GitHub](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2) |  \"\n    \"Created by [**TEG SINGH TIWANA (U2122816B)**](https://github.com/tej172) | \"\n    \"[Cloud Assignment 2 \u2013 GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n)\n\n# Create .env.example file if it doesn't exist\nif not os.path.exists(\".env.example\"):\n    create_env_example() \n\n--- File Index 6: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n--- File Index 7: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design2.md ---\n\n## layout: default title: \"System Design 2: Codebase Knowledge Builder + Streamlit Frontend\" nav\\_order: 3\n\n# System Design 2: Codebase Knowledge Builder + Streamlit Frontend \n\n### (System Design 2 is a extention from System Design: Codebase Knowledge Builder(in LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md) that is alredy built in this code base)\n\n> Builds on the implemented [design.md](docs/design.md) by adding:\n>\n> 1. A Streamlit-based web UI for interactive query input, credential management, filtering, and Markdown rendering.\n> 2. A GitHub API\u2013powered Smart Search that accepts natural\u2011language queries, direct URLs, or local paths.\n> 3. Advanced repository filters (stars, forks, issues, updated date) with clickable results driving the existing tutorial pipeline.\n\n## Requirements\n\n> **Notes for AI:** Mirror the detail of `design.md`, mapping high\u2011level user stories to concrete inputs/outputs.\n\n### User Stories\n\n1. **Tutorial Generation** (existing): As a developer onboarding a new codebase, I want an automated tutorial with core abstractions, relationships, and beginner\u2011friendly descriptions, so I can understand project structure quickly. ([design.md])\n2. **Web Interface**: As a user, I want a browser UI where I can:\n   - Enter a GitHub URL, natural language query, or local folder path.\n   - Configure LLM and GitHub API credentials.\n   - Apply repository filters.\n   - Trigger tutorial generation and view/download results\u2014all without leaving the page.\n3. **Smart GitHub Search**: As a researcher or student, I want to search GitHub by:\n   - **Natural Language**: e.g. \u201cframeworks for UI prototyping\u201d, converting text to search keywords via LLM.\n   - **Direct URL**: paste a repo URL to skip search.\n   - **Local Path**: select an existing clone to use directly.\n4. **Advanced Filtering**: As a power user, I want to sort/filter search results by stars, forks, issues, language, and last-updated to identify the most relevant repositories.\n5. **Seamless Pipeline Integration**: After selecting a repo, the existing tutorial-generation flow runs and displays rendered Markdown files inline, with an option to download a ZIP archive.\n\n### Inputs & Outputs\n\n| Category            | Inputs                                                                                                                                                 | Outputs                                                                                     |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |\n| **Search & Config** | - `query_input`: NL text, GitHub URL, or local path- `filter_params`: stars, forks, issues, language, updated- `.env` API credentials & model settings | - `search_results`: up to 10 `RepoMetadata`- Summary of each README                         |\n| **Tutorial Flow**   | - `repo_url` or `local_dir` set by selection                                                                                                           | - `index.md`, Mermaid diagram, ordered chapter `.md` files- ZIP archive of output directory |\n| **UI Rendering**    | - Generated Markdown files and ZIP path                                                                                                                | - Inline preview via `st.markdown`- `st.download_button` for ZIP                            |\n\n## End\u2011to\u2011End Flow Design\n\n```mermaid\nflowchart TD\n  UI[Streamlit UI] --> S1[SmartSearchRepo]\n  S1 --> F1[FilterRepos]\n  F1 --> Select[SelectRepository]\n  Select --> Fetch[FetchRepo]\n  Fetch --> Identify[IdentifyAbstractions]\n  Identify --> Analyze[AnalyzeRelationships]\n  Analyze --> Order[OrderChapters]\n  Order --> Batch[WriteChapters]\n  Batch --> Combine[CombineTutorial]\n  Combine --> UI_Display[Render & Download]\n```\n\n1. `StreamlitUI` (New Frontend)\n\n   - **Type:** Regular node\n   - **Responsibilities:**\n     - Present sidebar inputs: Query box, filter widgets, credential inputs, model selector, and action buttons.\n     - Display either search results (repo cards) or tutorial previews (Markdown pages).\n     - Handle user interactions: card clicks trigger downstream nodes.\n     - Provide ZIP download.\n\n2. `SmartSearchRepo`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.query_input`, detect `mode`:\n     ```python\n     if looks_like_url(query_input): mode='url'\n     elif os.path.isdir(query_input): mode='local'\n     else: mode='nl'\n     ```\n   - **Exec:**\n     - **NL Mode:** call `extract_keywords(query_input)` \u2192 keywords list, join as search string.\n     - **URL Mode:** validate URL, set `selected_repo`.\n     - **Local Mode:** verify path, set `selected_repo`.\n   - **Post:** Write `shared.search_mode`, `shared.keywords`, and (for URL/local) `shared.selected_repo`.\n\n3. `FilterRepos`\n\n   - **Type:** Regular node\n   - **Prep:** Read `search_mode`, `keywords`, `filter_params`, `github_token`.\n   - **Exec:**\n     - If NL mode: `github_search_repos(keywords, filter_params, token)`\n     - Else (URL/local): skip API search.\n   - **Post:** Store top-10 `shared.search_results` (`RepoMetadata` + README summary via `summarize_readme`).\n\n4. `SelectRepository`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.search_results`.\n   - **Exec:** On UI card click, set `shared.selected_repo` (URL or path).\n   - **Post:** Propagate into `shared.repo_url` or `shared.local_dir` for pipeline.\n\n5. **Existing Tutorial Flow** (`FetchRepo` \u2192 `IdentifyAbstractions` \u2192 `AnalyzeRelationships` \u2192 `OrderChapters` \u2192 `WriteChapters` \u2192 `CombineTutorial`)\n\n   - **Type:** Workflow + BatchNode for `WriteChapters`.\n   - **Unchanged:** Implements the tutorial generation as per [design.md].\n\n6. `RenderAndDownload`\n\n   - **Type:** Regular node\n   - **Prep:** After `CombineTutorial`, read `shared.final_output_dir` and loaded Markdown file contents.\n   - **Exec (Streamlit):**\n     - Loop through `.md` files: `st.markdown(file_content)` to render.\n     - ZIP directory with `zip_output_folder(output_dir)`.\n     - `st.download_button(\"Download Tutorial ZIP\", data=open(zip_path, 'rb'), file_name=\"tutorial.zip\")`\n   - **Post:** None (UI-only).\n\n## Utility Functions (New & Extended)\n\n| Function              | Module                 | Input                                                | Output                 | Role                                                                 |\n| --------------------- | ---------------------- | ---------------------------------------------------- | ---------------------- | -------------------------------------------------------------------- |\n| `extract_keywords`    | `utils/search_helpers` | `query_input: str`                                   | `keywords: List[str]`  | LLM\u2011powered extraction of search terms from free text.               |\n| `github_search_repos` | `utils/github_api`     | `keywords or query`, `filter_params`, `token: str`   | `List[RepoMetadata]`   | Calls GitHub search REST API, applies filters, returns top-10 repos. |\n| `summarize_readme`    | `utils/summarizer`     | `readme_text: str`                                   | `summary: str`         | Generates concise README summaries via LLM.                          |\n| `load_env_vars`       | `utils/env_loader`     | `.env` filepath                                      | Populated `os.environ` | Loads API keys and model names from `.env`.                          |\n| `zip_output_folder`   | `utils/io_helpers`     | `folder_path: str`                                   | `zip_path: str`        | Zips output tutorial directory for download.                         |\n| `crawl_github_files`  | as before              | `repo_url, token, patterns, max_size, use_rel_paths` | `files, stats`         | GitHub code crawling (unchanged).                                    |\n| `crawl_local_files`   | as before              | `directory, patterns, max_size, use_rel_paths`       | `files, stats`         | Local code crawling (unchanged).                                     |\n| `call_llm`            | as before              | `prompt: str, model: str`                            | `response: str`        | Central LLM interface for all analysis/generation steps.             |\n\n## Shared Store (Extended)\n\n```python\nshared = {\n  # Existing keys from design.md\n  \"repo_url\": None,\n  \"local_dir\": None,\n  \"project_name\": None,\n  \"github_token\": None,\n  \"output_dir\": \"output\",\n  \"files\": [],\n  \"abstractions\": [],\n  \"relationships\": {\"summary\": None, \"details\": []},\n  \"chapter_order\": [],\n  \"chapters\": [],\n  \"final_output_dir\": None,\n\n  # New keys for UI & search\n  \"query_input\": None,\n  \"search_mode\": None,\n  \"keywords\": [],\n  \"filter_params\": {},\n  \"search_results\": [],\n  \"selected_repo\": None,\n}\n```\n\n## Node Interface Mapping\n\n- \\`\\` \u2192 orchestrates all nodes, binds button callbacks to `SmartSearchRepo`, `FilterRepos`, `SelectRepository`, and downstream pipeline.\n- **New nodes** (`SmartSearchRepo`, `FilterRepos`, `SelectRepository`, `Render & Download`) plug into the existing `Flow` defined in `flow.py`, executed in the order shown in the flowchart.\n\n## Installation & Configuration\n\n1. **Clone & Setup**\n\nvenv\\Scripts\\activate\n\n pip install -r requirements.txt\n\n````\n2. **Environment**  \n   Copy `.env.example` to `.env` and fill:\n   ```env\n   GEMINI_API_KEY=<...>\n   GEMINI_PROJECT_ID=<...>\n   GITHUB_TOKEN=<...>\n   OPENAI_API_KEY=<...>\n   ANTHROPIC_API_KEY=<...>\n   GEMINI_MODEL=gemini-2.5-pro-exp-03-25\n   OPENAI_MODEL=gpt-4o-mini\n   ANTHROPIC_MODEL=claude-3-7-sonnet-latest\n````\n\n3. **Run App**\n   ```bash\n   streamlit run main.py\n   ```\n4. **Access UI**\\\n   Open [http://localhost:8501](http://localhost:8501) in your browser.\n\n---\n\n\n\n\n\n--- File Index 8: LLM-CODEBASE-DOCUMENTOR-main\\docs\\_config.yml ---\n# Basic site settings\ntitle: Assignment 2 (24S2-SC4052-CLOUD COMPUTING) (Building an AI-Enhanced Codebase Analyzer using GitHub APIs and LLMs)\n\n# Theme settings\nremote_theme: just-the-docs/just-the-docs\n\n# Navigation\nnav_sort: case_sensitive\n\n# Aux links (shown in upper right)\naux_links:\n  \"View on GitHub\":\n    - \"https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2\"\n    \n# Color scheme\ncolor_scheme: light\n\n# Author settings \nauthor:\n    name: Teg Singh Tiwana \n    studentID: U2122816B\n    url: https://github.com/tej172\n    github: tej172\n\n# Mermaid settings (Visualizing with AI | AI diagramming)\n# This is the default mermaid version, you can change it to the version you want\n# You can also use the latest version by using the following line\n# mermaid: latest\n# You can also use the following line to use a specific version\n# mermaid: 9.1.3\n# Create charts and diagrams and share them with your whole team using Mermaid Chart pro\nmermaid:\n  version: \"9.1.3\"  # can chnage to the version you want\n  # Default configuration\n  config: |\n    directionLR\n\n# Callouts settings\ncallouts:\n  warning:\n    title: Warning\n    color: red\n  note:\n    title: Note\n    color: blue\n  best-practice:\n    title: Best Practice\n    color: green\n  \n# The custom navigation\nnav:\n  - Home: index.md       # Link to your main docs index\n  - GitHub: \"https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2\"\n\n\n--- File Index 9: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\01_client_session.md ---\n# Chapter 1: Client Session\n\nWelcome to the `python-sdk` tutorial! We're excited to help you get started. This library helps your Python code talk to special servers called **MCP (Machine Communication Protocol) servers**. Think of these servers as assistants that have special tools or information you might want to use in your own programs.\n\nThis is the very first chapter, so we'll start with the basics!\n\n## What's the Big Idea? Connecting to the Assistant\n\nImagine you have a helpful assistant (the MCP server) living on another computer, or maybe running as a separate program on your own machine. This assistant has some cool abilities, like maybe it can do complex calculations, look up the weather, or generate text.\n\nHow do you, from your Python script, ask this assistant to do something? You need a way to connect to it, send your request (like \"Hey, what tools do you have?\"), and understand its reply. Doing this directly involves complicated networking or process communication details. Wouldn't it be nice if there was something simpler?\n\nThat's exactly what the `ClientSession` does!\n\n**`ClientSession` is like the dedicated phone line and translator between your Python code and the MCP server.** It manages the connection and handles all the back-and-forth communication for you.\n\nThink of it as your **control panel** for interacting with the remote MCP server. Instead of fiddling with complex communication protocols, you just use simple Python methods provided by the `ClientSession`, like:\n\n*   `session.list_tools()`: Ask the server what tools it offers.\n*   `session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`: Ask the server to use its \"calculator\" tool.\n\nThe `ClientSession` takes your Python command, translates it into the special MCP language the server understands, sends it off, waits for the server's reply, translates the reply back into Python, and gives it to you.\n\n## Getting Started: Listing Server Tools\n\nLet's try a simple task: connecting to an MCP server and asking it to list the tools it has available.\n\n**1. Import `ClientSession`**\n\nFirst, you need to import the necessary class from the `mcp` library.\n\n```python\n# main.py (simplified)\nimport asyncio\nfrom mcp import ClientSession, StdioServerParameters # We need these\nfrom mcp.client.stdio import stdio_client          # And this helper for local servers\n\n# Configure logging (helpful for seeing what's happening)\nimport logging\nlogging.basicConfig(level=logging.INFO)\n```\n\nThis code imports the main `ClientSession` class we'll be using. It also imports helpers (`StdioServerParameters`, `stdio_client`) that we'll use to connect to a server running locally on our machine via standard input/output (like a command-line tool). We also set up basic logging.\n\n**2. Prepare the Connection**\n\n`ClientSession` needs to know *how* to talk to the server. This involves setting up a communication channel. For this example, we'll assume the server is a command-line program we can run locally. The `stdio_client` helper function handles starting the server process and setting up the communication pipes (standard input/output).\n\n```python\n# main.py (continued - inside an async function)\n\n# Details for running the server command (replace with your actual server command)\nserver_params = StdioServerParameters(\n    command=\"your-mcp-server-command\", # The command to run the server\n    args=[\"--some-option\"],            # Any arguments the server needs\n)\n\n# Use stdio_client to start the server and get communication streams\n# 'async with' ensures resources are cleaned up automatically\nasync with stdio_client(server_params) as stdio_transport:\n    # stdio_transport gives us two things:\n    # read_stream: Where we receive messages FROM the server\n    # write_stream: Where we send messages TO the server\n    read_stream, write_stream = stdio_transport\n    # ... now we can create the ClientSession ...\n```\n\nThis snippet defines *how* to run the MCP server (using `StdioServerParameters`) and then uses the `stdio_client` helper. `stdio_client` starts the server process and gives us back `read_stream` and `write_stream` \u2013 the communication channels. Think of these like the \"in\" and \"out\" pipes for talking to the server process. We will cover other ways to connect in the [Transports](06_transports.md) chapter.\n\n**3. Create and Initialize the `ClientSession`**\n\nNow we have the communication streams, we can create our `ClientSession` instance and initialize the connection. Initialization is like a handshake: the client and server exchange greetings and basic information before starting real work.\n\n```python\n# main.py (continued - inside the 'async with stdio_client' block)\n\n    # Create the ClientSession using the streams\n    # 'async with' ensures the session is properly closed\n    async with ClientSession(read_stream, write_stream) as session:\n        logging.info(\"Session created. Initializing...\")\n        # Perform the initial handshake with the server\n        await session.initialize()\n        logging.info(\"Session initialized successfully!\")\n        # ... now we can use the session! ...\n```\n\nHere, we create the `ClientSession` instance, passing the `read_stream` and `write_stream` we got earlier. The `async with` statement is great because it automatically handles setting up and tearing down the session cleanly. The `await session.initialize()` line performs the crucial handshake with the server.\n\n**4. Use the Session to List Tools**\n\nWith the session initialized, we can finally ask the server about its tools!\n\n```python\n# main.py (continued - inside the 'async with ClientSession' block)\n\n        try:\n            logging.info(\"Asking the server for its tools...\")\n            # Call the list_tools method\n            tools_result = await session.list_tools()\n\n            logging.info(f\"Server responded with tools: {tools_result}\")\n\n            # Process the result (tools_result format detailed in MCP spec)\n            # Example: Extract tool names\n            tool_names = []\n            for item in tools_result: # The result is a list of tuples\n                if isinstance(item, tuple) and item[0] == \"tools\":\n                    tool_names = [tool.name for tool in item[1]]\n            logging.info(f\"Found tool names: {tool_names}\")\n\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n\n```\n\nThis is where the magic happens! `await session.list_tools()` sends the \"tools/list\" request to the server and waits for the response. The `ClientSession` handles the low-level communication. The server sends back a list of available [Tools](02_tools.md), which `session.list_tools()` returns as a Python object (specifically, a `ListToolsResult`, which is often a list containing a tuple like `('tools', [Tool(...), Tool(...)])`). We then log the raw result and extract just the names for clarity.\n\n**Putting it together (Simplified Main Structure):**\n\n```python\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_tools():\n    server_params = StdioServerParameters(\n        command=\"your-mcp-server-command\", # Replace with actual command\n        args=[],\n    )\n\n    try:\n        # Connect using stdio\n        async with stdio_client(server_params) as (read_stream, write_stream):\n            # Create and manage the session\n            async with ClientSession(read_stream, write_stream) as session:\n                await session.initialize() # Handshake!\n                logging.info(\"Session Initialized.\")\n\n                # Ask for tools\n                tools_result = await session.list_tools()\n                logging.info(f\"Tools result from server: {tools_result}\")\n\n                # Basic processing of the result\n                tool_names = []\n                for item in tools_result:\n                   if item[0] == \"tools\": tool_names = [t.name for t in item[1]]\n                logging.info(f\"Extracted Tool Names: {tool_names}\")\n\n    except Exception as e:\n        logging.error(f\"Failed to connect or list tools: {e}\")\n\nif __name__ == \"__main__\":\n    # Make sure to replace \"your-mcp-server-command\" before running!\n    # If you don't have one yet, you can't run this, but you see the structure.\n    # We'll learn about servers in the [FastMCP Server](05_fastmcp_server.md) chapter.\n    # asyncio.run(connect_and_list_tools())\n    logging.info(\"Example structure shown. Replace server command to run.\")\n```\n\nThis shows the complete flow: set up connection parameters, connect using `stdio_client`, create the `ClientSession`, initialize it, call `list_tools`, and process the result, all within `async with` blocks for proper resource management.\n\n## Under the Hood: How `list_tools()` Works\n\nWhen you call `await session.list_tools()`, what's actually happening?\n\n1.  **Method Call:** Your Python code calls the `list_tools` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates a specific message formatted according to the MCP protocol. For `list_tools`, this message essentially says: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`. The `\"id\"` is used to match the eventual response.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` (which `stdio_client` connected to the server's standard input).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) receives the message, understands it's a request to list tools, gathers the information about its available tools, and prepares a response message.\n5.  **Response Sending:** The server sends back a response message, like `{\"jsonrpc\": \"2.0\", \"result\": [[\"tools\", [{\"name\": \"calculator\", ...}, {\"name\": \"weather\", ...}]]], \"id\": 1}`, through its standard output (which `stdio_client` connected to our `read_stream`).\n6.  **Receiving:** The `ClientSession` is constantly listening on the `read_stream` for incoming messages. It receives the response.\n7.  **Matching & Parsing:** It matches the response `id` (1) to the original request it sent. It parses the JSON data in the `result` field.\n8.  **Returning:** It converts the parsed data into the appropriate Python object (a `ListToolsResult`) and returns it to your code where you called `await session.list_tools()`.\n\nHere's a simplified diagram of the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel (e.g., stdio)\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.list_tools()\n    CS->>CS: Create MCP \"tools/list\" request (ID=1)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Process request, find tools\n    Server->>Server: Create MCP response with tools (ID=1)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=1, parse result\n    CS-->>-YourCode: Return Python list of tools\n```\n\nThe `ClientSession` (along with the underlying transport mechanism like `stdio_client`) handles steps 2, 3, 6, 7, and 8, abstracting away the complexities of the MCP protocol and communication channel management.\n\nLooking at the code (`src/mcp/client/session.py`), you can see the `list_tools` method:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def list_tools(self) -> types.ListToolsResult:\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.ListToolsRequest(\n                method=\"tools/list\", # The MCP method name\n            )\n        )\n        # 2. Send it and expect a ListToolsResult back\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # The expected Python type of the result\n        )\n```\n\nIt creates a `ListToolsRequest` object (which knows its MCP method name is `\"tools/list\"`) and then calls the internal `send_request` method (part of the `BaseSession` it inherits from). `send_request` handles the actual sending, waiting for the response, and parsing it into the specified `types.ListToolsResult` type.\n\n## Conclusion\n\nYou've learned about the `ClientSession`, the core component in `python-sdk` for interacting with MCP servers. It acts as your control panel, simplifying communication by:\n\n1.  Managing the connection.\n2.  Translating Python method calls (like `list_tools()`) into MCP requests.\n3.  Sending requests and receiving responses/notifications.\n4.  Translating MCP responses back into Python objects.\n\nYou saw how to set up a connection (using `stdio_client` as an example), create and initialize a `ClientSession`, and use it to perform a basic action like listing the server's tools.\n\nNow that you know how to communicate with a server and ask what tools it has, let's dive deeper into what these [Tools](02_tools.md) actually are and how you can use them in the next chapter!\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 10: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\02_tools.md ---\n# Chapter 2: Tools - Asking the Server to Do Things\n\nIn [Chapter 1: Client Session](01_client_session.md), we learned how to connect to an MCP server using `ClientSession` and ask it a basic question: \"What tools do you have?\" using `session.list_tools()`. That's like looking at the buttons on a calculator or the commands available in a menu.\n\nBut just knowing the buttons isn't enough \u2013 we want to *press* them! This chapter is all about **Tools**: how to ask the server to actually *use* the capabilities it offers.\n\n## Motivation: Why Do We Need Tools?\n\nImagine your Python program needs to perform a specific action that it can't easily do by itself. Maybe:\n\n*   It needs to perform a complex mathematical calculation only the server knows how to do.\n*   It needs to take a screenshot of the user's screen (like in the `screenshot.py` example).\n*   It needs to fetch the latest weather information from a specialized service the server has access to.\n\nInstead of trying to build all this complex functionality into your own program, you can connect to an MCP server that already *has* these capabilities. The server exposes these capabilities as **Tools**.\n\n**Tools are like specialized commands or functions that the MCP server makes available for clients to run remotely.** Your program (the client) can ask the server to execute a specific tool, provide the necessary inputs, and get the result back.\n\nThink of the MCP server as a helpful assistant with a toolbox. You can ask the assistant, \"Please use your 'calculator' tool to add 5 and 3,\" or \"Use your 'screenshot' tool to capture the screen.\"\n\n## Using a Tool: Making the Request\n\nOkay, let's say we connected to a server (like we did in Chapter 1) and used `list_tools()` to find out it has a tool named `\"calculator\"`. How do we ask the server to use it?\n\nWe use the `session.call_tool()` method!\n\n**1. Connect and Initialize (Recap)**\n\nFirst, we need a connected and initialized `ClientSession`, just like in Chapter 1.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_use_tool(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can call tools!\n        # ... tool calling code goes here ...\n```\n\nThis code sets up the session. The `async with` block ensures the session is managed correctly, and `await session.initialize()` performs the handshake with the server.\n\n**2. Call the Tool**\n\nNow, inside that `async with` block, we can call `session.call_tool()`. We need to provide two main things:\n\n*   The **name** of the tool we want to use (e.g., `\"calculator\"`).\n*   A **dictionary** containing the specific inputs (arguments) the tool needs.\n\nLet's imagine our \"calculator\" tool needs an `\"operation\"` (like \"add\" or \"subtract\") and the numbers (`\"a\"` and `\"b\"`) to operate on.\n\n```python\n# Inside the async with block from above...\n\n        tool_name = \"calculator\"\n        tool_arguments = {\n            \"operation\": \"add\",\n            \"a\": 5,\n            \"b\": 3\n        }\n\n        try:\n            logging.info(f\"Calling tool '{tool_name}' with args: {tool_arguments}\")\n            # Ask the server to run the tool\n            result = await session.call_tool(tool_name, tool_arguments)\n\n            logging.info(f\"Server returned result: {result}\")\n\n            # Example: Extract the text result if available\n            text_result = None\n            if result and result.content and isinstance(result.content[0], TextContent):\n                 text_result = result.content[0].text\n            logging.info(f\"Extracted text from result: {text_result}\") # Should be '8'\n\n        except Exception as e:\n            logging.error(f\"Error calling tool '{tool_name}': {e}\")\n\n```\n\nHere's what's happening:\n1.  We define the `tool_name` we want to call (`\"calculator\"`).\n2.  We create a dictionary `tool_arguments` with the specific inputs the `calculator` tool needs for an addition.\n3.  `await session.call_tool(tool_name, tool_arguments)` sends the request to the server.\n4.  The server receives the request, finds its `calculator` tool, executes the `add` operation with `a=5` and `b=3`.\n5.  The server sends the result back.\n6.  `session.call_tool` receives the response and returns it as a Python object (typically a `CallToolResult`).\n7.  We log the result. A typical result for adding 5 and 3 might look something like `CallToolResult(content=[TextContent(type='text', text='8')], isError=False)`. We then extract the text part.\n\nIt's like telling the calculator assistant: \"Use the `calculator` tool. The operation is `add`, `a` is `5`, and `b` is `3`.\" The assistant does the math and tells you \"The result is 8.\"\n\n## Tool Inputs: What the Tool Needs (JSON Schema)\n\nHow does the server know *what* arguments (`operation`, `a`, `b`) the `calculator` tool needs? And how does the client know what to send?\n\nWhen a server defines a tool, it also defines an **input schema**. This schema is like a contract or a form that specifies exactly what inputs the tool expects, what their names are, and what type of data they should be (e.g., string, number, boolean). This schema is usually defined using a standard format called **JSON Schema**.\n\nRemember the `list_tools()` call from Chapter 1? The result it gives back includes not just the tool names but also their descriptions and their `inputSchema`.\n\nFor our hypothetical `calculator` tool, the `inputSchema` might look something like this (simplified):\n\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"operation\": {\n      \"type\": \"string\",\n      \"description\": \"The operation to perform (e.g., 'add', 'subtract')\"\n    },\n    \"a\": {\n      \"type\": \"number\",\n      \"description\": \"The first number\"\n    },\n    \"b\": {\n      \"type\": \"number\",\n      \"description\": \"The second number\"\n    }\n  },\n  \"required\": [\"operation\", \"a\", \"b\"]\n}\n```\n\nThis schema tells us:\n*   The input must be an object (like a Python dictionary).\n*   It must have properties named `operation`, `a`, and `b`.\n*   `operation` should be a string.\n*   `a` and `b` should be numbers.\n*   All three properties are required.\n\nThe `ClientSession` and the server use this schema to ensure that the arguments you provide in `call_tool` match what the tool expects. If you try to call the tool with missing or incorrect arguments, you'll likely get an error back from the server.\n\nSimilarly, the *result* of a tool call also has a structure, defined by the MCP protocol as `CallToolResult`, which contains a list of `content` items (like `TextContent`, `ImageContent`, or `EmbeddedResource`).\n\n## Under the Hood: How `call_tool()` Works\n\nWhen you call `await session.call_tool(\"calculator\", {\"operation\": \"add\", \"a\": 5, \"b\": 3})`, several steps happen behind the scenes:\n\n1.  **Method Call:** Your Python code calls the `call_tool` method on the `ClientSession` object.\n2.  **Request Creation:** The `ClientSession` creates an MCP request message for the `\"tools/call\"` method. This message includes the tool name (`\"calculator\"`) and the arguments dictionary. It looks something like: `{\"jsonrpc\": \"2.0\", \"method\": \"tools/call\", \"params\": {\"name\": \"calculator\", \"arguments\": {\"operation\": \"add\", \"a\": 5, \"b\": 3}}, \"id\": 2}`.\n3.  **Sending:** The `ClientSession` sends this formatted message through the `write_stream` to the server.\n4.  **Server Processing:** The MCP server receives the message. It identifies it as a `tools/call` request, finds the tool named `\"calculator\"`, validates the incoming arguments against the tool's `inputSchema`, and executes the tool's function (e.g., performs the addition).\n5.  **Response Creation:** The server prepares a response message containing the result (e.g., the number 8, formatted as `TextContent`). It looks something like: `{\"jsonrpc\": \"2.0\", \"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, \"id\": 2}`.\n6.  **Response Sending:** The server sends the response message back through its output stream to our `read_stream`.\n7.  **Receiving:** The `ClientSession` receives the response message.\n8.  **Matching & Parsing:** It matches the response `id` (2) to the original request and parses the JSON data in the `result` field.\n9.  **Returning:** It converts the parsed data into a `CallToolResult` Python object and returns it to your code.\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n\n    YourCode->>+CS: await session.call_tool(\"calculator\", {args})\n    CS->>CS: Create MCP \"tools/call\" request (ID=2)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>Server: Find \"calculator\" tool\n    Server->>Server: Validate args using inputSchema\n    Server->>Server: Execute tool function (add 5, 3)\n    Server->>Server: Create MCP response with result \"8\" (ID=2)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=2, parse result\n    CS-->>-YourCode: Return CallToolResult object\n```\n\nThe `ClientSession` handles all the tricky parts of formatting the request, sending/receiving messages, and parsing the response according to the MCP protocol.\n\nLooking at the `python-sdk` code (`src/mcp/client/session.py`), the `call_tool` method is similar in structure to `list_tools`:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n\n    async def call_tool(\n        self, name: str, arguments: dict[str, Any] | None = None\n    ) -> types.CallToolResult:\n        \"\"\"Send a tools/call request.\"\"\"\n        # 1. Create the specific request object\n        request_obj = types.ClientRequest(\n            types.CallToolRequest(\n                method=\"tools/call\", # The MCP method name\n                params=types.CallToolRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # 2. Send it and expect a CallToolResult back\n        return await self.send_request(\n            request_obj,\n            types.CallToolResult, # The expected Python type of the result\n        )\n```\n\nIt constructs a `CallToolRequest` (which knows its MCP method is `\"tools/call\"` and includes the `name` and `arguments`) and uses the internal `send_request` helper to manage the communication and result parsing.\n\n## A Glimpse at the Server Side\n\nWhile this tutorial focuses on the client (`python-sdk`), it's helpful to see briefly how a tool might be defined on the server side using the [FastMCP Server](05_fastmcp_server.md) framework (which we'll cover later).\n\nA server might define the `add` tool like this:\n\n```python\n# Example Server Code (e.g., using FastMCP)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Calculator Server\")\n\n@mcp.tool() # This decorator registers the function as a tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n# Server runs using mcp.run()...\n```\n\nNotice the `@mcp.tool()` decorator? That's how the `FastMCP` server framework makes the Python function `add` available as an MCP tool. The type hints (`a: int`, `b: int`, `-> int`) are automatically used to generate the `inputSchema` and determine the output type. Don't worry too much about the server code now; we'll explore it in Chapter 5.\n\n## Conclusion\n\nYou've now learned about **Tools** \u2013 the way MCP servers expose their specific capabilities. You saw how to:\n\n1.  Understand that Tools are functions run remotely on the server.\n2.  Use `session.call_tool()` to ask the server to execute a tool.\n3.  Provide the tool's name and the required arguments in a dictionary.\n4.  Understand that tools have defined inputs (via JSON Schema) and outputs (`CallToolResult`).\n5.  Get a basic idea of the request/response flow when calling a tool.\n\nYou can now not only ask a server *what* it can do (`list_tools`) but also ask it *to do* something (`call_tool`)!\n\nTools are powerful, but sometimes you just need to get data or information that the server holds, rather than asking it to perform an action. For this, MCP uses a concept called **Resources**. Let's explore them in the next chapter: [Resources](03_resources.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 11: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\03_resources.md ---\n# Chapter 3: Resources - Accessing Server Data\n\nIn the previous chapter, [Tools](02_tools.md), we learned how to ask the MCP server to *perform actions* using `session.call_tool()`. That's great for tasks like calculations or taking screenshots.\n\nBut what if we just need to *get some data* that the server has access to? Maybe the server can provide:\n\n*   The content of a specific configuration file.\n*   A list of files in a particular directory on the server machine.\n*   Data fetched from a web page or an API that only the server can reach.\n*   Dynamically generated information (like the current time on the server).\n\nThis is where **Resources** come in!\n\n## Motivation: Asking the Library for a Specific Book\n\nThink of the MCP server as a helpful librarian. We already know how to ask the librarian to *do* something (like \"calculate 5+3\" using a Tool). Now, we want to ask the librarian for a specific *item* from the library's collection.\n\n**Resources are like the specific books, documents, or data files that the library (MCP server) can provide access to.** Just like you need a catalog number to ask for a specific book, you need a **URI (Uniform Resource Identifier)** to ask the MCP server for a specific Resource.\n\nA URI looks like a web address or a file path, and it uniquely identifies the piece of data you want. Examples include:\n\n*   `file:///path/to/important_data.txt` (A specific file on the server's system)\n*   `http://example.com/latest_news` (Data fetched from a web URL by the server)\n*   `dir://desktop` (A special URI representing the server user's desktop directory)\n*   `resource://server-status` (A custom URI for dynamically generated status info)\n\nSo, the core idea is: Instead of asking the server to *do* something, we ask it to *give us the content* associated with a specific URI.\n\n## How Do We Know What Resources Are Available?\n\nBefore we can ask for a specific resource, we need to know which ones the server offers. Just like we used `session.list_tools()` to see the available tools, we use `session.list_resources()` to see the available resources.\n\n**1. Connect and Initialize (Recap)**\n\nAs always, we start with a connected and initialized `ClientSession`.\n\n```python\n# Simplified setup - assume we have streams from Chapter 1\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports from Ch 1\nfrom mcp.types import Resource # We'll need this type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_list_resources(read_stream, write_stream): # Assume streams are ready\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can list resources!\n        # ... listing code goes here ...\n```\n\nThis familiar setup ensures we have an active communication line to the server.\n\n**2. Listing the Resources**\n\nInside the `async with` block, we call `session.list_resources()`.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(\"Asking the server for available resources...\")\n            # Call the list_resources method\n            resource_list_result = await session.list_resources()\n\n            logging.info(f\"Server responded with resources: {resource_list_result}\")\n\n            # Example: Print the URI and name of each resource\n            if resource_list_result and resource_list_result.resources:\n                logging.info(\"Found Resources:\")\n                for resource in resource_list_result.resources:\n                    # resource is an mcp.types.Resource object\n                    logging.info(f\" - URI: {resource.uri}, Name: {resource.name}\")\n            else:\n                logging.info(\"Server reported no available resources.\")\n\n        except Exception as e:\n            logging.error(f\"Error listing resources: {e}\")\n\n```\n\nHere's the flow:\n1.  `await session.list_resources()` sends a request to the server asking for its resource catalog.\n2.  The server gathers information about all the resources it's configured to share.\n3.  The server sends back a response.\n4.  `session.list_resources()` receives the response and returns it as a `ListResourcesResult` object. This object contains a list called `resources`.\n5.  Each item in the `resources` list is a `Resource` object (defined in `mcp.types`), containing details like the `uri`, `name`, `description`, and `mimeType` (like 'text/plain' or 'image/png').\n6.  Our code then iterates through this list and prints the URI and name for each discovered resource.\n\nIf the server from the `simple-resource` example (`examples/servers/simple-resource/mcp_simple_resource/server.py`) was running, the output might show URIs like `file:///greeting.txt`, `file:///help.txt`, etc.\n\n## How Do We Read a Resource's Content?\n\nOkay, we've listed the resources and found one we're interested in, maybe `file:///greeting.txt`. How do we get its actual content (\"Hello! This is a sample text resource.\")?\n\nWe use the `session.read_resource()` method, providing the URI of the resource we want.\n\n**1. Connect, Initialize, and Find URI (Recap)**\n\nAssume we've already connected and maybe called `list_resources()` to get the URI we want.\n\n```python\n# Simplified setup - assume we have streams and know the URI\nimport asyncio\nimport logging\nfrom mcp import ClientSession # And other necessary imports\nfrom mcp.types import TextResourceContents # To check the result type\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_and_read_resource(read_stream, write_stream, resource_uri):\n    async with ClientSession(read_stream, write_stream) as session:\n        await session.initialize()\n        logging.info(\"Session initialized.\")\n        # Now we can read the resource!\n        # ... reading code goes here ...\n```\n\n**2. Reading the Resource**\n\nNow, we call `session.read_resource()` with the specific URI.\n\n```python\n# Inside the async with block from above...\n\n        try:\n            logging.info(f\"Asking server to read resource: {resource_uri}\")\n            # Call the read_resource method with the desired URI\n            read_result = await session.read_resource(resource_uri)\n\n            logging.info(f\"Server responded with content: {read_result}\")\n\n            # Example: Extract and print the text content if available\n            if read_result and read_result.contents:\n                first_content = read_result.contents[0]\n                # Check if it's text content\n                if isinstance(first_content, TextResourceContents):\n                    logging.info(f\"Resource content (text): '{first_content.text}'\")\n                else:\n                    # Could be binary (BlobResourceContents) or other types\n                    logging.info(f\"Resource content type: {type(first_content)}\")\n            else:\n                logging.info(\"Server returned no content for this resource.\")\n\n        except Exception as e:\n            # Handle potential errors, e.g., resource not found\n            logging.error(f\"Error reading resource '{resource_uri}': {e}\")\n\n```\n\nWhat happens here?\n1.  We specify the `resource_uri` we want to read (e.g., `\"file:///greeting.txt\"`).\n2.  `await session.read_resource(resource_uri)` sends a request to the server: \"Please give me the content for this URI.\"\n3.  The server finds the resource definition associated with that URI.\n4.  The server performs the action needed to get the data (e.g., opens and reads the file `/greeting.txt`, or calls a function, or fetches a URL).\n5.  The server packages the data into a response.\n6.  `session.read_resource()` receives the response and returns it as a `ReadResourceResult` object. This object contains a list called `contents`.\n7.  Each item in `contents` represents a piece of the resource's data. For simple text files, it's usually a single `TextResourceContents` object which has a `text` attribute holding the actual content. For binary files, it would be `BlobResourceContents` with base64-encoded data in a `blob` attribute.\n8.  Our code checks the type of the content and prints the text if available.\n\nIf we read `file:///greeting.txt` from the `simple-resource` example server, the extracted text would be `'Hello! This is a sample text resource.'`.\n\n## Under the Hood: How `read_resource()` Works\n\nWhen you call `await session.read_resource(\"file:///greeting.txt\")`:\n\n1.  **Method Call:** Your Python code calls `read_resource` on the `ClientSession`.\n2.  **Request Creation:** `ClientSession` creates an MCP request message for the `\"resources/read\"` method, including the URI `\"file:///greeting.txt\"` in the parameters. E.g., `{\"jsonrpc\": \"2.0\", \"method\": \"resources/read\", \"params\": {\"uri\": \"file:///greeting.txt\"}, \"id\": 3}`.\n3.  **Sending:** The `ClientSession` sends this request through the underlying [Transports](06_transports.md) (like stdio or SSE) to the server.\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or another MCP server) receives the request.\n5.  **Resource Lookup:** The server's `ResourceManager` looks up the URI `\"file:///greeting.txt\"` in its list of registered resources. It finds the corresponding `Resource` object (e.g., a `TextResource` or `FunctionResource`).\n6.  **Data Retrieval:** The server calls the `read()` method on that specific `Resource` object. This might involve reading a file (`FileResource`), returning stored text (`TextResource`), calling a Python function (`FunctionResource`), or fetching from a URL (`HttpResource`). In the `simple-resource` example, it looks up the name in the `SAMPLE_RESOURCES` dictionary.\n7.  **Response Creation:** The server takes the retrieved data (the string \"Hello! ...\") and creates an MCP response message. E.g., `{\"jsonrpc\": \"2.0\", \"result\": {\"contents\": [{\"type\": \"text\", \"text\": \"Hello!...\"}]}, \"id\": 3}`.\n8.  **Response Sending:** The server sends the response back through the [Transports](06_transports.md).\n9.  **Receiving:** `ClientSession` receives the response message.\n10. **Matching & Parsing:** It matches the response `id` (3) and parses the JSON data in the `result` field.\n11. **Returning:** It converts the parsed data into a `ReadResourceResult` object containing `TextResourceContents` and returns it to your code.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant ResMan as Server Resource Manager\n    participant DataSource as Actual Data (File, DB, etc)\n\n    YourCode->>+CS: await session.read_resource(\"file:///greeting.txt\")\n    CS->>CS: Create MCP \"resources/read\" request (ID=3)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+ResMan: Find resource for \"file:///greeting.txt\"\n    ResMan->>ResMan: Identify correct Resource object\n    ResMan->>+DataSource: Call object's read() method (e.g., read file)\n    DataSource-->>-ResMan: Return content (\"Hello!...\")\n    ResMan-->>-Server: Pass content back\n    Server->>Server: Create MCP response with content (ID=3)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=3, parse result\n    CS-->>-YourCode: Return ReadResourceResult object\n```\n\nThe client-side code in `python-sdk` (`src/mcp/client/session.py`) for `read_resource` follows the same pattern we saw for `list_tools` and `call_tool`: it creates a specific request object (`ReadResourceRequest`) and uses the internal `send_request` method to handle the communication and parsing into the expected result type (`ReadResourceResult`).\n\nOn the server side (like in `examples/fastmcp/desktop.py`), developers can easily define resources. The `@mcp.resource(\"dir://desktop\")` decorator, for example, registers the `desktop()` function. When a client requests `dir://desktop`, the server runs this function, gets the list of desktop files, and returns it. The `FastMCP` framework provides various `Resource` types (`TextResource`, `FileResource`, `FunctionResource`, etc.) to handle different data sources easily (see `src/mcp/server/fastmcp/resources/types.py`).\n\n## Conclusion\n\nYou've now learned about **Resources**, the MCP concept for accessing data exposed by a server.\n\n*   Resources represent data sources identified by **URIs**.\n*   They can be files, directories, web content, or dynamically generated data.\n*   You can discover available resources using `session.list_resources()`.\n*   You can fetch the content of a specific resource using `session.read_resource(uri)`.\n*   The server handles the details of how to retrieve the data based on the URI.\n\nNow you know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* (Resources). Sometimes, however, you might want to provide the server with structured input to generate more complex, context-aware responses, often for use with Large Language Models (LLMs). That's where the next concept comes in: [Prompts](04_prompts.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 12: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\04_prompts.md ---\n# Chapter 4: Prompts - Pre-written Recipes for LLMs\n\nIn the last chapter, [Resources](03_resources.md), we saw how to ask the server for specific pieces of data using URIs. We know how to ask the server to *do things* ([Tools](02_tools.md)) and how to *get data* ([Resources](03_resources.md)).\n\nBut what if we want the server to help us structure a more complex conversation, especially when talking to a Large Language Model (LLM)? Sometimes, we need more than just a single command or a single piece of data. We might need a pre-defined sequence of instructions or messages.\n\nThis is where **Prompts** come in handy!\n\n## Motivation: Giving Instructions to an AI Assistant\n\nImagine you want to use an LLM (like ChatGPT or Claude) to summarize a long piece of text. You can't just send the text and hope for the best. You need to *instruct* the LLM. A typical instruction might look like this:\n\n*   **You (User):** \"Please summarize the following text for me:\"\n*   **You (User):** \"[Your long text here]\"\n*   **AI (Assistant):** \"[The summary]\"\n\nNotice this isn't just one message, but a *sequence* of messages, setting up the context and the request.\n\nNow, imagine the MCP server is designed to help interact with LLMs. It could provide pre-written templates for common tasks like summarization, translation, or question answering. These templates ensure the LLM gets instructions in a format it understands well.\n\n**Prompts are like pre-written recipes or instruction manuals provided by the MCP server.** They define a sequence of messages, often with placeholders (arguments) that you can fill in. The server gives you the recipe, you add your specific ingredients (like the text to summarize), and the server (or the `python-sdk` using the server's definition) generates the final, formatted instructions ready to be sent to an LLM.\n\n## Key Concepts: Templates, Arguments, and Messages\n\n1.  **Prompt Template:** This is the structure defined by the server. It specifies the sequence of messages and where arguments can be inserted. For our summarization example, the template might define two messages: one asking for a summary and another containing the text itself.\n2.  **Arguments:** These are the specific details you provide when you use a prompt. In our example, the main argument would be the actual `text` you want summarized. Arguments are like filling in the blanks in the recipe. Prompts can be *static* (no arguments needed) or *dynamic* (requiring arguments).\n3.  **Generated Messages:** When you ask the server to \"get\" a prompt and provide the necessary arguments, it renders the template. The result is a list of `PromptMessage` objects, each with a `role` (\"user\" or \"assistant\") and `content` (like text or images). This list is exactly what you might send to an LLM API.\n\n## How to Use Prompts\n\nUsing prompts involves two main steps: finding out which prompt recipes the server has, and then asking the server to prepare one of those recipes with your specific ingredients.\n\n**1. Listing Available Prompts**\n\nFirst, you need to know what prompts the server offers. Similar to listing tools and resources, you use the `session.list_prompts()` method.\n\n```python\n# Assume 'session' is an initialized ClientSession from Chapter 1\n\nimport logging\nfrom mcp.types import Prompt # We need this type\n\n# Configure logging (helpful for seeing what's happening)\nlogging.basicConfig(level=logging.INFO)\n\nasync def list_server_prompts(session):\n    try:\n        logging.info(\"Asking the server for available prompts...\")\n        # Call the list_prompts method\n        list_result = await session.list_prompts() # Usually PaginatedResult\n\n        logging.info(f\"Server responded with prompts: {list_result}\")\n\n        # Example: Print the name and description of each prompt\n        if list_result and list_result.prompts:\n            logging.info(\"Found Prompts:\")\n            for prompt in list_result.prompts:\n                # prompt is an mcp.types.Prompt object\n                logging.info(f\" - Name: {prompt.name}\")\n                logging.info(f\"   Description: {prompt.description}\")\n                # We could also inspect prompt.arguments here\n        else:\n            logging.info(\"Server reported no available prompts.\")\n\n    except Exception as e:\n        logging.error(f\"Error listing prompts: {e}\")\n\n# Example usage (within an async function with an active session):\n# await list_server_prompts(session)\n```\n\n*   `await session.list_prompts()` sends a `prompts/list` request to the server.\n*   The server replies with a `ListPromptsResult`, which contains a list of `Prompt` objects.\n*   Each `Prompt` object tells you its `name` (like `\"summarize\"`), `description`, and importantly, what `arguments` it expects (if any).\n\nIf the server offered a summarization prompt, the output might include:\n\n```\nINFO:root:Found Prompts:\nINFO:root: - Name: summarize\nINFO:root:   Description: Summarizes the provided text.\n```\n\n**2. Getting a Specific Prompt (with Arguments)**\n\nOnce you know the name of the prompt you want (e.g., `\"summarize\"`) and what arguments it needs (e.g., a `text` argument), you can ask the server to generate the message sequence using `session.get_prompt()`.\n\nLet's assume the `\"summarize\"` prompt requires one argument named `\"text_to_summarize\"`.\n\n```python\n# Assume 'session' is an initialized ClientSession\n# Assume we know the prompt 'summarize' exists and needs 'text_to_summarize'\n\nimport logging\nfrom mcp.types import PromptMessage # For checking result types\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def get_summarize_prompt(session, input_text):\n    prompt_name = \"summarize\"\n    prompt_arguments = {\n        \"text_to_summarize\": input_text\n    }\n\n    try:\n        logging.info(f\"Getting prompt '{prompt_name}' with args: {prompt_arguments}\")\n        # Ask the server to render the prompt with our arguments\n        get_result = await session.get_prompt(prompt_name, prompt_arguments)\n\n        logging.info(f\"Server returned GetPromptResult: {get_result}\")\n\n        # Example: Print the generated messages\n        if get_result and get_result.messages:\n            logging.info(\"Generated Messages:\")\n            for message in get_result.messages:\n                # message is an mcp.types.PromptMessage object\n                logging.info(f\" - Role: {message.role}\")\n                # Content could be TextContent, ImageContent, etc.\n                logging.info(f\"   Content Type: {message.content.type}\")\n                if hasattr(message.content, 'text'):\n                    logging.info(f\"   Text: '{message.content.text[:50]}...'\") # Show start of text\n        else:\n            logging.info(\"Server returned no messages for this prompt.\")\n\n    except Exception as e:\n        logging.error(f\"Error getting prompt '{prompt_name}': {e}\")\n\n# Example usage:\n# my_text = \"This is a very long piece of text that needs summarizing...\"\n# await get_summarize_prompt(session, my_text)\n```\n\n*   We define the `prompt_name` and a dictionary `prompt_arguments` containing the required input.\n*   `await session.get_prompt(prompt_name, prompt_arguments)` sends a `prompts/get` request with the name and arguments.\n*   The server finds the `\"summarize\"` prompt template.\n*   It runs the logic associated with that template, inserting our `text_to_summarize` into the appropriate place(s).\n*   The server sends back a `GetPromptResult` containing the final list of `PromptMessage` objects.\n*   Our code then iterates through these messages and prints their roles and content.\n\nThe output might look something like this:\n\n```\nINFO:root:Generated Messages:\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'Please summarize the following text:'\nINFO:root: - Role: user\nINFO:root:   Content Type: text\nINFO:root:   Text: 'This is a very long piece of text that needs summa...'\n```\n\nThis sequence of messages is now perfectly formatted to be sent to an LLM for the summarization task!\n\n## Under the Hood: How `get_prompt()` Works\n\nWhat happens when you call `await session.get_prompt(\"summarize\", {\"text_to_summarize\": \"...\"})`?\n\n1.  **Method Call:** Your Python code calls `get_prompt`.\n2.  **Request Creation:** The `ClientSession` creates an MCP `prompts/get` request message, including the prompt name `\"summarize\"` and the provided `arguments` dictionary. `{\"jsonrpc\": \"2.0\", \"method\": \"prompts/get\", \"params\": {\"name\": \"summarize\", \"arguments\": {\"text_to_summarize\": \"...\"}}, \"id\": 4}`.\n3.  **Sending:** The request is sent to the server via the active [Transports](06_transports.md).\n4.  **Server Processing:** The [FastMCP Server](05_fastmcp_server.md) (or other MCP server) receives the request.\n5.  **Prompt Lookup:** The server's `PromptManager` looks up the prompt named `\"summarize\"`.\n6.  **Rendering:** The server executes the function or logic associated with the `\"summarize\"` prompt template, passing in the `arguments`. This function constructs the list of `PromptMessage` objects (e.g., the two user messages in our example).\n7.  **Response Creation:** The server creates an MCP response message containing the generated `messages` list within a `GetPromptResult`. `{\"jsonrpc\": \"2.0\", \"result\": {\"messages\": [...]}, \"id\": 4}`.\n8.  **Response Sending:** The server sends the response back.\n9.  **Receiving & Parsing:** The `ClientSession` receives the response, matches the `id`, and parses the `result` into a `GetPromptResult` object.\n10. **Returning:** The `GetPromptResult` object is returned to your code.\n\nHere\u2019s a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant YourCode as Your Python Code\n    participant CS as ClientSession\n    participant Transport as Communication Channel\n    participant Server as MCP Server\n    participant PromptMan as Server Prompt Manager\n\n    YourCode->>+CS: await session.get_prompt(\"summarize\", {args})\n    CS->>CS: Create MCP \"prompts/get\" request (ID=4)\n    CS->>+Transport: Send request message\n    Transport->>+Server: Deliver request message\n    Server->>+PromptMan: Find prompt \"summarize\"\n    PromptMan->>PromptMan: Get prompt definition/function\n    PromptMan->>PromptMan: Render messages using args (\"...\")\n    PromptMan-->>-Server: Return generated messages\n    Server->>Server: Create MCP response with messages (ID=4)\n    Server->>-Transport: Send response message\n    Transport->>-CS: Deliver response message\n    CS->>CS: Receive message, match ID=4, parse result\n    CS-->>-YourCode: Return GetPromptResult object\n```\n\n**Code Glimpse:**\n\nOn the client side (`src/mcp/client/session.py`), `get_prompt` follows the familiar pattern:\n\n```python\n# src/mcp/client/session.py (simplified snippet)\n    async def get_prompt(\n        self, name: str, arguments: dict[str, str] | None = None\n    ) -> types.GetPromptResult:\n        \"\"\"Send a prompts/get request.\"\"\"\n        # Create the specific request object\n        request_obj = types.ClientRequest(\n            types.GetPromptRequest(\n                method=\"prompts/get\", # The MCP method name\n                params=types.GetPromptRequestParams(\n                    name=name, arguments=arguments or {}\n                ),\n            )\n        )\n        # Send it and expect a GetPromptResult back\n        return await self.send_request(\n            request_obj,\n            types.GetPromptResult, # The expected Python type\n        )\n```\n\nOn the server side, using the [FastMCP Server](05_fastmcp_server.md) framework, defining a prompt can be quite straightforward. The `simple-prompt` example (`examples/servers/simple-prompt/mcp_simple_prompt/server.py`) shows a more manual approach, but `FastMCP` provides helpers. For instance, the `echo.py` example uses a decorator:\n\n```python\n# examples/fastmcp/echo.py (simplified snippet)\nfrom mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Echo Server\")\n\n# This function becomes the implementation for the 'echo' prompt\n@mcp.prompt(\"echo\") # Decorator registers the prompt\ndef echo_prompt(text: str) -> str: # Argument 'text' is automatically detected\n    # The returned string is automatically wrapped in a UserMessage\n    return text\n\n# Server runs...\n```\n\nThe `@mcp.prompt(\"echo\")` decorator registers the `echo_prompt` function. `FastMCP` inspects the function's parameters (like `text: str`) to determine the required `arguments` for the prompt. When `get_prompt(\"echo\", {\"text\": \"hello\"})` is called, this function runs, and its return value (`\"hello\"`) is automatically converted into the standard `GetPromptResult` structure with a `PromptMessage` containing `TextContent`. More complex prompts can return lists of message dictionaries or `Message` objects directly (see `src/mcp/server/fastmcp/prompts/base.py`).\n\n## Conclusion\n\nYou've now learned about **Prompts**, the server-provided templates for generating structured message sequences, particularly useful for interacting with LLMs.\n\n*   Prompts are like recipes defined by the server.\n*   They can be static or dynamic (accepting **arguments**).\n*   You can discover available prompts using `session.list_prompts()`.\n*   You can generate the final message sequence for a specific prompt using `session.get_prompt(name, arguments)`.\n*   The result is a `GetPromptResult` containing a list of `PromptMessage` objects, ready for use.\n\nWith Tools, Resources, and Prompts, you have a powerful toolkit for interacting with MCP servers. But how are these servers actually built? In the next chapter, we'll get a look behind the curtain at the server side with [FastMCP Server](05_fastmcp_server.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 13: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\05_fastmcp_server.md ---\n# Chapter 5: FastMCP Server - Building Your Own Assistant\n\nIn the previous chapters ([Client Session](01_client_session.md), [Tools](02_tools.md), [Resources](03_resources.md), [Prompts](04_prompts.md)), we learned how to *use* an existing MCP server from our Python code (the client). We learned how to connect, list capabilities, call tools, read resources, and get prompts.\n\nBut what if *you* want to create your own assistant? Maybe you have a special calculation, access to unique data, or a cool function you want to make available to other programs (or even to an AI like Claude). How do you build the *server* side?\n\n## Motivation: Making Server Creation Easy\n\nBuilding a server that speaks the MCP protocol might sound complicated. You'd need to handle:\n\n*   Receiving messages (like JSON-RPC requests).\n*   Parsing those messages correctly.\n*   Understanding requests like \"tools/list\" or \"tools/call\".\n*   Routing the request to the correct piece of your code.\n*   Generating JSON Schemas to describe your tool inputs.\n*   Formatting your results back into the correct MCP response format.\n*   Sending the response back.\n\nThat's a lot of boilerplate code just to handle the communication! Wouldn't it be great if you could just write your core Python functions (the actual logic for your tools, resources, etc.) and have something else handle all the tricky MCP communication details?\n\nThat's exactly what **FastMCP Server** does!\n\n**`FastMCP` is a high-level, user-friendly framework within `python-sdk` for creating MCP servers.** Think of it like a helpful construction kit for building your MCP assistant. You define the *capabilities* (tools, resources, prompts) using simple Python functions and special markers called **decorators** (`@mcp.tool()`, `@mcp.resource()`, `@mcp.prompt()`). `FastMCP` then automatically handles all the underlying MCP protocol details, generates the necessary descriptions (like JSON schemas), and routes incoming client requests to your functions.\n\nIt lets you focus on writing the *logic* for what your server can *do*, not the complex setup of how it *communicates*.\n\n**Our Goal:** Let's build a very simple MCP server that offers just one tool: adding two numbers.\n\n## Building Our Adder Server: Step-by-Step\n\nWe'll use `FastMCP` to create a server that a client can connect to and ask to add numbers.\n\n**1. Import `FastMCP`**\n\nFirst, we need to import the main class from the library.\n\n```python\n# adder_server.py\nfrom mcp.server.fastmcp import FastMCP\n\n# We'll add more code below...\n```\n\nThis line brings the `FastMCP` framework into our script.\n\n**2. Create a `FastMCP` Instance**\n\nNext, we create an instance of the `FastMCP` class. This represents our server. We can give it a name.\n\n```python\n# adder_server.py (continued)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n# We'll define tools next...\n```\n\nThink of `mcp` as the main control object for our server definition.\n\n**3. Define the Tool Logic (A Simple Function)**\n\nWe need the actual code that performs the addition. This is just a regular Python function. Notice the use of type hints (`a: int`, `b: int`, `-> int`). These are important!\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n# This function contains the logic for our tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Now, how do we tell FastMCP this is a tool?\n```\n\nThis is the core logic. It takes two integers and returns their sum. The `print` statements are just for us to see when it gets called.\n\n**4. Register the Function as a Tool (The Magic Decorator!)**\n\nHow does `FastMCP` know that our `add` function should be made available as an MCP tool? We use the `@mcp.tool()` **decorator**. A decorator is like a sticky note you put on a function to give it special powers or register it with a framework.\n\n```python\n# adder_server.py (continued)\n# ... (import and mcp instance creation) ...\n\n@mcp.tool() # <-- This is the FastMCP decorator!\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# We're almost ready to run the server...\n```\n\nBy adding `@mcp.tool()` right above our `add` function, we tell `FastMCP`:\n\n*   \"Make this function available as an MCP tool.\"\n*   **Automatically use the function name (`add`) as the tool name.** (You can specify a different name like `@mcp.tool(\"adder\")` if you want).\n*   **Automatically use the function's docstring (`\"\"\"Adds two numbers together.\"\"\"`) as the tool's description.**\n*   **Automatically look at the type hints (`a: int`, `b: int`) to generate the JSON `inputSchema`** that tells clients what arguments are needed (an integer `a` and an integer `b`).\n*   **Automatically understand the return type (`-> int`)** to know what kind of result the tool produces.\n\nThis single line saves us from writing a lot of configuration code!\n\n**5. Run the Server**\n\nFinally, we need to tell our server to start running and listening for connections.\n\n```python\n# adder_server.py (complete example)\nfrom mcp.server.fastmcp import FastMCP\n\n# Create the server instance\nmcp = FastMCP(\"Adder Server\")\n\n@mcp.tool()\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds two numbers together.\"\"\"\n    print(f\"Server: Received request to add {a} and {b}\")\n    result = a + b\n    print(f\"Server: Calculation result is {result}\")\n    return result\n\n# Add this block to start the server when the script is run\nif __name__ == \"__main__\":\n    print(\"Starting Adder Server...\")\n    # This runs the server, listening for connections via stdio by default\n    mcp.run()\n    print(\"Adder Server stopped.\")\n\n```\n\nThe `if __name__ == \"__main__\":` block is standard Python practice to make code run only when the script is executed directly. `mcp.run()` starts the `FastMCP` server. By default, it uses \"stdio\" (standard input/output) as the communication channel, meaning it behaves like a command-line application waiting for requests on its input and sending responses to its output. We'll learn more about different ways servers and clients can communicate in the next chapter on [Transports](06_transports.md).\n\nIf you save this as `adder_server.py` and run it (e.g., `python adder_server.py`), it will print \"Starting Adder Server...\" and wait. You could then use a client (like the one from [Chapter 2: Tools](02_tools.md), configured to run this script) to connect and call the `add` tool!\n\n## What Happens Under the Hood?\n\nLet's trace what happens when a client (using `ClientSession` from Chapter 1) calls `session.call_tool(\"add\", {\"a\": 5, \"b\": 3})` on our running `adder_server.py`:\n\n1.  **Client Sends Request:** The `ClientSession` creates an MCP `tools/call` request message (like `{\"method\": \"tools/call\", \"params\": {\"name\": \"add\", \"arguments\": {\"a\": 5, \"b\": 3}}, ...}`) and sends it over the communication channel (e.g., stdio).\n2.  **Transport Delivers:** The underlying transport layer ([Transports](06_transports.md)) delivers this raw message to the running `FastMCP` server.\n3.  **FastMCP Receives & Parses:** The `FastMCP` framework reads the incoming message. It understands the MCP protocol and parses the JSON to identify it's a `tools/call` request for the tool named `add` with arguments `a=5` and `b=3`.\n4.  **Finds the Tool Function:** `FastMCP` looks up the tool name \"add\" in its internal registry (where the `@mcp.tool()` decorator placed our `add` function).\n5.  **Validates Arguments:** It checks if the provided arguments (`{\"a\": 5, \"b\": 3}`) match the `inputSchema` it automatically generated from the `add(a: int, b: int)` type hints. In this case, they do. (If not, it would generate an error response).\n6.  **Calls Your Function:** `FastMCP` calls your actual Python function: `add(a=5, b=3)`. Your `print` statements will execute.\n7.  **Receives Return Value:** Your function returns the integer `8`.\n8.  **Formats Response:** `FastMCP` takes the return value (`8`) and automatically formats it into the standard MCP `CallToolResult` structure, likely as `TextContent`. The response message might look like `{\"result\": {\"content\": [{\"type\": \"text\", \"text\": \"8\"}]}, ...}`.\n9.  **Sends Response:** `FastMCP` sends this JSON response message back over the communication channel.\n10. **Client Receives:** The client's `ClientSession` receives the response, parses it, and returns the `CallToolResult` object to the calling code.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport as Communication Channel\n    participant FastMCP as FastMCP Framework\n    participant AdderFunc as Your add() function\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"add\", args={a:5, b:3})\n    Transport->>+FastMCP: Deliver raw message\n    FastMCP->>FastMCP: Parse message, find 'add' tool registration\n    FastMCP->>FastMCP: Validate args against auto-generated schema\n    FastMCP->>+AdderFunc: Call add(a=5, b=3)\n    Note over AdderFunc: Prints logs, returns 8\n    AdderFunc-->>-FastMCP: Return result (8)\n    FastMCP->>FastMCP: Format 8 into MCP response (e.g., TextContent)\n    FastMCP->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\nYou don't *need* to know the internals, but it can be helpful. Inside `FastMCP` (see `src/mcp/server/fastmcp/server.py`):\n\n*   The `FastMCP` class initializes internal managers like `ToolManager`, `ResourceManager`, and `PromptManager`.\n*   The `@mcp.tool()` decorator calls the `add_tool` method on the `ToolManager`.\n*   `add_tool` uses Python's `inspect` module to look at your function's signature (parameters, type hints, docstring). It uses this information (often with help from the Pydantic library) to build the `MCPTool` definition, including the `inputSchema`. It stores this definition.\n*   `FastMCP` sets up handlers (like `self._mcp_server.call_tool()(self.call_tool)`) that map incoming MCP methods (`tools/call`) to internal `FastMCP` methods.\n*   The internal `call_tool` method asks the `ToolManager` to execute the registered tool function, passing the arguments.\n*   Helper functions like `_convert_to_content` take your Python function's return value and package it into the standard MCP content types (`TextContent`, `ImageContent`, etc.).\n\nThe key takeaway is that `FastMCP` uses Python's introspection capabilities (looking at function details) and decorators to automate the tedious parts of building an MCP server.\n\n## Adding Resources and Prompts\n\nJust like `@mcp.tool()`, `FastMCP` provides decorators for defining resources and prompts:\n\n*   **`@mcp.resource(uri)`:** Registers a function to provide data for a specific URI. The URI can even include template parameters (like `greeting://{name}`) that match function arguments. `FastMCP` handles mapping the URI request to your function. (See [Resources](03_resources.md)).\n\n    ```python\n    # From examples/fastmcp/readme-quickstart.py\n    @mcp.resource(\"greeting://{name}\")\n    def get_greeting(name: str) -> str:\n        \"\"\"Get a personalized greeting\"\"\"\n        return f\"Hello, {name}!\"\n    ```\n\n*   **`@mcp.prompt(name)`:** Registers a function that generates a list of prompt messages, often taking arguments defined by the function's parameters. `FastMCP` handles generating the `MCPPrompt` definition and rendering the messages when requested. (See [Prompts](04_prompts.md)).\n\n    ```python\n    # From examples/fastmcp/echo.py\n    @mcp.prompt(\"echo\")\n    def echo_prompt(text: str) -> str:\n        # FastMCP automatically converts the returned string\n        # into a User message in the GetPromptResult\n        return text\n    ```\n\nThese work on the same principle: write a Python function with type hints and a docstring, add the decorator, and `FastMCP` handles the MCP integration.\n\n## Conclusion\n\nCongratulations! You've taken your first step into building your own MCP server using the `FastMCP` framework. You learned:\n\n*   `FastMCP` simplifies server development by handling complex MCP protocol details automatically.\n*   You can define server capabilities (Tools, Resources, Prompts) using standard Python functions.\n*   Decorators like `@mcp.tool()`, `@mcp.resource()`, and `@mcp.prompt()` are the magic that registers your functions with `FastMCP`.\n*   `FastMCP` uses type hints and docstrings to automatically generate schemas and descriptions.\n*   Running the server is as simple as calling `mcp.run()`.\n\nYou now have the power to create your own \"assistants\" that can offer custom capabilities over the MCP protocol.\n\nBut how exactly do the client and server *talk* to each other? Whether it's stdio, HTTP/SSE, or something else, the communication channel is crucial. In the next chapter, we'll explore these communication methods: [Transports](06_transports.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 14: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\06_transports.md ---\n# Chapter 6: Transports - The Delivery Service\n\nIn the last chapter, [FastMCP Server](05_fastmcp_server.md), we learned how to build our own MCP server assistant using the `FastMCP` framework. We saw how easy it is to define tools and have `FastMCP` handle the MCP protocol details.\n\nBut we skipped over a crucial detail: how does our client code, using the `ClientSession` from [Chapter 1: Client Session](01_client_session.md), actually *talk* to the server we built? They might be running on the same computer, or maybe the server is on a different machine across the internet. How do the request and response messages physically travel between them?\n\nThis is where **Transports** come in!\n\n## Motivation: Choosing Your Delivery Method\n\nImagine you wrote a letter (an MCP request message, like asking to call a tool). You need to send it to your friend (the MCP server). How do you get it there?\n\n*   You could hand it directly to them if they're right next to you (like a local program).\n*   You could use the postal service (like standard input/output for local programs).\n*   You could use a special courier service that provides continuous updates (like Server-Sent Events over the web).\n*   You could establish a direct, instant phone line (like WebSockets).\n\nEach method gets the letter delivered, but they work differently and are suited for different situations.\n\nIn the world of `python-sdk` and MCP, the **Transport** is the underlying communication mechanism chosen to send messages between the client and the server. It's the actual delivery service used for the MCP messages formatted by `ClientSession` and understood by the server.\n\n## What is a Transport?\n\nA **Transport** handles the low-level details of setting up a connection and sending/receiving the actual bytes that make up the MCP messages. Think of it as the plumbing that connects the client and the server.\n\nThe `ClientSession` we learned about in [Chapter 1: Client Session](01_client_session.md) is responsible for *formatting* the messages (like `tools/list` requests) and *interpreting* the replies. But it relies on a Transport to do the actual sending and receiving over the chosen communication channel.\n\nThe `python-sdk` provides built-in support for several common transports:\n\n1.  **Standard Input/Output (Stdio):**\n    *   **Analogy:** Sending letters via postal mail to a local address, or talking to a command-line tool.\n    *   **How it works:** The client starts the server as a local process (like running a command in your terminal). The client sends messages to the server's standard input (\"stdin\") and reads responses from the server's standard output (\"stdout\").\n    *   **Use Case:** Perfect when the client application needs to start and manage a local MCP server process running on the same machine. This is what we implicitly used in Chapter 1 and Chapter 5!\n\n2.  **Server-Sent Events (SSE over HTTP):**\n    *   **Analogy:** A continuous news ticker feed delivered over the web.\n    *   **How it works:** The client connects to a specific web URL on the server using standard HTTP. The server can then *push* messages (events) to the client over this connection whenever it wants. To send messages *to* the server, the client typically makes separate standard HTTP POST requests to another URL defined by the server.\n    *   **Use Case:** Good for web-based scenarios where the server needs to proactively send updates or notifications to the client (like progress updates, new messages).\n\n3.  **WebSockets:**\n    *   **Analogy:** A direct, two-way phone line established over the web.\n    *   **How it works:** The client and server establish a persistent, full-duplex (two-way) connection. Both the client and server can send messages to each other at any time over this single connection once it's established.\n    *   **Use Case:** Ideal for real-time, interactive applications requiring low-latency communication in both directions (e.g., chat applications, live dashboards, collaborative tools).\n\n## Using Transports with `ClientSession`\n\nThe beauty is that `ClientSession` itself doesn't need to know the complex details of *how* stdio, SSE, or WebSockets work. It just needs two things:\n\n*   A way to **read** incoming messages (`read_stream`).\n*   A way to **write** outgoing messages (`write_stream`).\n\nThe `python-sdk` provides helper functions (like `stdio_client`, `sse_client`, `websocket_client`) that set up the specific transport mechanism and give you back these two streams. You then pass these streams to `ClientSession`.\n\nLet's see how you'd use each transport helper.\n\n**1. Stdio Transport**\n\nThis is the one we saw in [Chapter 1: Client Session](01_client_session.md). We define how to run the server command and then use `stdio_client`.\n\n```python\n# Using stdio transport (like in Chapter 1)\nimport asyncio\nimport logging\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client # <-- Stdio helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_stdio(server_command: str):\n    server_params = StdioServerParameters(command=server_command)\n\n    # stdio_client starts the process and provides streams\n    async with stdio_client(server_params) as (read_stream, write_stream):\n        logging.info(\"Stdio transport connected.\")\n        # ClientSession uses the streams from stdio_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via stdio.\")\n            # ... use session.list_tools(), session.call_tool(), etc. ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via stdio: {tools}\")\n\n# Example: asyncio.run(connect_via_stdio(\"python your_server_script.py\"))\n```\n\n*   `StdioServerParameters` tells `stdio_client` *how* to run the server process.\n*   `stdio_client(...)` starts the server process and yields the `read_stream` (connected to server's stdout) and `write_stream` (connected to server's stdin).\n*   `ClientSession(read_stream, write_stream)` uses these streams to communicate.\n\n**2. SSE Transport**\n\nTo connect to an MCP server running with an SSE endpoint (e.g., hosted by a web server), you use `sse_client`.\n\n```python\n# Using SSE transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client # <-- SSE helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_sse(server_sse_url: str):\n    # sse_client connects to the URL and provides streams\n    # It handles the SSE event listening and POSTing for sending\n    async with sse_client(server_sse_url) as (read_stream, write_stream):\n        logging.info(\"SSE transport connected.\")\n        # ClientSession uses the streams from sse_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via SSE.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via SSE: {tools}\")\n\n# Example: asyncio.run(connect_via_sse(\"http://localhost:8000/sse\"))\n```\n\n*   `sse_client(server_sse_url)` connects to the specified HTTP endpoint, sets up the SSE listener for incoming messages (`read_stream`), and prepares to send outgoing messages via HTTP POST (`write_stream`).\n*   `ClientSession` uses these streams, unaware of the underlying HTTP/SSE mechanics.\n\n**3. WebSocket Transport**\n\nTo connect to an MCP server using a WebSocket endpoint, you use `websocket_client`.\n\n```python\n# Using WebSocket transport\nimport asyncio\nimport logging\nfrom mcp import ClientSession\nfrom mcp.client.websocket import websocket_client # <-- WebSocket helper\n\nlogging.basicConfig(level=logging.INFO)\n\nasync def connect_via_websocket(server_ws_url: str):\n    # websocket_client establishes the WebSocket connection\n    async with websocket_client(server_ws_url) as (read_stream, write_stream):\n        logging.info(\"WebSocket transport connected.\")\n        # ClientSession uses the streams from websocket_client\n        async with ClientSession(read_stream, write_stream) as session:\n            await session.initialize()\n            logging.info(\"Session initialized via WebSocket.\")\n            # ... use session methods ...\n            tools = await session.list_tools()\n            logging.info(f\"Tools via WebSocket: {tools}\")\n\n# Example: asyncio.run(connect_via_websocket(\"ws://localhost:8000/ws\"))\n```\n\n*   `websocket_client(server_ws_url)` connects to the WebSocket URL (`ws://...` or `wss://...`) and yields streams representing the bidirectional connection.\n*   `ClientSession` uses these streams to send and receive MCP messages over the WebSocket.\n\nNotice the pattern: You choose the appropriate `*_client` helper based on how the server is exposed, but the way you create and use `ClientSession` remains the same!\n\n```python\n# General Pattern\n# 1. Choose and configure the transport helper\n# async with stdio_client(...) as (read_stream, write_stream):\n# async with sse_client(...) as (read_stream, write_stream):\n# async with websocket_client(...) as (read_stream, write_stream):\n\n# 2. Pass the provided streams to ClientSession\n#     async with ClientSession(read_stream, write_stream) as session:\n#         # 3. Initialize and use the session as usual\n#         await session.initialize()\n#         result = await session.list_tools()\n#         # ...\n```\n\n## Under the Hood: How Transports Provide Streams\n\nHow do these `*_client` functions hide the complexity? They act as adapters.\n\n1.  **Connection:** Each helper establishes the specific type of connection (starts a process, makes HTTP requests, opens a WebSocket).\n2.  **Internal Queues:** They typically use internal memory queues or buffers (often using `anyio.create_memory_object_stream`).\n3.  **Reading:** They run background tasks that continuously read raw data from the connection (process stdout, SSE events, WebSocket frames), parse it into MCP messages (or detect errors), and put these messages/errors onto the internal *read* queue. The `read_stream` you get simply reads from this queue.\n4.  **Writing:** When you send a message to the `write_stream`, it goes into an internal *write* queue. Another background task reads from this queue, formats the message according to the transport's protocol (e.g., adds newline for stdio, makes HTTP POST for SSE, sends WebSocket frame), and sends it over the actual connection.\n\nThis setup decouples `ClientSession` from the specific transport details. `ClientSession` just interacts with the clean `read_stream` and `write_stream` interfaces.\n\n```mermaid\ngraph LR\n    subgraph Your Code\n        A[ClientSession]\n    end\n    subgraph Transport Helper (e.g., stdio_client)\n        direction LR\n        B[read_stream] -- Reads From --> C(Internal Read Queue)\n        D(Internal Write Queue) -- Writes To --> E[write_stream]\n        F[Background Reader Task] -- Writes To --> C\n        G[Background Writer Task] -- Reads From --> D\n    end\n    subgraph Actual Connection\n        H[Process Pipes / HTTP / WebSocket]\n    end\n    subgraph Server\n        I[MCP Server]\n    end\n\n    A -- Uses --> B\n    A -- Uses --> E\n\n    F -- Reads From --> H\n    G -- Writes To --> H\n    H -- Communicates With --> I\n```\n\nThe `Transport Helper` box manages the `Internal Queues` and `Background Tasks` to bridge the gap between the `Actual Connection` and the simple `read_stream`/`write_stream` used by `ClientSession`.\n\nYou can explore the implementation details in the `mcp.client` subdirectories:\n*   `src/mcp/client/stdio/`: Code for starting processes and managing stdin/stdout.\n*   `src/mcp/client/sse.py`: Code for handling SSE connections and HTTP POST requests.\n*   `src/mcp/client/websocket.py`: Code for managing WebSocket communication.\n\nSimilarly, the `mcp.server` directory contains corresponding transport implementations for the server side (e.g., `src/mcp/server/stdio.py`, `src/mcp/server/sse.py`, `src/mcp/server/websocket.py`) that frameworks like [FastMCP Server](05_fastmcp_server.md) can use.\n\n## Conclusion\n\nIn this chapter, we peeled back a layer to understand **Transports** \u2013 the delivery mechanisms that connect MCP clients and servers.\n\n*   Transports handle the low-level communication (stdio, SSE, WebSocket).\n*   They provide simple `read_stream` and `write_stream` interfaces for `ClientSession`.\n*   Helper functions (`stdio_client`, `sse_client`, `websocket_client`) manage the specifics of each transport type.\n*   You choose the transport based on how your client needs to connect to the server.\n\nUnderstanding transports gives you flexibility in how you deploy and connect your MCP applications.\n\nNow that we've covered the core components \u2013 sessions, tools, resources, prompts, servers, and transports \u2013 let's look at a more advanced server concept: how servers built with `FastMCP` can maintain state and context across multiple requests within a single client session. We'll explore this in the next chapter: [FastMCP Context](07_fastmcp_context.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 15: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\07_fastmcp_context.md ---\n# Chapter 7: FastMCP Context - Your In-Request Toolkit\n\nIn the previous chapter, [Transports](06_transports.md), we saw how clients and servers establish communication channels like stdio or SSE to exchange messages. Now, let's focus back on the server side, specifically when using the [FastMCP Server](05_fastmcp_server.md) framework.\n\nImagine you've built a tool for your FastMCP server, maybe one that performs a complex analysis or generates a large report. This might take a while! Wouldn't it be great if, while your tool function is running, it could:\n\n1.  Tell the client, \"Hey, I'm 20% done... now 50%... now 80%...\"?\n2.  Send status messages back to the client, like \"Starting analysis...\" or \"Fetching data...\"?\n3.  Maybe even access another piece of data (a [Resource](03_resources.md)) provided by the *same* server to help with its calculation?\n\nHow can your simple Python tool function, running inside the server, interact with the ongoing request and the server's capabilities?\n\n## Motivation: Giving Your Tool Function Superpowers\n\nWhen the [FastMCP Server](05_fastmcp_server.md) framework calls your tool or resource function to handle a client request, it doesn't just call it in isolation. It can provide a special helper object directly to your function. This helper object is called the **FastMCP Context**.\n\n**Think of the FastMCP Context (`Context`) as a special toolkit automatically handed to your tool or resource function when it's called.** This toolkit contains useful utilities relevant to the *current* request and the server session it belongs to.\n\nIt's like when you hire a plumber (your tool function); they don't just show up empty-handed. They bring their toolbox (the `Context`) which has tools (like `log`, `report_progress`, `read_resource`) specifically needed for the plumbing job (handling the client request).\n\n## What's in the Toolkit? Using the `Context` Object\n\nThe `Context` object gives your function access to several server capabilities during request handling:\n\n1.  **Logging to the Client:** Send log messages (debug, info, warning, error) back to the connected client. This is super useful for debugging or showing status updates.\n2.  **Progress Reporting:** If the client initiated the request with a progress token, your function can send updates like \"X% complete\" back to the client.\n3.  **Reading Resources:** Access other [Resources](03_resources.md) defined on the *same* FastMCP server, just as if you were a client calling `session.read_resource()`.\n4.  **Accessing Request Info:** Get details about the current request, like its unique ID or the client's identifier (if provided).\n\n## How to Get and Use the Context\n\nGetting access to this toolkit in your function is surprisingly simple: you just ask for it using a type hint!\n\n**1. Requesting the Context in Your Function**\n\nWhen defining your tool or resource function, simply add a parameter and type-hint it as `Context`. `FastMCP` will automatically see this and pass the `Context` object to your function when it's called.\n\n```python\n# server_code.py (inside your FastMCP server file)\nfrom mcp.server.fastmcp import FastMCP, Context # Import Context!\n\nmcp = FastMCP(\"MyServer\")\n\n@mcp.tool()\nasync def long_running_task(duration: int, ctx: Context): # <-- Request Context here!\n    \"\"\"Performs a task and reports progress.\"\"\"\n    ctx.info(f\"Starting task that will take {duration} seconds.\") # Use ctx!\n\n    for i in range(duration):\n        await asyncio.sleep(1) # Simulate work\n        progress = ((i + 1) / duration) * 100\n        # Report progress back to the client via the context\n        await ctx.report_progress(progress=i + 1, total=duration)\n        ctx.debug(f\"Progress: {progress:.0f}%\") # Send debug log\n\n    result_message = \"Task completed successfully!\"\n    ctx.info(result_message) # Log completion\n    return result_message\n\n# You don't *have* to call the parameter 'ctx', any name works\n# as long as the type hint is 'Context'.\n```\n\nIn this example, our `long_running_task` tool function now has a parameter `ctx` hinted as `Context`. When a client calls this tool, `FastMCP` will provide the appropriate `Context` object for that specific request.\n\n**2. Using Context Methods**\n\nInside the function, you can now use the methods provided by the `ctx` object:\n\n*   **Logging:**\n    ```python\n    # Inside your tool/resource function:\n    ctx.debug(\"This is detailed debug info.\")\n    ctx.info(\"Task phase 1 complete.\")\n    ctx.warning(\"Input value looks unusual.\")\n    ctx.error(\"Failed to process sub-task XYZ.\")\n    ```\n    These messages are sent as notifications back to the connected client, helping them understand what's happening.\n\n*   **Reporting Progress:**\n    ```python\n    # Inside your async tool/resource function:\n    total_steps = 10\n    for step in range(total_steps):\n        # ... do work for the step ...\n        await ctx.report_progress(progress=step + 1, total=total_steps)\n    ```\n    If the client supports it, they will receive these progress updates.\n\n*   **Reading Server Resources:**\n    ```python\n    # Inside your async tool/resource function:\n    try:\n        config_data_contents = await ctx.read_resource(\"resource://server-config\")\n        # Process the config_data_contents (which is like the result of session.read_resource)\n        if config_data_contents:\n             config_text = config_data_contents[0].content # Assuming text content\n             ctx.info(f\"Loaded config: {config_text[:30]}...\")\n    except Exception as e:\n        ctx.error(f\"Could not read resource 'resource://server-config': {e}\")\n    ```\n    This allows your tool to dynamically fetch data defined elsewhere on the same server.\n\n*   **Getting Request Info:**\n    ```python\n    # Inside your tool/resource function:\n    request_id = ctx.request_id\n    client_id = ctx.client_id # Might be None if client didn't provide one\n    ctx.info(f\"Handling request {request_id} for client {client_id or 'Unknown'}\")\n    ```\n    This can be useful for logging or tracking purposes.\n\n**Important:** Your tool/resource function doesn't *have* to accept the `Context`. If your function doesn't need these capabilities, just define it without a `Context` parameter, and `FastMCP` won't try to pass one.\n\n## Under the Hood: How FastMCP Provides Context\n\nHow does `FastMCP` magically inject this `Context` object?\n\n1.  **Request Arrives:** A client sends a request (e.g., `tools/call` for `long_running_task`) to the server via one of the [Transports](06_transports.md).\n2.  **Server Receives:** The underlying `MCPServer` receives the request and identifies the target method (e.g., `call_tool`).\n3.  **Context Creation:** `FastMCP` (specifically, the `MCPServer` part) creates an internal `RequestContext` object. This object holds information like the `request_id`, details about the specific client `session`, and any shared `lifespan_context` (server-wide state).\n4.  **Handler Lookup:** `FastMCP` routes the request to the appropriate handler method (e.g., its internal `call_tool` method).\n5.  **Tool Lookup:** The `ToolManager` (part of `FastMCP`) finds the registered Python function for the requested tool (`long_running_task`).\n6.  **Signature Inspection:** Before calling your function, the `ToolManager` (using helpers like `func_metadata` from `src/mcp/server/fastmcp/utilities/func_metadata.py`) inspects its signature (the parameters it expects).\n7.  **Context Injection:** It sees the `ctx: Context` parameter. `FastMCP` then creates the user-friendly `Context` object, wrapping the internal `RequestContext`.\n8.  **Function Call:** The `ToolManager` calls your Python function (`long_running_task`), passing the validated arguments from the client request *and* the newly created `Context` object to the `ctx` parameter.\n9.  **Context Usage:** Your function code now runs and can use the methods on the `ctx` object (like `ctx.info`, `ctx.report_progress`). These methods internally use the `RequestContext` to interact with the session (e.g., `session.send_log_message`, `session.send_progress_notification`).\n\nHere\u2019s a simplified view:\n\n```mermaid\nsequenceDiagram\n    participant Client\n    participant Transport\n    participant FastMCPServer as FastMCP Server Framework\n    participant ToolManager as Server Tool Manager\n    participant YourToolFunc as Your Tool Function (e.g., long_running_task)\n\n    Client->>+Transport: Send \"tools/call\" request (tool=\"long_running_task\")\n    Transport->>+FastMCPServer: Deliver request message\n    FastMCPServer->>FastMCPServer: Create internal RequestContext (holds session, req_id)\n    FastMCPServer->>+ToolManager: Request to execute \"long_running_task\" with args & RequestContext\n    ToolManager->>ToolManager: Find 'long_running_task' function registration\n    ToolManager->>ToolManager: Inspect function signature, see 'ctx: Context'\n    ToolManager->>ToolManager: Create user-friendly Context object (wraps RequestContext)\n    ToolManager->>+YourToolFunc: Call long_running_task(args=..., ctx=ContextObject)\n    YourToolFunc->>YourToolFunc: Run your code...\n    YourToolFunc->>ContextObject: Call ctx.info(\"Starting...\")\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_log_message(...)\n    Session->>Transport: Send log notification\n    Transport-->>Client: Deliver log notification\n    YourToolFunc->>ContextObject: Call await ctx.report_progress(...)\n    ContextObject->>RequestContext: Access session\n    RequestContext->>Session: session.send_progress_notification(...)\n    Session->>Transport: Send progress notification\n    Transport-->>Client: Deliver progress notification\n    Note over YourToolFunc: Continues execution...\n    YourToolFunc-->>-ToolManager: Return result\n    ToolManager-->>-FastMCPServer: Return result\n    FastMCPServer->>FastMCPServer: Format MCP response\n    FastMCPServer->>-Transport: Send response message\n    Transport-->>Client: Deliver response\n```\n\n**Code Glimpse:**\n\n*   The `Context` class itself is defined in `src/mcp/server/fastmcp/server.py`. It mostly acts as a wrapper around the lower-level `RequestContext` (defined in `src/mcp/shared/context.py`) and the `FastMCP` instance.\n\n    ```python\n    # src/mcp/server/fastmcp/server.py (Simplified Context Class)\n    class Context(BaseModel, Generic[ServerSessionT, LifespanContextT]):\n        _request_context: RequestContext[...] | None\n        _fastmcp: FastMCP | None\n        # ... (initializer) ...\n\n        @property\n        def request_context(self) -> RequestContext[...]:\n            # Provides access to the internal context\n            # ... (error handling) ...\n            return self._request_context\n\n        async def report_progress(self, progress: float, total: float | None = None) -> None:\n            # Uses request_context.session to send notification\n            progress_token = self.request_context.meta.progressToken # Simplified\n            if progress_token:\n                await self.request_context.session.send_progress_notification(...)\n\n        async def log(self, level: ..., message: str, ...) -> None:\n            # Uses request_context.session to send log message\n            await self.request_context.session.send_log_message(...)\n\n        # ... other methods like info(), error(), read_resource() ...\n    ```\n\n*   The `Tool` class (in `src/mcp/server/fastmcp/tools/base.py`) stores whether a function expects context (`context_kwarg`). The `ToolManager` (in `src/mcp/server/fastmcp/tools/tool_manager.py`) uses this information when calling the function via `tool.run()`.\n\n    ```python\n    # src/mcp/server/fastmcp/tools/base.py (Simplified Tool.run)\n    class Tool(BaseModel):\n        # ... fields: fn, name, context_kwarg ...\n\n        async def run(self, arguments: dict, context: Context | None = None) -> Any:\n            # Prepare kwargs, including context if needed\n            extra_kwargs = {}\n            if self.context_kwarg is not None and context is not None:\n                extra_kwargs[self.context_kwarg] = context\n\n            # Call the actual function with args and context\n            return await self.fn_metadata.call_fn_with_arg_validation(\n                self.fn, self.is_async, arguments, extra_kwargs\n            )\n    ```\n\n*   The main `FastMCP` class (`src/mcp/server/fastmcp/server.py`) provides the `get_context()` method which creates the `Context` object when needed, using the currently active `request_context` managed by the underlying `MCPServer`.\n\nYou don't need to interact with these internal details directly, but seeing them helps understand that the `Context` isn't magic \u2013 it's a well-defined object passed along during request handling by the `FastMCP` framework.\n\n## Conclusion\n\nYou've learned about the **FastMCP Context** (`Context`), a powerful object provided to your server-side tool and resource functions.\n\n*   It acts as a **toolkit** giving access to server capabilities *during* request handling.\n*   You request it simply by adding a parameter with the `Context` type hint to your function.\n*   It allows your function logic to send **logs** and **progress updates** back to the client.\n*   It enables your function to **read other resources** from the same server.\n*   It provides access to **request-specific information**.\n\nThe `Context` object bridges the gap between your isolated function logic and the broader server session, making your FastMCP server tools much more interactive and capable.\n\nWe've now covered the main concepts for both clients (`ClientSession`) and servers (`FastMCP`), including the core capabilities (Tools, Resources, Prompts), communication (Transports), and server-side interaction (Context). To round things off, the next chapter will provide a quick reference guide to the key data structures used in the MCP protocol itself: [MCP Protocol Types](08_mcp_protocol_types.md).\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 16: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\08_mcp_protocol_types.md ---\n# Chapter 8: MCP Protocol Types - The Shared Language\n\nWelcome to the final core chapter! In [Chapter 7: FastMCP Context](07_fastmcp_context.md), we explored how server-side functions can interact with the ongoing request using the `Context` object. We've covered clients, servers, tools, resources, prompts, transports, and server context.\n\nBut how do the client and server *know* exactly what a \"list tools request\" looks like, or how a \"tool call result\" should be structured? They need a precisely defined, shared language. This chapter introduces the **MCP Protocol Types**, the fundamental building blocks of that language.\n\n## Motivation: Why Do We Need Defined Types?\n\nImagine trying to communicate with someone who speaks a different language, and you don't have a shared dictionary or grammar rules. You might say \"Get food,\" but they might expect you to specify *what* food, *where* from, and *how much*. Without agreed-upon structures, communication breaks down.\n\nSimilarly, when a `python-sdk` client talks to an MCP server (like one built with [FastMCP Server](05_fastmcp_server.md)), they need to agree on the *exact* format of every message.\n\n*   When the client calls `session.list_tools()`, what information does the server expect? (Nothing, just the request itself).\n*   When the server responds, what fields will be in the response? (A list of `Tool` objects).\n*   When the client calls `session.call_tool(\"calculator\", {\"a\": 5, \"b\": 3})`, how must the arguments `{\"a\": 5, \"b\": 3}` be structured?\n*   When the server sends back the result `8`, how is that formatted? (As `TextContent` inside a `CallToolResult`).\n\n**MCP Protocol Types define this shared language.** They specify the precise structure of:\n\n1.  **Requests:** Messages asking the other side to do something (e.g., \"list tools,\" \"read resource\").\n2.  **Responses (Results):** The answers to requests (e.g., the list of tools, the resource content).\n3.  **Notifications:** One-way messages that don't expect a direct answer (e.g., progress updates, log messages).\n\nThink of them as the strict grammar and vocabulary rules both the client and server *must* follow. This ensures that when the client sends a message, the server knows exactly how to interpret it, and vice-versa. In `python-sdk`, these rules are defined using **Pydantic models**.\n\n## Key Concepts: Requests, Results, and Notifications\n\nThe core idea is that every interaction follows a predefined structure. Let's look at the main categories:\n\n1.  **Requests (`*Request`)**:\n    *   Sent when one side wants the other to perform an action or provide information.\n    *   Always expect a corresponding **Response** (either a Result or an Error).\n    *   Examples:\n        *   `InitializeRequest`: Sent by the client to start the session.\n        *   `ListToolsRequest`: Sent by the client to ask for available tools.\n        *   `CallToolRequest`: Sent by the client to execute a tool.\n        *   `ReadResourceRequest`: Sent by the client to get resource content.\n        *   `GetPromptRequest`: Sent by the client to get a rendered prompt.\n        *   `CreateMessageRequest`: Sent by the *server* to ask the *client* to generate text (e.g., via an LLM).\n\n2.  **Responses/Results (`*Result`)**:\n    *   Sent in direct reply to a **Request**.\n    *   Contain the information requested or confirm an action was taken.\n    *   Must match the specific `Request` they are answering.\n    *   Examples:\n        *   `InitializeResult`: Server's reply to `InitializeRequest`, contains server capabilities.\n        *   `ListToolsResult`: Server's reply to `ListToolsRequest`, contains a list of `Tool` objects.\n        *   `CallToolResult`: Server's reply to `CallToolRequest`, contains the tool's output (e.g., `TextContent`).\n        *   `ReadResourceResult`: Server's reply to `ReadResourceRequest`, contains the resource `contents`.\n        *   `GetPromptResult`: Server's reply to `GetPromptRequest`, contains the list of `PromptMessage` objects.\n        *   `CreateMessageResult`: Client's reply to `CreateMessageRequest`, contains the generated message.\n    *   **Errors:** If a request fails, the response is a `JSONRPCError` containing an `ErrorData` object instead of a `*Result`.\n\n3.  **Notifications (`*Notification`)**:\n    *   Sent one-way; they do *not* expect a direct response.\n    *   Used for status updates, events, or information that doesn't require an explicit request.\n    *   Examples:\n        *   `InitializedNotification`: Client tells the server it's ready after initialization.\n        *   `ProgressNotification`: Server sends progress updates for a long request.\n        *   `LoggingMessageNotification`: Server sends log messages to the client.\n        *   `ResourceUpdatedNotification`: Server tells the client a subscribed resource has changed.\n        *   `ToolListChangedNotification`: Server tells the client the list of tools has changed.\n\n## Using the Types (Implicitly)\n\nWhen you use methods like `session.list_tools()` or `session.call_tool()`, you are *implicitly* using these protocol types.\n\n*   When you call `await session.list_tools()`, the `ClientSession` internally creates a `ListToolsRequest` object.\n*   It serializes this object into JSON and sends it via the [Transports](06_transports.md).\n*   The server receives the JSON, deserializes it back into a `ListToolsRequest` object (using the same Pydantic definition).\n*   The server processes the request and prepares a `ListToolsResult` object containing `Tool` objects.\n*   The server serializes the `ListToolsResult` to JSON and sends it back.\n*   Your `ClientSession` receives the JSON, deserializes it into a `ListToolsResult` object, and returns it to your code.\n\n**Pydantic** is the library that makes this easy. It allows defining these data structures as Python classes with type hints. Pydantic handles:\n*   **Validation:** Ensuring incoming data matches the expected structure and types. If a server sends back a malformed `ListToolsResult`, Pydantic will raise an error on the client side during deserialization.\n*   **Serialization:** Converting Python objects (like `ListToolsRequest`) into JSON.\n*   **Deserialization:** Converting JSON back into Python objects.\n\nLet's look at a simplified example structure:\n\n```python\n# Simplified view from mcp.types\n\nfrom pydantic import BaseModel, Field\nfrom typing import Any, List, Literal\n\n# --- Tool Structures ---\nclass Tool(BaseModel):\n    \"\"\"Definition for a tool the client can call.\"\"\"\n    name: str\n    description: str | None = None\n    inputSchema: dict[str, Any] # JSON schema for tool arguments\n\n# --- Request for Listing Tools ---\nclass ListToolsRequest(BaseModel):\n    \"\"\"Internal representation of a tools/list request\"\"\"\n    method: Literal[\"tools/list\"] = \"tools/list\"\n    # No specific parameters needed for this request\n\n# --- Result for Listing Tools ---\nclass ListToolsResult(BaseModel):\n    \"\"\"The server's response to a tools/list request.\"\"\"\n    tools: List[Tool] # Contains a list of Tool objects\n```\n\nThis shows how `ListToolsRequest` is just a marker for the request, while `ListToolsResult` clearly defines that the response *must* contain a list named `tools`, where each item is a `Tool` object conforming to the `Tool` model.\n\nYou can find all these Pydantic models defined in `src/mcp/types.py`. You don't usually need to create these objects manually when using `ClientSession` or `FastMCP`, but understanding their structure helps you know what data to expect.\n\n## Under the Hood: The Journey of a Typed Message\n\nWhat happens when `ClientSession` sends a request like `list_tools`?\n\n1.  **Python Object Creation:** `ClientSession` creates an instance of the Pydantic model `ListToolsRequest`.\n2.  **Serialization (Pydantic):** Pydantic converts the `ListToolsRequest` object into its JSON representation (e.g., `{\"jsonrpc\": \"2.0\", \"method\": \"tools/list\", \"id\": 1}`).\n3.  **Transport Sending:** The JSON string is sent over the active [Transports](06_transports.md) (e.g., written to stdio, sent via WebSocket).\n4.  **Transport Receiving:** The server's transport layer receives the raw JSON bytes/string.\n5.  **Deserialization (Pydantic):** The server uses Pydantic to parse the JSON string back into a `ListToolsRequest` Python object. Pydantic validates that the JSON matches the expected structure.\n6.  **Server Logic:** The server processes the request.\n7.  **Response Object Creation:** The server creates a `ListToolsResult` object containing the list of `Tool` objects.\n8.  **Serialization (Pydantic):** Pydantic converts the `ListToolsResult` object into its JSON representation.\n9.  **Transport Sending:** The response JSON is sent back to the client.\n10. **Transport Receiving:** The client's transport layer receives the response JSON.\n11. **Deserialization (Pydantic):** The `ClientSession` uses Pydantic to parse the JSON back into a `ListToolsResult` Python object, again validating the structure.\n12. **Return Value:** The validated `ListToolsResult` object is returned by the `await session.list_tools()` call.\n\n```mermaid\nsequenceDiagram\n    participant ClientCode as Your Python Code\n    participant ClientSession as ClientSession (Python SDK)\n    participant Transport\n    participant Server as MCP Server (Python SDK)\n    participant ServerLogic as Server's Tool Handling\n\n    ClientCode->>+ClientSession: await session.list_tools()\n    ClientSession->>ClientSession: Create ListToolsRequest object\n    ClientSession->>ClientSession: Pydantic: Serialize object to JSON\n    ClientSession->>+Transport: Send JSON Request\n    Transport->>+Server: Deliver JSON Request\n    Server->>Server: Pydantic: Deserialize JSON to ListToolsRequest object\n    Server->>+ServerLogic: Process request\n    ServerLogic-->>-Server: Return list of Tool objects\n    Server->>Server: Create ListToolsResult object\n    Server->>Server: Pydantic: Serialize object to JSON\n    Server->>-Transport: Send JSON Response\n    Transport-->>-ClientSession: Deliver JSON Response\n    ClientSession->>ClientSession: Pydantic: Deserialize JSON to ListToolsResult object\n    ClientSession-->>-ClientCode: Return ListToolsResult object\n```\n\nThe MCP Protocol Types, defined as Pydantic models, are essential at the points where JSON is converted to Python objects and vice-versa, ensuring both sides adhere to the agreed-upon \"language\".\n\n## Code Glimpse: Where Types are Used\n\nWe saw the simplified Pydantic models above. Now let's see how `ClientSession` uses them (simplified from `src/mcp/client/session.py`):\n\n```python\n# src/mcp/client/session.py (Simplified Snippet)\nimport mcp.types as types # Import the protocol types\n\nclass ClientSession(...):\n    # ... (other methods) ...\n\n    async def list_tools(self) -> types.ListToolsResult: # Declares return type!\n        \"\"\"Send a tools/list request.\"\"\"\n        # 1. Create the specific Request object\n        request_obj = types.ClientRequest( # Wrapper for specific requests\n            types.ListToolsRequest(        # The actual request type\n                method=\"tools/list\",\n            )\n        )\n        # 2. Send it and expect the specific Result type back\n        # (internal send_request handles JSON conversion and validation)\n        return await self.send_request(\n            request_obj,\n            types.ListToolsResult, # Tell send_request what Pydantic model to expect\n        )\n```\n\nHere you can clearly see:\n1.  The function signature `-> types.ListToolsResult` explicitly states what type of Pydantic object this method will return.\n2.  Inside, it creates a `types.ListToolsRequest` object.\n3.  It calls the internal `send_request`, telling it to expect a response that can be validated and parsed into a `types.ListToolsResult`.\n\nThe `BaseSession` (which `ClientSession` inherits from) handles the details of using Pydantic for serialization and deserialization within its `_send_response`, `_receive_loop`, and related methods (see `src/mcp/shared/session.py`).\n\n## Conclusion\n\nYou've learned about the **MCP Protocol Types**, the precisely defined data structures that form the communication language between MCP clients and servers.\n\n*   They define the exact format for **Requests**, **Responses (Results)**, and **Notifications**.\n*   They act like a shared grammar and vocabulary, ensuring mutual understanding.\n*   In `python-sdk`, these types are implemented as **Pydantic models** (primarily in `src/mcp/types.py`).\n*   Pydantic handles the **validation**, **serialization** (Python object -> JSON), and **deserialization** (JSON -> Python object).\n*   While often used implicitly by `ClientSession` and `FastMCP`, understanding these types helps clarify what data is being exchanged.\n\nThis chapter concludes our exploration of the core concepts in the `python-sdk`. You now have a solid foundation for understanding how to build clients that interact with MCP servers, how to build servers using `FastMCP`, and the underlying principles of communication and data structures involved. From here, you can dive deeper into specific examples or advanced features!\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 17: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\index.md ---\n# Tutorial: python-sdk\n\nThis project provides a Python implementation for the **Model Context Protocol (MCP)**.\nIt allows developers to easily build both *MCP servers* and *MCP clients*.\nServers use the framework (especially **FastMCP**) to expose capabilities like remote functions (**Tools**), data access (**Resources**), or LLM instructions (**Prompts**).\nClients can then connect to these servers using different **Transports** (like standard I/O or WebSockets) to interact with those capabilities, often enabling AI models to use external functions and data.\n\n\n**Source Repository:** [https://github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)\n\n```mermaid\nflowchart TD\n    A0[\"MCP Protocol Types\"]\n    A1[\"FastMCP Server\"]\n    A2[\"Client Session\"]\n    A3[\"Tools\"]\n    A4[\"Resources\"]\n    A5[\"Transports\"]\n    A6[\"Prompts\"]\n    A7[\"FastMCP Context\"]\n    A0 -- \"Defines wire format for\" --> A5\n    A1 -- \"Defines\" --> A3\n    A1 -- \"Defines\" --> A4\n    A1 -- \"Defines\" --> A6\n    A1 -- \"Runs over\" --> A5\n    A1 -- \"Provides\" --> A7\n    A2 -- \"Connects via\" --> A5\n    A2 -- \"Calls\" --> A3\n    A2 -- \"Reads\" --> A4\n    A2 -- \"Gets\" --> A6\n    A3 -- \"Uses\" --> A7\n```\n\n## Chapters\n\n1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 18: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\01_model_input_shape.md ---\n# Chapter 1: What is a Model Input Shape?\n\nWelcome to the `SC4002-Group-Assignment` project tutorial! We're excited to have you here. This is the very first chapter, and we'll start with a fundamental concept in machine learning: the **Model Input Shape**.\n\n## Why Do We Need a Specific Input Shape?\n\nImagine you have a super-smart machine learning model that can read sentences and tell if they express a positive or negative feeling (this is called sentiment analysis). You want to feed it lots of sentences to learn from.\n\nBut here's the catch: the model is like a very specific machine in a factory. It's designed to handle items of a *precise* size and shape. If you try to feed it something differently shaped, the machine might jam or break down.\n\nOur machine learning model is similar. It needs its input data (the sentences, but represented as numbers) to be in a consistent format. This required format is called the **Input Shape**.\n\nThink of it like baking a cake:\n*   **Your Data (Ingredients):** The sentences you want to analyze.\n*   **The Model (Oven):** The machine learning algorithm that processes the data.\n*   **Input Shape (Baking Pan):** A specific size and shape of pan that the ingredients *must* fit into before going into the oven.\n\nIf your recipe requires a 9x13 inch pan, you can't just use a tiny cupcake tin or a giant cookie sheet! The ingredients (data) must fit the specified pan (input shape) for the oven (model) to work correctly.\n\n## Understanding `[null, 62]`\n\nIn our project, you'll often see the input shape defined as `[null, 62]`. Let's break this down:\n\n*   **It's a list with two dimensions:** This tells us the data should be structured like a table or a grid with rows and columns.\n*   **`null` (or `None`):** This is the first dimension. In machine learning, we often process data in groups called \"batches\". `null` means the model can handle batches of *any size*. One time we might feed it 10 sentences, another time 32, or even just 1. The model is flexible about *how many* sentences it sees at once. (Think: Our oven can bake a variable number of cakes simultaneously).\n*   **`62`:** This is the second dimension. It means *every single* piece of data (each sentence, after processing) must have a length of exactly 62 units (e.g., 62 numbers representing words or parts of words). If a sentence is longer, we might trim it. If it's shorter, we might add special \"padding\" values to make it reach 62. (Think: Each baking pan *must* have a diameter or length of exactly 62).\n\nSo, `[null, 62]` means the model expects data in batches, where the number of items per batch can vary (`null`), but each item in the batch must have a fixed length of 62 (`62`).\n\n## Why is Consistency Crucial?\n\nMachine learning models are typically built with layers, like floors in a building. The first layer receives the input data. The output of that layer becomes the input for the next layer, and so on.\n\nEach layer is designed expecting data of a specific shape from the previous one. If the initial input shape isn't consistent, the connections between these layers won't work correctly, and the model will fail. Sticking to the defined `[null, 62]` input shape ensures the data flows smoothly through all the model's layers.\n\n## Where We Define Input Shape: Configuration Files\n\nHow does our project know to use `[null, 62]`? We store this information in configuration files. Take a look at these snippets from different experiment \"trials\" in our project:\n\n--- File: `sentiment_transformer_tuning/trial_00/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n--- File: `sentiment_transformer_tuning/trial_09/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\nThese files (`build_config.json`) store settings used to build or train the model for different experiments (called \"trials\"). Notice that `\"input_shape\": [null, 62]` is the same in all these examples. This tells us that for these particular experiments, the fundamental structure expected for the input data remained consistent. Even if other settings were changed (like the model's internal complexity, which we'll explore later), the required \"entry door\" size for the data stayed the same: `[null, 62]`.\n\n## How It Works: From Sentence to Shaped Input\n\nLet's visualize the process using our baking analogy and a simple diagram:\n\n1.  **Raw Data (Ingredients):** You start with raw sentences like \"This movie was fantastic!\" or \"I did not like it.\"\n2.  **Preprocessing (Measuring & Mixing):** These sentences are converted into numbers (a process called tokenization) and then forced into the required length.\n    *   \"This movie was fantastic!\" -> `[101, 2023, 3185, 2001, 12411, 102]` (Maybe length 6) -> Needs padding! -> `[101, 2023, 3185, 2001, 12411, 102, 0, 0, ..., 0]` (Padded to length 62)\n    *   A very long sentence -> `[... lots of numbers ...]` (Maybe length 100) -> Needs trimming! -> `[... first 62 numbers ...]` (Trimmed to length 62)\n3.  **Shaped Data (Batter in Pan):** The processed numerical sequences, all exactly 62 units long, are grouped into batches. A batch might look like `[[seq1_len62], [seq2_len62], ..., [seq_batchsize_len62]]`. This batch has the shape `[batch_size, 62]`, which matches our required `[null, 62]` (since `batch_size` can vary).\n4.  **Model Processing (Baking):** The model takes these batches, one by one, and processes them through its layers.\n5.  **Output (Cake!):** The model produces a result, like a sentiment score (e.g., 0.9 for positive, 0.1 for negative).\n\nHere's a diagram showing the flow:\n\n```mermaid\nsequenceDiagram\n    participant Data as Raw Data (Sentences)\n    participant Preprocessor as Data Preparation\n    participant Trainer as Training Process\n    participant Model as ML Model (expects [null, 62])\n\n    Data->>Preprocessor: Feed raw sentences\n    Preprocessor->>Preprocessor: Convert text to numbers,<br/>Pad/Truncate to length 62\n    Preprocessor->>Trainer: Provide batches of shape [batch_size, 62]\n    Note right of Trainer: batch_size can vary (matches 'null')\n    Trainer->>Model: Feed one batch [batch_size, 62]\n    Model->>Model: Process data matching<br/>expected input shape\n    Model->>Trainer: Return predictions\n```\n\n## A Peek Inside: Defining the Shape in Code (Conceptual)\n\nWhile the `build_config.json` *stores* the shape, the actual enforcement happens when the model is defined in code. Using popular libraries like TensorFlow/Keras, it often looks something like this (this is a simplified, conceptual example):\n\n```python\n# Conceptual Python code using TensorFlow/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# 1. Define the shape for *one* sequence (batch size is handled automatically)\n# We know each sequence must have length 62.\nsingle_input_shape = (62,)\n\n# 2. Use this shape when creating the model's entry point\n# This Input layer tells the model what shape to expect for each item.\ninput_layer = keras.layers.Input(shape=single_input_shape)\n\n# 3. Build the rest of the model...\n# ... layers that process sequences of length 62 would follow ...\n# Example: output_layer = keras.layers.Dense(1, activation='sigmoid')(input_layer) # Simplified!\n# model = keras.Model(inputs=input_layer, outputs=output_layer)\n\n# This 'input_layer' now enforces that data must conform to the shape (None, 62)\n# where None represents the flexible batch size.\nprint(f\"Model expects sequences of shape: {single_input_shape}\")\n# Output: Model expects sequences of shape: (62,)\n```\n\n**Explanation:**\n\n1.  We define `single_input_shape` as `(62,)`. This tuple represents the dimensions of *one* input sample (one processed sentence). We only specify the sequence length (62) here.\n2.  We create an `Input` layer, which is like the official entrance to our model. We tell it the expected shape of each individual input using `shape=single_input_shape`.\n3.  Keras automatically understands that data will come in batches, so effectively, this `Input` layer expects data with the shape `[null, 62]` (or `(None, 62)` in Python terms). Any data fed to the model later during training *must* conform to this shape.\n\nThe `build_config.json` files we saw earlier store the value (`[null, 62]`) that would be used to configure this `Input` layer (or equivalent) when the actual model for that trial was constructed.\n\n## Conclusion\n\nYou've learned about the Model Input Shape, a crucial concept in machine learning!\n\n*   It defines the **required dimensions** for the data fed into a model.\n*   In our project, it's often `[null, 62]`, meaning a **variable number of items per batch (`null`)**, where each item has a **fixed length of 62**.\n*   It's like a **baking pan size** \u2013 the data (ingredients) must fit the shape (pan) before the model (oven) can process it.\n*   This shape is usually defined in configuration files (`build_config.json`) and enforced when building the model, typically in the first layer.\n*   **Consistency** is key for the model's internal layers to work together correctly.\n\nUnderstanding the input shape is the first step. Now that we know the *shape* of the data our model needs, how do we figure out the best *settings* for the model itself (like how complex it should be, or how quickly it should learn)? These settings are called hyperparameters.\n\nIn the next chapter, we'll explore the range of possible values for these settings.\n\nReady to move on? Let's dive into the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 19: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\02_tuning_hyperparameter_space.md ---\n# Chapter 2: Tuning Hyperparameter Space\n\nHello again! In the [previous chapter](01_model_input_shape.md), we learned about the **Model Input Shape**, which is like defining the exact size of the baking pan our data needs to fit into. We saw that our model needs data in the shape `[null, 62]`.\n\nNow that we know the shape of the *data*, let's think about the *model* itself. Building a good machine learning model often involves some trial and error. There are many \"settings\" we can adjust, kind of like knobs on a machine, to try and get the best performance. But how do we know which settings to even try? That's where the **Tuning Hyperparameter Space** comes in!\n\n## What's the Problem? Finding the Best Recipe Settings\n\nImagine you have a fantastic cake recipe, but it's a bit vague. It says \"use some eggs,\" \"add sugar,\" and \"bake until done.\" To get the *perfect* cake, you need more specific instructions, or at least some guidelines on what ranges are sensible. Maybe 2-4 eggs work best, and somewhere between 100g and 200g of sugar is ideal.\n\nOur machine learning models are similar. They have adjustable settings called **hyperparameters**. These are settings that *we* define *before* the model starts learning from the data. They control things like:\n\n*   How complex should the model be? (e.g., number of layers)\n*   How quickly should it learn? (e.g., learning rate)\n*   How much should it \"forget\" to avoid memorizing? (e.g., dropout rate)\n\nFinding the *best combination* of these hyperparameters can dramatically improve our model's ability to analyze sentiment correctly. But trying every single possible combination would take forever!\n\nSo, we need a way to tell our automatic \"tuning\" process (which tries different combinations for us) *where* to look. We need to define the **search space** \u2013 the sensible range of values for each hyperparameter.\n\n## Defining the Search Boundaries: The Hyperparameter Space\n\nThe **Tuning Hyperparameter Space** is exactly that: a definition of the possible range of values for each hyperparameter we want to optimize.\n\nThink back to the cookbook analogy:\n*   **Hyperparameter:** An ingredient or setting (e.g., number of eggs, amount of sugar, oven temperature).\n*   **Hyperparameter Space:** The section outlining the allowed variations (e.g., \"use 2 to 8 eggs\", \"adjust sugar between 100g and 500g\", \"bake between 170\u00b0C and 190\u00b0C\").\n\nAn automatic tuner will then explore different combinations *within these boundaries* to find the recipe that gives the best results (the best-performing model).\n\n## How We Define the Space in Our Project\n\nIn our `SC4002-Group-Assignment` project, we use a tool called Keras Tuner to help us find the best hyperparameters. This tool needs to know the space to search. This information is recorded in a file called `oracle.json` after the tuning process starts. Let's look at a part of that file:\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Relevant Snippet) ---\n```json\n{\n  \"hyperparameters\": {\n    \"space\": [\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_heads\",       // Name of the hyperparameter\n          \"min_value\": 2,            // Smallest value to try\n          \"max_value\": 8,            // Largest value to try\n          \"step\": 2                  // Increment value (try 2, 4, 6, 8)\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"dff\",             // Size of an internal layer\n          \"min_value\": 64,\n          \"max_value\": 512,\n          \"step\": 64                 // Try 64, 128, 192, ..., 512\n        }\n      },\n      {\n        \"class_name\": \"Int\",\n        \"config\": {\n          \"name\": \"num_layers\",      // How many layers in the model\n          \"min_value\": 1,\n          \"max_value\": 4,\n          \"step\": 1                  // Try 1, 2, 3, 4\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"dropout_rate\",   // How much regularization\n          \"min_value\": 0.1,\n          \"max_value\": 0.5,\n          \"step\": 0.1                // Try 0.1, 0.2, 0.3, 0.4, 0.5\n        }\n      },\n      {\n        \"class_name\": \"Float\",\n        \"config\": {\n          \"name\": \"learning_rate\",  // How fast the model learns\n          \"min_value\": 1e-05,        // 0.00001 (very small)\n          \"max_value\": 0.001,        // 0.001 (relatively larger)\n          \"sampling\": \"log\"          // Explore smaller values more thoroughly\n        }\n      }\n    ],\n    \"values\": { // Default or best values found so far might be listed here\n        \"num_heads\": 2, \"dff\": 64, \"num_layers\": 1, \"dropout_rate\": 0.1, \"learning_rate\": 1e-05\n    }\n  }\n  // ... other tuner information ...\n}\n```\n\n**Let's break this down:**\n\n*   `\"hyperparameters\"`: This section contains information about the settings we are tuning.\n*   `\"space\"`: This is a list defining the search boundaries for each hyperparameter.\n*   **Each item in the list** describes one hyperparameter:\n    *   `\"name\"`: The name we give to this setting (e.g., `num_heads`, `learning_rate`).\n    *   `\"class_name\"`: Tells the tuner if it's an integer (`Int`) or a decimal number (`Float`).\n    *   `\"config\"`: Contains the specific range details:\n        *   `\"min_value\"`: The lowest value the tuner should try.\n        *   `\"max_value\"`: The highest value the tuner should try.\n        *   `\"step\"`: (For `Int` and sometimes `Float`) How much to increase by when trying values (e.g., step 2 means try 2, 4, 6...). If not specified or `null`, the tuner might pick values more freely within the range.\n        *   `\"sampling\"`: (Optional) How the tuner should pick values. `\"linear\"` means pick uniformly across the range. `\"log\"` means focus more on smaller values, which is often useful for `learning_rate`.\n\nSo, this configuration tells the tuner:\n*   Try between 2 and 8 attention heads (`num_heads`), specifically 2, 4, 6, or 8.\n*   Try different internal layer sizes (`dff`) from 64 up to 512, in steps of 64.\n*   Try models with 1, 2, 3, or 4 layers (`num_layers`).\n*   Try dropout rates from 0.1 to 0.5 (like 10% to 50%).\n*   Try learning rates between 0.00001 and 0.001, paying closer attention to the smaller end of this range.\n\nThe tuner will then intelligently pick combinations from these allowed ranges (e.g., maybe `num_heads=4`, `dff=128`, `num_layers=2`, `dropout_rate=0.2`, `learning_rate=0.0001`) and test how well the resulting model performs.\n\n## Under the Hood: How the Tuner Uses This Space\n\nDefining the space is like giving instructions to the Keras Tuner. Here's a simplified view of what happens:\n\n1.  **You (the programmer)** define this hyperparameter space in your Python code when setting up the tuning process.\n2.  **The Tuner** (specifically, a component called the [Keras Tuner Oracle](05_keras_tuner_oracle.md)) reads and remembers this space definition.\n3.  **When it's time to run a test (a \"trial\")**: The Oracle consults the defined space.\n4.  **Sampling**: It picks a *specific value* for each hyperparameter from within its allowed range (e.g., it picks `num_heads=6`, `dff=256`, etc.).\n5.  **Trial Execution**: A model is built and trained using exactly these chosen hyperparameter values. We'll learn more about this in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n6.  **Feedback**: The performance of this trial is recorded. The Oracle uses this feedback to decide which combination to try next, aiming to find the best one within the defined space.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant User as You (Programmer)\n    participant TunerSetup as Tuning Setup Code\n    participant Oracle as Keras Tuner Oracle\n    participant Trial as A Single Tuning Trial\n\n    User->>TunerSetup: Define Hyperparameter Space (e.g., num_heads: 2-8)\n    TunerSetup->>Oracle: Store the defined space\n    Oracle->>Oracle: Need hyperparameter values for next trial\n    Oracle->>Oracle: Sample values from stored space (e.g., choose num_heads=4)\n    Oracle->>Trial: Start trial with chosen values (num_heads=4, ...)\n    Trial->>Trial: Build & Train Model\n    Trial->>Oracle: Report back performance (e.g., accuracy)\n    Oracle->>Oracle: Record result, use it to guide next sampling\n```\n\n## Defining the Space in Code (Conceptual Example)\n\nWhile `oracle.json` *records* the space, the actual definition happens in Python code, often inside a function that builds the model. Here's a simplified conceptual example using Keras Tuner's `hp` object:\n\n```python\n# Conceptual Python code using Keras Tuner\n# (This would typically be inside a function the tuner calls)\nimport keras_tuner as kt\n\n# Assume 'hp' is a HyperParameters object provided by the tuner\n# We use it to define the search space for each parameter\n\n# Define the search space for 'num_heads'\nnum_heads = hp.Int(\"num_heads\",           # Give it a name\n                   min_value=2,           # Minimum value\n                   max_value=8,           # Maximum value\n                   step=2)                # Step size\n\n# Define the search space for 'learning_rate'\nlearning_rate = hp.Float(\"learning_rate\", # Give it a name\n                         min_value=1e-5,  # Minimum value (0.00001)\n                         max_value=1e-3,  # Maximum value (0.001)\n                         sampling=\"log\")  # Use logarithmic sampling\n\n# Now, these variables (num_heads, learning_rate) hold values\n# sampled by the tuner for the current trial.\n# We would use them when building the actual model layers.\nprint(f\"This trial uses num_heads: {num_heads}\")\nprint(f\"This trial uses learning_rate: {learning_rate}\")\n\n# ... code to build the model using these specific values ...\n# Example (conceptual):\n# model_layer = SomeTransformerLayer(num_heads=num_heads, ...)\n# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n# ...\n```\n\n**Explanation:**\n\n1.  We import `keras_tuner`.\n2.  The tuner provides an object (conventionally named `hp`) that has methods like `hp.Int` and `hp.Float`.\n3.  We call these methods to define each hyperparameter's search space:\n    *   `hp.Int(\"num_heads\", min_value=2, max_value=8, step=2)` defines an integer hyperparameter named \"num_heads\" that the tuner should search within the range [2, 4, 6, 8].\n    *   `hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-3, sampling=\"log\")` defines a float hyperparameter named \"learning_rate\" to be searched between 0.00001 and 0.001, using log sampling.\n4.  When the tuner runs a specific trial, it replaces these definitions with actual sampled values (like `num_heads` might become `4` for one trial, and `6` for another).\n5.  This code tells the tuner *what ranges are allowed*. The tuner then takes care of picking specific values for each trial and records the space definition (similar to what we saw in `oracle.json`).\n\n## Conclusion\n\nYou've now learned about the **Tuning Hyperparameter Space**!\n\n*   It defines the **allowed range of values** for each setting (hyperparameter) we want to optimize in our model.\n*   It's like the **variation guidelines** in a recipe (e.g., \"2-8 eggs\", \"100g-500g sugar\").\n*   We define this space to guide the **automatic tuning process** (like Keras Tuner).\n*   The tuner explores combinations *only within these boundaries* to find the best model configuration.\n*   This definition is typically done in Python code using the tuner's tools (like `hp.Int`, `hp.Float`) and recorded in files like `oracle.json`.\n\nWe've defined the shape of our data ([Model Input Shape](01_model_input_shape.md)) and the search space for our tunable model settings (Hyperparameter Space). But are there any settings that we *don't* want to tune, settings that should remain fixed during the experiments?\n\nIn the next chapter, we'll look at [Training Constants](03_training_constants.md), which are the fixed parameters used during our model training and tuning process.\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 20: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\03_training_constants.md ---\n# Chapter 3: Training Constants\n\nWelcome back! In the [previous chapter](02_tuning_hyperparameter_space.md), we explored the **Tuning Hyperparameter Space**, which defines the range of possibilities for settings we want to experiment with, like searching for the best oven temperature or sugar amount in our recipe.\n\nBut what about the parts of the recipe that *don't* change? Like using a standard-sized measuring cup (e.g., always 1 cup) or a default oven preheat temperature if the recipe doesn't specify otherwise? In machine learning, we also have fixed values that we use consistently during the training process. These are our **Training Constants**.\n\n## The Problem: Keeping Standard Settings Consistent\n\nImagine you and your teammates are all working on training the same machine learning model. You're trying different approaches (different hyperparameters, like we saw in Chapter 2). But some basic settings should probably stay the same for everyone, at least initially, to ensure fair comparisons or provide a reliable baseline.\n\nFor example:\n*   How many data samples (sentences) should the model look at in one go (the \"batch size\")?\n*   If we use a standard learning algorithm (an \"optimizer\" like Adam or SGD) without tuning its learning speed, what default speed should we use?\n\nIf everyone picks a different batch size or a different default learning speed, it becomes hard to compare results or even understand what's going on. We need a way to define and use these standard, fixed values easily and consistently.\n\n## What are Training Constants?\n\n**Training Constants** are fixed numerical values defined in our project that are used during the model training process. They are *not* typically changed during hyperparameter tuning. Think of them as the standard tools or baseline settings in our machine learning \"kitchen\":\n\n*   **Standard Measuring Cup:** The `BATCH_SIZE` defines how many pieces of data we process together in one step. We set a standard size for this.\n*   **Default Oven Preheat:** If we aren't specifically tuning the learning rate for an optimizer like Adam or SGD, we might have a standard, default `LEARNING_RATE` to use as a starting point.\n\nThese constants provide:\n1.  **Consistency:** Everyone uses the same value for these basic settings.\n2.  **Baselines:** They offer default values, especially useful before we start fine-tuning hyperparameters.\n3.  **Readability:** Instead of seeing a raw number like `32` scattered throughout the code, we see a meaningful name like `BATCH_SIZE`.\n\n## Where We Define Constants: `constants.py`\n\nIn our `SC4002-Group-Assignment` project, we keep these fixed values together in a dedicated file called `constants.py`. This makes them easy to find and understand.\n\nLet's look inside this file:\n\n--- File: `constants.py` ---\n```python\n# Default number of samples processed in each training step\nBATCH_SIZE = 32\n\n# Default learning rate for the Adam optimizer\nADAM_LR = 0.001\n\n# Default learning rate for the SGD optimizer\nSGD_LR = 0.01\n```\n\n**Explanation:**\n\n*   `BATCH_SIZE = 32`: This line sets a constant named `BATCH_SIZE` to the value `32`. This means that during training, our model will typically process data in groups of 32 sentences at a time. Why 32? It's a common default value that often balances computational speed and learning stability.\n*   `ADAM_LR = 0.001`: This sets a constant for the default **L**earning **R**ate for an optimizer called \"Adam\". An optimizer helps the model learn by adjusting its internal parameters. The learning rate controls how big those adjustments are. `0.001` is a commonly used starting point for Adam.\n*   `SGD_LR = 0.01`: Similarly, this sets the default learning rate for another optimizer called \"SGD\" (Stochastic Gradient Descent). `0.01` is a typical default for SGD.\n\nThese values are *constants* \u2013 they are defined once here and aren't expected to be changed by the tuning process itself. If we *were* tuning the learning rate (like we saw possible in [Chapter 2](02_tuning_hyperparameter_space.md)), the tuner would override this default value for specific trials. But if we aren't tuning it, or if we need a fixed batch size, we use these constants.\n\n## How Constants Are Used in Code (Conceptual)\n\nOther parts of our project's code can easily access these predefined constants. Here's a simplified, conceptual example of how `BATCH_SIZE` might be used when preparing data for training:\n\n```python\n# Conceptual Python code showing usage of constants\n\n# 1. Import the constants from our special file\nfrom constants import BATCH_SIZE, ADAM_LR\n\n# (Imagine we have loaded our dataset: 'training_data')\n# ... load data ...\n\n# 2. Use the BATCH_SIZE constant when setting up the data loader\n# This tells the loader to provide data in chunks of 32 samples.\ndata_loader = SomeDataLoader(dataset=training_data, batch_size=BATCH_SIZE)\nprint(f\"Data will be processed in batches of size: {BATCH_SIZE}\")\n\n# 3. Use the ADAM_LR constant when setting up a default optimizer\n# (If we weren't tuning the learning rate for this specific setup)\ndefault_optimizer = tf.keras.optimizers.Adam(learning_rate=ADAM_LR)\nprint(f\"Using default Adam optimizer with LR: {ADAM_LR}\")\n\n# ... rest of the training setup code ...\n```\n\n**Explanation:**\n\n1.  We use `from constants import BATCH_SIZE, ADAM_LR` to bring the values defined in `constants.py` into our current script.\n2.  When creating a `DataLoader` (a tool to feed data to the model), we pass `BATCH_SIZE` to its `batch_size` parameter. Now, the data loader knows to use the standard size of 32.\n3.  When creating an Adam optimizer, we use `ADAM_LR` as the default learning rate.\n\nThis makes the code cleaner (`BATCH_SIZE` is more descriptive than `32`) and ensures we use the project's standard values.\n\n## Constants vs. Hyperparameters: What's the Difference?\n\nIt's important to distinguish between Training Constants and the Hyperparameters we discussed in [Chapter 2](02_tuning_hyperparameter_space.md).\n\n| Feature          | Training Constants (`constants.py`)             | Tunable Hyperparameters (`oracle.json` space) | Analogy                   |\n| :--------------- | :---------------------------------------------- | :-------------------------------------------- | :------------------------ |\n| **Purpose**      | Fixed, standard values for consistency/baseline | Settings to experiment with for optimization  | Standard measuring cup    | Variable recipe amount    |\n| **Value Source** | Defined directly in `constants.py`              | Sampled from a range by the tuner           | Fixed tool size           | Chef's choice (within range) |\n| **Variability**  | Stays the same across tuning trials             | Changes from one tuning trial to the next   | Always 1 cup              | Maybe 2 eggs, then 3 eggs |\n| **Example**      | `BATCH_SIZE = 32`                               | `hp.Int(\"num_layers\", 1, 4)`                  | Default preheat (180\u00b0C) | Testing 170\u00b0C vs 190\u00b0C    |\n\nTraining constants provide the stable foundation, while hyperparameters are the knobs we adjust during tuning to find the best performance.\n\n## Conclusion\n\nIn this chapter, we learned about **Training Constants**:\n\n*   They are **fixed numerical values** used throughout the training process (e.g., `BATCH_SIZE`, default learning rates like `ADAM_LR`).\n*   They ensure **consistency**, provide **baselines**, and improve code **readability**.\n*   They are typically defined in a central file like `constants.py`.\n*   They are different from **hyperparameters**, which are *tuned* and varied during experiments. Constants remain fixed.\n\nThink of them as the reliable, standard tools and settings in your ML kitchen, always there and always the same, allowing you to focus on adjusting the more experimental parts of your recipe (the hyperparameters).\n\nNow that we understand the data shape, the tunable settings (hyperparameters), and the fixed settings (constants), how do we actually put this all together to start the tuning process?\n\nLet's move on to the next chapter: [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md).\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 21: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\04_hyperparameter_tuning_setup.md ---\n# Chapter 4: Hyperparameter Tuning Setup\n\nHi there! In the [previous chapter](03_training_constants.md), we learned about **Training Constants** \u2013 the fixed settings like `BATCH_SIZE` that act as our standard tools. We now have:\n\n1.  The required data format: [Model Input Shape](01_model_input_shape.md) (`[null, 62]`).\n2.  The range of experimental settings: [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) (e.g., how many layers, learning rate).\n3.  The standard, fixed settings: [Training Constants](03_training_constants.md) (e.g., batch size).\n\nNow, how do we actually put all this together and *start the experiments* to find the best combination of hyperparameters? We need a system, a setup, to manage this process automatically. That's what this chapter is about!\n\n## The Problem: Running Many Experiments Systematically\n\nImagine you're determined to bake the absolute best chocolate cake. You have a basic recipe, but you know you can improve it by tweaking things like the amount of sugar, the number of eggs, or the baking time (these are like our hyperparameters).\n\nTrying just one variation isn't enough. You need to:\n*   Try *many* different combinations (recipes).\n*   Keep track of exactly what you did for each cake (which recipe variation).\n*   Record how well each cake turned out (taste test!).\n*   Compare the results to find the winning recipe.\n\nDoing this manually would be slow and messy. You need an organized *setup* for your baking experiments \u2013 maybe a dedicated corner of the kitchen, a notebook for results, and a clear plan.\n\nSimilarly, finding the best settings for our sentiment analysis model requires running many training experiments with different hyperparameter combinations. We need an organized way to manage these experiments, track their configurations, and store their results. This organized system is our **Hyperparameter Tuning Setup**.\n\n## What is the Hyperparameter Tuning Setup?\n\nThe **Hyperparameter Tuning Setup** is the overall **process and infrastructure** we use to automatically:\n\n1.  Define the experiments we want to run (using the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)).\n2.  Execute each experiment (called a \"trial\"). Each trial builds and trains a model with a specific set of hyperparameters.\n3.  Keep track of the hyperparameters used in each trial.\n4.  Record the performance (e.g., accuracy) of the model from each trial.\n5.  Organize all this information so we can later identify the best hyperparameters found.\n\nThink of it as setting up your **experimental kitchen** for finding the best cake recipe:\n\n*   **The Kitchen Space:** A dedicated place where all the experiments happen. In our project, this is the `sentiment_transformer_tuning` folder.\n*   **The Master Recipe Book:** A central place defining the range of ingredients to try ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)) and tracking the overall progress ([Keras Tuner Oracle](05_keras_tuner_oracle.md)).\n*   **Individual Recipe Cards & Notes:** For each specific cake baked (each trial), a record of the exact recipe used and how it turned out ([Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)).\n\n## Our Experimental Kitchen: The `sentiment_transformer_tuning` Folder\n\nIn our project, the main place where all the tuning experiments happen and their results are stored is a folder named `sentiment_transformer_tuning`.\n\nLet's peek at its structure and purpose:\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 README.md                    # A note about this folder\n\u251c\u2500\u2500 oracle.json                  # The 'master plan' / tuner's brain state\n\u251c\u2500\u2500 tuner0.json                  # Tuner configuration status (often empty or minimal)\n\u251c\u2500\u2500 trial_00/                    # Results & config for the 1st experiment\n\u2502   \u251c\u2500\u2500 build_config.json        # Input shape used for this trial\n\u2502   \u251c\u2500\u2500 trial.json               # Details of this specific trial (hyperparameters, score)\n\u2502   \u2514\u2500\u2500 ... (other files like model checkpoints)\n\u251c\u2500\u2500 trial_01/                    # Results & config for the 2nd experiment\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_02/\n...\n\u2514\u2500\u2500 trial_09/                    # Results & config for the 10th experiment\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\n**Explanation:**\n\n*   **`sentiment_transformer_tuning/`**: This is the main directory for our tuning process. Think of it as the logbook for our recipe experiments. The `README.md` inside just gives a brief description.\n    --- File: `sentiment_transformer_tuning/README.md` ---\n    ```\n    This folder contains trials 1-10 for sentiment transformer tuning, produced on 28/10/2024. This folder is merely used to store these intermediary files \n    ```\n*   **`oracle.json`**: This is a crucial file managed by the tuning tool (Keras Tuner). It stores the overall state of the tuning process, including the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined, which trials have been run, and maybe the best results found so far. We'll dive deeper into this in the next chapter: [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   **`tuner0.json`**: Stores some internal state for the tuner itself. For beginners, it's less important than `oracle.json`.\n*   **`trial_XX/` folders**: Each folder represents *one complete experiment* (one \"trial\") testing a specific combination of hyperparameters.\n    *   Inside each `trial_XX` folder, files like `trial.json` store the specific hyperparameter values used for *that trial* and the resulting performance score. Other files might store the trained model weights or configuration details like the input shape (`build_config.json`). We'll explore these in more detail in [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\nThis folder structure is automatically created and managed by the hyperparameter tuning tool (like Keras Tuner) when we run the tuning script. It provides a clean and organized way to store the results of potentially hundreds of experiments.\n\n## How the Tuning Setup Works (The Big Picture)\n\nImagine you start the automatic tuning process (usually by running a Python script, which we won't detail here). Here\u2019s a simplified step-by-step flow:\n\n1.  **Start:** You initiate the tuning process.\n2.  **Ask Oracle:** The system asks the [Keras Tuner Oracle](05_keras_tuner_oracle.md) (using the info in `oracle.json`) for the next set of hyperparameters to try, based on the defined [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) and past results.\n3.  **Run Trial:** A new trial (e.g., `trial_10`) is started:\n    *   A model is built using the specific hyperparameters suggested by the Oracle.\n    *   The model is trained using the data, respecting the [Model Input Shape](01_model_input_shape.md) and using [Training Constants](03_training_constants.md) where needed.\n    *   The model's performance is evaluated (e.g., accuracy is measured).\n4.  **Save Results:** The hyperparameters used, the final performance score, and other relevant information for this trial are saved inside the corresponding `trial_XX` folder (e.g., in `sentiment_transformer_tuning/trial_10/trial.json`).\n5.  **Update Oracle:** The performance result is reported back to the Oracle, which updates its state in `oracle.json`. This helps it make better decisions about which hyperparameters to try next.\n6.  **Repeat:** Steps 2-5 are repeated for a predefined number of trials or until a certain condition is met.\n7.  **Finish:** Once done, you can examine the `oracle.json` file or the `trial_XX` folders (especially their `trial.json` files) to find the trial that achieved the best performance and the hyperparameters that led to it.\n\nHere's a diagram illustrating this flow:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant TuningScript as Tuning Process\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    User->>TuningScript: Start Hyperparameter Tuning\n    loop For each trial\n        TuningScript->>Oracle: Get next Hyperparameters to test\n        Oracle-->>TuningScript: Provide Hyperparameter set\n        TuningScript->>TrialRunner: Run trial with these Hyperparameters\n        TrialRunner->>TrialRunner: Build Model\n        TrialRunner->>TrialRunner: Train Model\n        TrialRunner->>TrialRunner: Evaluate Performance (e.g., Accuracy)\n        TrialRunner->>TrialFolder: Save Trial Config & Results (trial.json)\n        TrialRunner-->>TuningScript: Return Performance Score\n        TuningScript->>Oracle: Report Score for these Hyperparameters\n        Oracle->>Oracle: Update internal state (oracle.json)\n    end\n    TuningScript-->>User: Tuning Finished (Results in folder)\n\n```\n\nThis whole setup allows us to systematically explore different model configurations without manual intervention, saving time and effort while increasing the chances of finding a high-performing model.\n\n## Conclusion\n\nYou've now learned about the **Hyperparameter Tuning Setup**:\n\n*   It's the **overall process and infrastructure** for automatically finding the best model settings (hyperparameters).\n*   It involves running multiple **trials** (experiments), each testing a different hyperparameter combination.\n*   It's like setting up an **experimental kitchen** to systematically test many cake recipes.\n*   In our project, the results and state are primarily stored in the **`sentiment_transformer_tuning` folder**.\n*   This folder contains the **`oracle.json`** file (the master plan) and individual **`trial_XX` folders** (results of each specific experiment).\n*   This setup automates the search, organizes results, and helps us find the best-performing model configuration.\n\nNow that we understand the overall structure, let's zoom in on the \"brain\" of this operation \u2013 the component that decides which hyperparameters to try next.\n\nReady to meet the mastermind? Let's explore the [Keras Tuner Oracle](05_keras_tuner_oracle.md) in the next chapter!\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 22: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\05_keras_tuner_oracle.md ---\n# Chapter 5: Keras Tuner Oracle\n\nWelcome back! In the [previous chapter](04_hyperparameter_tuning_setup.md), we learned about the overall **Hyperparameter Tuning Setup** \u2013 our organized \"experimental kitchen\" (`sentiment_transformer_tuning/` folder) for automatically running many model training experiments (trials) to find the best settings. We saw that this setup involves asking for hyperparameter combinations, running trials, and recording results.\n\nBut who decides *which* combination of hyperparameters to try next? And who keeps track of all the experiments and their outcomes? That's the job of the **Keras Tuner Oracle**!\n\n## The Problem: Who's Directing the Experiments?\n\nImagine you're in your experimental kitchen, ready to bake dozens of cakes to find the perfect recipe. You've defined the possible ranges for your ingredients ([Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)): 2-8 eggs, 100g-500g sugar, etc.\n\nYou *could* just randomly pick values from these ranges for each cake. But maybe there's a smarter way? Perhaps after baking a few cakes, you notice that cakes with more sugar tend to taste better. Shouldn't your next experiments focus more on the higher end of the sugar range?\n\nYou need a **director** or a **head chef** for your experiments. Someone (or something!) that:\n\n1.  Knows the allowed ingredient ranges (the hyperparameter space).\n2.  Decides the *exact* recipe (hyperparameter combination) for the *next* cake (trial).\n3.  Keeps a detailed logbook of every recipe tried and how well the cake turned out (the performance score).\n4.  Uses this information (and possibly a clever strategy) to guide future experiments towards better recipes.\n\nThis \"director\" role in Keras Tuner is filled by the **Oracle**.\n\n## What is the Keras Tuner Oracle?\n\nThe **Keras Tuner Oracle** is the central component that **manages the hyperparameter search process**. It's the \"brain\" behind the tuning operation within the Keras Tuner library.\n\nThink of it as the **Head Chef** in our experimental kitchen:\n\n*   **Knows the Recipe Boundaries:** It holds the definition of the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md).\n*   **Keeps the Master Logbook:** It tracks which hyperparameter combinations have been tried (`tried_so_far`), the results (scores) of those trials, and the overall state of the search.\n*   **Plans the Next Experiment:** Based on a defined strategy (like random search, or more advanced methods), it decides which specific combination of hyperparameters should be tested in the next trial.\n\nThe Oracle ensures that the search is conducted systematically and efficiently, guiding the process towards finding the best possible hyperparameters within the defined space.\n\n## The Oracle's Logbook: Peeking Inside `oracle.json`\n\nThe Oracle needs to store its knowledge and the history of the search somewhere. In our project setup ([Chapter 4](04_hyperparameter_tuning_setup.md)), this information is primarily stored in the `sentiment_transformer_tuning/oracle.json` file. This file acts as the Oracle's master logbook and state tracker.\n\nLet's look at some key parts of this file (we saw the full file in Chapter 4, here are the highlights):\n\n--- File: `sentiment_transformer_tuning/oracle.json` (Simplified Snippet) ---\n```json\n{\n  \"hyperparameters\": { // Section about the settings being tuned\n    \"space\": [ // The allowed ranges (defined in Chapter 2)\n      // ... definitions for num_heads, dff, num_layers, etc. ...\n    ]\n  },\n  \"tried_so_far\": [ // A list representing unique recipes already tried\n    \"198210495baf2451b73e907a02b1546c\", // Hash representing one combo\n    \"c4be04584b5ae928a62b872b508505ad\", // Another unique combo\n    // ... more hashes ...\n    \"29c605d5e8a77f9296610df81f15c87c\"\n  ],\n  \"id_to_hash\": { // Maps trial IDs (like folder names) to the recipe hash\n    \"00\": \"29c605d5e8a77f9296610df81f15c87c\",\n    \"01\": \"b0d65a09e62a5f5927ee1a53ac7d0e1a\",\n    // ... mapping for all trials (00 to 09) ...\n    \"09\": \"69e1b9791653b9499f6986d32cdc9cb0\"\n  },\n  \"seed\": 2999, // Starting point for random number generation\n  \"seed_state\": 3049, // Current state of random generation\n  // ... other details like trial start/end times ...\n}\n```\n\n**What this tells us about the Oracle:**\n\n*   **It knows the rules:** The `\"hyperparameters\"` section shows the Oracle has stored the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md) we defined \u2013 the allowed ranges for each setting.\n*   **It remembers the past:** The `\"tried_so_far\"` list contains unique identifiers (hashes) for each hyperparameter combination that has already been tested. This prevents the Oracle from trying the exact same recipe twice (unless intended). Like the chef crossing off recipes in the logbook.\n*   **It connects trials to recipes:** The `\"id_to_hash\"` dictionary links the trial number (like `00`, `01`) to the specific recipe (hash) used in that trial. This helps organize the results.\n*   **It manages randomness:** The `\"seed\"` and `\"seed_state\"` relate to how the Oracle uses random numbers (important for strategies like Random Search or breaking ties). Using seeds helps make experiments reproducible.\n\nThis `oracle.json` file essentially captures the Oracle's memory and current understanding of the search process.\n\n## How the Oracle Decides: The Strategy\n\nOkay, the Oracle remembers the past, but how does it decide what to try *next*? This depends on the **type** of Oracle being used, which corresponds to a specific search strategy.\n\nCommon strategies (types of Oracles) include:\n\n1.  **Random Search:** The Oracle simply picks the next hyperparameter combination randomly from the allowed space. It's like the chef randomly choosing ingredient amounts within the defined ranges for each cake. This is simple and surprisingly effective sometimes. The `oracle.json` we looked at likely came from a Random Search, as it explores different combinations without a clear pattern based only on the hashes shown.\n2.  **Bayesian Optimization:** A more advanced strategy. The Oracle builds a statistical model based on the results of previous trials. It uses this model to predict which *untried* combinations are most likely to yield good results. It tries to balance exploring new areas of the search space with exploiting areas that have already shown promise. Like the chef noticing that 3-egg cakes were good, and deciding to try 4 eggs next, based on that trend.\n3.  **Hyperband:** Another clever strategy that tries many combinations quickly with limited resources (e.g., training for fewer epochs initially) and then dedicates more resources to the most promising ones.\n\nThe choice of which Oracle (and thus strategy) to use is made when you set up the tuner in your Python code. The Oracle then implements that strategy, using the information in `oracle.json` to guide its decisions.\n\n## How it Works Under the Hood\n\nLet's visualize the Oracle's role in the tuning loop:\n\n1.  The main tuning process needs parameters for a new trial.\n2.  It asks the Oracle.\n3.  The Oracle consults its internal state (strategy, history from `oracle.json`, defined space).\n4.  It decides on a new set of hyperparameters.\n5.  It gives these back to the tuning process.\n6.  The trial runs with these parameters.\n7.  The tuning process reports the final score (e.g., accuracy) back to the Oracle.\n8.  The Oracle updates its state (`oracle.json`) with the results of this trial.\n\nHere's a simplified diagram:\n\n```mermaid\nsequenceDiagram\n    participant Tuner as Tuning Process\n    participant Oracle as Keras Tuner Oracle (manages oracle.json)\n    participant Trial as Trial Execution\n\n    Tuner->>Oracle: Request hyperparameters for next trial\n    Oracle->>Oracle: Consult space, history, strategy\n    Oracle->>Oracle: Decide next hyperparameters (e.g., num_heads=4, lr=0.0005)\n    Oracle-->>Tuner: Provide chosen hyperparameters\n    Tuner->>Trial: Run trial with these parameters\n    Trial->>Trial: Build, Train, Evaluate Model\n    Trial-->>Tuner: Return performance score (e.g., 0.85 accuracy)\n    Tuner->>Oracle: Report score for the tried hyperparameters\n    Oracle->>Oracle: Update history & state (in oracle.json)\n```\n\n## Using the Oracle (Implicitly)\n\nAs a beginner, you usually don't interact with the Oracle object directly *while* the tuning is running. You interact with it primarily during the initial **setup**:\n\n```python\n# Conceptual Python code for setting up a Tuner with an Oracle\n\nimport keras_tuner as kt\nimport tensorflow as tf # Just for context\n\n# Assume build_model is a function that defines your model\n# and uses hp.Int, hp.Float etc. to define the search space\n# (as shown in Chapter 2)\n# def build_model(hp):\n#   ... define model using hp ...\n#   return model\n\n# 1. CHOOSE the Oracle/Strategy: Here we choose RandomSearch\ntuner = kt.RandomSearch(\n    hypermodel=build_model,           # Function that builds the model\n    objective=\"val_accuracy\",        # What score to maximize\n    max_trials=10,                   # How many different combinations to try\n    executions_per_trial=1,          # How many times to train each combination\n    directory=\"sentiment_transformer_tuning\", # Where to store results (incl. oracle.json)\n    project_name=\"my_first_tuning\"   # Subfolder within the directory\n)\n\n# 2. START the search (The Tuner uses the Oracle internally)\n# tuner.search(train_data, epochs=5, validation_data=val_data)\n# ... The Oracle manages things behind the scenes during search ...\n\nprint(\"Tuning setup complete. The Oracle will manage the search.\")\n# When tuner.search() runs, the RandomSearch Oracle inside the 'tuner'\n# object will be consulted repeatedly.\n```\n\n**Explanation:**\n\n1.  We choose `kt.RandomSearch`. This creates a Tuner that *contains* a `RandomSearchOracle`. If we wanted Bayesian Optimization, we'd use `kt.BayesianOptimization(...)`.\n2.  We configure the Tuner, telling it which function defines our model and search space (`build_model`), what metric to optimize (`val_accuracy`), how many trials to run (`max_trials`), and where to save the results (`directory`).\n3.  When we would call `tuner.search(...)` (commented out here), the Tuner object takes over. It repeatedly asks its internal Oracle for hyperparameters, runs the trial, and reports back the score, all automatically. The `oracle.json` file is created and updated within the specified `directory`.\n\nSo, you choose the *type* of Oracle by choosing the type of Tuner, and the framework handles the rest!\n\n## Conclusion\n\nYou've now met the **Keras Tuner Oracle**, the mastermind behind hyperparameter tuning!\n\n*   It's the **central component managing the search**, like the head chef directing recipe experiments.\n*   It **knows the hyperparameter space**, **tracks tried combinations** and their **results**, and **decides which parameters to try next**.\n*   Its state and history are stored in the **`oracle.json`** file.\n*   The **strategy** it uses (e.g., Random Search, Bayesian Optimization) depends on the type of Oracle/Tuner you choose during setup.\n*   It works behind the scenes during the `tuner.search()` process to guide the experiments intelligently.\n\nThe Oracle manages the overall search, but what information is stored for each *individual* experiment or \"trial\" that it launches?\n\nLet's dive into the details of each specific experiment in the next chapter: [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md).\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 23: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\06_tuning_trial_configuration___state.md ---\n# Chapter 6: Tuning Trial Configuration & State\n\nHello again! In the [previous chapter](05_keras_tuner_oracle.md), we met the **Keras Tuner Oracle**, the \"Head Chef\" or \"director\" that manages our hyperparameter tuning experiments. The Oracle decides *which* combinations of settings (hyperparameters) to try next and keeps track of the overall progress in its master logbook (`oracle.json`).\n\nBut what about the detailed records for each *individual* experiment? When our Head Chef decides to try baking a specific cake (say, Cake #5 with 3 eggs, 200g sugar, baked for 35 minutes), we need a dedicated place to write down *that specific recipe* and *exactly how that cake turned out*. Where do we store the notes for each single attempt?\n\nThat's where the concept of **Tuning Trial Configuration & State** comes in!\n\n## The Problem: Keeping Track of Each Experiment's Details\n\nThe [Keras Tuner Oracle](05_keras_tuner_oracle.md) gives us the big picture \u2013 which recipes have been tried (`tried_so_far` in `oracle.json`), the overall search space, and maybe the best score found so far. But if we want to know the *exact* settings used for Trial #3, or the specific accuracy achieved by Trial #7, we need to look at the records for those individual trials.\n\nWe need a system to store:\n*   The **configuration** used for each trial (the specific hyperparameter values).\n*   The **state** of each trial (Did it start? Did it finish? What was the result?).\n\n## What is a \"Trial\" in Hyperparameter Tuning?\n\nA **Trial** represents **one single experiment run** within the larger hyperparameter tuning process.\n\nThink back to our baking analogy:\n*   **Hyperparameter Tuning Process:** The entire project of baking dozens of cakes to find the best recipe.\n*   **Trial:** Baking *one specific cake* using *one specific recipe variation* chosen by the Head Chef (the Oracle).\n\nEach trial involves:\n1.  Getting a specific set of hyperparameter values from the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n2.  Building a machine learning model using *exactly* those hyperparameters.\n3.  Training the model on our data.\n4.  Evaluating its performance (e.g., measuring its accuracy).\n5.  Saving the details: which hyperparameters were used, and what the final score was.\n\n## The Trial's Notebook: `trial_XX/` Folders\n\nRemember the `sentiment_transformer_tuning/` folder we saw in [Chapter 4: Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)? Inside that main folder, we find several subfolders named `trial_00/`, `trial_01/`, `trial_02/`, and so on.\n\n```\nsentiment_transformer_tuning/\n\u251c\u2500\u2500 oracle.json                  # The Oracle's master logbook\n\u251c\u2500\u2500 ... (other top-level files)\n\u251c\u2500\u2500 trial_00/                    # <--- Notebook page for Trial 0\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 trial_01/                    # <--- Notebook page for Trial 1\n\u2502   \u251c\u2500\u2500 build_config.json\n\u2502   \u251c\u2500\u2500 trial.json\n\u2502   \u2514\u2500\u2500 ...\n...\n\u2514\u2500\u2500 trial_09/                    # <--- Notebook page for Trial 9\n    \u251c\u2500\u2500 build_config.json\n    \u251c\u2500\u2500 trial.json\n    \u2514\u2500\u2500 ...\n```\n\nEach `trial_XX` folder is like a **dedicated page or section in our baking logbook** for one specific cake attempt. It contains all the important notes and results for that particular trial.\n\n## Inside the Trial Notebook: Key Files\n\nLet's open one of these trial folders, say `trial_05/`, and look at the most important files inside.\n\n### 1. `trial.json`: The Recipe Card and Result\n\nThis is the most crucial file within a trial folder. It stores the core information about this specific experiment: the exact configuration used and the outcome.\n\n--- File: `sentiment_transformer_tuning/trial_05/trial.json` (Conceptual Example Structure) ---\n```json\n{\n  \"trial_id\": \"05\", // Unique ID for this trial\n  \"hyperparameters\": {\n    \"space\": [ // Repeats the search space definition\n      // ... space definition ...\n    ],\n    \"values\": { // <-- THE IMPORTANT PART! Specific values used in THIS trial\n      \"num_heads\": 6,\n      \"dff\": 256,\n      \"num_layers\": 3,\n      \"dropout_rate\": 0.3,\n      \"learning_rate\": 0.00015 // Example values chosen by the Oracle for trial 05\n    }\n  },\n  \"score\": 0.885, // The performance metric (e.g., validation accuracy) achieved\n  \"best_step\": 4, // Which training step/epoch gave the best score\n  \"status\": \"COMPLETED\" // State of the trial (e.g., RUNNING, COMPLETED, FAILED)\n}\n```\n\n**Explanation:**\n\n*   `\"trial_id\"`: Matches the folder name (\"05\").\n*   `\"hyperparameters\"`:\n    *   `\"space\"`: Often repeats the overall search space definition (for completeness).\n    *   `\"values\"`: This is key! It shows the **exact hyperparameter values** that the [Keras Tuner Oracle](05_keras_tuner_oracle.md) selected *for this specific trial*. Think of this as the precise recipe used for Cake #5.\n*   `\"score\"`: The final evaluation score obtained by the model trained in this trial. This is the \"taste test\" result for Cake #5.\n*   `\"status\"`: Tells us if the trial finished successfully (`COMPLETED`), is still running (`RUNNING`), or encountered an error (`FAILED`).\n\nThis `trial.json` file gives you a complete snapshot of what happened in one specific experiment.\n\n### 2. `build_config.json`: Confirming the Setup\n\nThis file often stores configuration details that were fixed when the model for this trial was *built*, rather than settings that were tuned. A common example we've already seen is the [Model Input Shape](01_model_input_shape.md).\n\n--- File: `sentiment_transformer_tuning/trial_05/build_config.json` ---\n```json\n{\"input_shape\": [null, 62]}\n```\n\n**Explanation:**\n\n*   This file confirms that for Trial #05, the model was built expecting the standard input shape `[null, 62]`, as discussed in [Chapter 1](01_model_input_shape.md).\n*   You'll notice this file is often identical across many `trial_XX` folders (like the snippets provided show for trials 00 through 09). This indicates that the input shape was a constant part of the setup for all these trials, not something being tuned.\n*   Analogy: This file is like a note confirming we used the standard 9x13 inch baking pan for Cake #5, just like we did for Cakes #0 through #9.\n\n### Other Files\n\nYou might sometimes find other files inside a `trial_XX` folder, such as:\n*   `checkpoints/`: Saved versions (weights) of the model trained during this trial.\n*   `logs/`: Detailed logs from the training process.\n\nFor understanding the **configuration and state**, `trial.json` and `build_config.json` are usually the most important.\n\n## Connecting the Oracle and the Trials\n\nHow do the Oracle's master logbook (`oracle.json`) and the individual trial notebooks (`trial_XX/trial.json`) relate to each other?\n\n1.  **Oracle Decides:** The [Keras Tuner Oracle](05_keras_tuner_oracle.md) picks a set of hyperparameters (a specific recipe) based on its strategy and history (`oracle.json`).\n2.  **Trial Starts:** A new trial folder (e.g., `trial_10/`) is created. The chosen hyperparameters are recorded in `trial_10/trial.json` with status `RUNNING`. The Oracle also notes the start time in `oracle.json`.\n3.  **Trial Runs:** The model is built (using fixed settings like `input_shape` recorded in `build_config.json`) and trained using the specific hyperparameters from `trial_10/trial.json`.\n4.  **Trial Ends:** The performance score is measured. This score, along with the final status (`COMPLETED`), is saved in `trial_10/trial.json`.\n5.  **Report Back:** The score is reported back to the Oracle, which updates its master logbook (`oracle.json`) to include this result, helping it decide future trials.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Oracle as Keras Tuner Oracle (oracle.json)\n    participant TrialRunner as Trial Execution Process\n    participant TrialFolder as sentiment_transformer_tuning/trial_XX/\n\n    Oracle->>TrialRunner: Start Trial XX with specific Hyperparameters (HP_Set_XX)\n    TrialRunner->>TrialFolder: Create folder `trial_XX/`\n    TrialRunner->>TrialFolder: Write HP_Set_XX to `trial.json` (Status: RUNNING)\n    TrialRunner->>TrialFolder: Write build info (e.g., input shape) to `build_config.json`\n    Note right of TrialRunner: Build & Train Model using HP_Set_XX\n    TrialRunner->>TrialRunner: Evaluate Model, get Score_XX\n    TrialRunner->>TrialFolder: Update `trial.json` with Score_XX and Status: COMPLETED\n    TrialRunner-->>Oracle: Report Score_XX for Trial XX\n    Oracle->>Oracle: Update internal state (oracle.json) with Trial XX result\n\n```\n\n## How Are Trial Folders Created?\n\nYou don't need to create these `trial_XX` folders or the JSON files inside them manually! The Keras Tuner framework does this automatically for you when you run the tuning process (typically initiated by a `tuner.search(...)` command in your Python script).\n\nFor each trial the Oracle decides to run, the tuner framework handles:\n*   Creating the `trial_XX` directory.\n*   Populating `trial.json` with the chosen hyperparameters and initial status.\n*   Running your model-building function (which might save `build_config.json`).\n*   Executing the training and evaluation.\n*   Updating `trial.json` with the final score and status.\n\nIt's all part of the automated **Hyperparameter Tuning Setup** we discussed in [Chapter 4](04_hyperparameter_tuning_setup.md).\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual experiments within hyperparameter tuning: the **Trials**.\n\n*   A **Trial** is a single run testing one specific combination of hyperparameters chosen by the [Keras Tuner Oracle](05_keras_tuner_oracle.md).\n*   It's like baking **one specific cake** with **one specific recipe variation**.\n*   The details of each trial \u2013 its **configuration** (hyperparameters used) and **state** (status, score) \u2013 are stored in dedicated **`trial_XX` folders**.\n*   The key file inside is **`trial.json`**, acting as the \"recipe card and result note\" for that trial.\n*   **`build_config.json`** often stores fixed build-time settings like the [Model Input Shape](01_model_input_shape.md).\n*   These trial-specific records complement the Oracle's overall view and allow us to analyze each experiment individually.\n\nWe've now covered the core concepts of our tuning process: the data shape, the search space, constants, the overall setup, the Oracle director, and the individual trial records. Before you can run this project yourself, there's one last piece: knowing what software tools and libraries you need installed.\n\nLet's move on to the final chapter: [Project Dependencies](07_project_dependencies.md).\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 24: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\07_project_dependencies.md ---\n# Chapter 7: Project Dependencies\n\nWelcome to the final chapter of our `SC4002-Group-Assignment` tutorial! In the [previous chapter](06_tuning_trial_configuration___state.md), we looked closely at individual **Tuning Trials**, understanding how the configuration and results of each specific experiment are stored in the `trial_XX` folders. We've covered how data is shaped, how hyperparameters are tuned, and how results are tracked.\n\nNow, imagine you've just downloaded this project onto your computer, eager to run it. You try to execute the code, but... errors pop up everywhere! It complains about missing tools like \"TensorFlow\" or \"NLTK\". Why is this happening? It's because your computer doesn't yet have all the necessary \"appliances\" and \"ingredients\" (software libraries) required to run the project's \"recipe\" (the code). How do we make sure everyone has the exact same set of tools? That's where **Project Dependencies** come in.\n\n## The Problem: Missing Tools in Your Kitchen\n\nThink of our project's code as a complex recipe for baking a cake (analyzing sentiment). This recipe relies on specific kitchen tools and appliances:\n\n*   A powerful mixer (like **TensorFlow** for machine learning calculations)\n*   A specific brand of food processor (like **Keras Tuner** for hyperparameter optimization)\n*   A set of specialized measuring spoons (like **NLTK** for text processing)\n*   Even specific brands of flour or sugar (particular versions of these libraries)\n\nIf you try to follow the recipe, but your kitchen is missing the mixer, or you have a different brand of food processor that works slightly differently, the cake might not turn out right, or you might not be able to bake it at all!\n\nSimilarly, our Python code relies on many external software packages (libraries) written by other developers. To run our project, you need to have these libraries installed on your computer, and often, you need *specific versions* of them to ensure everything works together correctly. If you have the wrong version, or a library is missing, the code will fail.\n\nHow can we easily list all the required tools and their exact models (versions) so that anyone can set up their \"kitchen\" correctly?\n\n## What are Project Dependencies?\n\n**Project Dependencies** are the collection of external software libraries and tools that our project's code needs to function. They are \"dependencies\" because our code *depends* on them \u2013 it can't run without them.\n\nTo manage these dependencies, we typically use a simple text file, often named `requirements.txt`. This file acts like a shopping list for your kitchen setup:\n\n*   It lists every essential \"tool\" (library) needed.\n*   It specifies the exact \"model number\" (version) for each tool.\n\nWhy specific versions? Sometimes, newer versions of a library might change how things work, potentially breaking our code (like a new mixer model having different speed settings). Listing exact versions ensures that everyone running the project uses the *exact same set* of compatible tools, making the results reproducible and avoiding unexpected errors.\n\n## Our Project's Shopping List: `requirements.txt`\n\nIn our `SC4002-Group-Assignment` project, you'll find a file named `requirements.txt`. This file lists all the external Python packages required. Let's look at a small part of it:\n\n--- File: `requirements.txt` (Snippet) ---\n```txt\n# Core ML libraries\ntensorflow==2.18.0\nkeras==3.6.0\nkeras-tuner==1.4.7\n\n# Data handling and text processing\nnltk==3.9.1\npandas==2.2.3\ndatasets==3.1.0\nnumpy==1.26.4\n\n# Utilities and support libraries\nhuggingface-hub==0.26.2\ntqdm==4.67.0\nrequests==2.32.3\n# ... many more lines ...\n```\n\n**Explanation:**\n\n*   Each line specifies one required library.\n*   The format is typically `library_name==version_number`.\n    *   `tensorflow==2.18.0`: This means our project requires the TensorFlow library, specifically version 2.18.0.\n    *   `nltk==3.9.1`: Requires the Natural Language Toolkit (NLTK) version 3.9.1.\n    *   `keras-tuner==1.4.7`: Requires the Keras Tuner library (which manages the [Keras Tuner Oracle](05_keras_tuner_oracle.md) and [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)) version 1.4.7.\n*   Lines starting with `#` are comments and are ignored.\n*   Sometimes you might see `>=` (greater than or equal to) or `<` (less than), but using `==` (exactly equal to) provides the highest level of reproducibility.\n\nThis file is the definitive list of everything you need to install *after* you have Python itself set up.\n\n## How to Install the Dependencies\n\nOkay, you have the shopping list (`requirements.txt`). How do you actually get all these tools into your kitchen (install the libraries on your computer)? Python comes with a package manager called `pip` that can read this file and install everything automatically.\n\nYou typically run this command in your terminal or command prompt, making sure you are in the main directory of the project where `requirements.txt` is located:\n\n```bash\npip install -r requirements.txt\n```\n\n**Explanation:**\n\n*   `pip`: This is the command-line tool for installing Python packages.\n*   `install`: Tells `pip` that you want to install packages.\n*   `-r requirements.txt`: This is the crucial part. The `-r` flag tells `pip` to read the package names and versions from the specified file (`requirements.txt`) and install them.\n\n**What Happens Next?**\n\nWhen you run this command, `pip` will:\n1.  Read each line in `requirements.txt`.\n2.  Connect to the Python Package Index (PyPI) \u2013 a huge online repository of Python software.\n3.  Find the specified version of each library (e.g., TensorFlow 2.18.0).\n4.  Download the library and any additional libraries *it* might depend on.\n5.  Install them onto your system in a place where your Python interpreter can find them.\n\nYou'll see a lot of messages in your terminal showing the download and installation progress. Once it finishes without errors, you have successfully set up your \"kitchen\" with all the necessary tools!\n\n## Under the Hood: How `pip` Uses the List\n\nLet's visualize the process with a simple diagram:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Terminal as Command Line\n    participant Pip as pip command\n    participant PyPI as Python Package Index (Online Repository)\n    participant System as Your Computer\n\n    User->>Terminal: Enters `pip install -r requirements.txt`\n    Terminal->>Pip: Executes the pip command with the file argument\n    Pip->>Pip: Reads `requirements.txt` line by line (e.g., \"tensorflow==2.18.0\")\n    Pip->>PyPI: Asks for \"tensorflow\" version \"2.18.0\"\n    PyPI-->>Pip: Provides download link for the package\n    Pip->>System: Downloads the package file\n    Pip->>System: Installs the package\n    Note over Pip, System: Repeats for all lines in requirements.txt\n    System-->>User: Installation complete (or shows errors)\n```\n\nThis automated process ensures that you get the exact versions needed, saving you the hassle of finding and installing each library manually.\n\n## Why This Matters for Our Project\n\nHaving the correct dependencies installed is crucial for running the `SC4002-Group-Assignment` project:\n\n*   **Running the Code:** Without TensorFlow, Keras, and Keras Tuner, you can't build, train, or tune the sentiment analysis model.\n*   **Data Processing:** Libraries like NLTK, Pandas, and Datasets are essential for loading and preparing the text data before it fits the [Model Input Shape](01_model_input_shape.md).\n*   **Reproducibility:** Using the exact versions specified in `requirements.txt` ensures that if you run the code, you should get similar results to what the original developers got (assuming the same data and code). This is vital for scientific and group work.\n*   **Avoiding Conflicts:** Different libraries sometimes depend on *other* libraries. Using specific versions helps prevent situations where Library A needs Tool X version 1.0, but Library B needs Tool X version 2.0, causing a conflict. The `requirements.txt` represents a set of versions known to work together.\n\n## Conclusion\n\nCongratulations on reaching the end of the tutorial! In this chapter, we learned about **Project Dependencies**:\n\n*   They are the **external software libraries** our project needs to run (like TensorFlow, Keras Tuner, NLTK).\n*   They are listed, along with their specific **versions**, in the **`requirements.txt`** file \u2013 our project's \"shopping list\".\n*   Specifying exact versions ensures **reproducibility** and avoids **conflicts**.\n*   We use the command **`pip install -r requirements.txt`** to automatically download and install all listed dependencies.\n*   Having the correct dependencies is the final step needed to ensure you can successfully run the project code and reproduce the experiments we've discussed throughout these chapters.\n\nFrom understanding the [Model Input Shape](01_model_input_shape.md) to exploring the [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md), defining [Training Constants](03_training_constants.md), setting up the [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md), understanding the [Keras Tuner Oracle](05_keras_tuner_oracle.md), examining [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md), and now managing [Project Dependencies](07_project_dependencies.md), you've gained a solid overview of the key concepts involved in this `SC4002-Group-Assignment` project.\n\nWe hope this tutorial has been helpful. Happy coding!\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 25: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\index.md ---\n# Tutorial: SC4002-Group-Assignment\n\nThis project focuses on automatically finding the best settings (*hyperparameters*) for a **sentiment analysis transformer model**.\nIt uses the *Keras Tuner library* to run multiple **training experiments** (trials) with different configurations, exploring a defined **hyperparameter space** (e.g., learning rates, layer sizes).\nThe goal is to discover the *optimal model setup* for better performance without manual adjustment. The project relies on specific **software dependencies** listed in `requirements.txt`.\n\n\n**Source Repository:** [https://github.com/KristiyanCholakov/SC4002-Group-Assignment](https://github.com/KristiyanCholakov/SC4002-Group-Assignment)\n\n```mermaid\nflowchart TD\n    A0[\"Hyperparameter Tuning Setup\"]\n    A1[\"Keras Tuner Oracle\"]\n    A2[\"Tuning Hyperparameter Space\"]\n    A3[\"Tuning Trial Configuration & State\"]\n    A4[\"Model Input Shape\"]\n    A5[\"Project Dependencies\"]\n    A6[\"Training Constants\"]\n    A0 -- \"Uses\" --> A1\n    A1 -- \"Consults\" --> A2\n    A1 -- \"Manages Trials\" --> A3\n    A3 -- \"Specifies Input\" --> A4\n    A0 -- \"Requires\" --> A5\n    A0 -- \"Uses Defaults From\" --> A6\n```\n\n## Chapters\n\n1. [Model Input Shape](01_model_input_shape.md)\n2. [Tuning Hyperparameter Space](02_tuning_hyperparameter_space.md)\n3. [Training Constants](03_training_constants.md)\n4. [Hyperparameter Tuning Setup](04_hyperparameter_tuning_setup.md)\n5. [Keras Tuner Oracle](05_keras_tuner_oracle.md)\n6. [Tuning Trial Configuration & State](06_tuning_trial_configuration___state.md)\n7. [Project Dependencies](07_project_dependencies.md)\n\n\n---\n\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File Index 26: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\01_web_application_interface.md ---\n# Chapter 1: Web Application Interface\n\nWelcome to the SC4021 Information Retrieval project tutorial! We're starting our journey by looking at the most visible part of the system: the **Web Application Interface**.\n\nImagine you have a powerful search engine that can sift through thousands of news articles. How do *you* actually use it? You need some way to type in your search, maybe select some filters (like date ranges or news sources), and then see the results displayed nicely. That's exactly what the Web Application Interface does!\n\n**Use Case:** Let's say you want to find recent news articles about \"artificial intelligence in healthcare\" published after January 1st, 2023. You'd use the web interface to type your search query and set the date filter. The interface then takes your request, gets the relevant articles from the backend search engine, and displays them neatly on a webpage for you to read.\n\n## What is this Interface Made Of?\n\nOur web interface is built using a tool called **Flask**.\n\n*   **Flask:** Think of Flask as a lightweight toolkit for building websites and web applications in Python. It provides the basic building blocks needed to handle incoming web requests (like someone visiting a webpage or clicking a button) and send back responses (like displaying a webpage with search results). It's like having a simple, efficient frame to build our application's \"front door\" and \"display window\".\n\nHere's how we start our Flask application in `app.py`:\n\n```python\n# File: app.py\nfrom flask import Flask, request, render_template # Import necessary parts from Flask\n\n# Initialize Flask app\napp = Flask(__name__) # Create our web application object\napp.secret_key = 'your_secret_key_here' # Needed for some features like session management\n\n# ... (rest of the setup like Solr connection)\n```\n\nThis simple code sets up the basic Flask application. `Flask(__name__)` creates the core app object we'll use to define how it handles web requests.\n\n## Handling User Actions: Web Requests\n\nWhen you interact with the web interface \u2013 type a search query, click a filter, or go to the next page of results \u2013 your web browser sends a message to our Flask application. This message is called a **web request** (specifically, an HTTP GET request in our case).\n\nFlask listens for these requests at specific URLs (web addresses). We tell Flask what code to run when a request arrives at a certain URL using a special instruction called a **route**.\n\n```python\n# File: app.py\n\n# This tells Flask: When someone visits the main page ('/'), run the 'search' function\n@app.route('/', methods=['GET'])\ndef search():\n    # ... code to handle the request and show results ...\n    print(\"Someone visited the search page!\")\n    # This function will get the user's query, talk to the search engine,\n    # and prepare the results page.\n    return render_template('search.html', ...) # Sends back the HTML page\n```\n\nThe `@app.route('/')` line is like putting up a signpost: \"If a request comes for the main page (`/`), direct it to the `search` function.\" The `search` function then does all the work.\n\n## Getting User Input\n\nHow does the `search` function know *what* the user searched for? Flask provides the `request` object, which holds all the information from the incoming web request, including any data typed into forms (like the search box) or selected from filters.\n\n```python\n# File: app.py (inside the search function)\n\n# Get the search query from the URL, e.g., /?q=ai+healthcare\n# If 'q' isn't in the URL, default to an empty string ''\nquery = request.args.get('q', '')\n\n# Get selected categories, e.g., /?category=Technology&category=Healthcare\n# If none are selected, it defaults to an empty list\nselected_categories = request.args.getlist('category')\n\nprint(f\"User searched for: {query}\")\nprint(f\"Selected categories: {selected_categories}\")\n```\n\nThis code snippet shows how we extract the search term (`q`) and any selected categories (`category`) from the request. `request.args` holds data sent in the URL, and `.get()` or `.getlist()` retrieve specific pieces of information.\n\n## Talking to the Backend (The Search Engine)\n\nThe web interface itself doesn't store or search the articles. It's just the front counter. The real work of searching happens in the **backend**, which in our project is a search engine called **Solr**.\n\nOur Flask app needs to:\n1.  Take the user's query and filters.\n2.  Translate them into a format Solr understands (we'll cover this in [Chapter 2: Search Query Construction](02_search_query_construction.md)).\n3.  Send this structured query to Solr.\n4.  Receive the results back from Solr.\n\nThink of the Flask app as a librarian at the front desk. You tell the librarian what you're looking for (your query and filters). The librarian figures out how to phrase the request for the library's catalog system (Solr), goes to the system, gets the list of books (search results), and brings them back to you.\n\n```python\n# File: app.py (Simplified inside search function)\nimport pysolr # Library to talk to Solr\n\n# Assume 'solr' is already set up to connect to our Solr engine\nsolr = pysolr.Solr('http://localhost:8983/solr/mycore', timeout=10)\n\ndef search():\n    query = request.args.get('q', '')\n    # 1. Get user input (already done)\n\n    # 2. Prepare the Solr query (simplified - details in Chapter 2)\n    solr_params = {'q': f'text:{query}' if query else 'text:*'}\n    # ... add filters based on request.args ...\n\n    # 3. Send query to Solr and get results (details in Chapter 3)\n    results = solr.search(**solr_params)\n\n    # 4. Prepare and display results (see next section)\n    # ...\n    return render_template('search.html', results=results.docs, ...)\n```\n\nThis snippet shows the core idea: construct parameters (`solr_params`) and use the `solr.search()` function to interact with the backend search engine ([Chapter 3: Search Execution (Solr)](03_search_execution__solr_.md)).\n\n## Displaying the Results: Rendering Templates\n\nOnce Flask gets the search results back from Solr, it needs to present them nicely to the user. It does this using **HTML templates**.\n\nThink of a template like a fill-in-the-blanks document (`search.html` in our case). It defines the structure of the webpage (header, search bar, results area, footer), but leaves placeholders for the actual data (like the search query, the list of results, page numbers).\n\nFlask's `render_template` function takes the template file and fills in the blanks with the actual data we provide.\n\n```python\n# File: app.py (inside the search function)\nfrom flask import render_template\n\ndef search():\n    # ... (get query, get results from Solr) ...\n    query = request.args.get('q', '')\n    results_from_solr = solr.search(...) # Get results\n\n    # Pass data to the HTML template\n    return render_template(\n        'search.html', # The template file to use\n        query=query,   # Fill in the 'query' placeholder\n        results=results_from_solr.docs, # Fill in the 'results' placeholder\n        total_results=results_from_solr.hits # Fill in total count\n        # ... other data like page number, filters, etc. ...\n    )\n```\n\nThis code tells Flask: \"Take the `search.html` template, fill in the `query` placeholder with the user's search term, fill in the `results` placeholder with the documents we got from Solr, and so on.\" The final filled-in HTML is then sent back to the user's browser to be displayed. We also generate charts and graphs, which is covered in [Chapter 4: Data Visualization](04_data_visualization.md).\n\n## How it Works: Step-by-Step\n\nLet's trace the journey of our example use case: searching for \"AI in healthcare\".\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Browser\n    participant WA as Web App (Flask - app.py)\n    participant SE as Search Engine (Solr)\n\n    User->>Browser: Enters \"AI in healthcare\", clicks Search\n    Browser->>WA: Sends GET request to /?q=AI+in+healthcare\n    WA->>WA: Receives request via @app.route('/')\n    WA->>WA: Extracts query=\"AI in healthcare\" using request.args.get()\n    WA->>WA: Builds Solr query parameters (See Ch 2)\n    WA->>SE: Sends search request to Solr (See Ch 3)\n    SE->>SE: Finds matching articles\n    SE->>WA: Returns list of articles\n    WA->>WA: Receives results from Solr\n    WA->>WA: Prepares data for the webpage (results, query, page info)\n    WA->>Browser: Sends HTML page generated by render_template('search.html', ...)\n    Browser->>User: Displays search results page\n```\n\n1.  **User Action:** You type \"AI in healthcare\" into the search box on the webpage and click \"Search\".\n2.  **Browser Request:** Your web browser creates a URL like `http://your-app-address/?q=AI+in+healthcare` and sends a GET request to our Flask application.\n3.  **Flask Routing:** Flask sees the request for the main page (`/`) and directs it to the `search()` function in `app.py`.\n4.  **Input Processing:** Inside `search()`, the code `request.args.get('q', '')` retrieves \"AI in healthcare\". Other code retrieves filter values if any were selected.\n5.  **Query Building:** Helper functions (like `build_solr_params`) construct a query that Solr can understand based on the input. (More in [Chapter 2](02_search_query_construction.md)).\n6.  **Backend Interaction:** The `solr.search(...)` command sends the constructed query to the Solr search engine. (More in [Chapter 3](03_search_execution__solr_.md)).\n7.  **Solr Processing:** Solr searches its index for articles matching the query.\n8.  **Results Return:** Solr sends the found articles (documents) back to the Flask application.\n9.  **Response Preparation:** The `search()` function receives the results. It might also call functions to generate visualizations (like word clouds - see [Chapter 4](04_data_visualization.md)).\n10. **Template Rendering:** The `render_template('search.html', ...)` function is called. Flask takes the `search.html` file and inserts the query, the list of results, pagination info, etc., into the placeholders.\n11. **Browser Display:** Flask sends the complete HTML page back to your browser, which then displays the formatted search results page.\n\n## Conclusion\n\nThe Web Application Interface, powered by Flask, acts as the crucial intermediary between you (the user) and the powerful backend search engine (Solr). It handles your requests, translates them for the backend, and presents the results in a user-friendly format using HTML templates. It's the dashboard and control panel for our information retrieval system.\n\nWe've seen how it receives input and displays output. But how exactly is that user input turned into something the search engine understands? That's what we'll dive into next.\n\n**Next:** [Chapter 2: Search Query Construction](02_search_query_construction.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 27: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\02_search_query_construction.md ---\n# Chapter 2: Search Query Construction\n\nIn [Chapter 1: Web Application Interface](01_web_application_interface.md), we saw how users interact with our application through a web page built with Flask. Users type search terms, select filters like dates or categories, and click \"Search\". But how does our application take that plain language request and turn it into something the powerful backend search engine, Solr, can actually understand?\n\nThat's where **Search Query Construction** comes in! Think of it as a specialized translator.\n\n**Use Case:** Remember our example from Chapter 1? A user wants to find articles about \"artificial intelligence in healthcare\" published *after* January 1st, 2023, and perhaps only in the \"Technology\" category. The user simply types the words and selects options from dropdowns or date pickers. Our Search Query Construction component needs to translate this simple request into a precise set of instructions for Solr.\n\n## Why Do We Need to Translate?\n\nSearch engines like Solr are incredibly powerful but also very specific about how they receive instructions. You can't just tell Solr \"find me recent tech articles about AI in healthcare\". It needs a structured query, like a command written in its own technical language.\n\nImagine ordering a coffee:\n*   **Your Request (Plain English):** \"I'd like a medium latte with oat milk, please.\"\n*   **Barista's Internal Shorthand (Structured Query):** \"MED OAT LATTE\"\n\nOur Search Query Construction component acts like the barista, translating your everyday language request into the specific shorthand Solr understands.\n\n## Breaking Down the Translation Process\n\nThis translation happens in a few key steps, mostly handled by the `build_solr_params` function in our `app.py` file:\n\n1.  **Get the Raw Ingredients:** First, we grab all the information the user provided through the web interface \u2013 the search terms, selected dates, categories, polarities, etc. We saw in Chapter 1 how Flask uses `request.args` to get this information.\n\n2.  **Process the Search Terms (Lemmatization):** Users might type \"running\", \"runs\", or \"ran\". To the search engine, these should ideally all match articles containing the core concept \"run\". This process of reducing words to their base or dictionary form is called **lemmatization**. We use a Natural Language Processing (NLP) tool called `spaCy` to help with this.\n\n3.  **Handle the Filters:** The user might have selected specific date ranges, categories (like \"Technology\"), or sentiment polarities (like \"Positive\"). These selections need to be converted into **filter queries** (often abbreviated as `fq` in Solr). Filters narrow down the search results *after* the main keyword search.\n\n4.  **Build the Final Solr Query:** Finally, we assemble all the processed pieces \u2013 the lemmatized search terms and the filter queries \u2013 into a structured format (a Python dictionary in our case) that the `pysolr` library can use to talk to Solr.\n\n## Step-by-Step: Building the Query\n\nLet's see how these steps apply to our use case: \"artificial intelligence in healthcare\", after 2023-01-01, category \"Technology\".\n\n**1. Get Raw Input:**\nOur Flask app receives the request. Using `request.args`, it extracts:\n*   `q`: \"artificial intelligence in healthcare\"\n*   `date_from`: \"2023-01-01\"\n*   `date_to`: (empty, maybe defaults to \"now\")\n*   `category`: [\"Technology\"]\n\n**2. Process Search Terms (Lemmatization):**\nThe `build_solr_params` function takes the query \"artificial intelligence in healthcare\". It uses `spaCy` to break it down and find the base form (lemma) of each important word.\n*   \"artificial\" -> \"artificial\"\n*   \"intelligence\" -> \"intelligence\"\n*   \"in\" -> (often ignored as a \"stop word\")\n*   \"healthcare\" -> \"healthcare\"\nThe processed terms might become something like: `artificial OR intelligence OR healthcare`. (We use `OR` to find documents containing *any* of these key terms, and add `~` for fuzzy matching).\n\n```python\n# File: app.py (inside build_solr_params)\nimport spacy\nimport re\n\nnlp = spacy.load('en_core_web_sm') # Load the English language model\n\nquery = request.args.get('q', '').strip() # Get query: \"artificial intelligence in healthcare\"\nwords = []\nif query:\n    # Use spaCy to process the query\n    lemmatized = ' '.join([token.lemma_ for token in nlp(query)])\n    # Extract individual words (simplified)\n    words = re.findall(r'\\b\\w+\\b', lemmatized)\n    # Result: words = ['artificial', 'intelligence', 'healthcare'] (potentially)\n\n# Construct the main query part ('q') for Solr\n# If words exist, join them with OR and add '~' for fuzzy matching\nsolr_q = f'text:*' if not query else f'text:({\" OR \".join(words)}~)'\n# Result: solr_q = 'text:(artificial OR intelligence OR healthcare)~'\n```\nThis code snippet shows how we get the user's query string, process it with `spaCy` to get lemmatized words, and then format it into the main part of the Solr query (`q`). We search within the `text` field of our documents.\n\n**3. Handle Filters:**\nThe function then looks at the filter selections:\n*   **Date:** `date_from=\"2023-01-01\"` and `date_to=*` (meaning up to the latest) gets translated into a Solr date range query: `date:[2023-01-01T00:00:00Z TO *]`\n*   **Category:** `category=[\"Technology\"]` gets translated into: `category:(\"Technology\")` (If multiple were selected, it would be `category:(\"Tech\" OR \"Finance\")`).\n\nWe use helper functions like `get_date_param` and `get_list_param` to safely retrieve and format these values.\n\n```python\n# File: app.py (inside build_solr_params)\n\n# Example of getting and formatting the date filter\ndate_from_str = get_date_param(\"date_from\") # Gets \"2023-01-01\", converts to Solr format\ndate_to_str = get_date_param(\"date_to\")     # Gets \"\", defaults to \"*\" (now)\ndate_filter = f'date:[{date_from_str} TO {date_to_str}]'\n# Result: date_filter = 'date:[2023-01-01T00:00:00Z TO *]'\n\n# Example of getting and formatting the category filter\nselected_categories = get_list_param(\"category\") # Gets [\"Technology\"]\ncategory_filter = f'category:({\" OR \".join(selected_categories)})'\n# Result: category_filter = 'category:(\"Technology\")'\n\n# Other filters (polarity, source, etc.) are handled similarly...\n```\nThese snippets show how specific filter types (dates, lists of categories) are retrieved from the request and formatted into strings that Solr understands as filter queries (`fq`).\n\n**4. Build the Final Solr Query Structure:**\nAll these pieces are assembled into a Python dictionary that `pysolr` uses. The main keywords go into the `q` parameter, and all the filters go into a list under the `fq` (filter query) parameter. We also add parameters for sorting and pagination (controlling which page of results to show).\n\n```python\n# File: app.py (inside build_solr_params function)\n\ndef build_solr_params(request_args):\n    # ... (code from steps 1-3 to get solr_q, date_filter, category_filter, etc.) ...\n\n    # Collect all filter strings into a list\n    filter_queries = [\n        date_filter,\n        category_filter,\n        # ... other filters like polarity, source ...\n        f'polarity:({\" OR \".join(get_list_param(\"polarity\"))})', # Example\n    ]\n    # Remove any empty filters that might result from default values\n    filter_queries = [fq for fq in filter_queries if fq and ':' in fq and not fq.endswith(':(*)') and not fq.endswith(':[* TO *]')]\n\n\n    # Get pagination info\n    start_index = (int(request_args.get('page', 1)) - 1) * DEFAULT_RESULTS_PER_PAGE\n    rows_per_page = DEFAULT_RESULTS_PER_PAGE\n\n    # Assemble the final dictionary for pysolr\n    params = {\n        'q': solr_q,             # The main keyword query\n        'fq': filter_queries,    # The list of filter queries\n        'start': start_index,    # Which result number to start at\n        'rows': rows_per_page,   # How many results per page\n        'sort': f'{get_param('sort_field', 'id_num')} {get_param('sort_order', 'asc')}' # Sorting\n    }\n    return params\n```\nThis final snippet shows the assembly. The processed `solr_q` and the list of `filter_queries` are put into a dictionary, along with pagination (`start`, `rows`) and sorting information. This `params` dictionary is exactly what `pysolr` needs to send the request to Solr.\n\n## How It Works: Under the Hood\n\nLet's visualize the flow when the `build_solr_params` function is called:\n\n```mermaid\nsequenceDiagram\n    participant WA as Web App (Flask - app.py)\n    participant RA as Request Args (User Input)\n    participant QC as Query Constructor (build_solr_params)\n    participant NLP as spaCy (Lemmatizer)\n    participant SQ as Solr Query (Dictionary)\n\n    WA->>QC: Call build_solr_params(request.args)\n    QC->>RA: Get 'q', 'date_from', 'category', etc.\n    Note over QC,NLP: Process 'q' (search terms)\n    QC->>NLP: Send \"artificial intelligence in healthcare\"\n    NLP->>QC: Return lemmatized words ['artificial', 'intelligence', 'healthcare']\n    QC->>QC: Format main query: 'text:(artificial OR ...)'\n    Note over QC: Process Filters\n    QC->>RA: Get 'date_from' -> \"2023-01-01\"\n    QC->>QC: Format date filter: 'date:[2023-01-01T... TO *]'\n    QC->>RA: Get 'category' -> [\"Technology\"]\n    QC->>QC: Format category filter: 'category:(\"Technology\")'\n    Note over QC: Assemble final parameters\n    QC->>SQ: Create dictionary {'q': ..., 'fq': [...], 'start': ..., 'rows': ...}\n    QC->>WA: Return Solr Query dictionary\n```\n\n1.  The Flask `search` function gets the user's input (`request.args`).\n2.  It calls `build_solr_params`, passing this input.\n3.  `build_solr_params` reads the search terms (`q`).\n4.  It sends the terms to `spaCy` for lemmatization.\n5.  It formats the lemmatized terms into the main Solr query string (`q`).\n6.  It reads filter values (date, category, etc.) from the input.\n7.  It formats each filter into a Solr filter query string (`fq`).\n8.  It bundles the main query (`q`), the list of filter queries (`fq`), and other parameters like sorting and pagination into a Python dictionary.\n9.  This dictionary is returned to the Flask `search` function, ready to be sent to Solr.\n\n## Conclusion\n\nSearch Query Construction is the vital translator between the user's simple request and the search engine's complex requirements. By processing keywords using lemmatization and converting user selections into specific filter queries, it builds a structured set of instructions (the `params` dictionary) that Solr can execute efficiently. This ensures that users find the relevant information they're looking for, even when they express their needs in slightly different ways or apply various filters.\n\nNow that we have this perfectly structured query, how do we actually send it to the Solr engine and get the results back? That's the focus of our next chapter.\n\n**Next:** [Chapter 3: Search Execution (Solr)](03_search_execution__solr_.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 28: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\03_search_execution__solr_.md ---\n# Chapter 3: Search Execution (Solr)\n\nWelcome back! In [Chapter 2: Search Query Construction](02_search_query_construction.md), we learned how to translate a user's search request (like \"AI in healthcare\" after a certain date) into a precise set of instructions, a structured query, that our search engine, Apache Solr, can understand.\n\nBut having the instructions written down isn't enough. We need someone to *take* those instructions, go to the library's vast collection (the Solr index), find the matching items, and bring them back. That's exactly what **Search Execution** does!\n\n**Use Case:** Imagine our structured query from Chapter 2 is like a perfectly filled-out library request form for \"Books about 'AI' or 'healthcare', published after Jan 1, 2023, in the 'Technology' section\". Search Execution is the librarian who takes this form, walks into the library stacks (our Solr index), finds all the books matching the criteria, and brings them back to the front desk (our Flask application).\n\n## Connecting to the Search Engine (Solr)\n\nBefore our application can send any requests to Solr, it needs to know *where* Solr is and how to talk to it. We use a special Python library called `pysolr` that acts as our translator and messenger.\n\nThink of `pysolr` as the dedicated phone line connecting our Flask web application directly to the Solr search engine. We set up this connection when our application starts.\n\n```python\n# File: app.py (Near the top)\nimport pysolr # Import the library to talk to Solr\n\n# Solr setup: Tell pysolr where our Solr engine lives\n# 'http://localhost:8983/solr/mycore' is the address of our specific Solr collection\n# 'timeout=10' means we won't wait more than 10 seconds for a response\nsolr = pysolr.Solr('http://localhost:8983/solr/mycore', timeout=10)\n\nprint(\"Connected to Solr!\")\n```\n\nThis code creates a `solr` object. From now on, we can use this `solr` object to send commands (like search queries) to our Solr engine. It's like having the librarian's direct phone number saved in our contacts.\n\n## Sending the Query and Getting Results\n\nIn Chapter 2, we built a Python dictionary called `params` containing the structured query (like `{'q': 'text:(artificial OR intelligence OR healthcare)~', 'fq': ['date:[2023-01-01T00:00:00Z TO *]', 'category:(\"Technology\")'], 'rows': 5, 'start': 0}`).\n\nNow, we use our `solr` object (our connection to Solr) and its `search()` method to send these parameters.\n\n```python\n# File: app.py (Inside the 'search' function)\n\n# Assume 'params' is the dictionary built by build_solr_params (from Chapter 2)\n# params = {'q': 'text:(...)~', 'fq': [...], 'rows': 5, 'start': 0, ...}\n\nprint(f\"Sending search query to Solr: {params['q']}\")\n\n# Send the search parameters to Solr and wait for the results\n# The **params syntax unpacks the dictionary into arguments for the search function\nresults = solr.search(q=params['q'], fq=params['filters'], sort=params['sort'], start=params['start'], rows=params['rows'])\n\n# 'results' now holds the response from Solr\nprint(f\"Received {results.hits} matching documents from Solr.\")\n```\n\nIt's as simple as calling `solr.search()`! We pass the main query (`q`), the filters (`fq`, which stands for \"filter query\"), sorting instructions (`sort`), and pagination info (`start`, `rows`). The `pysolr` library takes care of formatting this into a web request that Solr understands, sending it, and parsing the response Solr sends back.\n\nThe `results` object returned by `solr.search()` contains everything Solr found:\n*   `results.docs`: A list of the actual documents (news articles, in our case) that matched the query for the requested page. Each document is usually a dictionary-like object containing its fields (like title, content, date, category).\n*   `results.hits`: The total number of documents that matched the query across *all* pages.\n\n## Getting All Results vs. Getting One Page\n\nSometimes, we need *all* the matching documents. For example, if we want to create a visualization like a word cloud ([Chapter 4: Data Visualization](04_data_visualization.md)) based on *every* article about \"AI in healthcare\", we need the full dataset.\n\nOther times, like when displaying results on the web interface ([Chapter 1: Web Application Interface](01_web_application_interface.md)), we only want a small number of results per page (e.g., 5 or 10). This is called **pagination**.\n\nOur application handles both scenarios:\n\n**1. Getting ALL Results (for Analysis/Visualization):**\nTo get all results, we send a query but set the `rows` parameter to a very large number, ensuring we get everything back in one go. This happens when the search filters change, so we can regenerate visualizations based on the complete matching dataset.\n\n```python\n# File: app.py (Inside the 'search' function - simplified logic)\n\n# Check if search parameters changed since last time\nif search_filters_changed:\n    print(\"Filters changed, fetching ALL results for analysis...\")\n    # Send query to get all results (e.g., up to 12372 rows, our total articles approx)\n    # We only pass 'q' and 'fq' because we don't need pagination here\n    all_results = solr.search(q=params['q'], fq=params['filters'], sort=params['sort'], rows=12372).docs\n    print(f\"Fetched {len(all_results)} total documents for analysis.\")\n    # Store these results (e.g., in session) or pass them to visualization functions\n    # generate_visualizations(all_results) # Covered in Chapter 4\n```\n\n**2. Getting Paginated Results (for Display):**\nTo display results page by page, we use the `start` and `rows` parameters that were calculated in `build_solr_params` (Chapter 2) based on which page the user requested.\n\n```python\n# File: app.py (Inside the 'search' function)\n\n# Fetch ONLY the results for the current page\nprint(f\"Fetching page {current_page} (start={params['start']}, rows={params['rows']})\")\npaginated_results = solr.search(\n    q=params['q'],\n    fq=params['filters'],\n    sort=params['sort'],\n    start=params['start'],\n    rows=params['rows']\n)\n\n# Now use paginated_results.docs to display on the web page\n# And use paginated_results.hits to show the total count and calculate page numbers\n\nprint(f\"Displaying {len(paginated_results.docs)} results out of {paginated_results.hits} total.\")\n```\n\nThis way, we don't overload the user's browser (or our application) by fetching thousands of results when they only need to see a few at a time.\n\n## How it Works: Under the Hood\n\nWhat actually happens when we call `solr.search()`?\n\n1.  **Your App Calls `pysolr`:** Your Flask code (in `app.py`) calls the `search()` method on the `solr` object we created.\n2.  **`pysolr` Builds an HTTP Request:** The `pysolr` library takes the parameters (`q`, `fq`, `rows`, etc.) and constructs a standard web request (an HTTP GET request) URL. It looks something like this (simplified):\n    `http://localhost:8983/solr/mycore/select?q=text:(...)~&fq=date:[...]&fq=category:(...)&rows=5&start=0&sort=...`\n3.  **Request Sent to Solr:** `pysolr` sends this HTTP request over the network to the Solr server running at `http://localhost:8983`.\n4.  **Solr Processes the Request:** The Solr engine receives the request, parses the parameters, searches its internal index using the powerful Lucene library, finds the matching documents, and gathers the requested fields.\n5.  **Solr Sends Back an HTTP Response:** Solr packages the results (the documents found, the total hit count, etc.) into a structured format, usually JSON, and sends it back as an HTTP response.\n6.  **`pysolr` Parses the Response:** The `pysolr` library receives this JSON response, parses it, and turns it into the convenient `results` object (with `.docs` and `.hits`) that your Flask application can easily use.\n\nLet's visualize this interaction:\n\n```mermaid\nsequenceDiagram\n    participant WA as Web App (app.py)\n    participant PY as pysolr Library\n    participant SE as Solr Engine\n\n    WA->>PY: Call solr.search(q=..., fq=..., rows=5, ...)\n    PY->>PY: Construct Solr HTTP Request URL\n    PY->>SE: Send HTTP GET Request (e.g., /select?q=...)\n    SE->>SE: Parse Request, Search Index (using Lucene)\n    SE->>PY: Send HTTP Response (JSON containing results)\n    PY->>PY: Parse JSON Response\n    PY->>WA: Return 'results' object (with .docs, .hits)\n    WA->>WA: Use results.docs for display, results.hits for total count\n```\n\nThis diagram shows our `app.py` code using the `pysolr` library as an intermediary to communicate with the external `Solr Engine`. `pysolr` handles the technical details of the web communication.\n\n## Conclusion\n\nSearch Execution is the bridge between wanting information and getting it. Using the `pysolr` library, our application establishes a connection to the Solr search engine. It takes the structured query crafted in the previous step ([Chapter 2: Search Query Construction](02_search_query_construction.md)), sends it to Solr using `solr.search()`, and receives the matching documents back. We learned how to fetch both paginated results for efficient display and all results when needed for deeper analysis or visualization.\n\nWe now have the raw results back from Solr! But just getting a list of documents isn't always the most insightful way to understand the information. How can we present this data visually, perhaps showing trends or distributions?\n\n**Next:** [Chapter 4: Data Visualization](04_data_visualization.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 29: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\04_data_visualization.md ---\n# Chapter 4: Data Visualization\n\nIn [Chapter 3: Search Execution (Solr)](03_search_execution__solr_.md), we saw how our application takes a structured query, sends it to the Solr search engine, and gets back a list of matching news articles. We learned how to get just one page of results for display, and also how to fetch *all* matching results when needed.\n\nBut what do we do with potentially hundreds or thousands of results? Just reading through them all is often impractical. How can we quickly understand the main themes, trends, or feelings hidden within that large set of information? That's where **Data Visualization** comes in!\n\n**Use Case:** Imagine you searched for \"AI impact on jobs\". You get back 500 articles. Instead of reading them one by one, wouldn't it be great to see:\n*   A **word cloud** showing the most frequently mentioned terms? (Maybe \"automation\", \"skills\", \"training\" pop out).\n*   A **pie chart** showing the overall sentiment? (Are most articles positive, negative, or neutral about the impact?).\n*   A **trend line** showing how sentiment changed over time? (Was discussion more optimistic last year?).\n\nData Visualization helps us create these visual summaries, turning raw data into easily understandable pictures.\n\n## What is Data Visualization?\n\nThink of data visualization as creating charts, graphs, and other pictures from data. Instead of looking at a spreadsheet full of numbers or a long list of text documents, you look at a visual representation.\n\nWhy is this helpful?\n*   **Spot Patterns Quickly:** Our brains are great at finding patterns in images. A rising line on a chart is much easier to spot than scanning through hundreds of numbers.\n*   **Understand the Big Picture:** Visuals can summarize complex information concisely. A pie chart instantly shows proportions.\n*   **Communicate Insights:** It's often easier to show someone a chart than to explain the underlying data verbally.\n\nIn our project, the Data Visualization component takes the *full set* of search results for your current query and filters, and generates various plots.\n\n## Tools of the Trade: Our Drawing Kit\n\nTo draw these plots, we use some powerful Python libraries:\n\n*   **Matplotlib:** This is like the fundamental drawing canvas and basic tools (pens, colors) for creating static plots like line charts, bar charts, and pie charts.\n*   **Seaborn:** Built on top of Matplotlib, Seaborn makes creating more complex and statistically informative plots (like heatmaps and distribution plots) easier and prettier.\n*   **WordCloud:** A specific library designed just for creating those cool word clouds where word size indicates frequency.\n*   **(Optional) Plotly:** While the provided code uses Matplotlib/Seaborn primarily, Plotly is another popular library often used for creating interactive charts (where you can hover over points for details, zoom in, etc.). Our project focuses mainly on static images generated by Matplotlib/Seaborn saved for display.\n\nThese libraries do the heavy lifting of figuring out how to arrange the data points, draw the axes, add labels, and color everything correctly. Our code just needs to prepare the data and tell the libraries what kind of plot to make.\n\nAll the code that *uses* these libraries to create our specific plots lives in the `plotVisuals.py` file.\n\n## Types of Plots We Generate\n\nOur `plotVisuals.py` file contains functions to create several types of visualizations from the search results:\n\n1.  **Word Cloud (`wordCloud`, `word_cloud_polarity`):** Shows the most frequent words in the text of the results. Bigger words mean they appeared more often. We generate an overall word cloud and separate ones for positive/negative sentiment text.\n2.  **Polarity Distribution (`polarityDistribution`):** A pie chart showing the percentage of search results classified as having positive, negative, or neutral sentiment.\n3.  **Sentiment Distribution by Industry (`sentiment_distribution_by_industry`):** A bar chart showing the count of positive, negative, and neutral articles for each industry category found in the results.\n4.  **Industry Sentiment Heatmap (`industry_sentiment_heatmap`):** A grid where rows are industries, columns are sentiments (positive, neutral, negative), and the color/number in each cell shows how many articles fall into that combination. Helps spot which industries have particularly strong sentiment leanings.\n5.  **AI Sentiment Trends Across Sectors (`ai_sentiment_trends_across_sectors`):** A line chart showing how the *volume* of discussion (number of articles) for different industries changed over time within the search results.\n\n## How We Create and Show the Plots\n\nGenerating these plots involves teamwork between our main application file (`app.py`) and the plotting functions (`plotVisuals.py`).\n\n**Step 1: Fetching All Data (in `app.py`)**\nAs we saw in [Chapter 3: Search Execution (Solr)](03_search_execution__solr_.md), when you change your search query or filters, `app.py` detects this change. It then performs a special Solr search to get *all* matching documents (not just the first page).\n\n```python\n# File: app.py (inside search function)\n# ... (build_solr_params creates 'params') ...\n\n# Check if the user changed the search query or filters\nsearch_params = { # Dictionary holding current filters }\nif session.get('search_params') != search_params:\n    session['search_params'] = search_params # Update saved filters\n    print(\"Filters changed, fetching ALL results for analysis...\")\n\n    # Get ALL results (e.g., up to 12372 rows)\n    all_results = solr.search(q=params['q'], fq=params['filters'], sort=params['sort'], rows=12372).docs\n    print(f\"Fetched {len(all_results)} total documents for analysis.\")\n\n    # Pass all results to the visualization generator\n    generate_visualizations(all_results)\n```\nThis code block checks if the `search_params` (like the query text, date range, categories) are different from the last search. If they are, it calls `solr.search` with a large `rows` value to get `all_results` and then calls `generate_visualizations`.\n\n**Step 2: Calling Plotting Functions (in `app.py`)**\nThe `generate_visualizations` function in `app.py` is simple. It just calls each specific plotting function defined in `plotVisuals.py`, passing the `all_results` data to them.\n\n```python\n# File: app.py\nfrom plotVisuals import ( # Import specific plot functions\n    wordCloud, polarityDistribution, sentiment_distribution_by_industry,\n    # ... other plot functions ...\n)\n\ndef generate_visualizations(results):\n    \"\"\"Generate various sentiment-related visualizations.\"\"\"\n    print(f\"Generating visualizations from {len(results)} results...\")\n    # List of functions to call from plotVisuals.py\n    visual_functions = [\n        wordCloud, polarityDistribution, sentiment_distribution_by_industry,\n        # ... other plot functions ...\n    ]\n    # Call each function with the results data\n    for func in visual_functions:\n        # Note: The actual plots are saved/handled within the plotVisuals functions\n        func(results)\n    print(\"Finished generating visualizations.\")\n```\nThis function acts as a coordinator, ensuring all the different plots are created using the same set of `results` data. It imports the necessary functions from `plotVisuals.py` and calls them one by one.\n\n**Step 3: Drawing the Plot (in `plotVisuals.py`)**\nEach function in `plotVisuals.py` (like `wordCloud` or `polarityDistribution`) takes the `results` data, processes it as needed (e.g., counts words, calculates percentages), and uses a library like Matplotlib or WordCloud to draw the actual plot.\n\nLet's look at a simplified `polarityDistribution`:\n\n```python\n# File: plotVisuals.py (Simplified Example)\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\nimport base64\n\ndef polarityDistribution(results):\n    # 1. Count sentiments from the results list\n    counts = {'positive': 0, 'neutral': 0, 'negative': 0}\n    for post in results:\n        sentiment = post.get('polarity', [None])[0] # Get sentiment\n        if sentiment in counts:\n            counts[sentiment] += 1\n\n    # Filter out sentiments with zero count\n    labels = [k for k, v in counts.items() if v > 0]\n    sizes = [v for v in counts.values() if v > 0]\n    colors = {'positive': 'green', 'neutral': 'gray', 'negative': 'red'}\n    plot_colors = [colors[label] for label in labels]\n\n    if not sizes: # Handle case with no data\n        print(\"No polarity data to plot.\")\n        # (Code to create/save a placeholder image would go here)\n        return None\n\n    # 2. Use Matplotlib to create a pie chart\n    fig, ax = plt.subplots()\n    ax.pie(sizes, labels=labels, colors=plot_colors, autopct='%1.1f%%', startangle=90)\n    ax.axis('equal') # Equal aspect ratio ensures a circle.\n    plt.title(\"Sentiment Polarity Distribution\")\n\n    # 3. Save the plot to memory (not a file)\n    img = BytesIO() # Create an in-memory file\n    plt.savefig(img, format='png', bbox_inches='tight') # Save the plot as PNG\n    plt.close(fig) # Close the plot figure to free memory\n    img.seek(0) # Go to the beginning of the in-memory file\n\n    # 4. Encode the image data as a Base64 string\n    plot_url = base64.b64encode(img.getvalue()).decode('utf8')\n\n    # 5. Store/Return the Base64 string (or save it for the template)\n    # In our app, the Flask template directly accesses saved plot URLs\n    # For simplicity here, let's imagine it returns the string:\n    # return plot_url\n    # (Actual project saves plot to be accessed by template later)\n    print(f\"Generated polarity distribution plot.\")\n```\nThis function:\n1.  Counts how many results have 'positive', 'negative', or 'neutral' polarity.\n2.  Uses `plt.pie()` from Matplotlib to draw the pie chart based on these counts.\n3.  Instead of saving to a file on disk, it saves the image data into `BytesIO`, which is like a temporary file in the computer's memory.\n4.  Converts the raw image data into a **Base64 string**. This is a way to represent binary data (like an image) as plain text, which can be easily embedded in an HTML webpage.\n5.  (In the full app) This Base64 string is stored so the HTML template can use it.\n\n**Step 4: Displaying the Plot (in `search.html` Template)**\nFinally, the Flask application uses `render_template` (as seen in [Chapter 1: Web Application Interface](01_web_application_interface.md)) to generate the `search.html` page. Inside the HTML template, there are `<img>` tags. Instead of a normal file path, the `src` attribute of these tags is set to the Base64 string generated in the previous step. The browser knows how to decode this string and display the image.\n\n```html\n<!-- File: templates/search.html (Simplified Snippet) -->\n\n<h2>Visualizations</h2>\n\n<!-- Display the Word Cloud -->\n<div>\n  <h3>Word Cloud</h3>\n  <!-- The src uses the Base64 data generated by plotVisuals.py -->\n  <img src=\"data:image/png;base64,{{ plot_url_wordcloud }}\" alt=\"Word Cloud\">\n</div>\n\n<!-- Display the Polarity Distribution -->\n<div>\n  <h3>Polarity Distribution</h3>\n  <img src=\"data:image/png;base64,{{ plot_url_polarity }}\" alt=\"Polarity Distribution\">\n</div>\n\n<!-- Other plots displayed similarly -->\n```\nThe `{{ plot_url_wordcloud }}` and `{{ plot_url_polarity }}` are placeholders that Flask fills in with the actual Base64 strings generated by our `plotVisuals.py` functions. The `data:image/png;base64,` part tells the browser that the following text is Base64 encoded PNG image data.\n\n## How it Works: Generating a Plot (Simplified Flow)\n\nLet's trace the journey of creating and displaying one plot, like the Polarity Distribution:\n\n```mermaid\nsequenceDiagram\n    participant WA as Web App (Flask app.py)\n    participant SE as Search Engine (Solr)\n    participant PV as Plotting Func (plotVisuals.polarityDistribution)\n    participant MPL as Matplotlib Library\n    participant Browser as User Browser\n\n    Note over WA: User changes filters, triggers search.\n    WA->>SE: Fetch ALL results for query/filters\n    SE->>WA: Return all_results (list of dicts)\n    WA->>PV: Call polarityDistribution(all_results)\n    PV->>PV: Count positive/neutral/negative items\n    PV->>MPL: Use plt.pie(counts) to draw chart\n    MPL->>PV: Generate plot image data\n    PV->>PV: Save image data to BytesIO (memory)\n    PV->>PV: Encode image data to Base64 string (plot_url)\n    PV->>WA: (Stores plot_url for template)\n    Note over WA: Prepare other data for page...\n    WA->>Browser: Send HTML ('search.html' filled with data, including plot_url)\n    Browser->>Browser: Decode Base64 string in <img> tag\n    Browser->>Browser: Display Polarity Pie Chart\n```\n\n1.  **Filters Change:** The user changes a filter (e.g., date range) and clicks Search.\n2.  **Fetch All Data:** `app.py` detects the change and asks Solr for *all* matching results.\n3.  **Call Plot Function:** `app.py` calls `polarityDistribution(all_results)`.\n4.  **Process & Draw:** `polarityDistribution` counts sentiments in the `all_results` list and tells Matplotlib (`plt.pie`) to create a pie chart.\n5.  **Encode:** Matplotlib generates the image data, which `polarityDistribution` saves to memory (`BytesIO`) and then encodes into a Base64 string (`plot_url`).\n6.  **Store & Render:** `app.py` stores this `plot_url` (along with URLs for other plots). When rendering `search.html`, it passes these URLs to the template.\n7.  **Display:** The user's browser receives the HTML, finds the `<img>` tag with the Base64 source, decodes it, and displays the pie chart image.\n\n## Conclusion\n\nData Visualization is a powerful technique for making sense of large amounts of search results. By using Python libraries like Matplotlib, Seaborn, and WordCloud within our `plotVisuals.py` file, we can generate insightful charts and graphs like word clouds and sentiment distributions.\n\nWe learned how the main application (`app.py`) triggers the generation of these plots when search filters change, how the plotting functions create the visuals and encode them as Base64 strings, and how these strings are embedded into the final HTML page (`search.html`) to be displayed in the user's browser. This allows users to quickly grasp key patterns and trends within the retrieved information, going beyond a simple list of documents.\n\nSo far, we've seen the user interface, how queries are built, how they are executed, and how results can be visualized. But where does the data that Solr searches actually come from? How are news articles collected, processed, and prepared *before* they even get into the search engine? That's the focus of the next few chapters, starting with the overall pipeline.\n\n**Next:** [Chapter 5: Data Processing Pipeline](05_data_processing_pipeline.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 30: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\05_data_processing_pipeline.md ---\n# Chapter 5: Data Processing Pipeline\n\nIn the previous chapter, [Chapter 4: Data Visualization](04_data_visualization.md), we saw how to create charts and graphs from the search results to understand them better. But where does the data that we search and visualize actually *come from*? How does raw information from the web, like Reddit posts, get transformed into the neat, searchable format our application uses?\n\nThat's the job of the **Data Processing Pipeline**! Think of it as the factory floor or an assembly line for our information.\n\n**Use Case:** Imagine we've just collected thousands of raw posts and comments from various Reddit subreddits discussing \"AI replacing jobs\" (`crawlReddit.py`). This raw data is messy! It might contain irrelevant posts, duplicates, spam, and it lacks useful tags like sentiment or topic category. Our goal is to take this raw pile of text and turn it into a clean, structured dataset where each item is relevant, tagged with its sentiment (positive/negative/neutral) and potential industry category (like \"Technology\" or \"Finance\"), and formatted perfectly as a JSON object ready to be put into our search engine ([Chapter 7: Data Indexing](07_data_indexing.md)).\n\n## What is a Data Processing Pipeline?\n\nIt's a series of automated steps (usually scripts) designed to take raw data, clean it up, transform it, add extra information, and get it ready for its final use \u2013 in our case, being searched by Solr.\n\nThink of it like making cookies:\n1.  **Get Raw Ingredients:** Flour, sugar, eggs, maybe some random things from the pantry (raw data).\n2.  **Sift and Mix:** Remove lumps, combine the right ingredients in the right order (cleaning, filtering).\n3.  **Add Flavor/Extras:** Add chocolate chips, vanilla essence (enrichment - sentiment, category).\n4.  **Shape:** Cut into cookie shapes (formatting into JSON).\n5.  **Bake:** (This is like [Chapter 7: Data Indexing](07_data_indexing.md) - getting it ready for consumption/search).\n\nOur pipeline uses several Python scripts, each acting as a \"station\" on the assembly line.\n\n## The Assembly Line Stations\n\nOur pipeline takes the raw data (from `crawlReddit.py` and potentially combined using `combine_json_datas.py`) and passes it through these main processing stations, implemented as Python scripts in the `formatting_data/` folder:\n\n1.  **Station 1: Filtering for Relevance (`p1_filterJson.py`)**\n    *   **Problem:** Our initial data collection might grab posts that mention \"AI\" or \"jobs\" but aren't really about the core topic of \"AI replacing jobs\". We need to remove this noise.\n    *   **Solution:** We use a smart technique involving **sentence transformers** (`SentenceTransformer` library). We compare each post's text to our main query (\"Thoughts On AI Replacing Jobs\"). If the meaning is too different (similarity score is too low), we discard the post.\n    *   **Input:** A JSON file with many posts (e.g., `OverallRecords.json`).\n    *   **Output:** A new JSON file containing *only* the posts that are relevant to our topic (e.g., `filteredOverallRecords.json`).\n\n    ```python\n    # File: formatting_data/p1_filterJson.py (Simplified)\n    from sentence_transformers import SentenceTransformer, util\n    import json\n\n    # Load the model that understands sentence meaning\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    query = \"Thoughts On AI Replacing Jobs\"\n    query_embedding = model.encode(query) # Turn query into numbers\n\n    # Load raw data\n    with open('OverallRecords.json', 'r') as file:\n        data = json.load(file)\n\n    filtered_data = []\n    for post in data:\n        # Turn post text into numbers and compare with query\n        post_embedding = model.encode(post['text'])\n        similarity = util.pytorch_cos_sim(query_embedding, post_embedding)\n\n        # Keep only posts with similarity > 0.5\n        if similarity.item() > 0.5:\n            filtered_data.append(post)\n\n    # Save the filtered data\n    with open('filteredOverallRecords.json', 'w') as file:\n        json.dump(filtered_data, file, indent=4)\n\n    print(f\"Filtered {len(data)} posts down to {len(filtered_data)} relevant ones.\")\n    ```\n    This script uses a pre-trained AI model to understand the *meaning* of the query and each post, keeping only those that are semantically similar.\n\n2.  **Station 2: Adding Sentiment & Subjectivity (`p2_polaritySubjectivity.py`)**\n    *   **Problem:** We want to know if a post expresses a positive, negative, or neutral opinion (sentiment) and whether it's more factual or opinionated (subjectivity). This is useful for analysis and filtering searches later.\n    *   **Solution:** We use a library called `TextBlob`. It reads the text of each post and automatically calculates these scores.\n    *   **Input:** The filtered JSON file (`filteredOverallRecords.json`).\n    *   **Output:** The *same* JSON file, but now each post has new fields: `polarity` (e.g., \"positive\") and `subjectivity` (e.g., \"opinionated\").\n\n    ```python\n    # File: formatting_data/p2_polaritySubjectivity.py (Simplified)\n    from textblob import TextBlob\n    import json\n\n    # Load the filtered data\n    with open('filteredOverallRecords.json', 'r') as file:\n        data = json.load(file)\n\n    for post in data:\n        analysis = TextBlob(post['text'])\n        # Determine sentiment\n        if analysis.sentiment.polarity > 0:\n            post['polarity'] = [\"positive\"] # Stored as a list\n        elif analysis.sentiment.polarity < 0:\n            post['polarity'] = [\"negative\"]\n        else:\n            post['polarity'] = [\"neutral\"]\n\n        # Determine subjectivity\n        if analysis.sentiment.subjectivity > 0.5:\n            post['subjectivity'] = [\"opinionated\"]\n        else:\n            post['subjectivity'] = [\"neutral\"] # Renamed from \"objective\"\n\n    # Save the enriched data back to the same file\n    with open('filteredOverallRecords.json', 'w') as file:\n        json.dump(data, file, indent=4)\n\n    print(\"Added polarity and subjectivity to posts.\")\n    ```\n    This script iterates through the posts and adds new key-value pairs for `polarity` and `subjectivity` based on TextBlob's analysis.\n\n3.  **Station 3: Adding Industry Categories (`p3_classification.py`)**\n    *   **Problem:** Knowing which industries (like \"Healthcare\", \"Finance\", \"Education\") are discussed in relation to AI job replacement is valuable.\n    *   **Solution:** This script uses more advanced NLP techniques (like `spaCy` and `transformers` library with zero-shot classification). It reads the text and tries to assign relevant industry labels from a predefined list. It even tries to \"mask\" generic AI terms first to avoid simply classifying everything as \"Technology & IT\".\n    *   **Input:** The enriched JSON file (`filteredOverallRecords.json`).\n    *   **Output:** The *same* JSON file, but now each post has another new field: `category` (e.g., `[\"Healthcare & Pharmaceuticals\", \"Technology & IT\"]`). It can have multiple categories.\n\n    ```python\n    # File: formatting_data/p3_classification.py (Highly Simplified)\n    import json\n    # (Assume setup for classifier and industry_labels is done)\n    # from transformers import pipeline\n    # classifier = pipeline(...)\n    # industry_labels = [\"Technology & IT\", \"Finance & Banking\", ...]\n\n    def classify_industry(text, labels):\n        # Placeholder for complex classification logic\n        # In reality, uses zero-shot classification model\n        # Let's pretend it returns top 2 relevant labels\n        if \"hospital\" in text.lower():\n            return [\"Healthcare & Pharmaceuticals\", \"Technology & IT\"]\n        if \"bank\" in text.lower():\n            return [\"Finance & Banking\", \"Technology & IT\"]\n        return [\"Technology & IT\"] # Default guess\n\n    # Load the data enriched with sentiment\n    with open('filteredOverallRecords.json', 'r') as file:\n        data = json.load(file)\n\n    for post in data:\n        # Classify the post's text into industry categories\n        post['category'] = classify_industry(post['text'], []) # Pass empty labels for dummy function\n\n    # Save the further enriched data back to the same file\n    with open('filteredOverallRecords.json', 'w') as file:\n        json.dump(data, file, indent=4)\n\n    print(\"Added industry categories to posts.\")\n    ```\n    This script adds the `category` field, containing a list of relevant industry tags identified by the NLP classification model.\n\n## How it Works: Data Flow\n\nThe data starts as raw files, potentially gets combined, and then flows through our `formatting_data` scripts one by one. Each script modifies the data and passes it along to the next station.\n\n```mermaid\nsequenceDiagram\n    participant RawData as Raw JSON Files (e.g., Reddit*.json)\n    participant FilterScript as Filter Relevance (p1_filterJson.py)\n    participant SentimentScript as Add Sentiment (p2_polaritySubjectivity.py)\n    participant CategoryScript as Add Categories (p3_classification.py)\n    participant FinalJSON as Cleaned & Enriched JSON (filteredOverallRecords.json)\n\n    RawData->>FilterScript: Load raw posts\n    FilterScript->>FilterScript: Calculate similarity, keep relevant ones\n    FilterScript->>SentimentScript: Pass filtered posts\n    SentimentScript->>SentimentScript: Add 'polarity' & 'subjectivity' fields\n    SentimentScript->>CategoryScript: Pass enriched posts\n    CategoryScript->>CategoryScript: Add 'category' field\n    CategoryScript->>FinalJSON: Save final structured data\n```\n\n1.  **Start:** We have raw JSON files from scraping (e.g., `OverallRecords.json`).\n2.  **Filter:** `p1_filterJson.py` reads this file, uses sentence transformers to check relevance against \"Thoughts On AI Replacing Jobs\", and writes *only* the relevant posts to `filteredOverallRecords.json`.\n3.  **Add Sentiment:** `p2_polaritySubjectivity.py` reads `filteredOverallRecords.json`, uses TextBlob to add `polarity` and `subjectivity` to each post, and saves the changes back to `filteredOverallRecords.json`.\n4.  **Add Category:** `p3_classification.py` reads the updated `filteredOverallRecords.json`, uses NLP classifiers to add the `category` list to each post, and saves the changes back again to `filteredOverallRecords.json`.\n5.  **End:** The `filteredOverallRecords.json` file now contains cleaned, relevant, enriched, and consistently formatted data, ready for the next step: indexing.\n\n## Conclusion\n\nThe Data Processing Pipeline is a critical behind-the-scenes component that transforms raw, messy data into a valuable, searchable asset. By chaining together scripts for filtering relevance, adding sentiment/subjectivity, and classifying by industry category, we create a clean, enriched, and consistently structured JSON dataset. This automated \"assembly line\" ensures that the data fed into our search engine ([Chapter 7: Data Indexing](07_data_indexing.md)) is high-quality and ready to power our application's search and visualization features.\n\nWe briefly touched upon how sentiment and categories are added. But how do those enrichment steps *really* work? Let's dive deeper into the specific techniques used.\n\n**Next:** [Chapter 6: Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 31: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\06_data_enrichment__sentiment_category_.md ---\n# Chapter 6: Data Enrichment (Sentiment/Category)\n\nWelcome back! In [Chapter 5: Data Processing Pipeline](05_data_processing_pipeline.md), we saw the big picture: an assembly line that takes raw data (like Reddit posts) and cleans it up, getting it ready for our search engine. We learned that this pipeline involves several steps, including filtering, adding sentiment, and adding categories.\n\nNow, let's zoom in on two crucial steps on that assembly line: **Data Enrichment**. This is where we add extra, valuable information to our data that wasn't there originally. Specifically, we'll look at how we figure out the *sentiment* (is the text positive, negative, or neutral?) and the *category* (what industry or topic is it about?).\n\n**Use Case:** Imagine we've collected thousands of posts about \"AI replacing jobs\" using the pipeline from Chapter 5. We have the text, but that's it. Data Enrichment helps us answer questions like:\n*   Are people generally feeling positive or negative about this topic?\n*   Which industries (like Healthcare, Finance, Education) are being discussed most often in relation to job replacement by AI?\n\nKnowing the sentiment and category makes our data much more insightful when we search or visualize it later ([Chapter 4: Data Visualization](04_data_visualization.md)).\n\n## What is Data Enrichment?\n\nThink about a librarian cataloging new books. They don't just record the title and author. They add extra tags:\n*   **Genre:** Fiction, Science, History (like our *Category*)\n*   **Keywords:** Space travel, dinosaurs, cooking (helps find related books)\n*   **Maybe a short summary or review score:** Tells you what the book is like or how others felt (like our *Sentiment*)\n\nData Enrichment is just like that! We take our basic data (the text of a post) and add extra \"tags\" or \"labels\" that describe it better. In our project, the main tags we add are:\n1.  **Sentiment:** The emotional tone (positive, negative, neutral).\n2.  **Subjectivity:** Whether the text seems like an opinion or a statement of fact.\n3.  **Category:** The relevant industry or topic (Technology, Healthcare, etc.).\n\nThese enrichments happen within specific scripts in our [Data Processing Pipeline](05_data_processing_pipeline.md), namely `p2_polaritySubjectivity.py` and `p3_classification.py`.\n\n## Adding Feelings: Sentiment and Subjectivity\n\nHow can a computer read a sentence and understand if it's happy, sad, or just stating a fact? We use a helpful Python library called **TextBlob**.\n\nTextBlob is like a mini-linguist. You give it some text, and it analyzes the words used, punctuation, and structure to make educated guesses about the feeling (polarity) and the style (subjectivity).\n\n*   **Polarity:** A score from -1.0 (very negative) to +1.0 (very positive). 0.0 is neutral.\n*   **Subjectivity:** A score from 0.0 (very objective/factual) to 1.0 (very subjective/opinionated).\n\nIn our script `p2_polaritySubjectivity.py`, we use TextBlob to calculate these scores for each post and then assign simple labels.\n\n```python\n# File: formatting_data/p2_polaritySubjectivity.py (Simplified Logic)\nfrom textblob import TextBlob\n\ndef get_sentiment(text):\n    # Create a TextBlob object to analyze the text\n    analysis = TextBlob(text)\n\n    # Determine polarity label\n    if analysis.sentiment.polarity > 0:\n        sentiment = \"positive\"\n    elif analysis.sentiment.polarity < 0:\n        sentiment = \"negative\"\n    else:\n        sentiment = \"neutral\"\n\n    # Determine subjectivity label\n    if analysis.sentiment.subjectivity > 0.5:\n        subjectivity = \"opinionated\" # We label highly subjective text\n    else:\n        subjectivity = \"neutral\" # Less subjective text (more factual)\n\n    return sentiment, subjectivity\n\n# Imagine 'data' is a list of posts (dictionaries) loaded from JSON\n# for post in data:\n#    text_content = post['text']\n#    # Calculate sentiment and subjectivity for the post's text\n#    sentiment_label, subjectivity_label = get_sentiment(text_content)\n#    # Add these new labels as fields to the post dictionary\n#    post['polarity'] = [sentiment_label] # Store as a list for consistency\n#    post['subjectivity'] = [subjectivity_label]\n#    print(f\"Analyzed post, added sentiment: {sentiment_label}\")\n```\n\nThis `get_sentiment` function takes the text from a post. It uses `TextBlob(text)` to perform the analysis. Then, based on the `polarity` and `subjectivity` scores TextBlob provides, it assigns simple labels like \"positive\" or \"opinionated\". The main part of the script (commented out above) would loop through all posts, call this function, and add the resulting labels as new fields (`polarity` and `subjectivity`) to each post's data.\n\n**Input:** A post dictionary like `{'id': 'abc', 'text': 'AI is amazing!'}`\n**Output:** The same dictionary with new fields: `{'id': 'abc', 'text': 'AI is amazing!', 'polarity': ['positive'], 'subjectivity': ['opinionated']}`\n\n## Adding Topics: Category Classification\n\nOkay, we know *how* people feel, but *what* are they talking about? Specifically, which industries? Assigning industry categories is trickier than sentiment. We use more advanced Natural Language Processing (NLP) tools for this:\n\n*   **SpaCy:** Helps with basic text processing, like identifying parts of speech or specific entities.\n*   **Transformers (Hugging Face library):** Provides access to powerful pre-trained AI models. We use a model capable of **zero-shot classification**.\n\n**What's Zero-Shot Classification?** Imagine you want to sort mail into bins labeled \"Bills\", \"Personal Letters\", and \"Junk Mail\". Zero-shot classification is like being able to do this *without* ever having seen examples of each category beforehand. The model understands language well enough to read a letter and decide which bin it likely belongs to based on the meaning, even if it hasn't been specifically trained on *your* exact categories.\n\nOur script `p3_classification.py` uses this technique. It takes the text of a post and asks the zero-shot model: \"Which of these industry labels best describe this text?\"\n\n**A Clever Trick: Masking AI Terms**\nOne challenge is that almost *all* our posts mention \"AI\". If we just classify the text, the model might always say \"Technology & IT\" because AI is so prominent. To find out what *else* the post is about (e.g., is it AI in *Healthcare*? AI in *Finance*?), we sometimes play a trick:\n1.  Temporarily hide or \"mask\" common AI-related words (like \"AI\", \"machine learning\") in the text, replacing them with a placeholder like `[MASK]`.\n2.  Classify *both* the original text and the masked text.\n3.  If the masked text gives us useful categories *other* than just \"Technology & IT\", we might prefer those results. This helps uncover the specific domain where AI is being discussed.\n\nHere's a highly simplified idea of how `p3_classification.py` works:\n\n```python\n# File: formatting_data/p3_classification.py (Highly Simplified Concept)\nimport random # Using random for this simplified example\n\n# Define the possible industry labels the model knows\nindustry_labels = [\n    \"Technology & IT\", \"Finance & Banking\", \"Healthcare & Pharmaceuticals\",\n    \"Education & Training\", \"Manufacturing & Engineering\", \"Retail & E-Commerce\",\n    # ... many more categories ...\n]\n\ndef classify_industry_simplified(text):\n    # --- This is a placeholder! ---\n    # The real function uses complex NLP models (Transformers zero-shot)\n    # and the masking technique described above.\n    # For this example, let's just pick 1 or 2 random labels.\n    num_labels = random.randint(1, 2)\n    chosen_labels = random.sample(industry_labels, num_labels)\n    print(f\"Simplified classification for '{text[:30]}...' -> {chosen_labels}\")\n    return chosen_labels\n\n# Imagine 'data' is a list of posts (dictionaries) loaded from JSON\n# for post in data:\n#    text_content = post['text']\n#    # Get the list of relevant industry categories\n#    category_labels = classify_industry_simplified(text_content)\n#    # Add this list as a new field to the post dictionary\n#    post['category'] = category_labels\n```\n\nThis simplified `classify_industry_simplified` function just picks random labels to illustrate the concept. The real `p3_classification.py` script contains sophisticated code using the `transformers` library to perform the zero-shot classification and the masking logic. The main part of the script (commented out) loops through posts, calls the classification function, and adds the resulting list of categories to the `category` field of each post.\n\n**Input:** A post dictionary like `{'id': 'def', 'text': 'Robots doing surgery is the future.', 'polarity': ['positive'], ...}`\n**Output:** The same dictionary with the new category field: `{'id': 'def', 'text': '...', 'polarity': ['positive'], ..., 'category': ['Healthcare & Pharmaceuticals', 'Technology & IT']}` (Note: `category` is a list, as a post might touch on multiple industries).\n\n## How it Works: The Enrichment Steps\n\nThese enrichment processes are specific steps within the larger [Data Processing Pipeline](05_data_processing_pipeline.md). The data flows from one step to the next, gaining more labels along the way.\n\n```mermaid\nsequenceDiagram\n    participant DP as Data Pipeline (Orchestrator)\n    participant FilteredJSON as Filtered Data (filteredOverallRecords.json)\n    participant SentimentScript as Add Sentiment (p2_polaritySubjectivity.py)\n    participant CategoryScript as Add Categories (p3_classification.py)\n    participant EnrichedJSON as Enriched Data (filteredOverallRecords.json)\n\n    DP->>SentimentScript: Start processing FilteredJSON\n    SentimentScript->>FilteredJSON: Read post data\n    SentimentScript->>SentimentScript: Use TextBlob to get polarity/subjectivity\n    SentimentScript->>FilteredJSON: Write updated post with new fields\n    Note over SentimentScript: Repeats for all posts\n    DP->>CategoryScript: Start processing updated FilteredJSON\n    CategoryScript->>FilteredJSON: Read post data (now includes sentiment)\n    CategoryScript->>CategoryScript: Use Transformers/SpaCy for category\n    CategoryScript->>FilteredJSON: Write updated post with category field\n    Note over CategoryScript: Repeats for all posts\n    CategoryScript->>EnrichedJSON: Final enriched data saved\n```\n\n1.  The pipeline first runs `p1_filterJson.py` to get relevant posts into `filteredOverallRecords.json`.\n2.  Then, `p2_polaritySubjectivity.py` reads this file, adds `polarity` and `subjectivity` using TextBlob, and saves the changes back to the *same file*.\n3.  Next, `p3_classification.py` reads the updated file (which now has sentiment info), adds the `category` list using complex NLP, and saves the final enriched data back to `filteredOverallRecords.json`.\n\n**Example: Before vs. After Enrichment**\n\n*   **Before (Output of `p1_filterJson.py`):**\n    ```json\n    {\n      \"id\": \"xyz789\",\n      \"title\": \"AI impact on banks?\",\n      \"text\": \"Will AI automate teller jobs in finance soon? Seems likely.\",\n      \"date\": \"2023-10-27T10:00:00Z\",\n      \"source\": \"Reddit: r/artificial\"\n    }\n    ```\n\n*   **After (Output of `p3_classification.py`):**\n    ```json\n    {\n      \"id\": \"xyz789\",\n      \"title\": \"AI impact on banks?\",\n      \"text\": \"Will AI automate teller jobs in finance soon? Seems likely.\",\n      \"date\": \"2023-10-27T10:00:00Z\",\n      \"source\": \"Reddit: r/artificial\",\n      \"polarity\": [\"neutral\"],\n      \"subjectivity\": [\"opinionated\"],\n      \"category\": [\"Finance & Banking\", \"Technology & IT\"]\n    }\n    ```\n    See how the `polarity`, `subjectivity`, and `category` fields have been added? This enriched data is much more useful for detailed searching and analysis.\n\n## Conclusion\n\nData Enrichment is like adding superpowers to our raw data. By using tools like TextBlob for sentiment analysis and advanced NLP models (SpaCy, Transformers) for category classification, we add meaningful labels (`polarity`, `subjectivity`, `category`) to each piece of text. These labels, generated by scripts like `p2_polaritySubjectivity.py` and `p3_classification.py` within our [Data Processing Pipeline](05_data_processing_pipeline.md), transform simple text posts into rich data points ready for sophisticated searching and insightful visualization.\n\nNow that our data is cleaned, filtered, and enriched, what's the final step before we can actually search it? We need to put it into our search engine, Solr. That process is called Indexing.\n\n**Next:** [Chapter 7: Data Indexing](07_data_indexing.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 32: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\07_data_indexing.md ---\n# Chapter 7: Data Indexing\n\nHello again! In the last chapter, [Chapter 6: Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md), we learned how our pipeline adds valuable labels like sentiment (positive/negative) and industry category (Technology, Healthcare) to our processed data. We ended up with a clean, enriched JSON file (`filteredOverallRecords.json`) packed with useful information about topics like \"AI replacing jobs\".\n\nBut having this great JSON file isn't enough. How does our search engine, Solr, actually *see* this data so we can search it through our web application? If the data file is just sitting on the computer's hard drive, Solr doesn't know about it!\n\nThat's where **Data Indexing** comes in. It's the crucial step of loading our finalized data into the search engine.\n\n**Use Case:** Remember our cleaned and enriched JSON file full of Reddit posts about \"AI replacing jobs\"? It contains the text, date, source, sentiment, and categories for each relevant post. Now, we need to feed this entire file into Solr. Once indexed, a user can go to our web application ([Chapter 1: Web Application Interface](01_web_application_interface.md)), type \"negative sentiment finance jobs AI\", and Solr will be able to instantly find the relevant posts because they are now *inside* its system.\n\n## What is Data Indexing?\n\nThink back to our library analogy. In [Chapter 5: Data Processing Pipeline](05_data_processing_pipeline.md) and [Chapter 6: Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md), we were like the library staff taking new books (raw data), checking if they fit the library's collection (filtering), adding genre stickers (categories), and maybe a little review note (sentiment). We ended up with a stack of fully processed and cataloged books (our final JSON file).\n\n**Data Indexing is like taking those fully cataloged books and carefully placing them onto the correct shelves in the library.**\n\nThe \"library\" is our Solr search engine, and the \"shelves\" are its internal data structure, called an **index**. Solr organizes the data in a highly efficient way (like using a super-fast card catalog) so that when you search later, it can find what you need almost instantly.\n\nUntil the data is *indexed* (put on the shelves), Solr can't find it.\n\n## The Prepared Package: Our JSON Data\n\nThanks to the hard work done in the previous steps ([Chapter 5: Data Processing Pipeline](05_data_processing_pipeline.md) and [Chapter 6: Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md)), our data is already perfectly formatted in a JSON file. Each entry in the file represents a \"document\" (a Reddit post, in our case) with fields that Solr expects, like `id`, `text`, `date`, `polarity`, `category`, etc.\n\nIt looks something like this (a single post):\n\n```json\n{\n  \"id\": \"xyz789\",\n  \"title\": \"AI impact on banks?\",\n  \"text\": \"Will AI automate teller jobs in finance soon? Seems likely.\",\n  \"date\": \"2023-10-27T10:00:00Z\",\n  \"source\": \"Reddit: r/artificial\",\n  \"polarity\": [\"neutral\"],\n  \"subjectivity\": [\"opinionated\"],\n  \"category\": [\"Finance & Banking\", \"Technology & IT\"],\n  \"id_num\": 123 // Added unique numeric ID\n}\n```\n\nThis format is exactly what Solr understands. Now we just need to send it.\n\n## The Tool for the Job: `importJson.py`\n\nTo get this JSON data into Solr, we use a simple Python script called `importJson.py`. Think of this script as the delivery truck that takes our box of cataloged books (the JSON file) from the processing room to the library (Solr) and tells the librarian exactly where to put them.\n\nThis script uses the popular `requests` library in Python, which is great for sending information over the web (like talking to our Solr server).\n\n## How `importJson.py` Works: Step-by-Step\n\nThe script performs a few key actions:\n\n1.  **Knows Solr's Address:** It defines the specific URL where Solr listens for incoming data updates.\n2.  **Reads the Data:** It opens our final JSON file (`filteredOverallRecords.json` or a similar file, maybe named `data.json` for the script) and reads its entire content.\n3.  **Sends the Data:** It sends the JSON content to the Solr URL using an HTTP POST request. This is like handing the box of books to the librarian.\n4.  **Tells Solr to Save:** After sending the data, it sends another command (an HTTP GET request with `commit=true`) telling Solr: \"Okay, I've given you all the new books, please update your main shelves now!\" This \"commit\" step makes the newly added data searchable.\n\nLet's look at the key parts of the script:\n\n```python\n# File: importJson.py (Simplified)\nimport requests\nimport json # Import json library to handle potential formatting\n\n# 1. Solr's Address for receiving JSON documents\n#    'mycore' is the name of our specific collection in Solr\nsolr_url = \"http://localhost:8983/solr/mycore/update/json/docs\"\n\n# Path to the JSON file we want to index\njson_file = \"filteredOverallRecords.json\" # Or could be \"data.json\"\n\nprint(f\"Reading data from {json_file}...\")\n# 2. Read the JSON data from the file\nwith open(json_file, \"r\", encoding='utf-8') as file:\n    # Read the whole file content as a string\n    json_data = file.read()\n    # It's good practice to ensure it's valid JSON before sending\n    # (Actual script might load/dump to validate)\n\nprint(f\"Sending data to Solr at {solr_url}...\")\n# 3. Send the JSON data to Solr\nheaders = {\"Content-Type\": \"application/json\"} # Tell Solr we're sending JSON\nresponse = requests.post(solr_url, data=json_data.encode('utf-8'), headers=headers)\n\n# Check if Solr received it okay\nif response.status_code == 200:\n    print(\"JSON data sent successfully to Solr!\")\nelse:\n    print(f\"Failed to send JSON data. Solr responded: {response.text}\")\n    exit() # Stop if sending failed\n\nprint(\"Asking Solr to commit the changes...\")\n# 4. Tell Solr to make the changes permanent and searchable\ncommit_url = \"http://localhost:8983/solr/mycore/update?commit=true\"\ncommit_response = requests.get(commit_url)\n\nif commit_response.status_code == 200:\n    print(\"Solr committed changes successfully! Data is now indexed.\")\nelse:\n    print(f\"Failed to commit changes. Solr responded: {commit_response.text}\")\n\n```\n\n*   **`solr_url`**: This is the specific web address (URL) on the Solr server that's designed to accept new data in JSON format. `http://localhost:8983` is the base address of our local Solr instance, `/solr/mycore` points to our specific data collection, and `/update/json/docs` tells Solr we're sending JSON documents to be added or updated.\n*   **`json_file`**: The name of the file containing our precious enriched data.\n*   **`with open(...)`**: This safely opens and reads the entire content of the JSON file into the `json_data` variable. We specify `encoding='utf-8'` to handle various text characters correctly.\n*   **`requests.post(...)`**: This is the core action! It sends the `json_data` (encoded into bytes) to the `solr_url`. The `headers` tell Solr that the data being sent is in JSON format. Solr receives this data and holds onto it temporarily.\n*   **`requests.get(commit_url)`**: This sends a simple request to a different Solr URL that just includes `?commit=true`. This tells Solr: \"Take all the data you received since the last commit and make it permanently part of your searchable index.\" Without this commit, the data might sit in Solr's memory but wouldn't be findable yet.\n\nWhen you run this script (`python importJson.py`), it reads your file and uploads it straight into Solr, making it ready for searching.\n\n## How it Works: Under the Hood\n\nWhat happens when the `importJson.py` script runs and talks to Solr?\n\n1.  **Script Reads File:** The Python script opens `filteredOverallRecords.json` and loads its content.\n2.  **Script Sends Data (POST):** The script sends an HTTP POST request containing the JSON data to the Solr server's `/update/json/docs` endpoint.\n3.  **Solr Receives Data:** Solr gets the JSON data. It looks at its configuration (called a \"schema\") to understand the fields (`text`, `date`, `category`, etc.).\n4.  **Solr Processes (Analyzes):** For text fields like `title` and `text`, Solr performs analysis. This might involve breaking text into words (tokenization), converting to lowercase, removing common words (\"the\", \"a\"), and reducing words to their root form (lemmatization, like we saw in [Chapter 2: Search Query Construction](02_search_query_construction.md)). This makes searching more flexible. For other fields like `date` or `category`, it stores them in appropriate formats.\n5.  **Solr Holds Data (Pre-Commit):** Solr keeps the processed data ready but doesn't make it searchable immediately. This is efficient if you're sending lots of updates.\n6.  **Script Sends Commit (GET):** The script sends an HTTP GET request to the Solr `/update?commit=true` endpoint.\n7.  **Solr Commits:** Solr takes all the pending changes and writes them permanently into its internal index (powered by a technology called Lucene).\n8.  **Data is Searchable:** Now, the data is fully indexed and can be found via search queries sent from our web application!\n\nHere's a diagram showing the interaction:\n\n```mermaid\nsequenceDiagram\n    participant Script as importJson.py\n    participant Solr as Solr Server (localhost:8983)\n    participant Index as Solr Index (Lucene)\n\n    Script->>Script: Read data from filteredOverallRecords.json\n    Script->>Solr: POST /solr/mycore/update/json/docs (with JSON data)\n    Solr->>Solr: Receive JSON, parse based on schema\n    Solr->>Solr: Analyze text fields (tokenize, lowercase, etc.)\n    Solr->>Solr: Hold processed data temporarily\n    Script->>Solr: GET /solr/mycore/update?commit=true\n    Solr->>Index: Write pending changes to the index\n    Index->>Solr: Confirm write successful\n    Solr->>Script: Respond OK (Commit successful)\n    Note over Solr,Index: Data is now live and searchable\n```\n\nThis shows the two main interactions: sending the data (`POST`) and then telling Solr to save it permanently (`GET commit=true`).\n\n## Conclusion\n\nData Indexing is the bridge that connects our prepared data to our search engine. By using the `importJson.py` script, we take the final, enriched JSON output from our [Data Processing Pipeline](05_data_processing_pipeline.md) and load it directly into Solr. The script sends the data and then issues a \"commit\" command, telling Solr to analyze the information and add it to its internal index. Once indexed, the data becomes instantly searchable through our web application, allowing users to explore the insights we worked hard to uncover.\n\nWe've now covered how data gets *into* Solr. But how do we manage the Solr engine itself? What if we need to check its status, manage its configuration, or even delete data?\n\n**Next:** [Chapter 8: Search Engine Management](08_search_engine_management.md)\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 33: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\08_search_engine_management.md ---\n# Chapter 8: Search Engine Management\n\nWelcome to the final chapter! In [Chapter 7: Data Indexing](07_data_indexing.md), we learned how to take our clean, enriched JSON data and load it into the Solr search engine using the `importJson.py` script. Now our data is \"on the shelves\" and ready to be searched.\n\nBut what about the library itself? What if the library (our Solr server) isn't even open? Or what if we need to set up a specific room (our data 'core') for the first time, or maybe clear it out later? That's where **Search Engine Management** comes in.\n\n**Use Case:** Imagine you just downloaded this project and want to run it for the first time. Before you can index any data ([Chapter 7: Data Indexing](07_data_indexing.md)) or search anything ([Chapter 3: Search Execution (Solr)](03_search_execution__solr_.md)), you need to make sure the Solr search engine program is actually running on your computer. You might also need to create the specific data storage area, which we call `mycore`, where our project's data will live. Search Engine Management provides the tools to do exactly this \u2013 starting the engine and preparing its storage.\n\nThink of it like the library's maintenance crew. They have the keys to open and close the library (start/stop Solr) and the tools to set up or remove specific sections (create/delete cores).\n\n## What Are We Managing?\n\nThere are two main things we need to manage:\n\n1.  **The Solr Server:** This is the main program that does all the searching. Like any program, it needs to be started before it can do anything, and sometimes you might need to stop it or restart it (maybe after changing some configuration).\n2.  **The Solr Core (`mycore`):** Inside the main Solr server, you can have different, independent collections of data, called \"cores\". Our project uses a specific core named `mycore` to store all the Reddit post data we processed. Before we can put data *into* `mycore`, we need to make sure this core actually *exists* within the running Solr server. If it doesn't (like the first time you run the project), we need to create it. Sometimes, if you want to start fresh, you might also want to delete the core and all the data inside it.\n\n## The Maintenance Toolkit: `start.py`\n\nDoing these tasks manually (by typing commands in a terminal) can be repetitive. So, we have a helpful Python script, `start.py`, that bundles these common management actions into easy-to-use functions.\n\nThis script acts as our control panel for the Solr engine.\n\n## Using the Toolkit: Common Tasks\n\nThe `start.py` script provides functions to handle the basic lifecycle of the Solr server and our `mycore`. Here's how you might use them (conceptually, you'd often run the script itself or call these functions from another script):\n\n**1. Starting the Solr Server:**\nBefore anything else, Solr needs to be running.\n\n```python\n# File: start.py (Conceptual Usage)\n# from start import start_solr # Assuming you import the function\n\nstart_solr()\n```\n\n*   **Input:** None.\n*   **Output:** The script will print messages like \"Starting Solr in the background...\" and tell you the address where you can access Solr's web interface (usually `http://localhost:8983/solr/`). It also waits a few seconds to give Solr time to fully start up.\n\n**2. Stopping the Solr Server:**\nWhen you're done working with the project, you can shut down the Solr server to free up computer resources.\n\n```python\n# File: start.py (Conceptual Usage)\n# from start import stop_solr\n\nstop_solr()\n```\n\n*   **Input:** None.\n*   **Output:** The script will print messages like \"Stopping Solr in the background...\" and confirm when it's done.\n\n**3. Restarting the Solr Server:**\nSometimes, if you make changes to Solr's configuration, you need to restart it.\n\n```python\n# File: start.py (Conceptual Usage)\n# from start import restart_solr\n\nrestart_solr()\n```\n\n*   **Input:** None.\n*   **Output:** The script will print \"Restart Solr in the background...\" and wait briefly for it to complete.\n\n**4. Creating the `mycore` Data Core:**\nThe very first time you set up the project, the `mycore` data collection won't exist yet. You need to create it before you can index data.\n\n```python\n# File: start.py (Conceptual Usage)\n# from start import create_core\n\ncreate_core()\n```\n\n*   **Input:** None.\n*   **Output:** The script first checks if `mycore` already exists. If not, it prints \"Core 'mycore' does not exist, creating it...\" and then \"Core 'mycore' created.\" If it already exists, it will just tell you.\n\n**5. Deleting the `mycore` Data Core:**\nIf you want to completely remove all the indexed data and start over, you can delete the core. **Warning:** This permanently deletes all data in `mycore`!\n\n```python\n# File: start.py (Conceptual Usage)\n# from start import delete_core\n\ndelete_core()\n```\n\n*   **Input:** None.\n*   **Output:** The script checks if `mycore` exists. If it does, it prints \"Core 'mycore' exist, deleting it...\" and then \"Core 'mycore' deleted.\" If it doesn't exist, it will inform you.\n\n*(Important Note: The actual `start.py` script provided in the project context runs `start_solr()` directly when executed. The other functions like `create_core`, `delete_core`, etc., are commented out by default but can be uncommented and run if needed.)*\n\n## How it Works: Under the Hood\n\nHow does the `start.py` script actually talk to Solr and tell it what to do? It uses two main techniques:\n\n1.  **Running Solr Commands (`subprocess`):** For starting, stopping, restarting the server, and creating/deleting cores, Solr provides command-line tools (like `solr.cmd` or `solr` depending on your system). Our Python script uses the built-in `subprocess` module to essentially tell the operating system: \"Please run this Solr command for me.\"\n2.  **Checking Status via Web Requests (`requests`):** Solr has a web interface and an API (Application Programming Interface). The `create_core` and `delete_core` functions first *check* if the core exists by sending a quick web request (using the `requests` library) to a specific Solr status URL. Solr replies with information about existing cores, and our script uses this to decide whether to proceed with creation or deletion.\n\nLet's visualize creating the core:\n\n```mermaid\nsequenceDiagram\n    participant Script as start.py (create_core)\n    participant PyReq as Python requests Lib\n    participant PySub as Python subprocess Lib\n    participant SolrAPI as Solr Admin API (via HTTP)\n    participant SolrCmd as Solr Command (solr.cmd)\n\n    Script->>PyReq: Ask: \"Check core status\"\n    PyReq->>SolrAPI: GET /solr/admin/cores?action=STATUS\n    SolrAPI->>PyReq: Respond: JSON listing cores (e.g., 'mycore' is NOT listed)\n    PyReq->>Script: Return status: 'mycore' does not exist\n    Script->>PySub: Tell OS: \"Run 'solr create -c mycore'\"\n    PySub->>SolrCmd: Execute 'solr create -c mycore' command\n    SolrCmd->>SolrCmd: Perform core creation steps\n    SolrCmd->>PySub: Command finished\n    PySub->>Script: Indicate command completed\n    Script->>Script: Print \"Core 'mycore' created.\"\n```\n\n1.  The `create_core` function in `start.py` starts.\n2.  It uses the `requests` library to send a web request to Solr's Admin API asking for the status of all cores.\n3.  Solr responds with a list (let's assume `mycore` isn't there).\n4.  The script sees `mycore` is missing.\n5.  It uses the `subprocess` module to run the actual Solr command-line instruction: `solr create -c mycore`.\n6.  The operating system executes this command, which tells the running Solr server to create the new core.\n7.  Once the command finishes, the script prints a success message.\n\n**Diving into the Code (Simplified Snippets)**\n\n*   **Starting Solr using `subprocess`:**\n\n```python\n# File: start.py (Simplified snippet for starting)\nimport subprocess\nimport time\nimport os\n\n# Define path to Solr's command-line tool\nsolr_bin = os.path.join(\"./solr-9.8.0/bin/solr.cmd\") # Path may vary!\n\ndef start_solr():\n    try:\n        print(\"Starting Solr...\")\n        # Use subprocess to run the 'solr start' command\n        # Popen runs it in the background without blocking our script\n        process = subprocess.Popen([solr_bin, \"start\", \"-p\", \"8983\"])\n        print(f\"Solr start command issued (PID: {process.pid}).\")\n        # Wait a bit for Solr to become ready\n        time.sleep(15)\n        print(\"Solr should be running at http://localhost:8983/solr/\")\n        # (The original script also terminates the Popen handle,\n        # but Solr itself keeps running in the background)\n    except Exception as e:\n        print(f\"Error starting Solr: {e}\")\n```\nThis uses `subprocess.Popen` to execute the `solr.cmd start -p 8983` command. `Popen` allows the script to continue while Solr starts up in the background. `time.sleep(15)` pauses the script to give Solr enough time to launch fully.\n\n*   **Checking Core Status using `requests`:**\n\n```python\n# File: start.py (Simplified snippet for checking core status)\nimport requests\n\nSOLR_PORT = '8983'\nSOLR_CORE = 'mycore'\nSOLR_ADMIN_URL = f\"http://localhost:{SOLR_PORT}/solr/admin/cores\"\n\ndef core_exists():\n    try:\n        # Send a web request to the Solr Admin API to get core status\n        response = requests.get(SOLR_ADMIN_URL, params={'action': 'STATUS'}, timeout=5)\n        response.raise_for_status() # Raise an error for bad responses (4xx or 5xx)\n\n        # Parse the JSON response from Solr\n        cores_status = response.json().get('status', {})\n        # Check if our specific core name is in the status list\n        return SOLR_CORE in cores_status\n    except requests.exceptions.RequestException as e:\n        print(f\"Error checking core status: {e}\")\n        return False # Assume it doesn't exist if we can't check\n\n# Usage example within create_core (simplified):\n# if not core_exists():\n#    print(f\"Core '{SOLR_CORE}' does not exist, creating it...\")\n#    # ... code using subprocess to run 'solr create' ...\n# else:\n#    print(f\"Core '{SOLR_CORE}' already exists.\")\n```\nThis uses `requests.get` to ask the Solr server (at its admin URL) for the status of all cores. It parses the JSON response to see if `mycore` is listed among the active cores. This check is crucial for the `create_core` and `delete_core` functions to know whether they need to act.\n\n*(Note: The provided `start.py` also includes checks for `JAVA_HOME`. Solr requires Java to run, so this environment variable must be set correctly on your system.)*\n\n## Conclusion\n\nSearch Engine Management is about controlling the search engine instance itself. Using the helper functions in `start.py`, we can easily start, stop, and restart the Solr server, as well as create or delete the specific `mycore` data collection used by our project. These scripts leverage Python's `subprocess` module to run Solr's command-line tools and the `requests` library to interact with Solr's API for status checks. This makes managing the backend search engine much simpler, especially when setting up the project or needing to perform maintenance.\n\nThis concludes our journey through the `SC4021-Information-Retrieval` project! We've seen how users interact via the [Web Application Interface](01_web_application_interface.md), how their searches are translated into precise queries ([Search Query Construction](02_search_query_construction.md)), how Solr executes those queries ([Search Execution (Solr)](03_search_execution__solr_.md)), and how results can be summarized visually ([Data Visualization](04_data_visualization.md)). We also explored the backend pipeline that prepares the data: the overall [Data Processing Pipeline](05_data_processing_pipeline.md), the specific steps of [Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md), the loading process via [Data Indexing](07_data_indexing.md), and finally, how to manage the Solr engine itself ([Search Engine Management](08_search_engine_management.md)).\n\nHopefully, this tutorial has given you a clear, step-by-step understanding of how all these pieces fit together to build a working information retrieval system. Happy searching!\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 34: LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\index.md ---\n# Tutorial: SC4021-Information-Retrieval\n\nThis project is a *web application* that lets users explore public opinions about **AI replacing jobs**, using data scraped from Reddit.\nUsers can **search** for specific terms, apply **filters** (like date range, sentiment polarity, or industry category), and view the relevant posts.\nThe application analyzes the sentiment and category of each post and presents the aggregated results through various **visualizations**, such as word clouds and charts, to reveal trends and patterns. It uses the **Apache Solr** search engine in the background to manage and query the data efficiently.\n\n\n**Source Repository:** [None](None)\n\n```mermaid\nflowchart TD\n    A0[\"Web Application Interface\"]\n    A1[\"Search Query Construction\"]\n    A2[\"Search Execution (Solr)\"]\n    A3[\"Data Processing Pipeline\"]\n    A4[\"Data Indexing\"]\n    A5[\"Data Visualization\"]\n    A6[\"Data Enrichment (Sentiment/Category)\"]\n    A7[\"Search Engine Management\"]\n    A0 -- \"Builds query from user input\" --> A1\n    A0 -- \"Initiates search requests\" --> A2\n    A0 -- \"Displays generated plots\" --> A5\n    A1 -- \"Provides formatted Solr query\" --> A2\n    A2 -- \"Returns search results\" --> A0\n    A2 -- \"Provides data for plots\" --> A5\n    A3 -- \"Sends processed data to\" --> A4\n    A3 -- \"Uses for analysis\" --> A6\n    A4 -- \"Loads data into Solr\" --> A2\n    A6 -- \"Enriches data within\" --> A3\n    A7 -- \"Controls Solr instance\" --> A2\n```\n\n## Chapters\n\n1. [Web Application Interface](01_web_application_interface.md)\n2. [Search Query Construction](02_search_query_construction.md)\n3. [Search Execution (Solr)](03_search_execution__solr_.md)\n4. [Data Visualization](04_data_visualization.md)\n5. [Data Processing Pipeline](05_data_processing_pipeline.md)\n6. [Data Enrichment (Sentiment/Category)](06_data_enrichment__sentiment_category_.md)\n7. [Data Indexing](07_data_indexing.md)\n8. [Search Engine Management](08_search_engine_management.md)\n\n\n---\n\nGenerated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)\n\n--- File Index 35: LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py ---\nfrom google import genai\nfrom anthropic import Anthropic\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\nimport logging\nimport json\nfrom datetime import datetime\nfrom enum import Enum\n\n# Load environment variables from .env file\nload_dotenv()  # Loads from .env file\n\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro gievn recent high bench marks\ndef call_llm(prompt: str, use_cache: bool = True, model: str=LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    \n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n                logger.warning(f\"Loaded and Using cache\")\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n        \n\n    # If not in cache, call the LLM\n    # Call the LLM if not in cache or cache disabled\n    if(model==LLMProvider_enum.GOOGLE):\n        # Use Google Gemini\n        \n        # client = genai.Client(\n        #     vertexai=True, \n        #     # TODO: change to your own project id and location\n        #     project=os.getenv(\"GEMINI_PROJECT_ID\", \"llm-code-explainer\"),\n        #     location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n        # )\n        # \n        client = genai.Client(\n            api_key=os.getenv(\"GEMINI_API_KEY\"),\n        )\n        model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n        response = client.models.generate_content(\n            model=model,\n            contents=[prompt]\n        )\n        response_text = response.text\n    elif(model==LLMProvider_enum.ANTHROPIC):\n        # Use Anthropic Claude\n        client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n        response = client.messages.create(\n            # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n            model=os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-20250219\"), \n            max_tokens=15000, #If have extra api budget, can increase this to 21000\n            thinking={\n                \"type\": \"enabled\", \n                \"budget_tokens\": 10000 # If have extra api budget, can increase this to 20000\n            },\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        response_text = response.content[1].text\n    else: # Assume OpenAI\n        # Use the default LLM, which is OpenAI (Use OpenAI o1/4o/gpt-4o-mini) depedning on the model & api budget\n        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        response = client.chat.completions.create(\n            model=os.environ.get(\"OPENAI_MODEL\", \"o4-mini\"),\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format={\n                \"type\": \"text\"\n            },\n            reasoning_effort=\"medium\",\n            store=False\n        )\n        response_text = response.choices[0].message.content\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you? What is the exact model are you using?\"\n    test_prompt2 = \"Hello, how bug is your model that are you using?\"\n    \n    # First call - should hit the API and return vals\n    print(\"Making call 1...\")\n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    response1 = call_llm(test_prompt, use_cache=False, model=LLMProvider_enum('openai-gpt'))\n    print(f\"Response: {response1}\\n======================\")\n\n    # Second call - should hit the cache and return vals\n    print(\"Making call 2...\")\n    response1 = call_llm(test_prompt, use_cache=True, model=LLMProvider_enum.GOOGLE)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File Index 36: LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (i.e. https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n                        (i.e. https://github.com/openai/codex/tree/cost-tracking)\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    # If URL explicitly specifies a tree/<ref>/\u2026 segment, use that\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        # Discover the real default branch from the GitHub API\n        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n\n        repo_api_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n        resp= requests.get(repo_api_url, headers=headers)\n        if resp.status_code == 200:\n            repo_data = resp.json()\n            ref = repo_data.get(\"default_branch\", \"main\")\n            print(f\"Default branch for {owner}/{repo} is {ref}..Pulling from {ref}\")\n        else:\n            print(f\"Error fetching repository data: {resp.status_code} - {resp.text}\")\n            # Fallback if the metadata call fails\n            # If no specific path is provided, use the root directory\n            # If no tree/<ref>/\u2026 segment, use the default branch\n            ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        # choose the correct endpoint: no slash for empty path\n        url_endpoint = (\n            f\"https://api.github.com/repos/{owner}/{repo}/contents\"\n            if not path else\n            f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        )\n\n\n        # url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url_endpoint, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    # repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    repo_url = \"https://github.com/openai/whisper\"\n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File Index 37: LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\n--- File Index 38: LLM-CODEBASE-DOCUMENTOR-main\\utils\\env_loader.py ---\nimport os\nimport dotenv\nimport logging\nfrom typing import Dict, Any, Optional\n\ndef load_env_vars(env_file_path: str = \".env\") -> Dict[str, str]:\n    \"\"\"\n    Load environment variables from .env file.\n    \n    Args:\n        env_file_path (str): Path to the .env file\n        \n    Returns:\n        Dict[str, str]: Dictionary of environment variables\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(env_file_path):\n        logging.warning(f\".env file not found at {env_file_path}\")\n        return {}\n    \n    # Load the .env file\n    dotenv.load_dotenv(env_file_path)\n    \n    # Return a dictionary of the loaded variables\n    env_vars = {}\n    try:\n        with open(env_file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                    \n                try:\n                    key, value = line.split('=', 1)\n                    env_vars[key.strip()] = value.strip().strip('\"\\'')\n                except ValueError:\n                    # Skip lines that don't have key=value format\n                    continue\n    except Exception as e:\n        logging.error(f\"Error reading .env file: {e}\")\n    \n    return env_vars\n\ndef get_api_keys() -> Dict[str, Optional[str]]:\n    \"\"\"\n    Get API keys from environment variables.\n    \n    Returns:\n        Dict[str, Optional[str]]: Dictionary of API keys\n    \"\"\"\n    return {\n        \"github_token\": os.environ.get(\"GITHUB_TOKEN\"),\n        \"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n        \"anthropic_api_key\": os.environ.get(\"ANTHROPIC_API_KEY\"),\n        \"gemini_api_key\": os.environ.get(\"GEMINI_API_KEY\")\n    }\n\ndef get_model_names() -> Dict[str, str]:\n    \"\"\"\n    Get model names from environment variables.\n    \n    Returns:\n        Dict[str, str]: Dictionary of model names\n    \"\"\"\n    return {\n        \"openai_model\": os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n        \"anthropic_model\": os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-latest\"),\n        \"gemini_model\": os.environ.get(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    }\n\ndef create_env_example() -> None:\n    \"\"\"\n    Create a .env.example file with placeholders.\n    \"\"\"\n    example_content = \"\"\"# API Keys\nGITHUB_TOKEN=your_github_token_here\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nGEMINI_API_KEY=your_gemini_key_here\n\n# Model Names\nGEMINI_MODEL=gemini-2.5-pro-exp-03-25\nOPENAI_MODEL=gpt-4o-mini\nANTHROPIC_MODEL=claude-3-7-sonnet-latest\n\"\"\"\n    \n    try:\n        with open(\".env.example\", 'w') as f:\n            f.write(example_content)\n        print(\"Created .env.example file\")\n    except Exception as e:\n        print(f\"Error creating .env.example file: {e}\")\n\nif __name__ == \"__main__\":\n    # Test the environment loading\n    env_vars = load_env_vars()\n    print(\"Loaded environment variables:\", list(env_vars.keys()))\n    \n    api_keys = get_api_keys()\n    print(\"API Keys available:\", [k for k, v in api_keys.items() if v])\n    \n    # Create example .env file\n    create_env_example() \n\n--- File Index 39: LLM-CODEBASE-DOCUMENTOR-main\\utils\\github_api.py ---\nimport requests\nimport os\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\n\nclass RepoMetadata:\n    \"\"\"Class to store GitHub repository metadata.\"\"\"\n    \n    def __init__(self, data: Dict[str, Any]):\n        self.name = data.get('name', '')\n        self.full_name = data.get('full_name', '')\n        self.description = data.get('description', 'No description available')\n        self.url = data.get('html_url', '')\n        self.api_url = data.get('url', '')\n        self.stars = data.get('stargazers_count', 0)\n        self.forks = data.get('forks_count', 0)\n        self.issues = data.get('open_issues_count', 0)\n        self.language = data.get('language', 'Unknown')\n        self.updated_at = data.get('updated_at', '')\n        self.readme_summary = None  # To be filled in later\n\n    def __str__(self):\n        return f\"{self.full_name} ({self.stars}\u2b50, {self.language})\"\n\ndef github_search_repos(\n    keywords: List[str], \n    filter_params: Optional[Dict[str, Any]] = None, \n    token: Optional[str] = None\n) -> List[RepoMetadata]:\n    \"\"\"\n    Search GitHub repositories using the GitHub Search API.\n    \n    Args:\n        keywords: List of keywords to search for\n        filter_params: Dictionary with filtering parameters \n            (min_stars, min_forks, language, sort_by, updated_since)\n        token: GitHub API token for authenticated requests\n    \n    Returns:\n        List of RepoMetadata objects\n    \"\"\"\n    # Default headers\n    headers = {\n        'Accept': 'application/vnd.github.v3+json'\n    }\n    \n    # Add authorization if token provided\n    if token:\n        headers['Authorization'] = f'token {token}'\n    \n    # Build query string\n    query = ' '.join(keywords)\n    \n    # Apply filters if provided\n    if filter_params:\n        if filter_params.get('min_stars'):\n            query += f\" stars:>={filter_params['min_stars']}\"\n        \n        if filter_params.get('min_forks'):\n            query += f\" forks:>={filter_params['min_forks']}\"\n        \n        if filter_params.get('language'):\n            query += f\" language:{filter_params['language']}\"\n        \n        if filter_params.get('updated_since'):\n            # Convert to ISO format for GitHub API\n            query += f\" pushed:>{filter_params['updated_since']}\"\n    \n    # Set up parameters for the search request\n    params = {\n        'q': query,\n        'sort': filter_params.get('sort_by', 'stars'),\n        'order': 'desc',\n        'per_page': 10  # Limit to 10 results\n    }\n    \n    url = 'https://api.github.com/search/repositories'\n    \n    try:\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        \n        data = response.json()\n        repos = [RepoMetadata(item) for item in data.get('items', [])]\n        \n        return repos\n    except requests.RequestException as e:\n        print(f\"GitHub API error: {e}\")\n        return []\n\ndef get_readme_content(repo_full_name: str, token: Optional[str] = None) -> str:\n    \"\"\"\n    Fetch README content for a GitHub repository.\n    \n    Args:\n        repo_full_name: Full name of the repository (owner/repo)\n        token: GitHub API token for authenticated requests\n    \n    Returns:\n        README content as string or error message\n    \"\"\"\n    # Default headers\n    headers = {\n        'Accept': 'application/vnd.github.v3.raw'\n    }\n    \n    # Add authorization if token provided\n    if token:\n        headers['Authorization'] = f'token {token}'\n    \n    url = f'https://api.github.com/repos/{repo_full_name}/readme'\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        \n        return response.text\n    except requests.RequestException as e:\n        return f\"Error fetching README: {e}\"\n\nif __name__ == \"__main__\":\n    # Test the GitHub API functions\n    token = os.environ.get('GITHUB_TOKEN')\n    test_keywords = [\"python\", \"llm\", \"framework\"]\n    test_filters = {\n        \"min_stars\": 100,\n        \"language\": \"Python\",\n        \"sort_by\": \"stars\"\n    }\n    \n    print(f\"Searching for: {test_keywords} with filters: {test_filters}\")\n    repos = github_search_repos(test_keywords, test_filters, token)\n    \n    for repo in repos:\n        print(f\"{repo.name}: {repo.stars}\u2b50 - {repo.description[:50]}...\") \n\n--- File Index 40: LLM-CODEBASE-DOCUMENTOR-main\\utils\\io_helpers.py ---\nimport os\nimport zipfile\nimport tempfile\nfrom typing import Optional\nimport shutil\nimport datetime\n\ndef zip_output_folder(folder_path: str, output_zip_path: Optional[str] = None) -> str:\n    \"\"\"\n    Create a ZIP archive of the output folder.\n    \n    Args:\n        folder_path (str): Path to the folder to zip\n        output_zip_path (str, optional): Path for the output ZIP file.\n                                         If None, a temporary file is created.\n    \n    Returns:\n        str: Path to the created ZIP file\n    \"\"\"\n    if not os.path.exists(folder_path):\n        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n    \n    if not output_zip_path:\n        # Generate a default filename based on the folder name and timestamp\n        folder_name = os.path.basename(folder_path)\n        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        output_zip_path = os.path.join(\n            tempfile.gettempdir(), \n            f\"{folder_name}_{timestamp}.zip\"\n        )\n    \n    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        # Walk through all files in the directory\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                # Calculate path relative to the folder root for the archive\n                arcname = os.path.relpath(file_path, folder_path)\n                zipf.write(file_path, arcname)\n    \n    return output_zip_path\n\ndef load_env_vars(env_file_path: str) -> None:\n    \"\"\"\n    Load environment variables from .env file if dotenv module is available.\n    Fallback to manual loading if not.\n    \n    Args:\n        env_file_path (str): Path to the .env file\n    \"\"\"\n    try:\n        from dotenv import load_dotenv\n        load_dotenv(env_file_path)\n    except ImportError:\n        print(\"python-dotenv not found, using manual .env loading\")\n        # Manual loading of .env as fallback\n        try:\n            with open(env_file_path, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    key, value = line.split('=', 1)\n                    os.environ[key.strip()] = value.strip().strip('\"\\'')\n        except Exception as e:\n            print(f\"Error loading .env file: {e}\")\n\ndef read_markdown_file(file_path: str) -> str:\n    \"\"\"\n    Read a Markdown file and return its content as a string.\n    \n    Args:\n        file_path (str): Path to the Markdown file\n    \n    Returns:\n        str: Content of the Markdown file\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\ndef read_all_markdown_files(directory: str) -> dict:\n    \"\"\"\n    Read all Markdown files in a directory.\n    \n    Args:\n        directory (str): Path to the directory\n    \n    Returns:\n        dict: Dictionary with filenames as keys and content as values\n    \"\"\"\n    result = {}\n    try:\n        for filename in os.listdir(directory):\n            if filename.endswith('.md'):\n                file_path = os.path.join(directory, filename)\n                result[filename] = read_markdown_file(file_path)\n    except Exception as e:\n        print(f\"Error reading directory: {e}\")\n    \n    return result\n\nif __name__ == \"__main__\":\n    # Test zipping a directory\n    test_dir = \".\"\n    zip_path = zip_output_folder(test_dir)\n    print(f\"Created ZIP file: {zip_path}\") \n\n--- File Index 41: LLM-CODEBASE-DOCUMENTOR-main\\utils\\search_helpers.py ---\nfrom utils.call_llm import call_llm, LLMProvider_enum\n\ndef extract_keywords(query_input: str, llm_provider=None) -> list:\n    \"\"\"\n    Extract search keywords from a natural language query using LLM.\n    \n    Args:\n        query_input (str): Natural language query string\n        llm_provider: LLM provider to use (default: None, which uses the default provider)\n        \n    Returns:\n        list: List of extracted keywords for GitHub search\n    \"\"\"\n    # prompt = f\"\"\"\n    # Extract the most relevant search keywords from this natural language query to find GitHub repositories relevant to my natural text.\n    # Return just a JSON array of strings, with no explanation.\n    \n    # Natural language query: \"{query_input}\"\n    \n    # Example output format:\n    # [\"keyword1\", \"keyword2\", \"keyword3\"]\n    # \"\"\"\n    \n\n    prompt = f\"\"\"\n    Rewrite my request from a natural language query into a simple 3-4 word query to find only relevant GitHub repositories to my natural text.\n    Return just a JSON array of strings, with no explanation.\n    \n    Natural language query: \"{query_input}\"\n    \n    Example output format:\n    [\"keyword1\", \"keyword2\", \"keyword3\"]\n    \"\"\"\n\n    try:\n        # Use the specified provider if available, otherwise use default\n        response = call_llm(prompt, model=llm_provider if llm_provider else LLMProvider_enum.GOOGLE)\n        # Simple parsing to extract array content\n        if response.strip().startswith('[') and response.strip().endswith(']'):\n            # Extract items between brackets and split by commas\n            keywords_str = response.strip()[1:-1]\n            # Split by commas and clean up each keyword\n            keywords = [k.strip().strip('\"\\'') for k in keywords_str.split(',')]\n            return keywords\n        else:\n            # Fallback: split the query into words, filter out common words\n            simple_keywords = [word for word in query_input.split() \n                              if len(word) > 3 and word.lower() not in {\n                                  'the', 'and', 'for', 'that', 'with', 'this', 'what', 'how'\n                              }]\n            return simple_keywords\n    except Exception as e:\n        print(f\"Error extracting keywords: {e}\")\n        # Fallback to simple keyword extraction on failure\n        return query_input.split()\n\ndef looks_like_url(text: str) -> bool:\n    \"\"\"\n    Check if input text looks like a URL.\n    \n    Args:\n        text (str): Input text to check\n        \n    Returns:\n        bool: True if text appears to be a URL\n    \"\"\"\n    return text.startswith(('http://', 'https://')) and 'github.com' in text.lower() \n\n--- File Index 42: LLM-CODEBASE-DOCUMENTOR-main\\utils\\summarizer.py ---\nfrom utils.call_llm import call_llm, LLMProvider_enum\nimport re\n\ndef summarize_readme(readme_text: str, max_length: int = 500, llm_provider=None) -> str:\n    \"\"\"\n    Generate a concise summary of a repository README using LLM.\n    \n    Args:\n        readme_text (str): README content as text\n        max_length (int): Maximum length of the summary in characters\n        llm_provider: LLM provider to use (default: None, which uses the default provider)\n        \n    Returns:\n        str: Concise summary of the README\n    \"\"\"\n    # Limit input length to prevent context overflow\n    truncated_text = truncate_text(readme_text, 6000)  # Limit to ~6k chars\n    \n    prompt = f\"\"\"\n    Provide a clear, concise summary of the following repository README.\n    Focus on what the repository does, key features, and its purpose.\n    Keep your response to about 2-3 sentences, under 100 words.\n    \n    README CONTENT:\n    {truncated_text}\n    \n    YOUR SUMMARY:\n    \"\"\"\n    \n    try:\n        # Use the specified provider if available, otherwise use default\n        summary = call_llm(prompt, model=llm_provider if llm_provider else LLMProvider_enum.GOOGLE)\n        # Ensure the summary doesn't exceed max_length\n        if len(summary) > max_length:\n            summary = summary[:max_length-3] + \"...\"\n        return summary\n    except Exception as e:\n        print(f\"Error summarizing README: {e}\")\n        # Fallback to manual extraction\n        return extract_first_paragraph(truncated_text, max_length)\n\ndef truncate_text(text: str, max_chars: int) -> str:\n    \"\"\"\n    Truncate text to maximum character limit while preserving complete sentences.\n    \n    Args:\n        text (str): Text to truncate\n        max_chars (int): Maximum number of characters\n        \n    Returns:\n        str: Truncated text\n    \"\"\"\n    if len(text) <= max_chars:\n        return text\n    \n    # Find the last sentence boundary before max_chars\n    truncated = text[:max_chars]\n    last_period = truncated.rfind('.')\n    last_question = truncated.rfind('?')\n    last_exclamation = truncated.rfind('!')\n    \n    # Find the last sentence boundary\n    last_sentence_end = max(last_period, last_question, last_exclamation)\n    \n    if last_sentence_end > 0:\n        return text[:last_sentence_end + 1] + \"...\"\n    else:\n        # If no sentence boundary found, just truncate\n        return truncated + \"...\"\n\ndef extract_first_paragraph(text: str, max_length: int = 500) -> str:\n    \"\"\"\n    Extract the first paragraph from a text as a fallback summary method.\n    \n    Args:\n        text (str): Text to extract from\n        max_length (int): Maximum length of the extracted paragraph\n        \n    Returns:\n        str: Extracted paragraph\n    \"\"\"\n    # Remove Markdown formatting\n    cleaned_text = re.sub(r'#+ ', '', text)\n    cleaned_text = re.sub(r'\\[([^\\]]+)\\]\\([^)]+\\)', r'\\1', cleaned_text)  # Replace links with text\n    cleaned_text = re.sub(r'!\\[([^\\]]*)\\]\\([^)]+\\)', '', cleaned_text)  # Remove images\n    \n    # Split by double newlines (paragraph breaks)\n    paragraphs = re.split(r'\\n\\s*\\n', cleaned_text)\n    \n    # Find first non-empty paragraph\n    for paragraph in paragraphs:\n        paragraph = paragraph.strip()\n        if paragraph and len(paragraph) > 30:  # Ensure it's substantial\n            if len(paragraph) > max_length:\n                return paragraph[:max_length-3] + \"...\"\n            return paragraph\n    \n    # Fallback to first 100 characters\n    return cleaned_text[:min(max_length, len(cleaned_text))] \n\n--- File Index 43: LLM-CODEBASE-DOCUMENTOR-main\\utils\\__init__.py ---\n\n\n\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        - 0 # LLM-CODEBASE-DOCUMENTOR-main\\flow.py\n- 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py\n- 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py\n- 3 # LLM-CODEBASE-DOCUMENTOR-main\\README.md\n- 4 # LLM-CODEBASE-DOCUMENTOR-main\\requirements.txt\n- 5 # LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py\n- 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md\n- 7 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design2.md\n- 8 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\_config.yml\n- 9 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\01_client_session.md\n- 10 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\02_tools.md\n- 11 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\03_resources.md\n- 12 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\04_prompts.md\n- 13 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\05_fastmcp_server.md\n- 14 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\06_transports.md\n- 15 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\07_fastmcp_context.md\n- 16 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\08_mcp_protocol_types.md\n- 17 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\index.md\n- 18 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\01_model_input_shape.md\n- 19 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\02_tuning_hyperparameter_space.md\n- 20 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\03_training_constants.md\n- 21 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\04_hyperparameter_tuning_setup.md\n- 22 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\05_keras_tuner_oracle.md\n- 23 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\06_tuning_trial_configuration___state.md\n- 24 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\07_project_dependencies.md\n- 25 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4002-Group-Assignment\\index.md\n- 26 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\01_web_application_interface.md\n- 27 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\02_search_query_construction.md\n- 28 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\03_search_execution__solr_.md\n- 29 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\04_data_visualization.md\n- 30 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\05_data_processing_pipeline.md\n- 31 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\06_data_enrichment__sentiment_category_.md\n- 32 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\07_data_indexing.md\n- 33 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\08_search_engine_management.md\n- 34 # LLM-CODEBASE-DOCUMENTOR-main\\output\\SC4021-Information-Retrieval\\index.md\n- 35 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py\n- 36 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_github_files.py\n- 37 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_local_files.py\n- 38 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\env_loader.py\n- 39 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\github_api.py\n- 40 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\io_helpers.py\n- 41 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\search_helpers.py\n- 42 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\summarizer.py\n- 43 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\__init__.py\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```": "```yaml\n- name: PocketFlow Workflow\n  description: |\n    The core engine orchestrating the tutorial generation process. It defines a sequence of steps (Nodes) where the output of one step feeds into the next. Think of it as an assembly line for creating the documentation. It manages the flow of data and execution order.\n  file_indices:\n    - 0 # LLM-CODEBASE-DOCUMENTOR-main\\flow.py\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py\n    - 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md\n\n- name: Node Abstraction\n  description: |\n    Individual processing units within the PocketFlow workflow. Each Node performs a specific task, like fetching code, analyzing relationships, or writing a chapter. Nodes have `prep`, `exec`, and `post` stages, making the workflow modular and testable. Examples include `FetchRepo`, `IdentifyAbstractions`, `WriteChapters`.\n  file_indices:\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py\n    - 0 # LLM-CODEBASE-DOCUMENTOR-main\\flow.py\n    - 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md\n\n- name: Code Fetching & Parsing\n  description: |\n    The initial step responsible for acquiring the source code from either a remote GitHub repository (using the GitHub API) or a local directory. It handles file filtering based on include/exclude patterns and size limits, preparing the codebase content for analysis.\n  file_indices:\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py (FetchRepo node)\n    - 36 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_github_files.py\n    - 37 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_local_files.py\n    - 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py (Handles CLI args for source)\n\n- name: LLM Analysis & Generation\n  description: |\n    Leverages Large Language Models (LLMs like Gemini, Claude, OpenAI) to understand the codebase and generate human-friendly explanations. This involves identifying key abstractions, analyzing their relationships, ordering tutorial chapters logically, and writing the actual chapter content.\n  file_indices:\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py (IdentifyAbstractions, AnalyzeRelationships, OrderChapters, WriteChapters nodes)\n    - 35 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py\n    - 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py (LLM selection argument)\n\n- name: Shared Data Store\n  description: |\n    A central Python dictionary (`shared`) that holds the state and data throughout the workflow execution. Nodes read input data from this store and write their results back into it, allowing information to flow between different processing steps.\n  file_indices:\n    - 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py (Initializes shared store)\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py (Nodes read/write to shared store via prep/post)\n    - 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md (Defines shared store structure)\n\n- name: Tutorial Structure Generation\n  description: |\n    The process of organizing the identified abstractions and generated content into a coherent tutorial structure. This includes determining the optimal chapter order based on relationships and creating the final `index.md` with summaries, diagrams, and links, along with individual chapter markdown files.\n  file_indices:\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py (OrderChapters, WriteChapters, CombineTutorial nodes)\n    - 17 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\index.md (Example output index)\n    - 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md\n\n- name: Streamlit UI & Search Flow\n  description: |\n    Provides a user-friendly web interface built with Streamlit. It includes features for smart repository searching (handling URLs, local paths, and natural language queries), filtering search results, selecting a repository, and displaying the generated tutorial content directly in the browser, including a download option.\n  file_indices:\n    - 5 # LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py\n    - 0 # LLM-CODEBASE-DOCUMENTOR-main\\flow.py (create_streamlit_flow function)\n    - 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py (SmartSearchRepo, FilterRepos, SelectRepository, RenderAndDownload nodes)\n    - 7 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design2.md\n\n- name: Configuration & Environment Management\n  description: |\n    Handles user inputs and environment settings. This includes parsing command-line arguments (for the CLI) or web inputs (for Streamlit), managing API keys securely using `.env` files, and setting up default file inclusion/exclusion patterns.\n  file_indices:\n    - 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py\n    - 38 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\env_loader.py\n    - 5 # LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py (Sidebar for API keys/settings)\n    - 35 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py (Uses env vars for keys)\n```", "\nBased on the following abstractions and relevant code snippets from the project `LLM -CODEBASE-DOCUMENTOR`:\n\nList of Abstraction Indices and Names:\n0 # PocketFlow Workflow\n1 # Node Abstraction\n2 # Code Fetching & Parsing\n3 # LLM Analysis & Generation\n4 # Shared Data Store\n5 # Tutorial Structure Generation\n6 # Streamlit UI & Search Flow\n7 # Configuration & Environment Management\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: PocketFlow Workflow (Relevant file indices: [0, 2, 6])\n  Description: The core engine orchestrating the tutorial generation process. It defines a sequence of steps (Nodes) where the output of one step feeds into the next. Think of it as an assembly line for creating the documentation. It manages the flow of data and execution order.\n\n- Index 1: Node Abstraction (Relevant file indices: [0, 2, 6])\n  Description: Individual processing units within the PocketFlow workflow. Each Node performs a specific task, like fetching code, analyzing relationships, or writing a chapter. Nodes have `prep`, `exec`, and `post` stages, making the workflow modular and testable. Examples include `FetchRepo`, `IdentifyAbstractions`, `WriteChapters`.\n\n- Index 2: Code Fetching & Parsing (Relevant file indices: [1, 2, 36, 37])\n  Description: The initial step responsible for acquiring the source code from either a remote GitHub repository (using the GitHub API) or a local directory. It handles file filtering based on include/exclude patterns and size limits, preparing the codebase content for analysis.\n\n- Index 3: LLM Analysis & Generation (Relevant file indices: [1, 2, 35])\n  Description: Leverages Large Language Models (LLMs like Gemini, Claude, OpenAI) to understand the codebase and generate human-friendly explanations. This involves identifying key abstractions, analyzing their relationships, ordering tutorial chapters logically, and writing the actual chapter content.\n\n- Index 4: Shared Data Store (Relevant file indices: [1, 2, 6])\n  Description: A central Python dictionary (`shared`) that holds the state and data throughout the workflow execution. Nodes read input data from this store and write their results back into it, allowing information to flow between different processing steps.\n\n- Index 5: Tutorial Structure Generation (Relevant file indices: [2, 6, 17])\n  Description: The process of organizing the identified abstractions and generated content into a coherent tutorial structure. This includes determining the optimal chapter order based on relationships and creating the final `index.md` with summaries, diagrams, and links, along with individual chapter markdown files.\n\n- Index 6: Streamlit UI & Search Flow (Relevant file indices: [0, 2, 5, 7])\n  Description: Provides a user-friendly web interface built with Streamlit. It includes features for smart repository searching (handling URLs, local paths, and natural language queries), filtering search results, selecting a repository, and displaying the generated tutorial content directly in the browser, including a download option.\n\n- Index 7: Configuration & Environment Management (Relevant file indices: [1, 5, 35, 38])\n  Description: Handles user inputs and environment settings. This includes parsing command-line arguments (for the CLI) or web inputs (for Streamlit), managing API keys securely using `.env` files, and setting up default file inclusion/exclusion patterns.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 0 # LLM-CODEBASE-DOCUMENTOR-main\\flow.py ---\nfrom pocketflow import Flow\n\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial,\n    # New nodes for Streamlit UI\n    SmartSearchRepo,\n    FilterRepos,\n    SelectRepository,\n    RenderAndDownload\n)\n\n\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow, fetch_repo, combine_tutorial\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n    \n    # Create the original tutorial flow and get the first and last nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n    \n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n    \n    # Connect the UI nodes\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n    \n    # Connect the select_repo node to the FetchRepo node\n    select_repo >> fetch_repo\n    \n    # Add the render and download node after CombineTutorial\n    combine_tutorial >> render_download\n    \n    # Create the full UI flow\n    ui_flow = Flow(start=smart_search)\n    \n    return ui_flow\n\n--- File: 1 # LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: 2 # LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: 5 # LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py ---\nimport streamlit as st\nimport os\nimport time\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\n\n# Import utilities\nfrom utils.env_loader import load_env_vars, get_api_keys, get_model_names, create_env_example\nfrom utils.call_llm import LLMProvider_enum\nfrom utils.github_api import RepoMetadata\n\n# Import the flow\nfrom flow import create_streamlit_flow\nfrom nodes import SmartSearchRepo, FilterRepos, SelectRepository, RenderAndDownload\n\n# Load environment variables\nload_env_vars()\n\n# 1. Page configuration (with a custom About menu item)\nst.set_page_config(\n    page_title=\"\ud83d\udcd6\ud83e\udd13 LLM Codebase Finder & Documentor\",\n    page_icon=\"\ud83e\uddd0\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'About': (\n            \"### LLM Codebase Documentor\\n\"\n            \"A&nbsp;Streamlit app to auto-generate beginner-friendly tutorials from any GitHub repo.\\n\\n\"\n            \"Built by **TEG SINGH TIWANA** for Cloud Assignment 2:\\n\"\n            \"[GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n        )\n    }\n)\n# Initialize session state if not already done\nif \"initialized\" not in st.session_state:\n    st.session_state.initialized = True\n    st.session_state.search_results = []\n    st.session_state.selected_repo = None\n    st.session_state.filter_params = {\n        \"min_stars\": 0,\n        \"min_forks\": 0,\n        \"language\": \"\",\n        \"sort_by\": \"stars\",\n        \"updated_since\": \"\"\n    }\n    st.session_state.shared_store = {}\n    st.session_state.markdown_files = {}\n    st.session_state.zip_path = None\n    st.session_state.task_completed = False\n    st.session_state.ui_view = \"search\"  # Possible values: search, results, tutorial\n    \n    # Get API keys from environment\n    api_keys = get_api_keys()\n    for key, value in api_keys.items():\n        st.session_state[key] = value or \"\"\n    \n    # Get model names from environment\n    model_names = get_model_names()\n    for key, value in model_names.items():\n        st.session_state[key] = value\n\n# --- Sidebar ---\nwith st.sidebar:\n    st.title(\"LLM Codebase Documentor\")\n    \n    # Create tabs for different sidebar sections\n    tab1, tab2, tab3 = st.tabs([\"Search\", \"API Keys\", \"Model Settings\"])\n    \n    with tab1:\n        # Search input and type\n        st.subheader(\"Repository Search\")\n        query_input = st.text_input(\n            \"Enter GitHub URL, local path, or natural language query:\",\n            placeholder=\"e.g., https://github.com/user/repo OR frameworks for UI prototyping\"\n        )\n        \n        # Filters (only shown for natural language search)\n        st.subheader(\"Filters\")\n        min_stars = st.number_input(\"Minimum Stars\", min_value=0, value=st.session_state.filter_params[\"min_stars\"])\n        min_forks = st.number_input(\"Minimum Forks\", min_value=0, value=st.session_state.filter_params[\"min_forks\"])\n        language = st.selectbox(\n            \"Language\", \n            [\"\", \"Python\", \"JavaScript\", \"TypeScript\", \"Java\", \"Go\", \"Rust\", \"C++\", \"Ruby\", \"PHP\"],\n            index=0\n        )\n        sort_by = st.selectbox(\n            \"Sort By\", \n            [\"stars\", \"forks\", \"updated\", \"help-wanted-issues\"],\n            index=0\n        )\n        updated_since = st.date_input(\n            \"Updated Since\", \n            value=None\n        )\n        \n        # Update filter params in session state\n        if st.button(\"Search\", key=\"search_button\"):\n            # Update filter params\n            st.session_state.filter_params = {\n                \"min_stars\": min_stars,\n                \"min_forks\": min_forks,\n                \"language\": language,\n                \"sort_by\": sort_by,\n                \"updated_since\": updated_since.isoformat() if updated_since else \"\"\n            }\n            \n            # Initialize shared store for the flow\n            st.session_state.shared_store = {\n                \"query_input\": query_input,\n                \"filter_params\": st.session_state.filter_params,\n                \"github_token\": st.session_state.github_token,\n                \"llm_provider\": LLMProvider_enum(st.session_state.provider_selection),\n                \n                # Default parameters from original flow\n                \"include_patterns\": {\n                    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n                    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n                    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n                    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n                    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n                    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \"*.md\", \"*.pdf\"\n                },\n                \"exclude_patterns\": {\n                    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n                    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n                    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n                },\n                \"max_file_size\": 300000,\n                \"output_dir\": \"output\"\n            }\n            \n            # Create the flow\n            flow = create_streamlit_flow()\n            \n            # Run the SmartSearchRepo and FilterRepos nodes\n            with st.spinner(\"Processing query...\"):\n                # Set state to indicate we're waiting for results\n                st.session_state.ui_view = \"results\"\n                \n                # Run the SmartSearch and FilterRepos nodes\n                smart_search = SmartSearchRepo()\n                filter_repos = FilterRepos()\n                \n                # Run nodes sequentially\n                smart_search.run(st.session_state.shared_store)\n                filter_repos.run(st.session_state.shared_store)\n                \n                # Update session state with search results\n                if \"search_results\" in st.session_state.shared_store:\n                    st.session_state.search_results = st.session_state.shared_store[\"search_results\"]\n                \n                # If we have a selected_repo, run the full pipeline\n                if \"selected_repo\" in st.session_state.shared_store:\n                    st.session_state.selected_repo = st.session_state.shared_store[\"selected_repo\"]\n                    with st.spinner(\"Generating tutorial...\"):\n                        st.session_state.ui_view = \"tutorial\"\n                        # Run the full flow\n                        flow.run(st.session_state.shared_store)\n                        \n                        # Get the rendered Markdown files and ZIP path if available\n                        if \"markdown_files\" in st.session_state.shared_store:\n                            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n                        if \"zip_path\" in st.session_state.shared_store:\n                            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n                        \n                        st.session_state.task_completed = True\n    \n    with tab2:\n        st.subheader(\"API Keys\")\n        \n        # GitHub token\n        github_token = st.text_input(\n            \"GitHub Token\",\n            value=st.session_state.github_token,\n            type=\"password\",\n            help=\"Required for private repos or to avoid rate limits\"\n        )\n        st.session_state.github_token = github_token\n        \n        # LLM API keys\n        openai_api_key = st.text_input(\n            \"OpenAI API Key\",\n            value=st.session_state.openai_api_key,\n            type=\"password\"\n        )\n        st.session_state.openai_api_key = openai_api_key\n        \n        anthropic_api_key = st.text_input(\n            \"Anthropic API Key\",\n            value=st.session_state.anthropic_api_key,\n            type=\"password\"\n        )\n        st.session_state.anthropic_api_key = anthropic_api_key\n        \n        gemini_api_key = st.text_input(\n            \"Google Gemini API Key\",\n            value=st.session_state.gemini_api_key,\n            type=\"password\"\n        )\n        st.session_state.gemini_api_key = gemini_api_key\n        \n        # Save API keys to environment\n        if st.button(\"Save API Keys\"):\n            os.environ[\"GITHUB_TOKEN\"] = github_token\n            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n            os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n            os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n            st.success(\"API keys saved to environment!\")\n    \n    with tab3:\n        st.subheader(\"Model Settings\")\n        \n        # LLM Provider selection\n        provider_selection = st.selectbox(\n            \"LLM Provider\",\n            options=[provider.value for provider in LLMProvider_enum],\n            index=0\n        )\n        st.session_state.provider_selection = provider_selection\n        \n        # Model name based on provider\n        if provider_selection == LLMProvider_enum.OPENAI.value:\n            model_name = st.text_input(\n                \"OpenAI Model\",\n                value=st.session_state.openai_model\n            )\n            st.session_state.openai_model = model_name\n            os.environ[\"OPENAI_MODEL\"] = model_name\n        \n        elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n            model_name = st.text_input(\n                \"Anthropic Model\",\n                value=st.session_state.anthropic_model\n            )\n            st.session_state.anthropic_model = model_name\n            os.environ[\"ANTHROPIC_MODEL\"] = model_name\n        \n        else:  # Gemini\n            model_name = st.text_input(\n                \"Google Gemini Model\",\n                value=st.session_state.gemini_model\n            )\n            st.session_state.gemini_model = model_name\n            os.environ[\"GEMINI_MODEL\"] = model_name\n            \n        # Add save button for model settings\n        if st.button(\"Save Model Settings\"):\n            # Save provider selection to session state\n            st.session_state.provider_selection = provider_selection\n            st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(provider_selection)\n            \n            # Save model name based on provider\n            if provider_selection == LLMProvider_enum.OPENAI.value:\n                os.environ[\"OPENAI_MODEL\"] = model_name\n                st.session_state.openai_model = model_name\n            elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n                os.environ[\"ANTHROPIC_MODEL\"] = model_name\n                st.session_state.anthropic_model = model_name\n            else:  # Gemini\n                os.environ[\"GEMINI_MODEL\"] = model_name\n                st.session_state.gemini_model = model_name\n                \n            st.success(f\"Model settings saved! Provider: {provider_selection}, Model: {model_name}\")\n\n# --- Main Content ---\ndef display_repo_card(repo: RepoMetadata, index: int):\n    \"\"\"Display a repository card with details and select button.\"\"\"\n    col1, col2 = st.columns([1, 4])\n    \n    with col1:\n        st.image(\"https://github.githubassets.com/favicons/favicon.png\", width=50)\n        st.button(f\"Select\", key=f\"select_{index}\", on_click=select_repository, args=(repo.url,))\n    \n    with col2:\n        st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n        st.markdown(f\"**Description:** {repo.description}\")\n        st.markdown(f\"**Summary:** {repo.readme_summary}\")\n        st.markdown(f\"**Language:** {repo.language} | **Stars:** {repo.stars} | **Forks:** {repo.forks}\")\n\ndef select_repository(repo_url: str):\n    \"\"\"Handle repository selection from search results.\"\"\"\n    st.session_state.selected_repo = repo_url\n    \n    # Make sure we have the shared store initialized\n    if not st.session_state.shared_store:\n        st.session_state.shared_store = {}\n    \n    # Set the selected repository URL in the shared store\n    st.session_state.shared_store[\"selected_repo\"] = repo_url\n    st.session_state.shared_store[\"repo_url\"] = repo_url\n    st.session_state.shared_store[\"local_dir\"] = None\n    \n    # Make sure we have other necessary parameters in the shared store\n    if \"include_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"include_patterns\"] = {\n            \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n            \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n            \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n            \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n            \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n            \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n        }\n    \n    if \"exclude_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"exclude_patterns\"] = {\n            \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n            \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n            \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n        }\n        \n    if \"max_file_size\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"max_file_size\"] = 300000\n        \n    if \"output_dir\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"output_dir\"] = \"output\"\n        \n    if \"github_token\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"github_token\"] = st.session_state.github_token\n        \n    if \"llm_provider\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(st.session_state.provider_selection)\n    \n    st.session_state.ui_view = \"tutorial\"\n    \n    # Create and run the flow\n    flow = create_streamlit_flow()\n    with st.spinner(\"Generating tutorial...\"):\n        # Run the full flow\n        flow.run(st.session_state.shared_store)\n        \n        # Get the rendered Markdown files and ZIP path if available\n        if \"markdown_files\" in st.session_state.shared_store:\n            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n        if \"zip_path\" in st.session_state.shared_store:\n            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n        \n        st.session_state.task_completed = True\n\n# Conditional display based on current view\nif st.session_state.ui_view == \"search\":\n    # Show welcome message and instructions\n    st.title(\"Welcome to **GITHUB** Finder & Documentor\ud83d\udcd6\ud83e\udd13\")\n    st.markdown(\"\"\"\n    **Built by [TEG SINGH TIWANA](https://github.com/tej172)** for _Cloud Assignment 2_:  \n    [GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n    #### Welcome to **LLM Codebase Documentor**, your friendly assistant for auto-generating beginner-friendly tutorials from any codebase! \n    ###### This tool helps you understand codebases by generating detailed tutorials explaining core abstractions, relationships, and code organization.\n    \n    ### Getting Started\n    1. In the **sidebar**, enter one of:\n       - A **GitHub URL** (e.g. `https://github.com/user/repo`)\n       - A **local folder path** on your machine\n       - A **natural language** query (e.g. \"frameworks for UI prototyping\")\n    2. If you chose natural language, adjust **advanced filters** (stars, forks, language, updated date).\n    3. Click **Search** to fetch repositories or **Generate** to build the tutorial.\n    \n    ### Features\n    - \ud83d\udd0d **Smart Search** powered by GPT / Gemini / Claude for effortless keyword extraction  \n    - \ud83d\udcc2 **Local Path** support\u2014point to any folder you've already cloned  \n    - \u2b50 **Advanced Filters**\u2014stars, forks, issues, language, last\u2010updated  \n    - \ud83d\udcc4 **Markdown Preview**\u2014click through `index.md` and each chapter in-app  \n    - \ud83d\udce5 **Downloadable ZIP**\u2014grab your entire tutorial with one click  \n\n    > _Tip_: Make sure your **API keys** (GITHUB_TOKEN, OPENAI_API_KEY, GEMINI_API_KEY, etc.) are set in the sidebar or in your `.env` file.\n    \"\"\")\n    \nelif st.session_state.ui_view == \"results\":\n    # Show search results\n    st.title(\"Search Results\")\n    \n    if st.session_state.search_results:\n        st.markdown(f\"Found {len(st.session_state.search_results)} repositories matching your query.\")\n        for i, repo in enumerate(st.session_state.search_results):\n            with st.container():\n                display_repo_card(repo, i)\n                st.markdown(\"---\")\n    else:\n        st.info(\"No results found. Try different keywords or filters.\")\n        \nelif st.session_state.ui_view == \"tutorial\":\n    # Show the generated tutorial\n    project_name = st.session_state.shared_store.get(\"project_name\", \"Repository\")\n    st.title(f\"Tutorial for {project_name}\")\n    \n    # Download button for ZIP if available\n    if st.session_state.zip_path:\n        with open(st.session_state.zip_path, \"rb\") as f:\n            st.download_button(\n                label=\"Download Tutorial ZIP\",\n                data=f,\n                file_name=f\"{project_name}_tutorial.zip\",\n                mime=\"application/zip\"\n            )\n    \n    # Display the Markdown files\n    if st.session_state.markdown_files:\n        # First show index.md if it exists\n        if \"index.md\" in st.session_state.markdown_files:\n            st.markdown(st.session_state.markdown_files[\"index.md\"])\n            st.markdown(\"---\")\n        \n        # Then show all other files in order\n        for filename in sorted([f for f in st.session_state.markdown_files if f != \"index.md\"]):\n            with st.expander(f\"{filename}\", expanded=True):\n                st.markdown(st.session_state.markdown_files[filename])\n    else:\n        st.info(\"Generating tutorial content... Please wait.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\n    \"LLM Codebase Documentor NTU \u00a9 2025 | \"\n    \"[View on GitHub](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2) |  \"\n    \"Created by [**TEG SINGH TIWANA (U2122816B)**](https://github.com/tej172) | \"\n    \"[Cloud Assignment 2 \u2013 GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n)\n\n# Create .env.example file if it doesn't exist\nif not os.path.exists(\".env.example\"):\n    create_env_example() \n\n--- File: 6 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n--- File: 7 # LLM-CODEBASE-DOCUMENTOR-main\\docs\\design2.md ---\n\n## layout: default title: \"System Design 2: Codebase Knowledge Builder + Streamlit Frontend\" nav\\_order: 3\n\n# System Design 2: Codebase Knowledge Builder + Streamlit Frontend \n\n### (System Design 2 is a extention from System Design: Codebase Knowledge Builder(in LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md) that is alredy built in this code base)\n\n> Builds on the implemented [design.md](docs/design.md) by adding:\n>\n> 1. A Streamlit-based web UI for interactive query input, credential management, filtering, and Markdown rendering.\n> 2. A GitHub API\u2013powered Smart Search that accepts natural\u2011language queries, direct URLs, or local paths.\n> 3. Advanced repository filters (stars, forks, issues, updated date) with clickable results driving the existing tutorial pipeline.\n\n## Requirements\n\n> **Notes for AI:** Mirror the detail of `design.md`, mapping high\u2011level user stories to concrete inputs/outputs.\n\n### User Stories\n\n1. **Tutorial Generation** (existing): As a developer onboarding a new codebase, I want an automated tutorial with core abstractions, relationships, and beginner\u2011friendly descriptions, so I can understand project structure quickly. ([design.md])\n2. **Web Interface**: As a user, I want a browser UI where I can:\n   - Enter a GitHub URL, natural language query, or local folder path.\n   - Configure LLM and GitHub API credentials.\n   - Apply repository filters.\n   - Trigger tutorial generation and view/download results\u2014all without leaving the page.\n3. **Smart GitHub Search**: As a researcher or student, I want to search GitHub by:\n   - **Natural Language**: e.g. \u201cframeworks for UI prototyping\u201d, converting text to search keywords via LLM.\n   - **Direct URL**: paste a repo URL to skip search.\n   - **Local Path**: select an existing clone to use directly.\n4. **Advanced Filtering**: As a power user, I want to sort/filter search results by stars, forks, issues, language, and last-updated to identify the most relevant repositories.\n5. **Seamless Pipeline Integration**: After selecting a repo, the existing tutorial-generation flow runs and displays rendered Markdown files inline, with an option to download a ZIP archive.\n\n### Inputs & Outputs\n\n| Category            | Inputs                                                                                                                                                 | Outputs                                                                                     |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |\n| **Search & Config** | - `query_input`: NL text, GitHub URL, or local path- `filter_params`: stars, forks, issues, language, updated- `.env` API credentials & model settings | - `search_results`: up to 10 `RepoMetadata`- Summary of each README                         |\n| **Tutorial Flow**   | - `repo_url` or `local_dir` set by selection                                                                                                           | - `index.md`, Mermaid diagram, ordered chapter `.md` files- ZIP archive of output directory |\n| **UI Rendering**    | - Generated Markdown files and ZIP path                                                                                                                | - Inline preview via `st.markdown`- `st.download_button` for ZIP                            |\n\n## End\u2011to\u2011End Flow Design\n\n```mermaid\nflowchart TD\n  UI[Streamlit UI] --> S1[SmartSearchRepo]\n  S1 --> F1[FilterRepos]\n  F1 --> Select[SelectRepository]\n  Select --> Fetch[FetchRepo]\n  Fetch --> Identify[IdentifyAbstractions]\n  Identify --> Analyze[AnalyzeRelationships]\n  Analyze --> Order[OrderChapters]\n  Order --> Batch[WriteChapters]\n  Batch --> Combine[CombineTutorial]\n  Combine --> UI_Display[Render & Download]\n```\n\n1. `StreamlitUI` (New Frontend)\n\n   - **Type:** Regular node\n   - **Responsibilities:**\n     - Present sidebar inputs: Query box, filter widgets, credential inputs, model selector, and action buttons.\n     - Display either search results (repo cards) or tutorial previews (Markdown pages).\n     - Handle user interactions: card clicks trigger downstream nodes.\n     - Provide ZIP download.\n\n2. `SmartSearchRepo`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.query_input`, detect `mode`:\n     ```python\n     if looks_like_url(query_input): mode='url'\n     elif os.path.isdir(query_input): mode='local'\n     else: mode='nl'\n     ```\n   - **Exec:**\n     - **NL Mode:** call `extract_keywords(query_input)` \u2192 keywords list, join as search string.\n     - **URL Mode:** validate URL, set `selected_repo`.\n     - **Local Mode:** verify path, set `selected_repo`.\n   - **Post:** Write `shared.search_mode`, `shared.keywords`, and (for URL/local) `shared.selected_repo`.\n\n3. `FilterRepos`\n\n   - **Type:** Regular node\n   - **Prep:** Read `search_mode`, `keywords`, `filter_params`, `github_token`.\n   - **Exec:**\n     - If NL mode: `github_search_repos(keywords, filter_params, token)`\n     - Else (URL/local): skip API search.\n   - **Post:** Store top-10 `shared.search_results` (`RepoMetadata` + README summary via `summarize_readme`).\n\n4. `SelectRepository`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.search_results`.\n   - **Exec:** On UI card click, set `shared.selected_repo` (URL or path).\n   - **Post:** Propagate into `shared.repo_url` or `shared.local_dir` for pipeline.\n\n5. **Existing Tutorial Flow** (`FetchRepo` \u2192 `IdentifyAbstractions` \u2192 `AnalyzeRelationships` \u2192 `OrderChapters` \u2192 `WriteChapters` \u2192 `CombineTutorial`)\n\n   - **Type:** Workflow + BatchNode for `WriteChapters`.\n   - **Unchanged:** Implements the tutorial generation as per [design.md].\n\n6. `RenderAndDownload`\n\n   - **Type:** Regular node\n   - **Prep:** After `CombineTutorial`, read `shared.final_output_dir` and loaded Markdown file contents.\n   - **Exec (Streamlit):**\n     - Loop through `.md` files: `st.markdown(file_content)` to render.\n     - ZIP directory with `zip_output_folder(output_dir)`.\n     - `st.download_button(\"Download Tutorial ZIP\", data=open(zip_path, 'rb'), file_name=\"tutorial.zip\")`\n   - **Post:** None (UI-only).\n\n## Utility Functions (New & Extended)\n\n| Function              | Module                 | Input                                                | Output                 | Role                                                                 |\n| --------------------- | ---------------------- | ---------------------------------------------------- | ---------------------- | -------------------------------------------------------------------- |\n| `extract_keywords`    | `utils/search_helpers` | `query_input: str`                                   | `keywords: List[str]`  | LLM\u2011powered extraction of search terms from free text.               |\n| `github_search_repos` | `utils/github_api`     | `keywords or query`, `filter_params`, `token: str`   | `List[RepoMetadata]`   | Calls GitHub search REST API, applies filters, returns top-10 repos. |\n| `summarize_readme`    | `utils/summarizer`     | `readme_text: str`                                   | `summary: str`         | Generates concise README summaries via LLM.                          |\n| `load_env_vars`       | `utils/env_loader`     | `.env` filepath                                      | Populated `os.environ` | Loads API keys and model names from `.env`.                          |\n| `zip_output_folder`   | `utils/io_helpers`     | `folder_path: str`                                   | `zip_path: str`        | Zips output tutorial directory for download.                         |\n| `crawl_github_files`  | as before              | `repo_url, token, patterns, max_size, use_rel_paths` | `files, stats`         | GitHub code crawling (unchanged).                                    |\n| `crawl_local_files`   | as before              | `directory, patterns, max_size, use_rel_paths`       | `files, stats`         | Local code crawling (unchanged).                                     |\n| `call_llm`            | as before              | `prompt: str, model: str`                            | `response: str`        | Central LLM interface for all analysis/generation steps.             |\n\n## Shared Store (Extended)\n\n```python\nshared = {\n  # Existing keys from design.md\n  \"repo_url\": None,\n  \"local_dir\": None,\n  \"project_name\": None,\n  \"github_token\": None,\n  \"output_dir\": \"output\",\n  \"files\": [],\n  \"abstractions\": [],\n  \"relationships\": {\"summary\": None, \"details\": []},\n  \"chapter_order\": [],\n  \"chapters\": [],\n  \"final_output_dir\": None,\n\n  # New keys for UI & search\n  \"query_input\": None,\n  \"search_mode\": None,\n  \"keywords\": [],\n  \"filter_params\": {},\n  \"search_results\": [],\n  \"selected_repo\": None,\n}\n```\n\n## Node Interface Mapping\n\n- \\`\\` \u2192 orchestrates all nodes, binds button callbacks to `SmartSearchRepo`, `FilterRepos`, `SelectRepository`, and downstream pipeline.\n- **New nodes** (`SmartSearchRepo`, `FilterRepos`, `SelectRepository`, `Render & Download`) plug into the existing `Flow` defined in `flow.py`, executed in the order shown in the flowchart.\n\n## Installation & Configuration\n\n1. **Clone & Setup**\n\nvenv\\Scripts\\activate\n\n pip install -r requirements.txt\n\n````\n2. **Environment**  \n   Copy `.env.example` to `.env` and fill:\n   ```env\n   GEMINI_API_KEY=<...>\n   GEMINI_PROJECT_ID=<...>\n   GITHUB_TOKEN=<...>\n   OPENAI_API_KEY=<...>\n   ANTHROPIC_API_KEY=<...>\n   GEMINI_MODEL=gemini-2.5-pro-exp-03-25\n   OPENAI_MODEL=gpt-4o-mini\n   ANTHROPIC_MODEL=claude-3-7-sonnet-latest\n````\n\n3. **Run App**\n   ```bash\n   streamlit run main.py\n   ```\n4. **Access UI**\\\n   Open [http://localhost:8501](http://localhost:8501) in your browser.\n\n---\n\n\n\n\n\n--- File: 17 # LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\index.md ---\n# Tutorial: python-sdk\n\nThis project provides a Python implementation for the **Model Context Protocol (MCP)**.\nIt allows developers to easily build both *MCP servers* and *MCP clients*.\nServers use the framework (especially **FastMCP**) to expose capabilities like remote functions (**Tools**), data access (**Resources**), or LLM instructions (**Prompts**).\nClients can then connect to these servers using different **Transports** (like standard I/O or WebSockets) to interact with those capabilities, often enabling AI models to use external functions and data.\n\n\n**Source Repository:** [https://github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)\n\n```mermaid\nflowchart TD\n    A0[\"MCP Protocol Types\"]\n    A1[\"FastMCP Server\"]\n    A2[\"Client Session\"]\n    A3[\"Tools\"]\n    A4[\"Resources\"]\n    A5[\"Transports\"]\n    A6[\"Prompts\"]\n    A7[\"FastMCP Context\"]\n    A0 -- \"Defines wire format for\" --> A5\n    A1 -- \"Defines\" --> A3\n    A1 -- \"Defines\" --> A4\n    A1 -- \"Defines\" --> A6\n    A1 -- \"Runs over\" --> A5\n    A1 -- \"Provides\" --> A7\n    A2 -- \"Connects via\" --> A5\n    A2 -- \"Calls\" --> A3\n    A2 -- \"Reads\" --> A4\n    A2 -- \"Gets\" --> A6\n    A3 -- \"Uses\" --> A7\n```\n\n## Chapters\n\n1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n--- File: 35 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py ---\nfrom google import genai\nfrom anthropic import Anthropic\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\nimport logging\nimport json\nfrom datetime import datetime\nfrom enum import Enum\n\n# Load environment variables from .env file\nload_dotenv()  # Loads from .env file\n\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro gievn recent high bench marks\ndef call_llm(prompt: str, use_cache: bool = True, model: str=LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    \n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n                logger.warning(f\"Loaded and Using cache\")\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n        \n\n    # If not in cache, call the LLM\n    # Call the LLM if not in cache or cache disabled\n    if(model==LLMProvider_enum.GOOGLE):\n        # Use Google Gemini\n        \n        # client = genai.Client(\n        #     vertexai=True, \n        #     # TODO: change to your own project id and location\n        #     project=os.getenv(\"GEMINI_PROJECT_ID\", \"llm-code-explainer\"),\n        #     location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n        # )\n        # \n        client = genai.Client(\n            api_key=os.getenv(\"GEMINI_API_KEY\"),\n        )\n        model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n        response = client.models.generate_content(\n            model=model,\n            contents=[prompt]\n        )\n        response_text = response.text\n    elif(model==LLMProvider_enum.ANTHROPIC):\n        # Use Anthropic Claude\n        client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n        response = client.messages.create(\n            # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n            model=os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-20250219\"), \n            max_tokens=15000, #If have extra api budget, can increase this to 21000\n            thinking={\n                \"type\": \"enabled\", \n                \"budget_tokens\": 10000 # If have extra api budget, can increase this to 20000\n            },\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        response_text = response.content[1].text\n    else: # Assume OpenAI\n        # Use the default LLM, which is OpenAI (Use OpenAI o1/4o/gpt-4o-mini) depedning on the model & api budget\n        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        response = client.chat.completions.create(\n            model=os.environ.get(\"OPENAI_MODEL\", \"o4-mini\"),\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format={\n                \"type\": \"text\"\n            },\n            reasoning_effort=\"medium\",\n            store=False\n        )\n        response_text = response.choices[0].message.content\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you? What is the exact model are you using?\"\n    test_prompt2 = \"Hello, how bug is your model that are you using?\"\n    \n    # First call - should hit the API and return vals\n    print(\"Making call 1...\")\n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    response1 = call_llm(test_prompt, use_cache=False, model=LLMProvider_enum('openai-gpt'))\n    print(f\"Response: {response1}\\n======================\")\n\n    # Second call - should hit the cache and return vals\n    print(\"Making call 2...\")\n    response1 = call_llm(test_prompt, use_cache=True, model=LLMProvider_enum.GOOGLE)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File: 36 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (i.e. https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n                        (i.e. https://github.com/openai/codex/tree/cost-tracking)\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    # If URL explicitly specifies a tree/<ref>/\u2026 segment, use that\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        # Discover the real default branch from the GitHub API\n        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n\n        repo_api_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n        resp= requests.get(repo_api_url, headers=headers)\n        if resp.status_code == 200:\n            repo_data = resp.json()\n            ref = repo_data.get(\"default_branch\", \"main\")\n            print(f\"Default branch for {owner}/{repo} is {ref}..Pulling from {ref}\")\n        else:\n            print(f\"Error fetching repository data: {resp.status_code} - {resp.text}\")\n            # Fallback if the metadata call fails\n            # If no specific path is provided, use the root directory\n            # If no tree/<ref>/\u2026 segment, use the default branch\n            ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        # choose the correct endpoint: no slash for empty path\n        url_endpoint = (\n            f\"https://api.github.com/repos/{owner}/{repo}/contents\"\n            if not path else\n            f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        )\n\n\n        # url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url_endpoint, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    # repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    repo_url = \"https://github.com/openai/whisper\"\n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: 37 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\n--- File: 38 # LLM-CODEBASE-DOCUMENTOR-main\\utils\\env_loader.py ---\nimport os\nimport dotenv\nimport logging\nfrom typing import Dict, Any, Optional\n\ndef load_env_vars(env_file_path: str = \".env\") -> Dict[str, str]:\n    \"\"\"\n    Load environment variables from .env file.\n    \n    Args:\n        env_file_path (str): Path to the .env file\n        \n    Returns:\n        Dict[str, str]: Dictionary of environment variables\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(env_file_path):\n        logging.warning(f\".env file not found at {env_file_path}\")\n        return {}\n    \n    # Load the .env file\n    dotenv.load_dotenv(env_file_path)\n    \n    # Return a dictionary of the loaded variables\n    env_vars = {}\n    try:\n        with open(env_file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                    \n                try:\n                    key, value = line.split('=', 1)\n                    env_vars[key.strip()] = value.strip().strip('\"\\'')\n                except ValueError:\n                    # Skip lines that don't have key=value format\n                    continue\n    except Exception as e:\n        logging.error(f\"Error reading .env file: {e}\")\n    \n    return env_vars\n\ndef get_api_keys() -> Dict[str, Optional[str]]:\n    \"\"\"\n    Get API keys from environment variables.\n    \n    Returns:\n        Dict[str, Optional[str]]: Dictionary of API keys\n    \"\"\"\n    return {\n        \"github_token\": os.environ.get(\"GITHUB_TOKEN\"),\n        \"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n        \"anthropic_api_key\": os.environ.get(\"ANTHROPIC_API_KEY\"),\n        \"gemini_api_key\": os.environ.get(\"GEMINI_API_KEY\")\n    }\n\ndef get_model_names() -> Dict[str, str]:\n    \"\"\"\n    Get model names from environment variables.\n    \n    Returns:\n        Dict[str, str]: Dictionary of model names\n    \"\"\"\n    return {\n        \"openai_model\": os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n        \"anthropic_model\": os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-latest\"),\n        \"gemini_model\": os.environ.get(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    }\n\ndef create_env_example() -> None:\n    \"\"\"\n    Create a .env.example file with placeholders.\n    \"\"\"\n    example_content = \"\"\"# API Keys\nGITHUB_TOKEN=your_github_token_here\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nGEMINI_API_KEY=your_gemini_key_here\n\n# Model Names\nGEMINI_MODEL=gemini-2.5-pro-exp-03-25\nOPENAI_MODEL=gpt-4o-mini\nANTHROPIC_MODEL=claude-3-7-sonnet-latest\n\"\"\"\n    \n    try:\n        with open(\".env.example\", 'w') as f:\n            f.write(example_content)\n        print(\"Created .env.example file\")\n    except Exception as e:\n        print(f\"Error creating .env.example file: {e}\")\n\nif __name__ == \"__main__\":\n    # Test the environment loading\n    env_vars = load_env_vars()\n    print(\"Loaded environment variables:\", list(env_vars.keys()))\n    \n    api_keys = get_api_keys()\n    print(\"API Keys available:\", [k for k, v in api_keys.items() if v])\n    \n    # Create example .env file\n    create_env_example() \n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  The `LLM-CODEBASE-DOCUMENTOR` project is designed to **automatically generate beginner-friendly tutorials** for software codebases. \n  It leverages a *PocketFlow Workflow* (Abstraction 0) to orchestrate a series of *Nodes* (Abstraction 1) that handle tasks such as *Code Fetching* (Abstraction 2), *LLM Analysis* (Abstraction 3), and *Tutorial Structure Generation* (Abstraction 5). \n  The project includes both a command-line interface and a *Streamlit UI* (Abstraction 6) for user interaction, search, and configuration. \n  Data flows between steps via a *Shared Data Store* (Abstraction 4), configured by *Configuration & Environment Management* (Abstraction 7).\nrelationships:\n  - from_abstraction: 0 # PocketFlow Workflow\n    to_abstraction: 1 # Node Abstraction\n    label: \"Orchestrates\"\n  - from_abstraction: 0 # PocketFlow Workflow\n    to_abstraction: 4 # Shared Data Store\n    label: \"Manages Data Flow Via\"\n  - from_abstraction: 1 # Node Abstraction\n    to_abstraction: 4 # Shared Data Store\n    label: \"Reads/Writes\"\n  - from_abstraction: 1 # Node Abstraction\n    to_abstraction: 3 # LLM Analysis & Generation\n    label: \"Invokes\"\n  - from_abstraction: 2 # Code Fetching & Parsing\n    to_abstraction: 4 # Shared Data Store\n    label: \"Populates\"\n  - from_abstraction: 3 # LLM Analysis & Generation\n    to_abstraction: 5 # Tutorial Structure Generation\n    label: \"Generates Content For\"\n  - from_abstraction: 4 # Shared Data Store\n    to_abstraction: 5 # Tutorial Structure Generation\n    label: \"Provides Data To\"\n  - from_abstraction: 6 # Streamlit UI & Search Flow\n    to_abstraction: 0 # PocketFlow Workflow\n    label: \"Integrates With / Extends\"\n  - from_abstraction: 6 # Streamlit UI & Search Flow\n    to_abstraction: 2 # Code Fetching & Parsing\n    label: \"Initiates\"\n  - from_abstraction: 7 # Configuration & Environment Management\n    to_abstraction: 0 # PocketFlow Workflow\n    label: \"Configures Initial State\"\n  - from_abstraction: 7 # Configuration & Environment Management\n    to_abstraction: 2 # Code Fetching & Parsing\n    label: \"Provides Patterns/Limits\"\n  - from_abstraction: 7 # Configuration & Environment Management\n    to_abstraction: 3 # LLM Analysis & Generation\n    label: \"Provides API Keys/Models\"\n  - from_abstraction: 7 # Configuration & Environment Management\n    to_abstraction: 6 # Streamlit UI & Search Flow\n    label: \"Manages Settings For\"\n  - from_abstraction: 1 # Node Abstraction # Ensure Node relates to Tutorial Generation\n    to_abstraction: 5 # Tutorial Structure Generation\n    label: \"Implements Steps For\" \n```", "\n        Given the following project abstractions and their relationships for the project ```` LLM -CODEBASE-DOCUMENTOR ````:\n\n        Abstractions (Index # Name):\n        - 0 # PocketFlow Workflow\n- 1 # Node Abstraction\n- 2 # Code Fetching & Parsing\n- 3 # LLM Analysis & Generation\n- 4 # Shared Data Store\n- 5 # Tutorial Structure Generation\n- 6 # Streamlit UI & Search Flow\n- 7 # Configuration & Environment Management\n\n        Context about relationships and project summary:\n        Project Summary:\nThe `LLM-CODEBASE-DOCUMENTOR` project is designed to **automatically generate beginner-friendly tutorials** for software codebases. \nIt leverages a *PocketFlow Workflow* (Abstraction 0) to orchestrate a series of *Nodes* (Abstraction 1) that handle tasks such as *Code Fetching* (Abstraction 2), *LLM Analysis* (Abstraction 3), and *Tutorial Structure Generation* (Abstraction 5). \nThe project includes both a command-line interface and a *Streamlit UI* (Abstraction 6) for user interaction, search, and configuration. \nData flows between steps via a *Shared Data Store* (Abstraction 4), configured by *Configuration & Environment Management* (Abstraction 7).\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (PocketFlow Workflow) to 1 (Node Abstraction): Orchestrates\n- From 0 (PocketFlow Workflow) to 4 (Shared Data Store): Manages Data Flow Via\n- From 1 (Node Abstraction) to 4 (Shared Data Store): Reads/Writes\n- From 1 (Node Abstraction) to 3 (LLM Analysis & Generation): Invokes\n- From 2 (Code Fetching & Parsing) to 4 (Shared Data Store): Populates\n- From 3 (LLM Analysis & Generation) to 5 (Tutorial Structure Generation): Generates Content For\n- From 4 (Shared Data Store) to 5 (Tutorial Structure Generation): Provides Data To\n- From 6 (Streamlit UI & Search Flow) to 0 (PocketFlow Workflow): Integrates With / Extends\n- From 6 (Streamlit UI & Search Flow) to 2 (Code Fetching & Parsing): Initiates\n- From 7 (Configuration & Environment Management) to 0 (PocketFlow Workflow): Configures Initial State\n- From 7 (Configuration & Environment Management) to 2 (Code Fetching & Parsing): Provides Patterns/Limits\n- From 7 (Configuration & Environment Management) to 3 (LLM Analysis & Generation): Provides API Keys/Models\n- From 7 (Configuration & Environment Management) to 6 (Streamlit UI & Search Flow): Manages Settings For\n- From 1 (Node Abstraction) to 5 (Tutorial Structure Generation): Implements Steps For\n\n\n        If you are going to make a tutorial for ```` LLM -CODEBASE-DOCUMENTOR ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        ": "```yaml\n- 2 # Code Fetching & Parsing (Starts with the input/problem domain)\n- 0 # PocketFlow Workflow (Explains the overall orchestration)\n- 3 # LLM Analysis & Generation (Core engine for creating content)\n- 5 # Tutorial Structure Generation (How the final output is shaped)\n- 1 # Node Abstraction (Implementation detail of the workflow steps)\n- 4 # Shared Data Store (Internal mechanism for data flow)\n- 6 # Streamlit UI & Search Flow (User interaction layer)\n- 7 # Configuration & Environment Management (Supporting aspect affecting all components)\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Code Fetching & Parsing\". This is Chapter 1.\n\n        Concept Details:\n        - Description:\n        The initial step responsible for acquiring the source code from either a remote GitHub repository (using the GitHub API) or a local directory. It handles file filtering based on include/exclude patterns and size limits, preparing the codebase content for analysis.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        This is the first chapter.\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_github_files.py ---\nimport requests\nimport base64\nimport os\nimport tempfile\nimport git\nimport time\nimport fnmatch\nfrom typing import Union, Set, List, Dict, Tuple, Any\nfrom urllib.parse import urlparse\n\ndef crawl_github_files(\n    repo_url, \n    token=None, \n    max_file_size: int = 1 * 1024 * 1024,  # 1 MB\n    use_relative_paths: bool = False,\n    include_patterns: Union[str, Set[str]] = None,\n    exclude_patterns: Union[str, Set[str]] = None\n):\n    \"\"\"\n    Crawl files from a specific path in a GitHub repository at a specific commit.\n\n    Args:\n        repo_url (str): URL of the GitHub repository with specific path and commit\n                        (i.e. https://github.com/microsoft/autogen/tree/e45a15766746d95f8cfaaa705b0371267bec812e/python/packages/autogen-core/src/autogen_core')\n                        (i.e. https://github.com/openai/codex/tree/cost-tracking)\n        token (str, optional): **GitHub personal access token.**\n            - **Required for private repositories.**\n            - **Recommended for public repos to avoid rate limits.**\n            - Can be passed explicitly or set via the `GITHUB_TOKEN` environment variable.\n        max_file_size (int, optional): Maximum file size in bytes to download (default: 1 MB)\n        use_relative_paths (bool, optional): If True, file paths will be relative to the specified subdirectory\n        include_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to include (e.g., \"*.py\", {\"*.md\", \"*.txt\"}).\n                                                       If None, all files are included.\n        exclude_patterns (str or set of str, optional): Pattern or set of patterns specifying which files to exclude.\n                                                       If None, no files are excluded.\n\n    Returns:\n        dict: Dictionary with files and statistics\n    \"\"\"\n    # Convert single pattern to set\n    if include_patterns and isinstance(include_patterns, str):\n        include_patterns = {include_patterns}\n    if exclude_patterns and isinstance(exclude_patterns, str):\n        exclude_patterns = {exclude_patterns}\n\n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n\n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n\n        return include_file\n\n    # Detect SSH URL (git@ or .git suffix)\n    is_ssh_url = repo_url.startswith(\"git@\") or repo_url.endswith(\".git\")\n\n    if is_ssh_url:\n        # Clone repo via SSH to temp dir\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            print(f\"Cloning SSH repo {repo_url} to temp dir {tmpdirname} ...\")\n            try:\n                repo = git.Repo.clone_from(repo_url, tmpdirname)\n            except Exception as e:\n                print(f\"Error cloning repo: {e}\")\n                return {\"files\": {}, \"stats\": {\"error\": str(e)}}\n\n            # Attempt to checkout specific commit/branch if in URL\n            # Parse ref and subdir from SSH URL? SSH URLs don't have branch info embedded\n            # So rely on default branch, or user can checkout manually later\n            # Optionally, user can pass ref explicitly in future API\n\n            # Walk directory\n            files = {}\n            skipped_files = []\n\n            for root, dirs, filenames in os.walk(tmpdirname):\n                for filename in filenames:\n                    abs_path = os.path.join(root, filename)\n                    rel_path = os.path.relpath(abs_path, tmpdirname)\n\n                    # Check file size\n                    try:\n                        file_size = os.path.getsize(abs_path)\n                    except OSError:\n                        continue\n\n                    if file_size > max_file_size:\n                        skipped_files.append((rel_path, file_size))\n                        print(f\"Skipping {rel_path}: size {file_size} exceeds limit {max_file_size}\")\n                        continue\n\n                    # Check include/exclude patterns\n                    if not should_include_file(rel_path, filename):\n                        print(f\"Skipping {rel_path}: does not match include/exclude patterns\")\n                        continue\n\n                    # Read content\n                    try:\n                        with open(abs_path, \"r\", encoding=\"utf-8\") as f:\n                            content = f.read()\n                        files[rel_path] = content\n                        print(f\"Added {rel_path} ({file_size} bytes)\")\n                    except Exception as e:\n                        print(f\"Failed to read {rel_path}: {e}\")\n\n            return {\n                \"files\": files,\n                \"stats\": {\n                    \"downloaded_count\": len(files),\n                    \"skipped_count\": len(skipped_files),\n                    \"skipped_files\": skipped_files,\n                    \"base_path\": None,\n                    \"include_patterns\": include_patterns,\n                    \"exclude_patterns\": exclude_patterns,\n                    \"source\": \"ssh_clone\"\n                }\n            }\n\n    # Parse GitHub URL to extract owner, repo, commit/branch, and path\n    parsed_url = urlparse(repo_url)\n    path_parts = parsed_url.path.strip('/').split('/')\n    \n    if len(path_parts) < 2:\n        raise ValueError(f\"Invalid GitHub URL: {repo_url}\")\n    \n    # Extract the basic components\n    owner = path_parts[0]\n    repo = path_parts[1]\n    \n    # Check if URL contains a specific branch/commit\n    # If URL explicitly specifies a tree/<ref>/\u2026 segment, use that\n    if 'tree' in path_parts:\n        tree_index = path_parts.index('tree')\n        ref = path_parts[tree_index + 1]\n        # Combine all parts after the ref as the path\n        path_start = tree_index + 2\n        specific_path = '/'.join(path_parts[path_start:]) if path_start < len(path_parts) else \"\"\n    else:\n        # Discover the real default branch from the GitHub API\n        headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n        if token:\n            headers[\"Authorization\"] = f\"token {token}\"\n\n\n        repo_api_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n        resp= requests.get(repo_api_url, headers=headers)\n        if resp.status_code == 200:\n            repo_data = resp.json()\n            ref = repo_data.get(\"default_branch\", \"main\")\n            print(f\"Default branch for {owner}/{repo} is {ref}..Pulling from {ref}\")\n        else:\n            print(f\"Error fetching repository data: {resp.status_code} - {resp.text}\")\n            # Fallback if the metadata call fails\n            # If no specific path is provided, use the root directory\n            # If no tree/<ref>/\u2026 segment, use the default branch\n            ref = \"main\"  # Default branch\n        specific_path = \"\"\n    \n    # Setup for GitHub API\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    # Dictionary to store path -> content mapping\n    files = {}\n    skipped_files = []\n    \n    def should_include_file(file_path: str, file_name: str) -> bool:\n        \"\"\"Determine if a file should be included based on patterns\"\"\"\n        # If no include patterns are specified, include all files\n        if not include_patterns:\n            include_file = True\n        else:\n            # Check if file matches any include pattern\n            include_file = any(fnmatch.fnmatch(file_name, pattern) for pattern in include_patterns)\n        \n        # If exclude patterns are specified, check if file should be excluded\n        if exclude_patterns and include_file:\n            # Exclude if file matches any exclude pattern\n            exclude_file = any(fnmatch.fnmatch(file_path, pattern) for pattern in exclude_patterns)\n            return not exclude_file\n        \n        return include_file\n    \n    def fetch_contents(path):\n        \"\"\"Fetch contents of the repository at a specific path and commit\"\"\"\n        # choose the correct endpoint: no slash for empty path\n        url_endpoint = (\n            f\"https://api.github.com/repos/{owner}/{repo}/contents\"\n            if not path else\n            f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        )\n\n\n        # url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}\"\n        params = {\"ref\": ref}\n        \n        response = requests.get(url_endpoint, headers=headers, params=params)\n        \n        if response.status_code == 403 and 'rate limit exceeded' in response.text.lower():\n            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n            wait_time = max(reset_time - time.time(), 0) + 1\n            print(f\"Rate limit exceeded. Waiting for {wait_time:.0f} seconds...\")\n            time.sleep(wait_time)\n            return fetch_contents(path)\n            \n        if response.status_code == 404:\n            if not token:\n                print(f\"Error 404: Repository not found or is private.\\n\"\n                      f\"If this is a private repository, please provide a valid GitHub token via the 'token' argument or set the GITHUB_TOKEN environment variable.\")\n            else:\n                print(f\"Error 404: Path '{path}' not found in repository or insufficient permissions with the provided token.\\n\"\n                      f\"Please verify the token has access to this repository and the path exists.\")\n            return\n            \n        if response.status_code != 200:\n            print(f\"Error fetching {path}: {response.status_code} - {response.text}\")\n            return\n        \n        contents = response.json()\n        \n        # Handle both single file and directory responses\n        if not isinstance(contents, list):\n            contents = [contents]\n        \n        for item in contents:\n            item_path = item[\"path\"]\n            \n            # Calculate relative path if requested\n            if use_relative_paths and specific_path:\n                # Make sure the path is relative to the specified subdirectory\n                if item_path.startswith(specific_path):\n                    rel_path = item_path[len(specific_path):].lstrip('/')\n                else:\n                    rel_path = item_path\n            else:\n                rel_path = item_path\n            \n            if item[\"type\"] == \"file\":\n                # Check if file should be included based on patterns\n                if not should_include_file(rel_path, item[\"name\"]):\n                    print(f\"Skipping {rel_path}: Does not match include/exclude patterns\")\n                    continue\n                \n                # Check file size if available\n                file_size = item.get(\"size\", 0)\n                if file_size > max_file_size:\n                    skipped_files.append((item_path, file_size))\n                    print(f\"Skipping {rel_path}: File size ({file_size} bytes) exceeds limit ({max_file_size} bytes)\")\n                    continue\n                \n                # For files, get raw content\n                if \"download_url\" in item and item[\"download_url\"]:\n                    file_url = item[\"download_url\"]\n                    file_response = requests.get(file_url, headers=headers)\n                    \n                    # Final size check in case content-length header is available but differs from metadata\n                    content_length = int(file_response.headers.get('content-length', 0))\n                    if content_length > max_file_size:\n                        skipped_files.append((item_path, content_length))\n                        print(f\"Skipping {rel_path}: Content length ({content_length} bytes) exceeds limit ({max_file_size} bytes)\")\n                        continue\n                        \n                    if file_response.status_code == 200:\n                        files[rel_path] = file_response.text\n                        print(f\"Downloaded: {rel_path} ({file_size} bytes) \")\n                    else:\n                        print(f\"Failed to download {rel_path}: {file_response.status_code}\")\n                else:\n                    # Alternative method if download_url is not available\n                    content_response = requests.get(item[\"url\"], headers=headers)\n                    if content_response.status_code == 200:\n                        content_data = content_response.json()\n                        if content_data.get(\"encoding\") == \"base64\" and \"content\" in content_data:\n                            # Check size of base64 content before decoding\n                            if len(content_data[\"content\"]) * 0.75 > max_file_size:  # Approximate size calculation\n                                estimated_size = int(len(content_data[\"content\"]) * 0.75)\n                                skipped_files.append((item_path, estimated_size))\n                                print(f\"Skipping {rel_path}: Encoded content exceeds size limit\")\n                                continue\n                                \n                            file_content = base64.b64decode(content_data[\"content\"]).decode('utf-8')\n                            files[rel_path] = file_content\n                            print(f\"Downloaded: {rel_path} ({file_size} bytes)\")\n                        else:\n                            print(f\"Unexpected content format for {rel_path}\")\n                    else:\n                        print(f\"Failed to get content for {rel_path}: {content_response.status_code}\")\n            \n            elif item[\"type\"] == \"dir\":\n                # Recursively process subdirectories\n                fetch_contents(item_path)\n    \n    # Start crawling from the specified path\n    fetch_contents(specific_path)\n    \n    return {\n        \"files\": files,\n        \"stats\": {\n            \"downloaded_count\": len(files),\n            \"skipped_count\": len(skipped_files),\n            \"skipped_files\": skipped_files,\n            \"base_path\": specific_path if use_relative_paths else None,\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns\n        }\n    }\n\n# Example usage\nif __name__ == \"__main__\":\n    # Get token from environment variable (recommended for private repos)\n    github_token = os.environ.get(\"GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Warning: No GitHub token found in environment variable 'GITHUB_TOKEN'.\\n\"\n              \"Private repositories will not be accessible without a token.\\n\"\n              \"To access private repos, set the environment variable or pass the token explicitly.\")\n    \n    # repo_url = \"https://github.com/pydantic/pydantic/tree/6c38dc93f40a47f4d1350adca9ec0d72502e223f/pydantic\"\n    repo_url = \"https://github.com/openai/whisper\"\n    # Example: Get Python and Markdown files, but exclude test files\n    result = crawl_github_files(\n        repo_url, \n        token=github_token,\n        max_file_size=1 * 1024 * 1024,  # 1 MB in bytes\n        use_relative_paths=True,  # Enable relative paths\n        include_patterns={\"*.py\", \"*.md\"},  # Include Python and Markdown files\n    )\n    \n    files = result[\"files\"]\n    stats = result[\"stats\"]\n    \n    print(f\"\\nDownloaded {stats['downloaded_count']} files.\")\n    print(f\"Skipped {stats['skipped_count']} files due to size limits or patterns.\")\n    print(f\"Base path for relative paths: {stats['base_path']}\")\n    print(f\"Include patterns: {stats['include_patterns']}\")\n    print(f\"Exclude patterns: {stats['exclude_patterns']}\")\n    \n    # Display all file paths in the dictionary\n    print(\"\\nFiles in dictionary:\")\n    for file_path in sorted(files.keys()):\n        print(f\"  {file_path}\")\n    \n    # Example: accessing content of a specific file\n    if files:\n        sample_file = next(iter(files))\n        print(f\"\\nSample file: {sample_file}\")\n        print(f\"Content preview: {files[sample_file][:200]}...\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\utils\\crawl_local_files.py ---\nimport os\nimport fnmatch\n\ndef crawl_local_files(directory, include_patterns=None, exclude_patterns=None, max_file_size=None, use_relative_paths=True):\n    \"\"\"\n    Crawl files in a local directory with similar interface as crawl_github_files.\n    \n    Args:\n        directory (str): Path to local directory\n        include_patterns (set): File patterns to include (e.g. {\"*.py\", \"*.js\"})\n        exclude_patterns (set): File patterns to exclude (e.g. {\"tests/*\"})\n        max_file_size (int): Maximum file size in bytes\n        use_relative_paths (bool): Whether to use paths relative to directory\n        \n    Returns:\n        dict: {\"files\": {filepath: content}}\n    \"\"\"\n    if not os.path.isdir(directory):\n        raise ValueError(f\"Directory does not exist: {directory}\")\n        \n    files_dict = {}\n    \n    for root, _, files in os.walk(directory):\n        for filename in files:\n            filepath = os.path.join(root, filename)\n            \n            # Get path relative to directory if requested\n            if use_relative_paths:\n                relpath = os.path.relpath(filepath, directory)\n            else:\n                relpath = filepath\n                \n            # Check if file matches any include pattern\n            included = False\n            if include_patterns:\n                for pattern in include_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        included = True\n                        break\n            else:\n                included = True\n                \n            # Check if file matches any exclude pattern\n            excluded = False\n            if exclude_patterns:\n                for pattern in exclude_patterns:\n                    if fnmatch.fnmatch(relpath, pattern):\n                        excluded = True\n                        break\n                        \n            if not included or excluded:\n                continue\n                \n            # Check file size\n            if max_file_size and os.path.getsize(filepath) > max_file_size:\n                continue\n                \n            try:\n                with open(filepath, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                files_dict[relpath] = content\n            except Exception as e:\n                print(f\"Warning: Could not read file {filepath}: {e}\")\n                \n    return {\"files\": files_dict}\n\nif __name__ == \"__main__\":\n    print(\"--- Crawling parent directory ('..') ---\")\n    files_data = crawl_local_files(\"..\", exclude_patterns={\"*.pyc\", \"__pycache__/*\", \".git/*\", \"output/*\"})\n    print(f\"Found {len(files_data['files'])} files:\")\n    for path in files_data[\"files\"]:\n        print(f\"  {path}\")\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 1: Code Fetching & Parsing`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"PocketFlow Workflow\". This is Chapter 2.\n\n        Concept Details:\n        - Description:\n        The core engine orchestrating the tutorial generation process. It defines a sequence of steps (Nodes) where the output of one step feeds into the next. Think of it as an assembly line for creating the documentation. It manages the flow of data and execution order.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\flow.py ---\nfrom pocketflow import Flow\n\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial,\n    # New nodes for Streamlit UI\n    SmartSearchRepo,\n    FilterRepos,\n    SelectRepository,\n    RenderAndDownload\n)\n\n\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow, fetch_repo, combine_tutorial\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n    \n    # Create the original tutorial flow and get the first and last nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n    \n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n    \n    # Connect the UI nodes\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n    \n    # Connect the select_repo node to the FetchRepo node\n    select_repo >> fetch_repo\n    \n    # Add the render and download node after CombineTutorial\n    combine_tutorial >> render_download\n    \n    # Create the full UI flow\n    ui_flow = Flow(start=smart_search)\n    \n    return ui_flow\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 2: PocketFlow Workflow`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"LLM Analysis & Generation\". This is Chapter 3.\n\n        Concept Details:\n        - Description:\n        Leverages Large Language Models (LLMs like Gemini, Claude, OpenAI) to understand the codebase and generate human-friendly explanations. This involves identifying key abstractions, analyzing their relationships, ordering tutorial chapters logically, and writing the actual chapter content.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py ---\nfrom google import genai\nfrom anthropic import Anthropic\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\nimport logging\nimport json\nfrom datetime import datetime\nfrom enum import Enum\n\n# Load environment variables from .env file\nload_dotenv()  # Loads from .env file\n\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro gievn recent high bench marks\ndef call_llm(prompt: str, use_cache: bool = True, model: str=LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    \n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n                logger.warning(f\"Loaded and Using cache\")\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n        \n\n    # If not in cache, call the LLM\n    # Call the LLM if not in cache or cache disabled\n    if(model==LLMProvider_enum.GOOGLE):\n        # Use Google Gemini\n        \n        # client = genai.Client(\n        #     vertexai=True, \n        #     # TODO: change to your own project id and location\n        #     project=os.getenv(\"GEMINI_PROJECT_ID\", \"llm-code-explainer\"),\n        #     location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n        # )\n        # \n        client = genai.Client(\n            api_key=os.getenv(\"GEMINI_API_KEY\"),\n        )\n        model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n        response = client.models.generate_content(\n            model=model,\n            contents=[prompt]\n        )\n        response_text = response.text\n    elif(model==LLMProvider_enum.ANTHROPIC):\n        # Use Anthropic Claude\n        client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n        response = client.messages.create(\n            # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n            model=os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-20250219\"), \n            max_tokens=15000, #If have extra api budget, can increase this to 21000\n            thinking={\n                \"type\": \"enabled\", \n                \"budget_tokens\": 10000 # If have extra api budget, can increase this to 20000\n            },\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        response_text = response.content[1].text\n    else: # Assume OpenAI\n        # Use the default LLM, which is OpenAI (Use OpenAI o1/4o/gpt-4o-mini) depedning on the model & api budget\n        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        response = client.chat.completions.create(\n            model=os.environ.get(\"OPENAI_MODEL\", \"o4-mini\"),\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format={\n                \"type\": \"text\"\n            },\n            reasoning_effort=\"medium\",\n            store=False\n        )\n        response_text = response.choices[0].message.content\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you? What is the exact model are you using?\"\n    test_prompt2 = \"Hello, how bug is your model that are you using?\"\n    \n    # First call - should hit the API and return vals\n    print(\"Making call 1...\")\n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    response1 = call_llm(test_prompt, use_cache=False, model=LLMProvider_enum('openai-gpt'))\n    print(f\"Response: {response1}\\n======================\")\n\n    # Second call - should hit the cache and return vals\n    print(\"Making call 2...\")\n    response1 = call_llm(test_prompt, use_cache=True, model=LLMProvider_enum.GOOGLE)\n    print(f\"Response: {response1}\")\n    \n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 3: LLM Analysis & Generation`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Tutorial Structure Generation\". This is Chapter 4.\n\n        Concept Details:\n        - Description:\n        The process of organizing the identified abstractions and generated content into a coherent tutorial structure. This includes determining the optimal chapter order based on relationships and creating the final `index.md` with summaries, diagrams, and links, along with individual chapter markdown files.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n---\n# Chapter 3: LLM Analysis & Generation\n\n```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\output\\python-sdk\\index.md ---\n# Tutorial: python-sdk\n\nThis project provides a Python implementation for the **Model Context Protocol (MCP)**.\nIt allows developers to easily build both *MCP servers* and *MCP clients*.\nServers use the framework (especially **FastMCP**) to expose capabilities like remote functions (**Tools**), data access (**Resources**), or LLM instructions (**Prompts**).\nClients can then connect to these servers using different **Transports** (like standard I/O or WebSockets) to interact with those capabilities, often enabling AI models to use external functions and data.\n\n\n**Source Repository:** [https://github.com/modelcontextprotocol/python-sdk](https://github.com/modelcontextprotocol/python-sdk)\n\n```mermaid\nflowchart TD\n    A0[\"MCP Protocol Types\"]\n    A1[\"FastMCP Server\"]\n    A2[\"Client Session\"]\n    A3[\"Tools\"]\n    A4[\"Resources\"]\n    A5[\"Transports\"]\n    A6[\"Prompts\"]\n    A7[\"FastMCP Context\"]\n    A0 -- \"Defines wire format for\" --> A5\n    A1 -- \"Defines\" --> A3\n    A1 -- \"Defines\" --> A4\n    A1 -- \"Defines\" --> A6\n    A1 -- \"Runs over\" --> A5\n    A1 -- \"Provides\" --> A7\n    A2 -- \"Connects via\" --> A5\n    A2 -- \"Calls\" --> A3\n    A2 -- \"Reads\" --> A4\n    A2 -- \"Gets\" --> A6\n    A3 -- \"Uses\" --> A7\n```\n\n## Chapters\n\n1. [Client Session](01_client_session.md)\n2. [Tools](02_tools.md)\n3. [Resources](03_resources.md)\n4. [Prompts](04_prompts.md)\n5. [FastMCP Server](05_fastmcp_server.md)\n6. [Transports](06_transports.md)\n7. [FastMCP Context](07_fastmcp_context.md)\n8. [MCP Protocol Types](08_mcp_protocol_types.md)\n\n\n---\n\nGenerated by [Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 4: Tutorial Structure Generation`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "```markdown\n# Chapter 4: Tutorial Structure Generation\n\nWelcome back! In [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md), we saw how the \"AI brain\" (the LLM) helps us understand the code by identifying key concepts (abstractions), figuring out how they relate, deciding the best order to teach them, and even writing the first draft of each chapter's content.\n\nNow, imagine you've written all the individual recipes for your cookbook. You have the text for the cake recipe, the cookie recipe, the bread recipe, etc., all on separate pieces of paper. You also have a summary of the whole cookbook and maybe a diagram showing which recipes use similar ingredients. What's the next step? You need to **assemble the cookbook**!\n\nThis chapter is all about that final assembly process: **Tutorial Structure Generation**.\n\n**Goal:** Understand how the tool takes all the generated pieces \u2013 the project summary, the concept relationships, the chapter order, and the written chapter text \u2013 and organizes them into a final, polished tutorial structure, complete with an index file, diagrams, and correctly linked chapter files.\n\n## The Problem: From Scattered Notes to a Polished Book \ud83d\udcda\n\nAfter the previous steps, we have several valuable pieces of information floating around:\n\n1.  **Abstractions:** The list of key concepts identified by the LLM (e.g., \"Client Session\", \"Tools\").\n2.  **Relationships:** How these concepts connect, including a project summary (e.g., \"Client Session calls Tools\").\n3.  **Chapter Order:** The best sequence to explain these concepts (e.g., explain \"Client Session\" first, then \"Tools\").\n4.  **Chapter Content:** The actual Markdown text generated by the LLM for each concept.\n\nBut these are just separate bits of data inside the computer's memory. We need to transform them into something a human can actually use: a set of neatly organized Markdown files that form a coherent tutorial.\n\n**Use Case:** We want to take all the intermediate results generated by the LLMs and the workflow and produce the final output directory. This directory should contain:\n*   An `index.md` file acting as the table of contents, with a summary and a relationship diagram.\n*   Individual chapter files (like `01_client_session.md`, `02_tools.md`) containing the generated text.\n*   Proper links connecting the index to the chapters and the chapters to each other.\n\n## The Solution: The `CombineTutorial` Node - Our Assembler \ud83d\udee0\ufe0f\n\nThe final step in our documentation assembly line ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) is handled by a specific node called `CombineTutorial`. Think of this node as the **final assembler** or the **bookbinder**. Its job is to take all the finished components and put them together correctly.\n\nHere's what the `CombineTutorial` node does:\n\n1.  **Gathers Inputs:** It collects all the necessary pieces created by the previous nodes:\n    *   The project summary and relationship details (from `AnalyzeRelationships`).\n    *   The list of abstractions (from `IdentifyAbstractions`).\n    *   The determined chapter order (from `OrderChapters`).\n    *   The generated Markdown content for each chapter (from `WriteChapters`).\n    *   The project name and desired output location.\n\n2.  **Generates the Relationship Diagram:** It uses the relationship data (which abstraction connects to which, and the label for that connection) to automatically create a visual flowchart using Mermaid syntax. This helps users see the connections at a glance.\n\n3.  **Creates `index.md` Content:** It prepares the text for the main `index.md` file. This includes:\n    *   The project title.\n    *   The high-level summary generated earlier.\n    *   A link to the source code repository (if available).\n    *   The Mermaid diagram it just generated.\n    *   An ordered list of chapters, with links to the corresponding chapter files (e.g., \"1. [Client Session](01_client_session.md)\").\n\n4.  **Prepares Chapter Files:** For each chapter's generated Markdown content:\n    *   It determines the correct filename based on the chapter number and a \"safe\" version of the abstraction name (e.g., replacing spaces with underscores).\n    *   It adds a standard attribution line at the end.\n\n5.  **Writes Everything to Disk:**\n    *   It creates the final output directory (usually named after the project, inside an `output` folder).\n    *   It writes the prepared `index.md` content to `index.md` inside that directory.\n    *   It writes each chapter's prepared content to its corresponding filename (e.g., `01_client_session.md`, `02_tools.md`, etc.) inside the same directory.\n\n## How It Works Under the Hood (Simplified Walkthrough) \ud83d\udeb6\u200d\u2640\ufe0f\n\nLet's follow the `CombineTutorial` node step-by-step:\n\n1.  **Get Ready (`prep`):** The node first gathers all the ingredients it needs from the shared data store: the summary, relationship details (`relationships['details']`), chapter order (`chapter_order`), abstraction list (`abstractions`), chapter content (`chapters`), project name, and output directory path.\n2.  **Draw the Map (`prep`):** It looks at the `relationships['details']` (which contains pairs like `from: 0, to: 1, label: \"Calls\"`) and generates the Mermaid code for the diagram.\n3.  **Write the Introduction (`prep`):** It starts building the `index.md` text string, adding the title, summary, and the Mermaid diagram code.\n4.  **Create the Table of Contents (`prep`):** It iterates through the `chapter_order` list. For each abstraction index in the order, it finds the abstraction's name, creates a safe filename (like `01_client_session.md`), and adds a numbered link like `1. [Client Session](01_client_session.md)` to the `index.md` text. It also prepares a list of all chapter filenames and their corresponding content (adding the attribution).\n5.  **Build the Output Folder (`exec`):** The node creates the actual folder on your computer (e.g., `output/my_project/`).\n6.  **Save the Index (`exec`):** It saves the complete `index.md` text string into the `index.md` file within the new folder.\n7.  **Save the Chapters (`exec`):** It goes through the list of prepared chapter filenames and content, saving each one as a separate `.md` file in the folder.\n8.  **Done! (`post`):** It records the path to the final output directory in the shared store and prints a success message.\n\n**Visualizing the Flow:**\n\nHere's a simple sequence diagram for the `CombineTutorial` node:\n\n```mermaid\nsequenceDiagram\n    participant CT_Prep as CombineTutorial (Prep Phase)\n    participant SharedData as Shared Data Store\n    participant CT_Exec as CombineTutorial (Exec Phase)\n    participant FileSystem as Your Computer's Files\n\n    CT_Prep ->> SharedData: Read summary, relationships, order, abstractions, chapters, project name\n    CT_Prep ->> CT_Prep: Generate Mermaid Diagram from relationships\n    CT_Prep ->> CT_Prep: Prepare index.md content (title, summary, diagram, links)\n    CT_Prep ->> CT_Prep: Prepare list of chapter filenames & content\n    CT_Prep -->> CT_Exec: Pass prepared index & chapter data\n\n    CT_Exec ->> FileSystem: Create output directory (e.g., output/project_name)\n    CT_Exec ->> FileSystem: Write index.md content to file\n    loop For each chapter file\n        CT_Exec ->> FileSystem: Write chapter content to its .md file\n    end\n    CT_Exec ->> SharedData: Store final output path\n    CT_Exec -->> PocketFlow: Signal completion\n```\n\n## Code Snippets (`nodes.py` - `CombineTutorial`)\n\nLet's look at simplified snippets from the `CombineTutorial` node to see how it prepares and writes the files.\n\n**1. Preparing the Index and Chapter Data (`prep` method):**\n\nThis part gathers data and builds the structure in memory *before* writing any files.\n\n```python\n# In nodes.py, inside CombineTutorial.prep method\n\n# --- Assume these are loaded from shared data ---\n# project_name = \"My Cool Project\"\n# relationships_data = {\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"Uses\"}, ...]}\n# chapter_order = [0, 1, 2] # Indices of abstractions in order\n# abstractions = [{\"name\": \"Core Logic\", ...}, {\"name\": \"Data Handler\", ...}, {\"name\": \"API Interface\", ...}]\n# chapters_content = [\"Markdown for Core Logic...\", \"Markdown for Data Handler...\", \"...\"]\n# output_base_dir = \"output\"\n# repo_url = \"https://github.com/user/repo\"\n# -------------------------------------------------\n\noutput_path = os.path.join(output_base_dir, project_name) # e.g., \"output/My Cool Project\"\n\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes (e.g., A0[\"Core Logic\"])\nfor i, abstr in enumerate(abstractions):\n    node_id = f\"A{i}\"\n    node_label = abstr['name'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges (e.g., A0 -- \"Uses\" --> A1)\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    edge_label = rel['label'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n\n# --- Prepare index.md content ---\nindex_content = f\"# Tutorial: {project_name}\\n\\n\"\nindex_content += f\"{relationships_data['summary']}\\n\\n\"\nif repo_url:\n    index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\nindex_content += \"```mermaid\\n\" + mermaid_diagram + \"\\n```\\n\\n\"\nindex_content += \"## Chapters\\n\\n\"\n\nchapter_files = [] # To store {filename, content} pairs\n# Generate chapter links and prepare chapter file data\nfor i, abstraction_index in enumerate(chapter_order):\n    abstraction_name = abstractions[abstraction_index][\"name\"]\n    # Create safe filename (e.g., 01_core_logic.md)\n    safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n    filename = f\"{i+1:02d}_{safe_name}.md\"\n    # Add link to index.md content\n    index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n\n    # Prepare chapter content (add attribution)\n    chapter_content = chapters_content[i] + \"\\n\\n---\\n\\nGenerated by ...\" # Add attribution\n    # Store filename and content for later writing\n    chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n\n# Add attribution to index.md\nindex_content += \"\\n\\n---\\n\\nGenerated by ...\"\n\n# Return all prepared data for the exec step\nreturn {\n    \"output_path\": output_path,\n    \"index_content\": index_content,\n    \"chapter_files\": chapter_files\n}\n```\n*Explanation:* The `prep` method acts like the planning stage. It uses the data from previous steps (`relationships`, `chapter_order`, etc.) to generate the Mermaid diagram string and the full text content for `index.md`, including formatted links. It also prepares a list (`chapter_files`) containing the final filename and content for each chapter.\n\n**2. Writing the Files (`exec` method):**\n\nThis part takes the prepared data and actually creates the files and folders.\n\n```python\n# In nodes.py, inside CombineTutorial.exec method\n\n# Get the prepared data from the prep step\noutput_path = prep_res[\"output_path\"]       # e.g., \"output/My Cool Project\"\nindex_content = prep_res[\"index_content\"]   # The full text for index.md\nchapter_files = prep_res[\"chapter_files\"]   # List of {\"filename\": \"...\", \"content\": \"...\"}\n\nprint(f\"Combining tutorial into directory: {output_path}\")\n# Create the directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Write index.md\nindex_filepath = os.path.join(output_path, \"index.md\")\nwith open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n    f.write(index_content)\nprint(f\"  - Wrote {index_filepath}\")\n\n# Write chapter files\nfor chapter_info in chapter_files:\n    chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n    with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(chapter_info[\"content\"])\n    print(f\"  - Wrote {chapter_filepath}\")\n\n# Return the path where the tutorial was saved\nreturn output_path\n```\n*Explanation:* The `exec` method is the action stage. It takes the `output_path`, `index_content`, and `chapter_files` prepared by `prep`. It creates the directory, then simply writes the `index_content` into `index.md` and loops through `chapter_files`, writing each chapter's content into its respective `.md` file.\n\n## Connecting the Dots\n\n*   This `CombineTutorial` node relies heavily on the outputs produced by previous nodes like `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters`. Without their results, it wouldn't have anything to assemble!\n*   The use of indices throughout the process (identifying files, relationships, and order by index) makes it easier for `CombineTutorial` to correctly link everything together at the end.\n\n## Conclusion\n\nIn this chapter, we learned about the final assembly step: **Tutorial Structure Generation**.\n\n*   The `CombineTutorial` node acts as the \"bookbinder\" for our project.\n*   It takes the project summary, relationship data, chapter order, and chapter content generated in earlier steps.\n*   It creates the final output directory.\n*   It generates a **Mermaid diagram** visualizing the relationships between concepts.\n*   It writes the main **`index.md`** file, including the summary, diagram, and an ordered, linked list of chapters.\n*   It writes the individual **chapter `.md` files** with the LLM-generated content and proper filenames.\n\nThis node transforms the intermediate data into a usable, structured set of documentation files, ready for a developer to read!\n\n**Next Up:** We've seen several \"Nodes\" mentioned (`FetchRepo`, `IdentifyAbstractions`, `CombineTutorial`, etc.). What exactly *is* a Node, and how are they defined? Let's explore the building blocks of our workflow in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Node Abstraction\". This is Chapter 5.\n\n        Concept Details:\n        - Description:\n        Individual processing units within the PocketFlow workflow. Each Node performs a specific task, like fetching code, analyzing relationships, or writing a chapter. Nodes have `prep`, `exec`, and `post` stages, making the workflow modular and testable. Examples include `FetchRepo`, `IdentifyAbstractions`, `WriteChapters`.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n---\n# Chapter 3: LLM Analysis & Generation\n\n```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```\n---\n# Chapter 4: Tutorial Structure Generation\n\n```markdown\n# Chapter 4: Tutorial Structure Generation\n\nWelcome back! In [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md), we saw how the \"AI brain\" (the LLM) helps us understand the code by identifying key concepts (abstractions), figuring out how they relate, deciding the best order to teach them, and even writing the first draft of each chapter's content.\n\nNow, imagine you've written all the individual recipes for your cookbook. You have the text for the cake recipe, the cookie recipe, the bread recipe, etc., all on separate pieces of paper. You also have a summary of the whole cookbook and maybe a diagram showing which recipes use similar ingredients. What's the next step? You need to **assemble the cookbook**!\n\nThis chapter is all about that final assembly process: **Tutorial Structure Generation**.\n\n**Goal:** Understand how the tool takes all the generated pieces \u2013 the project summary, the concept relationships, the chapter order, and the written chapter text \u2013 and organizes them into a final, polished tutorial structure, complete with an index file, diagrams, and correctly linked chapter files.\n\n## The Problem: From Scattered Notes to a Polished Book \ud83d\udcda\n\nAfter the previous steps, we have several valuable pieces of information floating around:\n\n1.  **Abstractions:** The list of key concepts identified by the LLM (e.g., \"Client Session\", \"Tools\").\n2.  **Relationships:** How these concepts connect, including a project summary (e.g., \"Client Session calls Tools\").\n3.  **Chapter Order:** The best sequence to explain these concepts (e.g., explain \"Client Session\" first, then \"Tools\").\n4.  **Chapter Content:** The actual Markdown text generated by the LLM for each concept.\n\nBut these are just separate bits of data inside the computer's memory. We need to transform them into something a human can actually use: a set of neatly organized Markdown files that form a coherent tutorial.\n\n**Use Case:** We want to take all the intermediate results generated by the LLMs and the workflow and produce the final output directory. This directory should contain:\n*   An `index.md` file acting as the table of contents, with a summary and a relationship diagram.\n*   Individual chapter files (like `01_client_session.md`, `02_tools.md`) containing the generated text.\n*   Proper links connecting the index to the chapters and the chapters to each other.\n\n## The Solution: The `CombineTutorial` Node - Our Assembler \ud83d\udee0\ufe0f\n\nThe final step in our documentation assembly line ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) is handled by a specific node called `CombineTutorial`. Think of this node as the **final assembler** or the **bookbinder**. Its job is to take all the finished components and put them together correctly.\n\nHere's what the `CombineTutorial` node does:\n\n1.  **Gathers Inputs:** It collects all the necessary pieces created by the previous nodes:\n    *   The project summary and relationship details (from `AnalyzeRelationships`).\n    *   The list of abstractions (from `IdentifyAbstractions`).\n    *   The determined chapter order (from `OrderChapters`).\n    *   The generated Markdown content for each chapter (from `WriteChapters`).\n    *   The project name and desired output location.\n\n2.  **Generates the Relationship Diagram:** It uses the relationship data (which abstraction connects to which, and the label for that connection) to automatically create a visual flowchart using Mermaid syntax. This helps users see the connections at a glance.\n\n3.  **Creates `index.md` Content:** It prepares the text for the main `index.md` file. This includes:\n    *   The project title.\n    *   The high-level summary generated earlier.\n    *   A link to the source code repository (if available).\n    *   The Mermaid diagram it just generated.\n    *   An ordered list of chapters, with links to the corresponding chapter files (e.g., \"1. [Client Session](01_client_session.md)\").\n\n4.  **Prepares Chapter Files:** For each chapter's generated Markdown content:\n    *   It determines the correct filename based on the chapter number and a \"safe\" version of the abstraction name (e.g., replacing spaces with underscores).\n    *   It adds a standard attribution line at the end.\n\n5.  **Writes Everything to Disk:**\n    *   It creates the final output directory (usually named after the project, inside an `output` folder).\n    *   It writes the prepared `index.md` content to `index.md` inside that directory.\n    *   It writes each chapter's prepared content to its corresponding filename (e.g., `01_client_session.md`, `02_tools.md`, etc.) inside the same directory.\n\n## How It Works Under the Hood (Simplified Walkthrough) \ud83d\udeb6\u200d\u2640\ufe0f\n\nLet's follow the `CombineTutorial` node step-by-step:\n\n1.  **Get Ready (`prep`):** The node first gathers all the ingredients it needs from the shared data store: the summary, relationship details (`relationships['details']`), chapter order (`chapter_order`), abstraction list (`abstractions`), chapter content (`chapters`), project name, and output directory path.\n2.  **Draw the Map (`prep`):** It looks at the `relationships['details']` (which contains pairs like `from: 0, to: 1, label: \"Calls\"`) and generates the Mermaid code for the diagram.\n3.  **Write the Introduction (`prep`):** It starts building the `index.md` text string, adding the title, summary, and the Mermaid diagram code.\n4.  **Create the Table of Contents (`prep`):** It iterates through the `chapter_order` list. For each abstraction index in the order, it finds the abstraction's name, creates a safe filename (like `01_client_session.md`), and adds a numbered link like `1. [Client Session](01_client_session.md)` to the `index.md` text. It also prepares a list of all chapter filenames and their corresponding content (adding the attribution).\n5.  **Build the Output Folder (`exec`):** The node creates the actual folder on your computer (e.g., `output/my_project/`).\n6.  **Save the Index (`exec`):** It saves the complete `index.md` text string into the `index.md` file within the new folder.\n7.  **Save the Chapters (`exec`):** It goes through the list of prepared chapter filenames and content, saving each one as a separate `.md` file in the folder.\n8.  **Done! (`post`):** It records the path to the final output directory in the shared store and prints a success message.\n\n**Visualizing the Flow:**\n\nHere's a simple sequence diagram for the `CombineTutorial` node:\n\n```mermaid\nsequenceDiagram\n    participant CT_Prep as CombineTutorial (Prep Phase)\n    participant SharedData as Shared Data Store\n    participant CT_Exec as CombineTutorial (Exec Phase)\n    participant FileSystem as Your Computer's Files\n\n    CT_Prep ->> SharedData: Read summary, relationships, order, abstractions, chapters, project name\n    CT_Prep ->> CT_Prep: Generate Mermaid Diagram from relationships\n    CT_Prep ->> CT_Prep: Prepare index.md content (title, summary, diagram, links)\n    CT_Prep ->> CT_Prep: Prepare list of chapter filenames & content\n    CT_Prep -->> CT_Exec: Pass prepared index & chapter data\n\n    CT_Exec ->> FileSystem: Create output directory (e.g., output/project_name)\n    CT_Exec ->> FileSystem: Write index.md content to file\n    loop For each chapter file\n        CT_Exec ->> FileSystem: Write chapter content to its .md file\n    end\n    CT_Exec ->> SharedData: Store final output path\n    CT_Exec -->> PocketFlow: Signal completion\n```\n\n## Code Snippets (`nodes.py` - `CombineTutorial`)\n\nLet's look at simplified snippets from the `CombineTutorial` node to see how it prepares and writes the files.\n\n**1. Preparing the Index and Chapter Data (`prep` method):**\n\nThis part gathers data and builds the structure in memory *before* writing any files.\n\n```python\n# In nodes.py, inside CombineTutorial.prep method\n\n# --- Assume these are loaded from shared data ---\n# project_name = \"My Cool Project\"\n# relationships_data = {\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"Uses\"}, ...]}\n# chapter_order = [0, 1, 2] # Indices of abstractions in order\n# abstractions = [{\"name\": \"Core Logic\", ...}, {\"name\": \"Data Handler\", ...}, {\"name\": \"API Interface\", ...}]\n# chapters_content = [\"Markdown for Core Logic...\", \"Markdown for Data Handler...\", \"...\"]\n# output_base_dir = \"output\"\n# repo_url = \"https://github.com/user/repo\"\n# -------------------------------------------------\n\noutput_path = os.path.join(output_base_dir, project_name) # e.g., \"output/My Cool Project\"\n\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes (e.g., A0[\"Core Logic\"])\nfor i, abstr in enumerate(abstractions):\n    node_id = f\"A{i}\"\n    node_label = abstr['name'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges (e.g., A0 -- \"Uses\" --> A1)\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    edge_label = rel['label'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n\n# --- Prepare index.md content ---\nindex_content = f\"# Tutorial: {project_name}\\n\\n\"\nindex_content += f\"{relationships_data['summary']}\\n\\n\"\nif repo_url:\n    index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\nindex_content += \"```mermaid\\n\" + mermaid_diagram + \"\\n```\\n\\n\"\nindex_content += \"## Chapters\\n\\n\"\n\nchapter_files = [] # To store {filename, content} pairs\n# Generate chapter links and prepare chapter file data\nfor i, abstraction_index in enumerate(chapter_order):\n    abstraction_name = abstractions[abstraction_index][\"name\"]\n    # Create safe filename (e.g., 01_core_logic.md)\n    safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n    filename = f\"{i+1:02d}_{safe_name}.md\"\n    # Add link to index.md content\n    index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n\n    # Prepare chapter content (add attribution)\n    chapter_content = chapters_content[i] + \"\\n\\n---\\n\\nGenerated by ...\" # Add attribution\n    # Store filename and content for later writing\n    chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n\n# Add attribution to index.md\nindex_content += \"\\n\\n---\\n\\nGenerated by ...\"\n\n# Return all prepared data for the exec step\nreturn {\n    \"output_path\": output_path,\n    \"index_content\": index_content,\n    \"chapter_files\": chapter_files\n}\n```\n*Explanation:* The `prep` method acts like the planning stage. It uses the data from previous steps (`relationships`, `chapter_order`, etc.) to generate the Mermaid diagram string and the full text content for `index.md`, including formatted links. It also prepares a list (`chapter_files`) containing the final filename and content for each chapter.\n\n**2. Writing the Files (`exec` method):**\n\nThis part takes the prepared data and actually creates the files and folders.\n\n```python\n# In nodes.py, inside CombineTutorial.exec method\n\n# Get the prepared data from the prep step\noutput_path = prep_res[\"output_path\"]       # e.g., \"output/My Cool Project\"\nindex_content = prep_res[\"index_content\"]   # The full text for index.md\nchapter_files = prep_res[\"chapter_files\"]   # List of {\"filename\": \"...\", \"content\": \"...\"}\n\nprint(f\"Combining tutorial into directory: {output_path}\")\n# Create the directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Write index.md\nindex_filepath = os.path.join(output_path, \"index.md\")\nwith open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n    f.write(index_content)\nprint(f\"  - Wrote {index_filepath}\")\n\n# Write chapter files\nfor chapter_info in chapter_files:\n    chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n    with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(chapter_info[\"content\"])\n    print(f\"  - Wrote {chapter_filepath}\")\n\n# Return the path where the tutorial was saved\nreturn output_path\n```\n*Explanation:* The `exec` method is the action stage. It takes the `output_path`, `index_content`, and `chapter_files` prepared by `prep`. It creates the directory, then simply writes the `index_content` into `index.md` and loops through `chapter_files`, writing each chapter's content into its respective `.md` file.\n\n## Connecting the Dots\n\n*   This `CombineTutorial` node relies heavily on the outputs produced by previous nodes like `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters`. Without their results, it wouldn't have anything to assemble!\n*   The use of indices throughout the process (identifying files, relationships, and order by index) makes it easier for `CombineTutorial` to correctly link everything together at the end.\n\n## Conclusion\n\nIn this chapter, we learned about the final assembly step: **Tutorial Structure Generation**.\n\n*   The `CombineTutorial` node acts as the \"bookbinder\" for our project.\n*   It takes the project summary, relationship data, chapter order, and chapter content generated in earlier steps.\n*   It creates the final output directory.\n*   It generates a **Mermaid diagram** visualizing the relationships between concepts.\n*   It writes the main **`index.md`** file, including the summary, diagram, and an ordered, linked list of chapters.\n*   It writes the individual **chapter `.md` files** with the LLM-generated content and proper filenames.\n\nThis node transforms the intermediate data into a usable, structured set of documentation files, ready for a developer to read!\n\n**Next Up:** We've seen several \"Nodes\" mentioned (`FetchRepo`, `IdentifyAbstractions`, `CombineTutorial`, etc.). What exactly *is* a Node, and how are they defined? Let's explore the building blocks of our workflow in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n```\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\flow.py ---\nfrom pocketflow import Flow\n\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial,\n    # New nodes for Streamlit UI\n    SmartSearchRepo,\n    FilterRepos,\n    SelectRepository,\n    RenderAndDownload\n)\n\n\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow, fetch_repo, combine_tutorial\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n    \n    # Create the original tutorial flow and get the first and last nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n    \n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n    \n    # Connect the UI nodes\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n    \n    # Connect the select_repo node to the FetchRepo node\n    select_repo >> fetch_repo\n    \n    # Add the render and download node after CombineTutorial\n    combine_tutorial >> render_download\n    \n    # Create the full UI flow\n    ui_flow = Flow(start=smart_search)\n    \n    return ui_flow\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 5: Node Abstraction`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "```markdown\n# Chapter 5: Node Abstraction\n\nWelcome back! In [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md), we saw how the `CombineTutorial` step acts like a bookbinder, taking all the generated pieces \u2013 summaries, chapter text, diagrams \u2013 and assembling them into the final tutorial files.\n\nNow, let's zoom out a bit. We've mentioned several specific steps like `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, and `CombineTutorial`. These are the core workers in our documentation \"assembly line\". How are these individual workers defined? What makes them tick?\n\nThis chapter dives into the fundamental building block of our workflow: the **Node Abstraction**.\n\n**Goal:** Understand what a \"Node\" is in the PocketFlow workflow, learn about its three stages (`prep`, `exec`, `post`), and see how this structure helps keep our complex process organized, modular, and easier to manage.\n\n## The Problem: Managing a Complex Recipe \ud83c\udf5d\n\nImagine trying to cook a complex multi-course meal. If you just had one giant page of instructions mixing everything together \u2013 chopping vegetables for the salad while boiling pasta and preheating the oven for dessert \u2013 it would be incredibly confusing and prone to errors.\n\nSimilarly, generating a code tutorial involves many different tasks: getting code, analyzing it with AI, figuring out relationships, deciding the order, writing text, formatting output. Trying to write one massive script to do all of this would be a nightmare!\n\n**Use Case:** How can we break down the complex task of generating a tutorial into smaller, manageable, and reusable pieces? We need a way to define distinct steps, handle their inputs and outputs cleanly, and ensure they work together smoothly.\n\n## The Solution: Nodes - The Specialized Workers \ud83e\uddd1\u200d\ud83d\udd27\n\nIn the `LLM-CODEBASE-DOCUMENTOR`, each specific task is handled by a **Node**. Think of our PocketFlow workflow (from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) as an assembly line, and each **Node** is a specialized station or worker on that line.\n\n*   `FetchRepo` is the station that **gets the raw materials** (source code).\n*   `IdentifyAbstractions` is the station that **inspects the materials** to find the main parts.\n*   `AnalyzeRelationships` figures out **how the parts connect**.\n*   `OrderChapters` decides the **assembly sequence**.\n*   `WriteChapters` **builds each component** (writes the chapter text).\n*   `CombineTutorial` performs the **final assembly and packaging**.\n\nEach Node has a very specific job description. It knows what inputs it needs, what task it performs, and what output it produces.\n\n## The Three Stages of a Node: Prep, Exec, Post\n\nTo make these workers efficient and organized, every Node follows a standard three-stage process:\n\n1.  **`prep` (Prepare / Get Ready):**\n    *   **What it does:** Gathers all the necessary inputs (ingredients and tools) needed for the main task. It usually reads data from the central `shared` data store (we'll cover this in [Chapter 6: Shared Data Store](06_shared_data_store.md)). It might also do some initial setup or validation.\n    *   **Analogy:** A chef reading the recipe, gathering ingredients from the pantry, and getting out the right knives and bowls.\n    *   **Output:** Passes the prepared inputs to the `exec` stage.\n\n2.  **`exec` (Execute / Do the Work):**\n    *   **What it does:** Performs the Node's main job. This is where the core logic happens \u2013 calling an external service (like the GitHub API or an LLM), running a calculation, processing data, etc.\n    *   **Analogy:** The chef chopping the vegetables, mixing the batter, or putting the dish in the oven.\n    *   **Output:** Returns the result of its work to the `post` stage.\n\n3.  **`post` (Post-process / Clean Up & Store):**\n    *   **What it does:** Takes the raw result from the `exec` stage, cleans it up or formats it if necessary, and most importantly, saves the final output back into the `shared` data store so that subsequent Nodes can use it.\n    *   **Analogy:** The chef plating the finished dish, garnishing it, and putting the plate on the serving counter (the shared store) for the waiter (the next Node). They might also quickly clean their station.\n\nThis `prep -> exec -> post` structure ensures that each Node clearly separates gathering inputs, doing the work, and storing outputs.\n\n## Example Walkthrough: The `FetchRepo` Node\n\nLet's revisit the `FetchRepo` node, which is responsible for getting the source code. Here's how its `prep`, `exec`, and `post` stages work (simplified):\n\n**1. `prep` Stage:**\n\n*   **Goal:** Get the repository URL or local directory path, filtering rules, and GitHub token (if needed) from the shared store. Prepare these for the crawler functions.\n*   **Analogy:** Get the cookbook (shared store), find the recipe source (URL or local path), check for ingredient preferences (filters), and get the special key to the pantry (token).\n\n```python\n# Simplified from nodes.py - FetchRepo.prep\n\ndef prep(self, shared):\n    # 1. Read necessary info from the shared store\n    repo_url = shared.get(\"repo_url\")\n    local_dir = shared.get(\"local_dir\")\n    github_token = shared.get(\"github_token\")\n    include_patterns = shared[\"include_patterns\"]\n    exclude_patterns = shared[\"exclude_patterns\"]\n    max_file_size = shared[\"max_file_size\"]\n\n    # (Determine project_name if not already set - omitted for simplicity)\n\n    # 2. Prepare a dictionary of inputs for the 'exec' stage\n    inputs_for_exec = {\n        \"repo_url\": repo_url,\n        \"local_dir\": local_dir,\n        \"token\": github_token,\n        \"include_patterns\": include_patterns,\n        \"exclude_patterns\": exclude_patterns,\n        \"max_file_size\": max_file_size,\n        \"use_relative_paths\": True # Example setting\n    }\n    return inputs_for_exec # Pass prepared inputs to exec\n```\n\n**2. `exec` Stage:**\n\n*   **Goal:** Based on the inputs from `prep`, call the correct utility function (`crawl_github_files` or `crawl_local_files`) to actually fetch the code.\n*   **Analogy:** If you have a URL, call the \"Online Shopper\" service. If you have a local path, call the \"Pantry Raider\" service. Get the list of ingredients (files).\n\n```python\n# Simplified from nodes.py - FetchRepo.exec\n\n# 'prep_res' contains the dictionary returned by the 'prep' stage\ndef exec(self, prep_res):\n    # 1. Check if we are fetching from GitHub or local directory\n    if prep_res[\"repo_url\"]:\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Call the GitHub utility function with prepared inputs\n        result = crawl_github_files(\n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    elif prep_res[\"local_dir\"]:\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Call the local directory utility function\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    else:\n        # Handle case where neither is provided (should not happen normally)\n        result = {\"files\": {}}\n\n    # 2. Process the result (convert dict to list of (path, content))\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n\n    return files_list # Return the raw result (list of files) to post\n```\n\n**3. `post` Stage:**\n\n*   **Goal:** Take the list of files returned by `exec` and save it into the shared store under the key `\"files\"`.\n*   **Analogy:** Take the bag of ingredients (list of files) and place it neatly on the shared kitchen counter (shared store) labeled \"Fetched Files\".\n\n```python\n# Simplified from nodes.py - FetchRepo.post\n\n# 'shared' is the shared data store\n# 'prep_res' is the output of prep (not usually needed here)\n# 'exec_res' is the list of files returned by exec\ndef post(self, shared, prep_res, exec_res):\n    # 1. Save the result from 'exec' into the shared store\n    shared[\"files\"] = exec_res\n    print(\"Saved fetched files list to shared store.\")\n    # (Could also save project_name here if prep determined it)\n```\n\n**Internal Node Flow Diagram:**\n\nHere's how the data flows within a typical Node like `FetchRepo`:\n\n```mermaid\nsequenceDiagram\n    participant Prep as FetchRepo.prep\n    participant SharedIn as Shared Store (Input)\n    participant Exec as FetchRepo.exec\n    participant Crawler as (crawl_github_files / crawl_local_files)\n    participant Post as FetchRepo.post\n    participant SharedOut as Shared Store (Output)\n\n    Prep ->> SharedIn: Read repo_url, local_dir, patterns, etc.\n    Prep -->> Exec: Pass prepared parameters (e.g., token, patterns)\n    Exec ->> Crawler: Call appropriate crawler function\n    Crawler -->> Exec: Return raw file data (dict)\n    Exec -->> Post: Pass processed result (list of files)\n    Post ->> SharedOut: Write file list to shared[\"files\"]\n```\n\nThis clear separation of concerns (`prep`, `exec`, `post`) makes each Node easier to understand and manage.\n\n## Benefits of Node Abstraction\n\nUsing Nodes to structure our workflow provides several advantages:\n\n*   **Modularity:** Each Node is a self-contained unit with a single responsibility. This makes the overall system easier to understand, as you can focus on one task at a time. Need to change how code is fetched? Just look at the `FetchRepo` node. Need to improve chapter writing? Focus on `WriteChapters`.\n*   **Reusability:** While not heavily utilized in this specific project, Nodes designed for common tasks could potentially be reused across different workflows.\n*   **Testability:** This is a huge benefit! You can test each Node individually. You can create fake input data (`shared` state before the node runs), run the Node's `prep`, `exec`, and `post` stages, and then check if the output in the `shared` store is what you expected. This makes debugging much easier.\n*   **Clarity:** When you look at the workflow definition in `flow.py` (remember the `>>` connections from Chapter 2?), you see a high-level view of the process connecting these well-defined Nodes. It's like reading a flowchart of the main steps, rather than digging through one giant script.\n\n```python\n# From flow.py (Illustrating clarity)\n\n# Define the stations (Nodes)\nfetch_repo = FetchRepo()\nidentify_abstractions = IdentifyAbstractions(...)\n# ... other nodes\n\n# Connect them clearly\nfetch_repo >> identify_abstractions >> analyze_relationships # ... etc\n\n# Define the starting point\ntutorial_flow = Flow(start=fetch_repo)\n```\n\n## Conclusion\n\nIn this chapter, we learned about the **Node Abstraction**, the core building block of the PocketFlow workflow used in `LLM-CODEBASE-DOCUMENTOR`.\n\n*   A Node is like a **specialized worker** performing one specific task in the documentation generation process (e.g., `FetchRepo`, `WriteChapters`).\n*   Each Node follows a three-stage lifecycle:\n    *   **`prep`**: Gathers inputs from the shared store.\n    *   **`exec`**: Performs the main task.\n    *   **`post`**: Stores the results back into the shared store.\n*   This structure promotes **modularity**, **reusability**, **testability**, and **clarity** in our complex workflow.\n\nNodes handle *what* gets done and *how* it's done internally. But how do they communicate? How does the output of `FetchRepo` actually become the input for `IdentifyAbstractions`? That's where the central communication hub comes in.\n\n**Next Up:** Explore the mechanism that allows Nodes to share data with each other in [Chapter 6: Shared Data Store](06_shared_data_store.md).\n```", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Shared Data Store\". This is Chapter 6.\n\n        Concept Details:\n        - Description:\n        A central Python dictionary (`shared`) that holds the state and data throughout the workflow execution. Nodes read input data from this store and write their results back into it, allowing information to flow between different processing steps.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n---\n# Chapter 3: LLM Analysis & Generation\n\n```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```\n---\n# Chapter 4: Tutorial Structure Generation\n\n```markdown\n# Chapter 4: Tutorial Structure Generation\n\nWelcome back! In [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md), we saw how the \"AI brain\" (the LLM) helps us understand the code by identifying key concepts (abstractions), figuring out how they relate, deciding the best order to teach them, and even writing the first draft of each chapter's content.\n\nNow, imagine you've written all the individual recipes for your cookbook. You have the text for the cake recipe, the cookie recipe, the bread recipe, etc., all on separate pieces of paper. You also have a summary of the whole cookbook and maybe a diagram showing which recipes use similar ingredients. What's the next step? You need to **assemble the cookbook**!\n\nThis chapter is all about that final assembly process: **Tutorial Structure Generation**.\n\n**Goal:** Understand how the tool takes all the generated pieces \u2013 the project summary, the concept relationships, the chapter order, and the written chapter text \u2013 and organizes them into a final, polished tutorial structure, complete with an index file, diagrams, and correctly linked chapter files.\n\n## The Problem: From Scattered Notes to a Polished Book \ud83d\udcda\n\nAfter the previous steps, we have several valuable pieces of information floating around:\n\n1.  **Abstractions:** The list of key concepts identified by the LLM (e.g., \"Client Session\", \"Tools\").\n2.  **Relationships:** How these concepts connect, including a project summary (e.g., \"Client Session calls Tools\").\n3.  **Chapter Order:** The best sequence to explain these concepts (e.g., explain \"Client Session\" first, then \"Tools\").\n4.  **Chapter Content:** The actual Markdown text generated by the LLM for each concept.\n\nBut these are just separate bits of data inside the computer's memory. We need to transform them into something a human can actually use: a set of neatly organized Markdown files that form a coherent tutorial.\n\n**Use Case:** We want to take all the intermediate results generated by the LLMs and the workflow and produce the final output directory. This directory should contain:\n*   An `index.md` file acting as the table of contents, with a summary and a relationship diagram.\n*   Individual chapter files (like `01_client_session.md`, `02_tools.md`) containing the generated text.\n*   Proper links connecting the index to the chapters and the chapters to each other.\n\n## The Solution: The `CombineTutorial` Node - Our Assembler \ud83d\udee0\ufe0f\n\nThe final step in our documentation assembly line ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) is handled by a specific node called `CombineTutorial`. Think of this node as the **final assembler** or the **bookbinder**. Its job is to take all the finished components and put them together correctly.\n\nHere's what the `CombineTutorial` node does:\n\n1.  **Gathers Inputs:** It collects all the necessary pieces created by the previous nodes:\n    *   The project summary and relationship details (from `AnalyzeRelationships`).\n    *   The list of abstractions (from `IdentifyAbstractions`).\n    *   The determined chapter order (from `OrderChapters`).\n    *   The generated Markdown content for each chapter (from `WriteChapters`).\n    *   The project name and desired output location.\n\n2.  **Generates the Relationship Diagram:** It uses the relationship data (which abstraction connects to which, and the label for that connection) to automatically create a visual flowchart using Mermaid syntax. This helps users see the connections at a glance.\n\n3.  **Creates `index.md` Content:** It prepares the text for the main `index.md` file. This includes:\n    *   The project title.\n    *   The high-level summary generated earlier.\n    *   A link to the source code repository (if available).\n    *   The Mermaid diagram it just generated.\n    *   An ordered list of chapters, with links to the corresponding chapter files (e.g., \"1. [Client Session](01_client_session.md)\").\n\n4.  **Prepares Chapter Files:** For each chapter's generated Markdown content:\n    *   It determines the correct filename based on the chapter number and a \"safe\" version of the abstraction name (e.g., replacing spaces with underscores).\n    *   It adds a standard attribution line at the end.\n\n5.  **Writes Everything to Disk:**\n    *   It creates the final output directory (usually named after the project, inside an `output` folder).\n    *   It writes the prepared `index.md` content to `index.md` inside that directory.\n    *   It writes each chapter's prepared content to its corresponding filename (e.g., `01_client_session.md`, `02_tools.md`, etc.) inside the same directory.\n\n## How It Works Under the Hood (Simplified Walkthrough) \ud83d\udeb6\u200d\u2640\ufe0f\n\nLet's follow the `CombineTutorial` node step-by-step:\n\n1.  **Get Ready (`prep`):** The node first gathers all the ingredients it needs from the shared data store: the summary, relationship details (`relationships['details']`), chapter order (`chapter_order`), abstraction list (`abstractions`), chapter content (`chapters`), project name, and output directory path.\n2.  **Draw the Map (`prep`):** It looks at the `relationships['details']` (which contains pairs like `from: 0, to: 1, label: \"Calls\"`) and generates the Mermaid code for the diagram.\n3.  **Write the Introduction (`prep`):** It starts building the `index.md` text string, adding the title, summary, and the Mermaid diagram code.\n4.  **Create the Table of Contents (`prep`):** It iterates through the `chapter_order` list. For each abstraction index in the order, it finds the abstraction's name, creates a safe filename (like `01_client_session.md`), and adds a numbered link like `1. [Client Session](01_client_session.md)` to the `index.md` text. It also prepares a list of all chapter filenames and their corresponding content (adding the attribution).\n5.  **Build the Output Folder (`exec`):** The node creates the actual folder on your computer (e.g., `output/my_project/`).\n6.  **Save the Index (`exec`):** It saves the complete `index.md` text string into the `index.md` file within the new folder.\n7.  **Save the Chapters (`exec`):** It goes through the list of prepared chapter filenames and content, saving each one as a separate `.md` file in the folder.\n8.  **Done! (`post`):** It records the path to the final output directory in the shared store and prints a success message.\n\n**Visualizing the Flow:**\n\nHere's a simple sequence diagram for the `CombineTutorial` node:\n\n```mermaid\nsequenceDiagram\n    participant CT_Prep as CombineTutorial (Prep Phase)\n    participant SharedData as Shared Data Store\n    participant CT_Exec as CombineTutorial (Exec Phase)\n    participant FileSystem as Your Computer's Files\n\n    CT_Prep ->> SharedData: Read summary, relationships, order, abstractions, chapters, project name\n    CT_Prep ->> CT_Prep: Generate Mermaid Diagram from relationships\n    CT_Prep ->> CT_Prep: Prepare index.md content (title, summary, diagram, links)\n    CT_Prep ->> CT_Prep: Prepare list of chapter filenames & content\n    CT_Prep -->> CT_Exec: Pass prepared index & chapter data\n\n    CT_Exec ->> FileSystem: Create output directory (e.g., output/project_name)\n    CT_Exec ->> FileSystem: Write index.md content to file\n    loop For each chapter file\n        CT_Exec ->> FileSystem: Write chapter content to its .md file\n    end\n    CT_Exec ->> SharedData: Store final output path\n    CT_Exec -->> PocketFlow: Signal completion\n```\n\n## Code Snippets (`nodes.py` - `CombineTutorial`)\n\nLet's look at simplified snippets from the `CombineTutorial` node to see how it prepares and writes the files.\n\n**1. Preparing the Index and Chapter Data (`prep` method):**\n\nThis part gathers data and builds the structure in memory *before* writing any files.\n\n```python\n# In nodes.py, inside CombineTutorial.prep method\n\n# --- Assume these are loaded from shared data ---\n# project_name = \"My Cool Project\"\n# relationships_data = {\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"Uses\"}, ...]}\n# chapter_order = [0, 1, 2] # Indices of abstractions in order\n# abstractions = [{\"name\": \"Core Logic\", ...}, {\"name\": \"Data Handler\", ...}, {\"name\": \"API Interface\", ...}]\n# chapters_content = [\"Markdown for Core Logic...\", \"Markdown for Data Handler...\", \"...\"]\n# output_base_dir = \"output\"\n# repo_url = \"https://github.com/user/repo\"\n# -------------------------------------------------\n\noutput_path = os.path.join(output_base_dir, project_name) # e.g., \"output/My Cool Project\"\n\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes (e.g., A0[\"Core Logic\"])\nfor i, abstr in enumerate(abstractions):\n    node_id = f\"A{i}\"\n    node_label = abstr['name'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges (e.g., A0 -- \"Uses\" --> A1)\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    edge_label = rel['label'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n\n# --- Prepare index.md content ---\nindex_content = f\"# Tutorial: {project_name}\\n\\n\"\nindex_content += f\"{relationships_data['summary']}\\n\\n\"\nif repo_url:\n    index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\nindex_content += \"```mermaid\\n\" + mermaid_diagram + \"\\n```\\n\\n\"\nindex_content += \"## Chapters\\n\\n\"\n\nchapter_files = [] # To store {filename, content} pairs\n# Generate chapter links and prepare chapter file data\nfor i, abstraction_index in enumerate(chapter_order):\n    abstraction_name = abstractions[abstraction_index][\"name\"]\n    # Create safe filename (e.g., 01_core_logic.md)\n    safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n    filename = f\"{i+1:02d}_{safe_name}.md\"\n    # Add link to index.md content\n    index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n\n    # Prepare chapter content (add attribution)\n    chapter_content = chapters_content[i] + \"\\n\\n---\\n\\nGenerated by ...\" # Add attribution\n    # Store filename and content for later writing\n    chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n\n# Add attribution to index.md\nindex_content += \"\\n\\n---\\n\\nGenerated by ...\"\n\n# Return all prepared data for the exec step\nreturn {\n    \"output_path\": output_path,\n    \"index_content\": index_content,\n    \"chapter_files\": chapter_files\n}\n```\n*Explanation:* The `prep` method acts like the planning stage. It uses the data from previous steps (`relationships`, `chapter_order`, etc.) to generate the Mermaid diagram string and the full text content for `index.md`, including formatted links. It also prepares a list (`chapter_files`) containing the final filename and content for each chapter.\n\n**2. Writing the Files (`exec` method):**\n\nThis part takes the prepared data and actually creates the files and folders.\n\n```python\n# In nodes.py, inside CombineTutorial.exec method\n\n# Get the prepared data from the prep step\noutput_path = prep_res[\"output_path\"]       # e.g., \"output/My Cool Project\"\nindex_content = prep_res[\"index_content\"]   # The full text for index.md\nchapter_files = prep_res[\"chapter_files\"]   # List of {\"filename\": \"...\", \"content\": \"...\"}\n\nprint(f\"Combining tutorial into directory: {output_path}\")\n# Create the directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Write index.md\nindex_filepath = os.path.join(output_path, \"index.md\")\nwith open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n    f.write(index_content)\nprint(f\"  - Wrote {index_filepath}\")\n\n# Write chapter files\nfor chapter_info in chapter_files:\n    chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n    with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(chapter_info[\"content\"])\n    print(f\"  - Wrote {chapter_filepath}\")\n\n# Return the path where the tutorial was saved\nreturn output_path\n```\n*Explanation:* The `exec` method is the action stage. It takes the `output_path`, `index_content`, and `chapter_files` prepared by `prep`. It creates the directory, then simply writes the `index_content` into `index.md` and loops through `chapter_files`, writing each chapter's content into its respective `.md` file.\n\n## Connecting the Dots\n\n*   This `CombineTutorial` node relies heavily on the outputs produced by previous nodes like `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters`. Without their results, it wouldn't have anything to assemble!\n*   The use of indices throughout the process (identifying files, relationships, and order by index) makes it easier for `CombineTutorial` to correctly link everything together at the end.\n\n## Conclusion\n\nIn this chapter, we learned about the final assembly step: **Tutorial Structure Generation**.\n\n*   The `CombineTutorial` node acts as the \"bookbinder\" for our project.\n*   It takes the project summary, relationship data, chapter order, and chapter content generated in earlier steps.\n*   It creates the final output directory.\n*   It generates a **Mermaid diagram** visualizing the relationships between concepts.\n*   It writes the main **`index.md`** file, including the summary, diagram, and an ordered, linked list of chapters.\n*   It writes the individual **chapter `.md` files** with the LLM-generated content and proper filenames.\n\nThis node transforms the intermediate data into a usable, structured set of documentation files, ready for a developer to read!\n\n**Next Up:** We've seen several \"Nodes\" mentioned (`FetchRepo`, `IdentifyAbstractions`, `CombineTutorial`, etc.). What exactly *is* a Node, and how are they defined? Let's explore the building blocks of our workflow in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n```\n---\n# Chapter 5: Node Abstraction\n\n```markdown\n# Chapter 5: Node Abstraction\n\nWelcome back! In [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md), we saw how the `CombineTutorial` step acts like a bookbinder, taking all the generated pieces \u2013 summaries, chapter text, diagrams \u2013 and assembling them into the final tutorial files.\n\nNow, let's zoom out a bit. We've mentioned several specific steps like `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, and `CombineTutorial`. These are the core workers in our documentation \"assembly line\". How are these individual workers defined? What makes them tick?\n\nThis chapter dives into the fundamental building block of our workflow: the **Node Abstraction**.\n\n**Goal:** Understand what a \"Node\" is in the PocketFlow workflow, learn about its three stages (`prep`, `exec`, `post`), and see how this structure helps keep our complex process organized, modular, and easier to manage.\n\n## The Problem: Managing a Complex Recipe \ud83c\udf5d\n\nImagine trying to cook a complex multi-course meal. If you just had one giant page of instructions mixing everything together \u2013 chopping vegetables for the salad while boiling pasta and preheating the oven for dessert \u2013 it would be incredibly confusing and prone to errors.\n\nSimilarly, generating a code tutorial involves many different tasks: getting code, analyzing it with AI, figuring out relationships, deciding the order, writing text, formatting output. Trying to write one massive script to do all of this would be a nightmare!\n\n**Use Case:** How can we break down the complex task of generating a tutorial into smaller, manageable, and reusable pieces? We need a way to define distinct steps, handle their inputs and outputs cleanly, and ensure they work together smoothly.\n\n## The Solution: Nodes - The Specialized Workers \ud83e\uddd1\u200d\ud83d\udd27\n\nIn the `LLM-CODEBASE-DOCUMENTOR`, each specific task is handled by a **Node**. Think of our PocketFlow workflow (from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) as an assembly line, and each **Node** is a specialized station or worker on that line.\n\n*   `FetchRepo` is the station that **gets the raw materials** (source code).\n*   `IdentifyAbstractions` is the station that **inspects the materials** to find the main parts.\n*   `AnalyzeRelationships` figures out **how the parts connect**.\n*   `OrderChapters` decides the **assembly sequence**.\n*   `WriteChapters` **builds each component** (writes the chapter text).\n*   `CombineTutorial` performs the **final assembly and packaging**.\n\nEach Node has a very specific job description. It knows what inputs it needs, what task it performs, and what output it produces.\n\n## The Three Stages of a Node: Prep, Exec, Post\n\nTo make these workers efficient and organized, every Node follows a standard three-stage process:\n\n1.  **`prep` (Prepare / Get Ready):**\n    *   **What it does:** Gathers all the necessary inputs (ingredients and tools) needed for the main task. It usually reads data from the central `shared` data store (we'll cover this in [Chapter 6: Shared Data Store](06_shared_data_store.md)). It might also do some initial setup or validation.\n    *   **Analogy:** A chef reading the recipe, gathering ingredients from the pantry, and getting out the right knives and bowls.\n    *   **Output:** Passes the prepared inputs to the `exec` stage.\n\n2.  **`exec` (Execute / Do the Work):**\n    *   **What it does:** Performs the Node's main job. This is where the core logic happens \u2013 calling an external service (like the GitHub API or an LLM), running a calculation, processing data, etc.\n    *   **Analogy:** The chef chopping the vegetables, mixing the batter, or putting the dish in the oven.\n    *   **Output:** Returns the result of its work to the `post` stage.\n\n3.  **`post` (Post-process / Clean Up & Store):**\n    *   **What it does:** Takes the raw result from the `exec` stage, cleans it up or formats it if necessary, and most importantly, saves the final output back into the `shared` data store so that subsequent Nodes can use it.\n    *   **Analogy:** The chef plating the finished dish, garnishing it, and putting the plate on the serving counter (the shared store) for the waiter (the next Node). They might also quickly clean their station.\n\nThis `prep -> exec -> post` structure ensures that each Node clearly separates gathering inputs, doing the work, and storing outputs.\n\n## Example Walkthrough: The `FetchRepo` Node\n\nLet's revisit the `FetchRepo` node, which is responsible for getting the source code. Here's how its `prep`, `exec`, and `post` stages work (simplified):\n\n**1. `prep` Stage:**\n\n*   **Goal:** Get the repository URL or local directory path, filtering rules, and GitHub token (if needed) from the shared store. Prepare these for the crawler functions.\n*   **Analogy:** Get the cookbook (shared store), find the recipe source (URL or local path), check for ingredient preferences (filters), and get the special key to the pantry (token).\n\n```python\n# Simplified from nodes.py - FetchRepo.prep\n\ndef prep(self, shared):\n    # 1. Read necessary info from the shared store\n    repo_url = shared.get(\"repo_url\")\n    local_dir = shared.get(\"local_dir\")\n    github_token = shared.get(\"github_token\")\n    include_patterns = shared[\"include_patterns\"]\n    exclude_patterns = shared[\"exclude_patterns\"]\n    max_file_size = shared[\"max_file_size\"]\n\n    # (Determine project_name if not already set - omitted for simplicity)\n\n    # 2. Prepare a dictionary of inputs for the 'exec' stage\n    inputs_for_exec = {\n        \"repo_url\": repo_url,\n        \"local_dir\": local_dir,\n        \"token\": github_token,\n        \"include_patterns\": include_patterns,\n        \"exclude_patterns\": exclude_patterns,\n        \"max_file_size\": max_file_size,\n        \"use_relative_paths\": True # Example setting\n    }\n    return inputs_for_exec # Pass prepared inputs to exec\n```\n\n**2. `exec` Stage:**\n\n*   **Goal:** Based on the inputs from `prep`, call the correct utility function (`crawl_github_files` or `crawl_local_files`) to actually fetch the code.\n*   **Analogy:** If you have a URL, call the \"Online Shopper\" service. If you have a local path, call the \"Pantry Raider\" service. Get the list of ingredients (files).\n\n```python\n# Simplified from nodes.py - FetchRepo.exec\n\n# 'prep_res' contains the dictionary returned by the 'prep' stage\ndef exec(self, prep_res):\n    # 1. Check if we are fetching from GitHub or local directory\n    if prep_res[\"repo_url\"]:\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Call the GitHub utility function with prepared inputs\n        result = crawl_github_files(\n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    elif prep_res[\"local_dir\"]:\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Call the local directory utility function\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    else:\n        # Handle case where neither is provided (should not happen normally)\n        result = {\"files\": {}}\n\n    # 2. Process the result (convert dict to list of (path, content))\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n\n    return files_list # Return the raw result (list of files) to post\n```\n\n**3. `post` Stage:**\n\n*   **Goal:** Take the list of files returned by `exec` and save it into the shared store under the key `\"files\"`.\n*   **Analogy:** Take the bag of ingredients (list of files) and place it neatly on the shared kitchen counter (shared store) labeled \"Fetched Files\".\n\n```python\n# Simplified from nodes.py - FetchRepo.post\n\n# 'shared' is the shared data store\n# 'prep_res' is the output of prep (not usually needed here)\n# 'exec_res' is the list of files returned by exec\ndef post(self, shared, prep_res, exec_res):\n    # 1. Save the result from 'exec' into the shared store\n    shared[\"files\"] = exec_res\n    print(\"Saved fetched files list to shared store.\")\n    # (Could also save project_name here if prep determined it)\n```\n\n**Internal Node Flow Diagram:**\n\nHere's how the data flows within a typical Node like `FetchRepo`:\n\n```mermaid\nsequenceDiagram\n    participant Prep as FetchRepo.prep\n    participant SharedIn as Shared Store (Input)\n    participant Exec as FetchRepo.exec\n    participant Crawler as (crawl_github_files / crawl_local_files)\n    participant Post as FetchRepo.post\n    participant SharedOut as Shared Store (Output)\n\n    Prep ->> SharedIn: Read repo_url, local_dir, patterns, etc.\n    Prep -->> Exec: Pass prepared parameters (e.g., token, patterns)\n    Exec ->> Crawler: Call appropriate crawler function\n    Crawler -->> Exec: Return raw file data (dict)\n    Exec -->> Post: Pass processed result (list of files)\n    Post ->> SharedOut: Write file list to shared[\"files\"]\n```\n\nThis clear separation of concerns (`prep`, `exec`, `post`) makes each Node easier to understand and manage.\n\n## Benefits of Node Abstraction\n\nUsing Nodes to structure our workflow provides several advantages:\n\n*   **Modularity:** Each Node is a self-contained unit with a single responsibility. This makes the overall system easier to understand, as you can focus on one task at a time. Need to change how code is fetched? Just look at the `FetchRepo` node. Need to improve chapter writing? Focus on `WriteChapters`.\n*   **Reusability:** While not heavily utilized in this specific project, Nodes designed for common tasks could potentially be reused across different workflows.\n*   **Testability:** This is a huge benefit! You can test each Node individually. You can create fake input data (`shared` state before the node runs), run the Node's `prep`, `exec`, and `post` stages, and then check if the output in the `shared` store is what you expected. This makes debugging much easier.\n*   **Clarity:** When you look at the workflow definition in `flow.py` (remember the `>>` connections from Chapter 2?), you see a high-level view of the process connecting these well-defined Nodes. It's like reading a flowchart of the main steps, rather than digging through one giant script.\n\n```python\n# From flow.py (Illustrating clarity)\n\n# Define the stations (Nodes)\nfetch_repo = FetchRepo()\nidentify_abstractions = IdentifyAbstractions(...)\n# ... other nodes\n\n# Connect them clearly\nfetch_repo >> identify_abstractions >> analyze_relationships # ... etc\n\n# Define the starting point\ntutorial_flow = Flow(start=fetch_repo)\n```\n\n## Conclusion\n\nIn this chapter, we learned about the **Node Abstraction**, the core building block of the PocketFlow workflow used in `LLM-CODEBASE-DOCUMENTOR`.\n\n*   A Node is like a **specialized worker** performing one specific task in the documentation generation process (e.g., `FetchRepo`, `WriteChapters`).\n*   Each Node follows a three-stage lifecycle:\n    *   **`prep`**: Gathers inputs from the shared store.\n    *   **`exec`**: Performs the main task.\n    *   **`post`**: Stores the results back into the shared store.\n*   This structure promotes **modularity**, **reusability**, **testability**, and **clarity** in our complex workflow.\n\nNodes handle *what* gets done and *how* it's done internally. But how do they communicate? How does the output of `FetchRepo` actually become the input for `IdentifyAbstractions`? That's where the central communication hub comes in.\n\n**Next Up:** Explore the mechanism that allows Nodes to share data with each other in [Chapter 6: Shared Data Store](06_shared_data_store.md).\n```\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md ---\n---\nlayout: default\ntitle: \"System Design\"\nnav_order: 2\n---\n\n# System Design: Codebase Knowledge Builder\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n**User Story:** As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code.\n\n**Input:**\n- A publicly accessible GitHub repository URL.\n- A project name (optional, will be derived from the URL if not provided).\n\n**Output:**\n- A directory named after the project containing:\n    - An `index.md` file with:\n        - A high-level project summary.\n        - A Mermaid flowchart diagram visualizing relationships between abstractions.\n        - Textual descriptions of the relationships.\n        - An ordered list of links to chapter files.\n    - Individual Markdown files for each chapter (`01_chapter_one.md`, `02_chapter_two.md`, etc.) detailing core abstractions in a logical order.\n\n## Flow Design\n\n> Notes for AI:\n> 1. Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit.\n> 2. Present a concise, high-level description of the workflow.\n\n### Applicable Design Pattern:\n\nThis project primarily uses a **Workflow** pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a **BatchNode** (a form of MapReduce) to process each abstraction individually.\n\n1.  **Workflow:** The overall process follows a defined sequence: fetch code -> identify abstractions -> analyze relationships -> determine order -> write chapters -> combine tutorial into files.\n2.  **Batch Processing:** The `WriteChapters` node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce).\n\n### Flow high-level Design:\n\n1.  **`FetchRepo`**: Crawls the specified GitHub repository path using `crawl_github_files` utility, retrieving relevant source code file contents.\n2.  **`IdentifyAbstractions`**: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (allowing multi-line), and list the *indices* of files related to each abstraction.\n3.  **`AnalyzeRelationships`**: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions, specifying *source* and *target* abstraction indices and a concise label for each interaction.\n4.  **`OrderChapters`**: Determines the most logical order (as indices) to present the abstractions in the tutorial, likely based on importance or dependencies identified in the previous step.\n5.  **`WriteChapters` (BatchNode)**: Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter, using the relevant code files (accessed via indices) and summaries of previously generated chapters as context.\n6.  **`CombineTutorial`**: Creates an output directory, generates a Mermaid diagram from the relationship data, and writes the project summary, relationship diagram/details (in `index.md`), and individually generated chapters (as separate `.md` files, named and ordered according to `chapter_order`) into it.\n\n```mermaid\nflowchart TD\n    A[FetchRepo] --> B[IdentifyAbstractions];\n    B --> C[AnalyzeRelationships];\n    C --> D[OrderChapters];\n    D --> E[Batch WriteChapters];\n    E --> F[CombineTutorial];\n```\n\n## Utility Functions\n\n> Notes for AI:\n> 1. Understand the utility function definition thoroughly by reviewing the doc.\n> 2. Include only the necessary utility functions, based on nodes in the flow.\n\n1.  **`crawl_github_files`** (`utils/crawl_github_files.py`) - *External Dependency: requests*\n    *   *Input*: `repo_url` (str), `token` (str, optional), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]) and `stats`.\n    *   *Necessity*: Required by `FetchRepo` to download and read source code from GitHub if a `repo_url` is provided. Handles cloning logic implicitly via API calls, filtering, and file reading.\n2.  **`crawl_local_files`** (`utils/crawl_local_files.py`) - *External Dependency: None*\n    *   *Input*: `directory` (str), `max_file_size` (int, optional), `use_relative_paths` (bool, optional), `include_patterns` (set, optional), `exclude_patterns` (set, optional)\n    *   *Output*: `dict` containing `files` (dict[str, str]).\n    *   *Necessity*: Required by `FetchRepo` to read source code from a local directory if a `local_dir` path is provided. Handles directory walking, filtering, and file reading.\n3.  **`call_llm`** (`utils/call_llm.py`) - *External Dependency: LLM Provider API (e.g., OpenAI, Anthropic)*\n    *   *Input*: `prompt` (str)\n    *   *Output*: `response` (str)\n    *   *Necessity*: Used by `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters` for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via `yaml.safe_load` which raises errors).\n\n## Node Design\n\n### Shared Store\n\n> Notes for AI: Try to minimize data redundancy\n\nThe shared Store structure is organized as follows:\n\n```python\nshared = {\n    \"repo_url\": None, # Input: Provided by the user/main script\n    \"project_name\": None, # Input: Optional, derived from repo_url if not provided\n    \"github_token\": None, # Input: Optional, from environment or config\n    \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str)\n    \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str, \"description\": str (can be multi-line), \"files\": [int]} (indices into shared[\"files\"])\n    \"relationships\": { # Output of AnalyzeRelationships\n         \"summary\": None, # Overall project summary (can be multi-line)\n         \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str} describing relationships between abstraction indices with a concise label.\n     },\n    \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order\n    \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown), ordered according to chapter_order\n    \"output_dir\": \"output\", # Input/Default: Base directory for output\n    \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\")\n}\n```\n\n### Node Steps\n\n> Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node's built-in fault tolerance.\n\n1.  **`FetchRepo`**\n    *   *Purpose*: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `repo_url` (if provided), `local_dir` (if provided), optional `github_token`, `output_dir` from shared store. Define `include_patterns` (e.g., `{\"*.py\", \"*.js\", \"*.md\"}`) and `exclude_patterns` (e.g., `{\"*test*\", \"docs/*\"}`). Set `max_file_size` and `use_relative_paths` flags. Determine `project_name` from `repo_url` or `local_dir` if not present in shared.\n        *   `exec`: If `repo_url` is present, call `crawl_github_files(...)`. Otherwise, call `crawl_local_files(...)`. Convert the resulting `files` dictionary into a list of `(path, content)` tuples.\n        *   `post`: Write the list of `files` tuples and the derived `project_name` (if applicable) to the shared store.\n\n2.  **`IdentifyAbstractions`**\n    *   *Purpose*: Analyze the code to identify key concepts/abstractions using indices.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `files` (list of tuples) from shared store. Create context using `create_llm_context` helper which adds file indices. Format the list of `index # path` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking it to identify ~5-10 core abstractions, provide a simple description (allowing multi-line YAML string) for each, and list the relevant *file indices* (e.g., `- 0 # path/to/file.py`). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like `0 # path...` to just the integer `0`.\n        *   `post`: Write the validated list of `abstractions` (e.g., `[{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]`) containing file *indices* to the shared store.\n\n3.  **`AnalyzeRelationships`**\n    *   *Purpose*: Generate a project summary and describe how the identified abstractions interact using indices and concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `files` from shared store. Format context for the LLM, including abstraction names *and indices*, descriptions, and content snippets from related files (referenced by `index # path` using `get_content_for_indices` helper). Prepare the list of `index # AbstractionName` for the prompt.\n        *   `exec`: Construct a prompt for `call_llm` asking for (1) a high-level summary (allowing multi-line YAML string) and (2) a list of relationships, each specifying `from_abstraction` (e.g., `0 # Abstraction1`), `to_abstraction` (e.g., `1 # Abstraction2`), and a concise `label` (string, just a few words). Request structured YAML output. Parse and validate, converting referenced abstractions to indices (`from: 0, to: 1`).\n        *   `post`: Parse the LLM response and write the `relationships` dictionary (`{\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}`) with indices to the shared store.\n\n4.  **`OrderChapters`**\n    *   *Purpose*: Determine the sequence (as indices) in which abstractions should be presented.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `abstractions` and `relationships` from the shared store. Prepare context including the list of `index # AbstractionName` and textual descriptions of relationships referencing indices and using the concise `label`.\n        *   `exec`: Construct a prompt for `call_llm` asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of `index # AbstractionName`. Parse and validate, extracting only the indices and ensuring all are present exactly once.\n        *   `post`: Write the validated ordered list of indices (`chapter_order`) to the shared store.\n\n5.  **`WriteChapters`**\n    *   *Purpose*: Generate the detailed content for each chapter of the tutorial.\n    *   *Type*: **BatchNode**\n    *   *Steps*:\n        *   `prep`: Read `chapter_order` (list of indices), `abstractions`, and `files` from shared store. Initialize an empty instance variable `self.chapters_written_so_far`. Return an iterable list where each item corresponds to an *abstraction index* from `chapter_order`. Each item should contain chapter number, abstraction details, and a map of related file content (`{ \"idx # path\": content }` obtained via `get_content_for_indices`).\n        *   `exec(item)`: Construct a prompt for `call_llm`. Ask it to write a beginner-friendly Markdown chapter about the current abstraction. Provide its description. Include a summary of previously written chapters (from `self.chapters_written_so_far`). Provide relevant code snippets (referenced by `index # path`). Add the generated chapter content to `self.chapters_written_so_far` for the next iteration's context. Return the chapter content.\n        *   `post(shared, prep_res, exec_res_list)`: `exec_res_list` contains the generated chapter Markdown content strings, ordered correctly. Assign this list directly to `shared[\"chapters\"]`. Clean up `self.chapters_written_so_far`.\n\n6.  **`CombineTutorial`**\n    *   *Purpose*: Assemble the final tutorial files, including a Mermaid diagram using concise labels.\n    *   *Type*: Regular\n    *   *Steps*:\n        *   `prep`: Read `project_name`, `relationships`, `chapter_order` (indices), `abstractions`, and `chapters` (list of content) from shared store. Generate a Mermaid `flowchart TD` string based on `relationships[\"details\"]`, using indices to identify nodes and the concise `label` for edges. Construct the content for `index.md` (including summary, Mermaid diagram, textual relationship details using the `label`, and ordered links to chapters derived using `chapter_order` and `abstractions`). Define the output directory path (e.g., `./output_dir/project_name`). Prepare a list of `{ \"filename\": \"01_...\", \"content\": \"...\" }` for chapters.\n        *   `exec`: Create the output directory. Write the generated `index.md` content. Iterate through the prepared chapter file list and write each chapter's content to its corresponding `.md` file in the output directory.\n        *   `post`: Write the final `output_dir` path to `shared[\"final_output_dir\"]`. Log completion.\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 6: Shared Data Store`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 6: Shared Data Store\n\nWelcome back! In [Chapter 5: Node Abstraction](05_node_abstraction.md), we learned about the specialized workers, called \"Nodes,\" that perform specific tasks like fetching code (`FetchRepo`) or writing chapters (`WriteChapters`). We saw how each Node has a `prep -> exec -> post` lifecycle.\n\nBut how do these workers talk to each other? How does the `FetchRepo` node hand off the list of code files it found to the `IdentifyAbstractions` node that needs them? If they couldn't share information, our assembly line would grind to a halt!\n\nThis chapter introduces the central communication hub: the **Shared Data Store**.\n\n**Goal:** Understand the central Python dictionary (`shared`) that acts as a shared workspace, allowing different Nodes in the workflow to read inputs left by previous Nodes and write their results for subsequent Nodes.\n\n## The Problem: Workers Need to Communicate! \ud83d\udce2\n\nImagine our documentation assembly line again ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)).\n*   Station 1 (`FetchRepo`) gathers the ingredients (code files).\n*   Station 2 (`IdentifyAbstractions`) needs those ingredients to figure out the main concepts.\n*   Station 3 (`AnalyzeRelationships`) needs the concepts from Station 2 to see how they connect.\n*   ...and so on.\n\nIf each station worked in complete isolation, how would the ingredients get from Station 1 to Station 2? How would the list of concepts get from Station 2 to Station 3? The whole process relies on **passing information** between steps.\n\n**Use Case:** We need a mechanism to make the output of one Node (like the list of files from `FetchRepo`) available as input to the next Node in the sequence (like `IdentifyAbstractions`), enabling the entire workflow defined in `flow.py` to function.\n\n## The Solution: The `shared` Dictionary - Our Shared Whiteboard \u26aa\n\nThe `LLM-CODEBASE-DOCUMENTOR` solves this communication problem with a simple yet effective approach: a central **Python dictionary** called `shared`.\n\nThink of this `shared` dictionary as a **shared whiteboard** or a central **project binder** that gets passed along the assembly line.\n\nHere's how it works:\n\n1.  **Initialization:** When you start the `main.py` script, a dictionary named `shared` is created. It's initially filled with the inputs you provided (like the repo URL, model choice, include/exclude patterns) and some empty placeholders for results.\n2.  **Passing Along:** The PocketFlow engine takes this *single* `shared` dictionary and passes it to *every* Node as it runs through the workflow sequence (`FetchRepo >> IdentifyAbstractions >> ...`).\n3.  **Reading Inputs (`prep`):** When a Node starts its `prep` stage, it looks at the `shared` dictionary to find the information it needs (e.g., `IdentifyAbstractions` reads the `\"files\"` key).\n4.  **Writing Outputs (`post`):** After a Node finishes its `exec` stage, its `post` stage takes the results and writes them *back into the same* `shared` dictionary under specific keys (e.g., `FetchRepo` writes the list of files into `shared[\"files\"]`).\n5.  **Flowing Data:** Because every Node reads from and writes to the *exact same* dictionary object, the data automatically flows. When `IdentifyAbstractions` runs, the `shared` dictionary it receives already contains the `\"files\"` list written by `FetchRepo`.\n\n**Analogy:** Imagine a team building a car.\n*   The `shared` dictionary is the main blueprint and parts checklist that gets passed from team to team.\n*   Team 1 (`FetchRepo`) gets the blueprint, gathers the chassis and engine parts, and checks them off on the list (`shared[\"files\"] = ...`).\n*   They pass the updated blueprint/checklist to Team 2 (`IdentifyAbstractions`).\n*   Team 2 reads the checklist (`files = shared[\"files\"]`), identifies which major components they are (`abstractions = ...`), and adds *that* information to the blueprint (`shared[\"abstractions\"] = ...`).\n*   This continues until the final team (`CombineTutorial`) uses all the collected information on the blueprint to assemble the final car documentation.\n\n## How It Looks in the Code\n\nLet's see how the `shared` dictionary is used in practice.\n\n**1. Initialization in `main.py`:**\n\nThe `shared` dictionary is created right at the beginning, populated with command-line arguments and defaults.\n\n```python\n# Simplified from main.py\n\ndef main():\n    # ... (argparse setup to get args like args.repo, args.model, etc.) ...\n    args = parser.parse_args()\n\n    # Get GitHub token (example of combining arg and env var)\n    github_token = args.token or os.environ.get('GITHUB_TOKEN')\n\n    # Initialize the shared dictionary\n    shared = {\n        # Inputs from user or defaults\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Might be None initially\n        \"github_token\": github_token,\n        \"output_dir\": args.output,\n        \"model_used\": args.model,\n        \"llm_provider\": LLMProvider_enum(args.model),\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Placeholders for outputs from Nodes\n        \"files\": [],             # To be filled by FetchRepo\n        \"abstractions\": [],      # To be filled by IdentifyAbstractions\n        \"relationships\": {},     # To be filled by AnalyzeRelationships\n        \"chapter_order\": [],     # To be filled by OrderChapters\n        \"chapters\": [],          # To be filled by WriteChapters\n        \"final_output_dir\": None # To be filled by CombineTutorial\n    }\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow, passing the shared dictionary\n    # PocketFlow manages passing 'shared' to each Node internally\n    tutorial_flow.run(shared)\n\n# ...\n```\n*Explanation:* We create a standard Python dictionary `shared`. We fill it with initial values derived from user input (`args`) or defaults. Crucially, we also include keys like `\"files\"`, `\"abstractions\"`, etc., initially empty, ready to be populated by the Nodes later in the workflow. This entire dictionary is then passed to `tutorial_flow.run()`.\n\n**2. Reading from `shared` in a Node's `prep` stage:**\n\nNodes access the `shared` dictionary passed to their `prep` method to get the inputs they need.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.prep\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared): # 'shared' is the dictionary passed by PocketFlow\n        # Read the list of files generated by FetchRepo\n        files_data = shared[\"files\"]\n        # Read the project name (might have been set by FetchRepo)\n        project_name = shared[\"project_name\"]\n        # Read the chosen LLM provider\n        llm_provider = shared[\"llm_provider\"]\n\n        # ... (prepare context using files_data) ...\n\n        # Return necessary items for the 'exec' stage\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider\n```\n*Explanation:* The `prep` method receives the `shared` dictionary as its argument. It simply accesses the required keys (like `shared[\"files\"]`, `shared[\"project_name\"]`) to retrieve the data needed for its task.\n\n**3. Writing to `shared` in a Node's `post` stage:**\n\nNodes write their results back into the `shared` dictionary in their `post` method.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.post\n\nclass IdentifyAbstractions(Node):\n    # ... prep and exec methods ...\n\n    # 'shared' is the dictionary, 'exec_res' is the result from exec()\n    def post(self, shared, prep_res, exec_res):\n        # 'exec_res' contains the list of identified abstractions\n        # Write this list back into the shared dictionary\n        shared[\"abstractions\"] = exec_res\n        print(f\"Saved {len(exec_res)} abstractions to shared store.\")\n```\n*Explanation:* The `post` method also receives the `shared` dictionary. It takes the result produced by the `exec` stage (`exec_res`) and assigns it to the appropriate key (e.g., `shared[\"abstractions\"] = ...`). This modification happens directly on the `shared` dictionary object that PocketFlow is managing.\n\n## How PocketFlow Orchestrates the Sharing\n\nYou might wonder how the *same* dictionary gets passed around. It's handled automatically by the PocketFlow engine when you call `flow.run(shared)`:\n\n1.  **Start:** PocketFlow receives the initial `shared` dictionary.\n2.  **Node 1 (`FetchRepo`):**\n    *   It calls `FetchRepo.prep(shared)`.\n    *   It calls `FetchRepo.exec(...)` which returns `files_list`.\n    *   It calls `FetchRepo.post(shared, ..., files_list)`. The `post` method modifies the `shared` dictionary by adding the `\"files\"` key.\n3.  **Node 2 (`IdentifyAbstractions`):**\n    *   PocketFlow now takes the *modified* `shared` dictionary (which now contains `\"files\"`) and passes it to the next node in the sequence.\n    *   It calls `IdentifyAbstractions.prep(shared)`. This `prep` method can now successfully read `shared[\"files\"]`.\n    *   It calls `IdentifyAbstractions.exec(...)` which returns `abstractions_list`.\n    *   It calls `IdentifyAbstractions.post(shared, ..., abstractions_list)`. This modifies `shared` again by adding the `\"abstractions\"` key.\n4.  **Continue:** This process repeats for every node in the workflow (`AnalyzeRelationships`, `OrderChapters`, etc.), with each node potentially reading data written by previous nodes and writing its own results back into the *same* `shared` dictionary.\n\n**Sequence Diagram: Data Flow via `shared`**\n\n```mermaid\nsequenceDiagram\n    participant Main as main.py\n    participant PF as PocketFlow Engine\n    participant Shared as Shared Dictionary\n    participant Node1Prep as FetchRepo.prep\n    participant Node1Post as FetchRepo.post\n    participant Node2Prep as IdentifyAbstractions.prep\n    participant Node2Post as IdentifyAbstractions.post\n\n    Main->>Shared: Create shared dict (with inputs)\n    Main->>PF: flow.run(shared)\n    PF->>Node1Prep: prep(shared)\n    Node1Prep->>Shared: Read repo_url, etc.\n    PF->>Node1Post: post(shared, ..., files_list)\n    Node1Post->>Shared: Write shared[\"files\"] = files_list\n    Note over Shared: shared now contains \"files\"\n    PF->>Node2Prep: prep(shared)\n    Node2Prep->>Shared: Read shared[\"files\"]\n    PF->>Node2Post: post(shared, ..., abstractions_list)\n    Node2Post->>Shared: Write shared[\"abstractions\"] = abstractions_list\n    Note over Shared: shared now contains \"files\", \"abstractions\"\n    PF-->>Main: Flow finished (shared contains all results)\n```\n\nThis diagram shows how the single `shared` dictionary object is passed along and modified by each Node's `post` method, making the results available to subsequent Nodes.\n\n## Conclusion\n\nIn this chapter, we explored the **Shared Data Store**, the central communication hub of the `LLM-CODEBASE-DOCUMENTOR` workflow.\n\n*   It's implemented as a simple **Python dictionary** named `shared`.\n*   It's initialized in `main.py` with user inputs and placeholders.\n*   The **PocketFlow engine** passes this *same* dictionary object sequentially to each Node in the workflow.\n*   Nodes **read** their required inputs from `shared` in their `prep` stage.\n*   Nodes **write** their results back into `shared` in their `post` stage.\n*   This mechanism allows data (like fetched files, identified abstractions, chapter content) to **flow smoothly** between the different processing steps.\n\nThe `shared` dictionary acts as the vital link connecting our specialized workers (Nodes), turning a series of isolated tasks into a cohesive, functional documentation generation pipeline.\n\nNow that we understand the core workflow, the node structure, and how data flows between them, let's look at how a user might interact with this system through a graphical interface.\n\n**Next Up:** Explore the user interface and a different workflow focused on searching for code in [Chapter 7: Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Streamlit UI & Search Flow\". This is Chapter 7.\n\n        Concept Details:\n        - Description:\n        Provides a user-friendly web interface built with Streamlit. It includes features for smart repository searching (handling URLs, local paths, and natural language queries), filtering search results, selecting a repository, and displaying the generated tutorial content directly in the browser, including a download option.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n---\n# Chapter 3: LLM Analysis & Generation\n\n```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```\n---\n# Chapter 4: Tutorial Structure Generation\n\n```markdown\n# Chapter 4: Tutorial Structure Generation\n\nWelcome back! In [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md), we saw how the \"AI brain\" (the LLM) helps us understand the code by identifying key concepts (abstractions), figuring out how they relate, deciding the best order to teach them, and even writing the first draft of each chapter's content.\n\nNow, imagine you've written all the individual recipes for your cookbook. You have the text for the cake recipe, the cookie recipe, the bread recipe, etc., all on separate pieces of paper. You also have a summary of the whole cookbook and maybe a diagram showing which recipes use similar ingredients. What's the next step? You need to **assemble the cookbook**!\n\nThis chapter is all about that final assembly process: **Tutorial Structure Generation**.\n\n**Goal:** Understand how the tool takes all the generated pieces \u2013 the project summary, the concept relationships, the chapter order, and the written chapter text \u2013 and organizes them into a final, polished tutorial structure, complete with an index file, diagrams, and correctly linked chapter files.\n\n## The Problem: From Scattered Notes to a Polished Book \ud83d\udcda\n\nAfter the previous steps, we have several valuable pieces of information floating around:\n\n1.  **Abstractions:** The list of key concepts identified by the LLM (e.g., \"Client Session\", \"Tools\").\n2.  **Relationships:** How these concepts connect, including a project summary (e.g., \"Client Session calls Tools\").\n3.  **Chapter Order:** The best sequence to explain these concepts (e.g., explain \"Client Session\" first, then \"Tools\").\n4.  **Chapter Content:** The actual Markdown text generated by the LLM for each concept.\n\nBut these are just separate bits of data inside the computer's memory. We need to transform them into something a human can actually use: a set of neatly organized Markdown files that form a coherent tutorial.\n\n**Use Case:** We want to take all the intermediate results generated by the LLMs and the workflow and produce the final output directory. This directory should contain:\n*   An `index.md` file acting as the table of contents, with a summary and a relationship diagram.\n*   Individual chapter files (like `01_client_session.md`, `02_tools.md`) containing the generated text.\n*   Proper links connecting the index to the chapters and the chapters to each other.\n\n## The Solution: The `CombineTutorial` Node - Our Assembler \ud83d\udee0\ufe0f\n\nThe final step in our documentation assembly line ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) is handled by a specific node called `CombineTutorial`. Think of this node as the **final assembler** or the **bookbinder**. Its job is to take all the finished components and put them together correctly.\n\nHere's what the `CombineTutorial` node does:\n\n1.  **Gathers Inputs:** It collects all the necessary pieces created by the previous nodes:\n    *   The project summary and relationship details (from `AnalyzeRelationships`).\n    *   The list of abstractions (from `IdentifyAbstractions`).\n    *   The determined chapter order (from `OrderChapters`).\n    *   The generated Markdown content for each chapter (from `WriteChapters`).\n    *   The project name and desired output location.\n\n2.  **Generates the Relationship Diagram:** It uses the relationship data (which abstraction connects to which, and the label for that connection) to automatically create a visual flowchart using Mermaid syntax. This helps users see the connections at a glance.\n\n3.  **Creates `index.md` Content:** It prepares the text for the main `index.md` file. This includes:\n    *   The project title.\n    *   The high-level summary generated earlier.\n    *   A link to the source code repository (if available).\n    *   The Mermaid diagram it just generated.\n    *   An ordered list of chapters, with links to the corresponding chapter files (e.g., \"1. [Client Session](01_client_session.md)\").\n\n4.  **Prepares Chapter Files:** For each chapter's generated Markdown content:\n    *   It determines the correct filename based on the chapter number and a \"safe\" version of the abstraction name (e.g., replacing spaces with underscores).\n    *   It adds a standard attribution line at the end.\n\n5.  **Writes Everything to Disk:**\n    *   It creates the final output directory (usually named after the project, inside an `output` folder).\n    *   It writes the prepared `index.md` content to `index.md` inside that directory.\n    *   It writes each chapter's prepared content to its corresponding filename (e.g., `01_client_session.md`, `02_tools.md`, etc.) inside the same directory.\n\n## How It Works Under the Hood (Simplified Walkthrough) \ud83d\udeb6\u200d\u2640\ufe0f\n\nLet's follow the `CombineTutorial` node step-by-step:\n\n1.  **Get Ready (`prep`):** The node first gathers all the ingredients it needs from the shared data store: the summary, relationship details (`relationships['details']`), chapter order (`chapter_order`), abstraction list (`abstractions`), chapter content (`chapters`), project name, and output directory path.\n2.  **Draw the Map (`prep`):** It looks at the `relationships['details']` (which contains pairs like `from: 0, to: 1, label: \"Calls\"`) and generates the Mermaid code for the diagram.\n3.  **Write the Introduction (`prep`):** It starts building the `index.md` text string, adding the title, summary, and the Mermaid diagram code.\n4.  **Create the Table of Contents (`prep`):** It iterates through the `chapter_order` list. For each abstraction index in the order, it finds the abstraction's name, creates a safe filename (like `01_client_session.md`), and adds a numbered link like `1. [Client Session](01_client_session.md)` to the `index.md` text. It also prepares a list of all chapter filenames and their corresponding content (adding the attribution).\n5.  **Build the Output Folder (`exec`):** The node creates the actual folder on your computer (e.g., `output/my_project/`).\n6.  **Save the Index (`exec`):** It saves the complete `index.md` text string into the `index.md` file within the new folder.\n7.  **Save the Chapters (`exec`):** It goes through the list of prepared chapter filenames and content, saving each one as a separate `.md` file in the folder.\n8.  **Done! (`post`):** It records the path to the final output directory in the shared store and prints a success message.\n\n**Visualizing the Flow:**\n\nHere's a simple sequence diagram for the `CombineTutorial` node:\n\n```mermaid\nsequenceDiagram\n    participant CT_Prep as CombineTutorial (Prep Phase)\n    participant SharedData as Shared Data Store\n    participant CT_Exec as CombineTutorial (Exec Phase)\n    participant FileSystem as Your Computer's Files\n\n    CT_Prep ->> SharedData: Read summary, relationships, order, abstractions, chapters, project name\n    CT_Prep ->> CT_Prep: Generate Mermaid Diagram from relationships\n    CT_Prep ->> CT_Prep: Prepare index.md content (title, summary, diagram, links)\n    CT_Prep ->> CT_Prep: Prepare list of chapter filenames & content\n    CT_Prep -->> CT_Exec: Pass prepared index & chapter data\n\n    CT_Exec ->> FileSystem: Create output directory (e.g., output/project_name)\n    CT_Exec ->> FileSystem: Write index.md content to file\n    loop For each chapter file\n        CT_Exec ->> FileSystem: Write chapter content to its .md file\n    end\n    CT_Exec ->> SharedData: Store final output path\n    CT_Exec -->> PocketFlow: Signal completion\n```\n\n## Code Snippets (`nodes.py` - `CombineTutorial`)\n\nLet's look at simplified snippets from the `CombineTutorial` node to see how it prepares and writes the files.\n\n**1. Preparing the Index and Chapter Data (`prep` method):**\n\nThis part gathers data and builds the structure in memory *before* writing any files.\n\n```python\n# In nodes.py, inside CombineTutorial.prep method\n\n# --- Assume these are loaded from shared data ---\n# project_name = \"My Cool Project\"\n# relationships_data = {\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"Uses\"}, ...]}\n# chapter_order = [0, 1, 2] # Indices of abstractions in order\n# abstractions = [{\"name\": \"Core Logic\", ...}, {\"name\": \"Data Handler\", ...}, {\"name\": \"API Interface\", ...}]\n# chapters_content = [\"Markdown for Core Logic...\", \"Markdown for Data Handler...\", \"...\"]\n# output_base_dir = \"output\"\n# repo_url = \"https://github.com/user/repo\"\n# -------------------------------------------------\n\noutput_path = os.path.join(output_base_dir, project_name) # e.g., \"output/My Cool Project\"\n\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes (e.g., A0[\"Core Logic\"])\nfor i, abstr in enumerate(abstractions):\n    node_id = f\"A{i}\"\n    node_label = abstr['name'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges (e.g., A0 -- \"Uses\" --> A1)\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    edge_label = rel['label'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n\n# --- Prepare index.md content ---\nindex_content = f\"# Tutorial: {project_name}\\n\\n\"\nindex_content += f\"{relationships_data['summary']}\\n\\n\"\nif repo_url:\n    index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\nindex_content += \"```mermaid\\n\" + mermaid_diagram + \"\\n```\\n\\n\"\nindex_content += \"## Chapters\\n\\n\"\n\nchapter_files = [] # To store {filename, content} pairs\n# Generate chapter links and prepare chapter file data\nfor i, abstraction_index in enumerate(chapter_order):\n    abstraction_name = abstractions[abstraction_index][\"name\"]\n    # Create safe filename (e.g., 01_core_logic.md)\n    safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n    filename = f\"{i+1:02d}_{safe_name}.md\"\n    # Add link to index.md content\n    index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n\n    # Prepare chapter content (add attribution)\n    chapter_content = chapters_content[i] + \"\\n\\n---\\n\\nGenerated by ...\" # Add attribution\n    # Store filename and content for later writing\n    chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n\n# Add attribution to index.md\nindex_content += \"\\n\\n---\\n\\nGenerated by ...\"\n\n# Return all prepared data for the exec step\nreturn {\n    \"output_path\": output_path,\n    \"index_content\": index_content,\n    \"chapter_files\": chapter_files\n}\n```\n*Explanation:* The `prep` method acts like the planning stage. It uses the data from previous steps (`relationships`, `chapter_order`, etc.) to generate the Mermaid diagram string and the full text content for `index.md`, including formatted links. It also prepares a list (`chapter_files`) containing the final filename and content for each chapter.\n\n**2. Writing the Files (`exec` method):**\n\nThis part takes the prepared data and actually creates the files and folders.\n\n```python\n# In nodes.py, inside CombineTutorial.exec method\n\n# Get the prepared data from the prep step\noutput_path = prep_res[\"output_path\"]       # e.g., \"output/My Cool Project\"\nindex_content = prep_res[\"index_content\"]   # The full text for index.md\nchapter_files = prep_res[\"chapter_files\"]   # List of {\"filename\": \"...\", \"content\": \"...\"}\n\nprint(f\"Combining tutorial into directory: {output_path}\")\n# Create the directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Write index.md\nindex_filepath = os.path.join(output_path, \"index.md\")\nwith open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n    f.write(index_content)\nprint(f\"  - Wrote {index_filepath}\")\n\n# Write chapter files\nfor chapter_info in chapter_files:\n    chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n    with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(chapter_info[\"content\"])\n    print(f\"  - Wrote {chapter_filepath}\")\n\n# Return the path where the tutorial was saved\nreturn output_path\n```\n*Explanation:* The `exec` method is the action stage. It takes the `output_path`, `index_content`, and `chapter_files` prepared by `prep`. It creates the directory, then simply writes the `index_content` into `index.md` and loops through `chapter_files`, writing each chapter's content into its respective `.md` file.\n\n## Connecting the Dots\n\n*   This `CombineTutorial` node relies heavily on the outputs produced by previous nodes like `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters`. Without their results, it wouldn't have anything to assemble!\n*   The use of indices throughout the process (identifying files, relationships, and order by index) makes it easier for `CombineTutorial` to correctly link everything together at the end.\n\n## Conclusion\n\nIn this chapter, we learned about the final assembly step: **Tutorial Structure Generation**.\n\n*   The `CombineTutorial` node acts as the \"bookbinder\" for our project.\n*   It takes the project summary, relationship data, chapter order, and chapter content generated in earlier steps.\n*   It creates the final output directory.\n*   It generates a **Mermaid diagram** visualizing the relationships between concepts.\n*   It writes the main **`index.md`** file, including the summary, diagram, and an ordered, linked list of chapters.\n*   It writes the individual **chapter `.md` files** with the LLM-generated content and proper filenames.\n\nThis node transforms the intermediate data into a usable, structured set of documentation files, ready for a developer to read!\n\n**Next Up:** We've seen several \"Nodes\" mentioned (`FetchRepo`, `IdentifyAbstractions`, `CombineTutorial`, etc.). What exactly *is* a Node, and how are they defined? Let's explore the building blocks of our workflow in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n```\n---\n# Chapter 5: Node Abstraction\n\n```markdown\n# Chapter 5: Node Abstraction\n\nWelcome back! In [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md), we saw how the `CombineTutorial` step acts like a bookbinder, taking all the generated pieces \u2013 summaries, chapter text, diagrams \u2013 and assembling them into the final tutorial files.\n\nNow, let's zoom out a bit. We've mentioned several specific steps like `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, and `CombineTutorial`. These are the core workers in our documentation \"assembly line\". How are these individual workers defined? What makes them tick?\n\nThis chapter dives into the fundamental building block of our workflow: the **Node Abstraction**.\n\n**Goal:** Understand what a \"Node\" is in the PocketFlow workflow, learn about its three stages (`prep`, `exec`, `post`), and see how this structure helps keep our complex process organized, modular, and easier to manage.\n\n## The Problem: Managing a Complex Recipe \ud83c\udf5d\n\nImagine trying to cook a complex multi-course meal. If you just had one giant page of instructions mixing everything together \u2013 chopping vegetables for the salad while boiling pasta and preheating the oven for dessert \u2013 it would be incredibly confusing and prone to errors.\n\nSimilarly, generating a code tutorial involves many different tasks: getting code, analyzing it with AI, figuring out relationships, deciding the order, writing text, formatting output. Trying to write one massive script to do all of this would be a nightmare!\n\n**Use Case:** How can we break down the complex task of generating a tutorial into smaller, manageable, and reusable pieces? We need a way to define distinct steps, handle their inputs and outputs cleanly, and ensure they work together smoothly.\n\n## The Solution: Nodes - The Specialized Workers \ud83e\uddd1\u200d\ud83d\udd27\n\nIn the `LLM-CODEBASE-DOCUMENTOR`, each specific task is handled by a **Node**. Think of our PocketFlow workflow (from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) as an assembly line, and each **Node** is a specialized station or worker on that line.\n\n*   `FetchRepo` is the station that **gets the raw materials** (source code).\n*   `IdentifyAbstractions` is the station that **inspects the materials** to find the main parts.\n*   `AnalyzeRelationships` figures out **how the parts connect**.\n*   `OrderChapters` decides the **assembly sequence**.\n*   `WriteChapters` **builds each component** (writes the chapter text).\n*   `CombineTutorial` performs the **final assembly and packaging**.\n\nEach Node has a very specific job description. It knows what inputs it needs, what task it performs, and what output it produces.\n\n## The Three Stages of a Node: Prep, Exec, Post\n\nTo make these workers efficient and organized, every Node follows a standard three-stage process:\n\n1.  **`prep` (Prepare / Get Ready):**\n    *   **What it does:** Gathers all the necessary inputs (ingredients and tools) needed for the main task. It usually reads data from the central `shared` data store (we'll cover this in [Chapter 6: Shared Data Store](06_shared_data_store.md)). It might also do some initial setup or validation.\n    *   **Analogy:** A chef reading the recipe, gathering ingredients from the pantry, and getting out the right knives and bowls.\n    *   **Output:** Passes the prepared inputs to the `exec` stage.\n\n2.  **`exec` (Execute / Do the Work):**\n    *   **What it does:** Performs the Node's main job. This is where the core logic happens \u2013 calling an external service (like the GitHub API or an LLM), running a calculation, processing data, etc.\n    *   **Analogy:** The chef chopping the vegetables, mixing the batter, or putting the dish in the oven.\n    *   **Output:** Returns the result of its work to the `post` stage.\n\n3.  **`post` (Post-process / Clean Up & Store):**\n    *   **What it does:** Takes the raw result from the `exec` stage, cleans it up or formats it if necessary, and most importantly, saves the final output back into the `shared` data store so that subsequent Nodes can use it.\n    *   **Analogy:** The chef plating the finished dish, garnishing it, and putting the plate on the serving counter (the shared store) for the waiter (the next Node). They might also quickly clean their station.\n\nThis `prep -> exec -> post` structure ensures that each Node clearly separates gathering inputs, doing the work, and storing outputs.\n\n## Example Walkthrough: The `FetchRepo` Node\n\nLet's revisit the `FetchRepo` node, which is responsible for getting the source code. Here's how its `prep`, `exec`, and `post` stages work (simplified):\n\n**1. `prep` Stage:**\n\n*   **Goal:** Get the repository URL or local directory path, filtering rules, and GitHub token (if needed) from the shared store. Prepare these for the crawler functions.\n*   **Analogy:** Get the cookbook (shared store), find the recipe source (URL or local path), check for ingredient preferences (filters), and get the special key to the pantry (token).\n\n```python\n# Simplified from nodes.py - FetchRepo.prep\n\ndef prep(self, shared):\n    # 1. Read necessary info from the shared store\n    repo_url = shared.get(\"repo_url\")\n    local_dir = shared.get(\"local_dir\")\n    github_token = shared.get(\"github_token\")\n    include_patterns = shared[\"include_patterns\"]\n    exclude_patterns = shared[\"exclude_patterns\"]\n    max_file_size = shared[\"max_file_size\"]\n\n    # (Determine project_name if not already set - omitted for simplicity)\n\n    # 2. Prepare a dictionary of inputs for the 'exec' stage\n    inputs_for_exec = {\n        \"repo_url\": repo_url,\n        \"local_dir\": local_dir,\n        \"token\": github_token,\n        \"include_patterns\": include_patterns,\n        \"exclude_patterns\": exclude_patterns,\n        \"max_file_size\": max_file_size,\n        \"use_relative_paths\": True # Example setting\n    }\n    return inputs_for_exec # Pass prepared inputs to exec\n```\n\n**2. `exec` Stage:**\n\n*   **Goal:** Based on the inputs from `prep`, call the correct utility function (`crawl_github_files` or `crawl_local_files`) to actually fetch the code.\n*   **Analogy:** If you have a URL, call the \"Online Shopper\" service. If you have a local path, call the \"Pantry Raider\" service. Get the list of ingredients (files).\n\n```python\n# Simplified from nodes.py - FetchRepo.exec\n\n# 'prep_res' contains the dictionary returned by the 'prep' stage\ndef exec(self, prep_res):\n    # 1. Check if we are fetching from GitHub or local directory\n    if prep_res[\"repo_url\"]:\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Call the GitHub utility function with prepared inputs\n        result = crawl_github_files(\n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    elif prep_res[\"local_dir\"]:\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Call the local directory utility function\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    else:\n        # Handle case where neither is provided (should not happen normally)\n        result = {\"files\": {}}\n\n    # 2. Process the result (convert dict to list of (path, content))\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n\n    return files_list # Return the raw result (list of files) to post\n```\n\n**3. `post` Stage:**\n\n*   **Goal:** Take the list of files returned by `exec` and save it into the shared store under the key `\"files\"`.\n*   **Analogy:** Take the bag of ingredients (list of files) and place it neatly on the shared kitchen counter (shared store) labeled \"Fetched Files\".\n\n```python\n# Simplified from nodes.py - FetchRepo.post\n\n# 'shared' is the shared data store\n# 'prep_res' is the output of prep (not usually needed here)\n# 'exec_res' is the list of files returned by exec\ndef post(self, shared, prep_res, exec_res):\n    # 1. Save the result from 'exec' into the shared store\n    shared[\"files\"] = exec_res\n    print(\"Saved fetched files list to shared store.\")\n    # (Could also save project_name here if prep determined it)\n```\n\n**Internal Node Flow Diagram:**\n\nHere's how the data flows within a typical Node like `FetchRepo`:\n\n```mermaid\nsequenceDiagram\n    participant Prep as FetchRepo.prep\n    participant SharedIn as Shared Store (Input)\n    participant Exec as FetchRepo.exec\n    participant Crawler as (crawl_github_files / crawl_local_files)\n    participant Post as FetchRepo.post\n    participant SharedOut as Shared Store (Output)\n\n    Prep ->> SharedIn: Read repo_url, local_dir, patterns, etc.\n    Prep -->> Exec: Pass prepared parameters (e.g., token, patterns)\n    Exec ->> Crawler: Call appropriate crawler function\n    Crawler -->> Exec: Return raw file data (dict)\n    Exec -->> Post: Pass processed result (list of files)\n    Post ->> SharedOut: Write file list to shared[\"files\"]\n```\n\nThis clear separation of concerns (`prep`, `exec`, `post`) makes each Node easier to understand and manage.\n\n## Benefits of Node Abstraction\n\nUsing Nodes to structure our workflow provides several advantages:\n\n*   **Modularity:** Each Node is a self-contained unit with a single responsibility. This makes the overall system easier to understand, as you can focus on one task at a time. Need to change how code is fetched? Just look at the `FetchRepo` node. Need to improve chapter writing? Focus on `WriteChapters`.\n*   **Reusability:** While not heavily utilized in this specific project, Nodes designed for common tasks could potentially be reused across different workflows.\n*   **Testability:** This is a huge benefit! You can test each Node individually. You can create fake input data (`shared` state before the node runs), run the Node's `prep`, `exec`, and `post` stages, and then check if the output in the `shared` store is what you expected. This makes debugging much easier.\n*   **Clarity:** When you look at the workflow definition in `flow.py` (remember the `>>` connections from Chapter 2?), you see a high-level view of the process connecting these well-defined Nodes. It's like reading a flowchart of the main steps, rather than digging through one giant script.\n\n```python\n# From flow.py (Illustrating clarity)\n\n# Define the stations (Nodes)\nfetch_repo = FetchRepo()\nidentify_abstractions = IdentifyAbstractions(...)\n# ... other nodes\n\n# Connect them clearly\nfetch_repo >> identify_abstractions >> analyze_relationships # ... etc\n\n# Define the starting point\ntutorial_flow = Flow(start=fetch_repo)\n```\n\n## Conclusion\n\nIn this chapter, we learned about the **Node Abstraction**, the core building block of the PocketFlow workflow used in `LLM-CODEBASE-DOCUMENTOR`.\n\n*   A Node is like a **specialized worker** performing one specific task in the documentation generation process (e.g., `FetchRepo`, `WriteChapters`).\n*   Each Node follows a three-stage lifecycle:\n    *   **`prep`**: Gathers inputs from the shared store.\n    *   **`exec`**: Performs the main task.\n    *   **`post`**: Stores the results back into the shared store.\n*   This structure promotes **modularity**, **reusability**, **testability**, and **clarity** in our complex workflow.\n\nNodes handle *what* gets done and *how* it's done internally. But how do they communicate? How does the output of `FetchRepo` actually become the input for `IdentifyAbstractions`? That's where the central communication hub comes in.\n\n**Next Up:** Explore the mechanism that allows Nodes to share data with each other in [Chapter 6: Shared Data Store](06_shared_data_store.md).\n```\n---\n# Chapter 6: Shared Data Store\n\nWelcome back! In [Chapter 5: Node Abstraction](05_node_abstraction.md), we learned about the specialized workers, called \"Nodes,\" that perform specific tasks like fetching code (`FetchRepo`) or writing chapters (`WriteChapters`). We saw how each Node has a `prep -> exec -> post` lifecycle.\n\nBut how do these workers talk to each other? How does the `FetchRepo` node hand off the list of code files it found to the `IdentifyAbstractions` node that needs them? If they couldn't share information, our assembly line would grind to a halt!\n\nThis chapter introduces the central communication hub: the **Shared Data Store**.\n\n**Goal:** Understand the central Python dictionary (`shared`) that acts as a shared workspace, allowing different Nodes in the workflow to read inputs left by previous Nodes and write their results for subsequent Nodes.\n\n## The Problem: Workers Need to Communicate! \ud83d\udce2\n\nImagine our documentation assembly line again ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)).\n*   Station 1 (`FetchRepo`) gathers the ingredients (code files).\n*   Station 2 (`IdentifyAbstractions`) needs those ingredients to figure out the main concepts.\n*   Station 3 (`AnalyzeRelationships`) needs the concepts from Station 2 to see how they connect.\n*   ...and so on.\n\nIf each station worked in complete isolation, how would the ingredients get from Station 1 to Station 2? How would the list of concepts get from Station 2 to Station 3? The whole process relies on **passing information** between steps.\n\n**Use Case:** We need a mechanism to make the output of one Node (like the list of files from `FetchRepo`) available as input to the next Node in the sequence (like `IdentifyAbstractions`), enabling the entire workflow defined in `flow.py` to function.\n\n## The Solution: The `shared` Dictionary - Our Shared Whiteboard \u26aa\n\nThe `LLM-CODEBASE-DOCUMENTOR` solves this communication problem with a simple yet effective approach: a central **Python dictionary** called `shared`.\n\nThink of this `shared` dictionary as a **shared whiteboard** or a central **project binder** that gets passed along the assembly line.\n\nHere's how it works:\n\n1.  **Initialization:** When you start the `main.py` script, a dictionary named `shared` is created. It's initially filled with the inputs you provided (like the repo URL, model choice, include/exclude patterns) and some empty placeholders for results.\n2.  **Passing Along:** The PocketFlow engine takes this *single* `shared` dictionary and passes it to *every* Node as it runs through the workflow sequence (`FetchRepo >> IdentifyAbstractions >> ...`).\n3.  **Reading Inputs (`prep`):** When a Node starts its `prep` stage, it looks at the `shared` dictionary to find the information it needs (e.g., `IdentifyAbstractions` reads the `\"files\"` key).\n4.  **Writing Outputs (`post`):** After a Node finishes its `exec` stage, its `post` stage takes the results and writes them *back into the same* `shared` dictionary under specific keys (e.g., `FetchRepo` writes the list of files into `shared[\"files\"]`).\n5.  **Flowing Data:** Because every Node reads from and writes to the *exact same* dictionary object, the data automatically flows. When `IdentifyAbstractions` runs, the `shared` dictionary it receives already contains the `\"files\"` list written by `FetchRepo`.\n\n**Analogy:** Imagine a team building a car.\n*   The `shared` dictionary is the main blueprint and parts checklist that gets passed from team to team.\n*   Team 1 (`FetchRepo`) gets the blueprint, gathers the chassis and engine parts, and checks them off on the list (`shared[\"files\"] = ...`).\n*   They pass the updated blueprint/checklist to Team 2 (`IdentifyAbstractions`).\n*   Team 2 reads the checklist (`files = shared[\"files\"]`), identifies which major components they are (`abstractions = ...`), and adds *that* information to the blueprint (`shared[\"abstractions\"] = ...`).\n*   This continues until the final team (`CombineTutorial`) uses all the collected information on the blueprint to assemble the final car documentation.\n\n## How It Looks in the Code\n\nLet's see how the `shared` dictionary is used in practice.\n\n**1. Initialization in `main.py`:**\n\nThe `shared` dictionary is created right at the beginning, populated with command-line arguments and defaults.\n\n```python\n# Simplified from main.py\n\ndef main():\n    # ... (argparse setup to get args like args.repo, args.model, etc.) ...\n    args = parser.parse_args()\n\n    # Get GitHub token (example of combining arg and env var)\n    github_token = args.token or os.environ.get('GITHUB_TOKEN')\n\n    # Initialize the shared dictionary\n    shared = {\n        # Inputs from user or defaults\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Might be None initially\n        \"github_token\": github_token,\n        \"output_dir\": args.output,\n        \"model_used\": args.model,\n        \"llm_provider\": LLMProvider_enum(args.model),\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Placeholders for outputs from Nodes\n        \"files\": [],             # To be filled by FetchRepo\n        \"abstractions\": [],      # To be filled by IdentifyAbstractions\n        \"relationships\": {},     # To be filled by AnalyzeRelationships\n        \"chapter_order\": [],     # To be filled by OrderChapters\n        \"chapters\": [],          # To be filled by WriteChapters\n        \"final_output_dir\": None # To be filled by CombineTutorial\n    }\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow, passing the shared dictionary\n    # PocketFlow manages passing 'shared' to each Node internally\n    tutorial_flow.run(shared)\n\n# ...\n```\n*Explanation:* We create a standard Python dictionary `shared`. We fill it with initial values derived from user input (`args`) or defaults. Crucially, we also include keys like `\"files\"`, `\"abstractions\"`, etc., initially empty, ready to be populated by the Nodes later in the workflow. This entire dictionary is then passed to `tutorial_flow.run()`.\n\n**2. Reading from `shared` in a Node's `prep` stage:**\n\nNodes access the `shared` dictionary passed to their `prep` method to get the inputs they need.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.prep\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared): # 'shared' is the dictionary passed by PocketFlow\n        # Read the list of files generated by FetchRepo\n        files_data = shared[\"files\"]\n        # Read the project name (might have been set by FetchRepo)\n        project_name = shared[\"project_name\"]\n        # Read the chosen LLM provider\n        llm_provider = shared[\"llm_provider\"]\n\n        # ... (prepare context using files_data) ...\n\n        # Return necessary items for the 'exec' stage\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider\n```\n*Explanation:* The `prep` method receives the `shared` dictionary as its argument. It simply accesses the required keys (like `shared[\"files\"]`, `shared[\"project_name\"]`) to retrieve the data needed for its task.\n\n**3. Writing to `shared` in a Node's `post` stage:**\n\nNodes write their results back into the `shared` dictionary in their `post` method.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.post\n\nclass IdentifyAbstractions(Node):\n    # ... prep and exec methods ...\n\n    # 'shared' is the dictionary, 'exec_res' is the result from exec()\n    def post(self, shared, prep_res, exec_res):\n        # 'exec_res' contains the list of identified abstractions\n        # Write this list back into the shared dictionary\n        shared[\"abstractions\"] = exec_res\n        print(f\"Saved {len(exec_res)} abstractions to shared store.\")\n```\n*Explanation:* The `post` method also receives the `shared` dictionary. It takes the result produced by the `exec` stage (`exec_res`) and assigns it to the appropriate key (e.g., `shared[\"abstractions\"] = ...`). This modification happens directly on the `shared` dictionary object that PocketFlow is managing.\n\n## How PocketFlow Orchestrates the Sharing\n\nYou might wonder how the *same* dictionary gets passed around. It's handled automatically by the PocketFlow engine when you call `flow.run(shared)`:\n\n1.  **Start:** PocketFlow receives the initial `shared` dictionary.\n2.  **Node 1 (`FetchRepo`):**\n    *   It calls `FetchRepo.prep(shared)`.\n    *   It calls `FetchRepo.exec(...)` which returns `files_list`.\n    *   It calls `FetchRepo.post(shared, ..., files_list)`. The `post` method modifies the `shared` dictionary by adding the `\"files\"` key.\n3.  **Node 2 (`IdentifyAbstractions`):**\n    *   PocketFlow now takes the *modified* `shared` dictionary (which now contains `\"files\"`) and passes it to the next node in the sequence.\n    *   It calls `IdentifyAbstractions.prep(shared)`. This `prep` method can now successfully read `shared[\"files\"]`.\n    *   It calls `IdentifyAbstractions.exec(...)` which returns `abstractions_list`.\n    *   It calls `IdentifyAbstractions.post(shared, ..., abstractions_list)`. This modifies `shared` again by adding the `\"abstractions\"` key.\n4.  **Continue:** This process repeats for every node in the workflow (`AnalyzeRelationships`, `OrderChapters`, etc.), with each node potentially reading data written by previous nodes and writing its own results back into the *same* `shared` dictionary.\n\n**Sequence Diagram: Data Flow via `shared`**\n\n```mermaid\nsequenceDiagram\n    participant Main as main.py\n    participant PF as PocketFlow Engine\n    participant Shared as Shared Dictionary\n    participant Node1Prep as FetchRepo.prep\n    participant Node1Post as FetchRepo.post\n    participant Node2Prep as IdentifyAbstractions.prep\n    participant Node2Post as IdentifyAbstractions.post\n\n    Main->>Shared: Create shared dict (with inputs)\n    Main->>PF: flow.run(shared)\n    PF->>Node1Prep: prep(shared)\n    Node1Prep->>Shared: Read repo_url, etc.\n    PF->>Node1Post: post(shared, ..., files_list)\n    Node1Post->>Shared: Write shared[\"files\"] = files_list\n    Note over Shared: shared now contains \"files\"\n    PF->>Node2Prep: prep(shared)\n    Node2Prep->>Shared: Read shared[\"files\"]\n    PF->>Node2Post: post(shared, ..., abstractions_list)\n    Node2Post->>Shared: Write shared[\"abstractions\"] = abstractions_list\n    Note over Shared: shared now contains \"files\", \"abstractions\"\n    PF-->>Main: Flow finished (shared contains all results)\n```\n\nThis diagram shows how the single `shared` dictionary object is passed along and modified by each Node's `post` method, making the results available to subsequent Nodes.\n\n## Conclusion\n\nIn this chapter, we explored the **Shared Data Store**, the central communication hub of the `LLM-CODEBASE-DOCUMENTOR` workflow.\n\n*   It's implemented as a simple **Python dictionary** named `shared`.\n*   It's initialized in `main.py` with user inputs and placeholders.\n*   The **PocketFlow engine** passes this *same* dictionary object sequentially to each Node in the workflow.\n*   Nodes **read** their required inputs from `shared` in their `prep` stage.\n*   Nodes **write** their results back into `shared` in their `post` stage.\n*   This mechanism allows data (like fetched files, identified abstractions, chapter content) to **flow smoothly** between the different processing steps.\n\nThe `shared` dictionary acts as the vital link connecting our specialized workers (Nodes), turning a series of isolated tasks into a cohesive, functional documentation generation pipeline.\n\nNow that we understand the core workflow, the node structure, and how data flows between them, let's look at how a user might interact with this system through a graphical interface.\n\n**Next Up:** Explore the user interface and a different workflow focused on searching for code in [Chapter 7: Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md).\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\flow.py ---\nfrom pocketflow import Flow\n\n# Import all node classes from nodes.py\nfrom nodes import (\n    FetchRepo,\n    IdentifyAbstractions,\n    AnalyzeRelationships,\n    OrderChapters,\n    WriteChapters,\n    CombineTutorial,\n    # New nodes for Streamlit UI\n    SmartSearchRepo,\n    FilterRepos,\n    SelectRepository,\n    RenderAndDownload\n)\n\n\n\ndef create_tutorial_flow():\n    \"\"\"Creates and returns the codebase tutorial generation flow.\"\"\"\n\n    # Instantiate nodes\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(max_retries=3, wait=10)\n    analyze_relationships = AnalyzeRelationships(max_retries=3, wait=10)\n    order_chapters = OrderChapters(max_retries=3, wait=10)\n    write_chapters = WriteChapters(max_retries=3, wait=10) # This is a BatchNode\n    combine_tutorial = CombineTutorial()\n\n    # Connect nodes in sequence based on the design\n    fetch_repo >> identify_abstractions\n    identify_abstractions >> analyze_relationships\n    analyze_relationships >> order_chapters\n    order_chapters >> write_chapters\n    write_chapters >> combine_tutorial\n\n    # Create the flow starting with FetchRepo\n    tutorial_flow = Flow(start=fetch_repo)\n\n    return tutorial_flow, fetch_repo, combine_tutorial\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n    \n    # Create the original tutorial flow and get the first and last nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n    \n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n    \n    # Connect the UI nodes\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n    \n    # Connect the select_repo node to the FetchRepo node\n    select_repo >> fetch_repo\n    \n    # Add the render and download node after CombineTutorial\n    combine_tutorial >> render_download\n    \n    # Create the full UI flow\n    ui_flow = Flow(start=smart_search)\n    \n    return ui_flow\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\nodes.py ---\nimport os\nimport yaml\nfrom pocketflow import Node, BatchNode\nfrom utils.crawl_github_files import crawl_github_files\nfrom utils.call_llm import call_llm \nfrom utils.crawl_local_files import crawl_local_files\nfrom utils.search_helpers import extract_keywords, looks_like_url\nfrom utils.github_api import github_search_repos, get_readme_content, RepoMetadata\nfrom utils.summarizer import summarize_readme\nfrom utils.io_helpers import zip_output_folder, read_all_markdown_files\n\n# Helper to get content for specific file indices\ndef get_content_for_indices(files_data, indices):\n    content_map = {}\n    for i in indices:\n        if 0 <= i < len(files_data):\n            path, content = files_data[i]\n            content_map[f\"{i} # {path}\"] = content # Use index + path as key for context\n    return content_map\n\nclass FetchRepo(Node):\n    def prep(self, shared):\n        repo_url = shared.get(\"repo_url\")\n        local_dir = shared.get(\"local_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        if not project_name:\n            # Basic name derivation from URL or directory\n            if repo_url:\n                project_name = repo_url.split('/')[-1].replace('.git', '')\n            elif local_dir:\n                project_name = os.path.basename(os.path.abspath(local_dir))\n            else:\n                # Fallback if neither repo_url nor local_dir is provided\n                project_name = \"untitled-project\"\n            shared[\"project_name\"] = project_name\n\n        # Get file patterns directly from shared\n        include_patterns = shared[\"include_patterns\"]\n        exclude_patterns = shared[\"exclude_patterns\"]\n        max_file_size = shared[\"max_file_size\"]\n\n        return {\n            \"repo_url\": repo_url,\n            \"local_dir\": local_dir,\n            \"token\": shared.get(\"github_token\"),\n            \"include_patterns\": include_patterns,\n            \"exclude_patterns\": exclude_patterns,\n            \"max_file_size\": max_file_size,\n            \"use_relative_paths\": True\n        }\n\n    def exec(self, prep_res):\n        if prep_res[\"repo_url\"]:\n            print(f\"Crawling repository: {prep_res['repo_url']}...\")\n            result = crawl_github_files(\n                repo_url=prep_res[\"repo_url\"],\n                token=prep_res[\"token\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        elif prep_res[\"local_dir\"]:\n            print(f\"Crawling directory: {prep_res['local_dir']}...\")\n            result = crawl_local_files(\n                directory=prep_res[\"local_dir\"],\n                include_patterns=prep_res[\"include_patterns\"],\n                exclude_patterns=prep_res[\"exclude_patterns\"],\n                max_file_size=prep_res[\"max_file_size\"],\n                use_relative_paths=prep_res[\"use_relative_paths\"]\n            )\n        else:\n            print(\"No repository URL or local directory provided. Cannot proceed.\")\n            # Return empty result\n            result = {\"files\": {}}\n            \n        # Convert dict to list of tuples: [(path, content), ...]\n        files_list = list(result.get(\"files\", {}).items())\n        print(f\"Fetched {len(files_list)} files.\")\n        return files_list\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"files\"] = exec_res # List of (path, content) tuples\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared):\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider  = shared[\"llm_provider\"]\n        \n        # Helper to create context from files, respecting limits (basic example)\n        def create_llm_context(files_data):\n            context = \"\"\n            file_info = [] # Store tuples of (index, path)\n            for i, (path, content) in enumerate(files_data):\n                entry = f\"--- File Index {i}: {path} ---\\n{content}\\n\\n\"\n                context += entry\n                file_info.append((i, path))\n\n            return context, file_info # file_info is list of (index, path)\n\n        context, file_info = create_llm_context(files_data)\n        # Format file info for the prompt (comment is just a hint for LLM)\n        file_listing_for_prompt = \"\\n\".join([f\"- {idx} # {path}\" for idx, path in file_info])\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, file_listing_for_prompt, file_count, project_name, llm_provider = prep_res  # Unpack project name\n        print(\"Identifying abstractions using LLM...\")\n        prompt = f\"\"\"\n        For the project `{project_name}`:\n\n        Codebase Context:\n        {context}\n\n        Analyze the codebase context.\n        Identify the top 6-10 core most important abstractions to help those new to the codebase.\n\n        For each abstraction, provide:\n        1. A concise `name`.\n        2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n        3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\n        List of file indices and paths present in the context:\n        {file_listing_for_prompt}\n\n        Format the output as a YAML list of dictionaries:\n\n        ```yaml\n        - name: Query Processing\n        description: | \n            Explains what the abstraction does.\n            It's like a central dispatcher routing requests.\n        file_indices:\n            - 0 # path/to/file1.py\n            - 3 # path/to/related.py\n        - name: Query Optimization\n        description: |\n            Another core concept, similar to a blueprint for objects.\n        file_indices:\n            - 5 # path/to/another.js\n        # ... up to 10 abstractions\n        ```\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        abstractions = yaml.safe_load(yaml_str)\n\n        if not isinstance(abstractions, list):\n            raise ValueError(\"LLM Output is not a list\")\n\n        validated_abstractions = []\n        for item in abstractions:\n            if not isinstance(item, dict) or not all(k in item for k in [\"name\", \"description\", \"file_indices\"]):\n                raise ValueError(f\"Missing keys in abstraction item: {item}\")\n            if not isinstance(item[\"description\"], str):\n                 raise ValueError(f\"description is not a string in item: {item}\")\n            if not isinstance(item[\"file_indices\"], list):\n                 raise ValueError(f\"file_indices is not a list in item: {item}\")\n\n            # Validate indices\n            validated_indices = []\n            for idx_entry in item[\"file_indices\"]:\n                 try:\n                     if isinstance(idx_entry, int):\n                         idx = idx_entry\n                     elif isinstance(idx_entry, str) and '#' in idx_entry:\n                          idx = int(idx_entry.split('#')[0].strip())\n                     else:\n                          idx = int(str(idx_entry).strip())\n\n                     if not (0 <= idx < file_count):\n                         raise ValueError(f\"Invalid file index {idx} found in item {item['name']}. Max index is {file_count - 1}.\")\n                     validated_indices.append(idx)\n                 except (ValueError, TypeError):\n                      raise ValueError(f\"Could not parse index from entry: {idx_entry} in item {item['name']}\")\n\n            item[\"files\"] = sorted(list(set(validated_indices)))\n            # Store only the required fields\n            validated_abstractions.append({\n                \"name\": item[\"name\"],\n                \"description\": item[\"description\"],\n                \"files\": item[\"files\"]\n            })\n\n        print(f\"Identified {len(validated_abstractions)} abstractions.\")\n        return validated_abstractions\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"abstractions\"] = exec_res # List of {\"name\": str, \"description\": str, \"files\": [int]}\n\nclass AnalyzeRelationships(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"] # Now contains 'files' list of indices\n        files_data = shared[\"files\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider = shared[\"llm_provider\"]\n\n        # Create context with abstraction names, indices, descriptions, and relevant file snippets\n        context = \"Identified Abstractions:\\n\"\n        all_relevant_indices = set()\n        abstraction_info_for_prompt = []\n        for i, abstr in enumerate(abstractions):\n            # Use 'files' which contains indices directly\n            file_indices_str = \", \".join(map(str, abstr['files']))\n            info_line = f\"- Index {i}: {abstr['name']} (Relevant file indices: [{file_indices_str}])\\n  Description: {abstr['description']}\"\n            context += info_line + \"\\n\"\n            abstraction_info_for_prompt.append(f\"{i} # {abstr['name']}\")\n            all_relevant_indices.update(abstr['files'])\n\n        context += \"\\nRelevant File Snippets (Referenced by Index and Path):\\n\"\n        # Get content for relevant files using helper\n        relevant_files_content_map = get_content_for_indices(\n            files_data,\n            sorted(list(all_relevant_indices))\n        )\n        # Format file content for context\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path} ---\\n{content}\"\n            for idx_path, content in relevant_files_content_map.items()\n        )\n        context += file_context_str\n\n        return context, \"\\n\".join(abstraction_info_for_prompt), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        context, abstraction_listing, project_name, llm_provider  = prep_res  # Unpack project name\n        print(\"Analyzing relationships using LLM...\")\n        prompt = f\"\"\"\nBased on the following abstractions and relevant code snippets from the project `{project_name}`:\n\nList of Abstraction Indices and Names:\n{abstraction_listing}\n\nContext (Abstractions, Descriptions, Code):\n{context}\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n\"\"\"\n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider )\n\n        # --- Validation ---\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        relationships_data = yaml.safe_load(yaml_str)\n\n        if not isinstance(relationships_data, dict) or not all(k in relationships_data for k in [\"summary\", \"relationships\"]):\n            raise ValueError(\"LLM output is not a dict or missing keys ('summary', 'relationships')\")\n        if not isinstance(relationships_data[\"summary\"], str):\n             raise ValueError(\"summary is not a string\")\n        if not isinstance(relationships_data[\"relationships\"], list):\n             raise ValueError(\"relationships is not a list\")\n\n        # Validate relationships structure\n        validated_relationships = []\n        num_abstractions = len(abstraction_listing.split('\\n'))\n        for rel in relationships_data[\"relationships\"]:\n             # Check for 'label' key\n             if not isinstance(rel, dict) or not all(k in rel for k in [\"from_abstraction\", \"to_abstraction\", \"label\"]):\n                  raise ValueError(f\"Missing keys (expected from_abstraction, to_abstraction, label) in relationship item: {rel}\")\n             # Validate 'label' is a string\n             if not isinstance(rel[\"label\"], str):\n                  raise ValueError(f\"Relationship label is not a string: {rel}\")\n\n             # Validate indices\n             try:\n                 from_idx = int(str(rel[\"from_abstraction\"]).split('#')[0].strip())\n                 to_idx = int(str(rel[\"to_abstraction\"]).split('#')[0].strip())\n                 if not (0 <= from_idx < num_abstractions and 0 <= to_idx < num_abstractions):\n                      raise ValueError(f\"Invalid index in relationship: from={from_idx}, to={to_idx}. Max index is {num_abstractions-1}.\")\n                 validated_relationships.append({\n                     \"from\": from_idx,\n                     \"to\": to_idx,\n                     \"label\": rel[\"label\"] \n                 })\n             except (ValueError, TypeError):\n                  raise ValueError(f\"Could not parse indices from relationship: {rel}\")\n\n        print(\"Generated project summary and relationship details.\")\n        return {\n            \"summary\": relationships_data[\"summary\"],\n            \"details\": validated_relationships # Store validated, index-based relationships\n        }\n\n\n    def post(self, shared, prep_res, exec_res):\n        # Structure is now {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        shared[\"relationships\"] = exec_res\n\nclass OrderChapters(Node):\n    def prep(self, shared):\n        abstractions = shared[\"abstractions\"]\n        relationships = shared[\"relationships\"]\n        project_name = shared[\"project_name\"]  # Get project name\n        llm_provider   = shared[\"llm_provider\"]\n\n        # Prepare context for the LLM\n        abstraction_info_for_prompt = []\n        for i, a in enumerate(abstractions):\n            abstraction_info_for_prompt.append(f\"- {i} # {a['name']}\")\n        abstraction_listing = \"\\n\".join(abstraction_info_for_prompt)\n\n        context = f\"Project Summary:\\n{relationships['summary']}\\n\\n\"\n        context += \"Relationships (Indices refer to abstractions above):\\n\"\n        for rel in relationships['details']:\n             from_name = abstractions[rel['from']]['name']\n             to_name = abstractions[rel['to']]['name']\n             # Use 'label' instead of 'desc'\n             context += f\"- From {rel['from']} ({from_name}) to {rel['to']} ({to_name}): {rel['label']}\\n\"\n\n        return abstraction_listing, context, len(abstractions), project_name, llm_provider  # Return project name\n\n    def exec(self, prep_res):\n        abstraction_listing, context, num_abstractions, project_name, llm_provider = prep_res\n        print(\"Determining chapter order using LLM...\")\n        prompt = f\"\"\"\n        Given the following project abstractions and their relationships for the project ```` {project_name} ````:\n\n        Abstractions (Index # Name):\n        {abstraction_listing}\n\n        Context about relationships and project summary:\n        {context}\n\n        If you are going to make a tutorial for ```` {project_name} ````, what is the best order to explain these abstractions, from first to last?\n        Ideally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\n        Output the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n        ```yaml\n        - 2 # FoundationalConcept\n        - 0 # CoreClassA\n        - 1 # CoreClassB (uses CoreClassA)\n        - ...\n        ```\n\n        Now, provide the YAML output:\n        \"\"\" \n        response = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n        # --- Validation ---\n        # Rely on Node's built-in retry/fallback\n        yaml_str = response.strip().split(\"```yaml\")[1].split(\"```\")[0].strip()\n        ordered_indices_raw = yaml.safe_load(yaml_str)\n\n        if not isinstance(ordered_indices_raw, list):\n            raise ValueError(\"LLM output is not a list\")\n\n        ordered_indices = []\n        seen_indices = set()\n        for entry in ordered_indices_raw:\n            try:\n                 if isinstance(entry, int):\n                     idx = entry\n                 elif isinstance(entry, str) and '#' in entry:\n                      idx = int(entry.split('#')[0].strip())\n                 else:\n                      idx = int(str(entry).strip())\n\n                 if not (0 <= idx < num_abstractions):\n                      raise ValueError(f\"Invalid index {idx} in ordered list. Max index is {num_abstractions-1}.\")\n                 if idx in seen_indices:\n                     raise ValueError(f\"Duplicate index {idx} found in ordered list.\")\n                 ordered_indices.append(idx)\n                 seen_indices.add(idx)\n\n            except (ValueError, TypeError):\n                 raise ValueError(f\"Could not parse index from ordered list entry: {entry}\")\n\n        # Check if all abstractions are included\n        if len(ordered_indices) != num_abstractions:\n             raise ValueError(f\"Ordered list length ({len(ordered_indices)}) does not match number of abstractions ({num_abstractions}). Missing indices: {set(range(num_abstractions)) - seen_indices}\")\n\n        print(f\"Determined chapter order (indices): {ordered_indices}\")\n        return ordered_indices # Return the list of indices\n\n    def post(self, shared, prep_res, exec_res):\n        # exec_res is already the list of ordered indices\n        shared[\"chapter_order\"] = exec_res # List of indices\n\nclass WriteChapters(BatchNode):\n    def prep(self, shared):\n        chapter_order = shared[\"chapter_order\"] # List of indices\n        abstractions = shared[\"abstractions\"]   # List of dicts, now using 'files' with indices\n        files_data = shared[\"files\"]\n        llm_provider  = shared[\"llm_provider\"]\n        # Get already written chapters to provide context\n        # We store them temporarily during the batch run, not in shared memory yet\n        # The 'previous_chapters_summary' will be built progressively in the exec context\n        self.chapters_written_so_far = [] # Use instance variable for temporary storage across exec calls\n\n        # Create a complete list of all chapters\n        all_chapters = []\n        chapter_filenames = {} # Store chapter filename mapping for linking\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                chapter_num = i + 1\n                chapter_name = abstractions[abstraction_index][\"name\"]\n                # Create safe filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in chapter_name).lower()\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                # Format with link\n                all_chapters.append(f\"{chapter_num}. [{chapter_name}]({filename})\")\n                # Store mapping of chapter index to filename for linking\n                chapter_filenames[abstraction_index] = {\"num\": chapter_num, \"name\": chapter_name, \"filename\": filename}\n        \n        # Create a formatted string with all chapters\n        full_chapter_listing = \"\\n\".join(all_chapters)\n\n        items_to_process = []\n        for i, abstraction_index in enumerate(chapter_order):\n            if 0 <= abstraction_index < len(abstractions):\n                abstraction_details = abstractions[abstraction_index]\n                # Use 'files' (list of indices) directly\n                related_file_indices = abstraction_details.get(\"files\", [])\n                # Get content using helper, passing indices\n                related_files_content_map = get_content_for_indices(files_data, related_file_indices)\n                \n                # Get previous chapter info for transitions\n                prev_chapter = None\n                if i > 0:\n                    prev_idx = chapter_order[i-1]\n                    prev_chapter = chapter_filenames[prev_idx]\n                \n                # Get next chapter info for transitions\n                next_chapter = None\n                if i < len(chapter_order) - 1:\n                    next_idx = chapter_order[i+1]\n                    next_chapter = chapter_filenames[next_idx]\n\n                items_to_process.append({\n                    \"chapter_num\": i + 1,\n                    \"abstraction_index\": abstraction_index,\n                    \"abstraction_details\": abstraction_details,\n                    \"related_files_content_map\": related_files_content_map,\n                    \"project_name\": shared[\"project_name\"],  # Add project name\n                    \"full_chapter_listing\": full_chapter_listing,  # Add the full chapter listing\n                    \"chapter_filenames\": chapter_filenames,  # Add chapter filenames mapping\n                    \"prev_chapter\": prev_chapter,  # Add previous chapter info\n                    \"next_chapter\": next_chapter,  # Add next chapter info\n                    \"llm_provider\": llm_provider,  # Pass the LLM provider\n                    # previous_chapters_summary will be added dynamically in exec\n                })\n            else:\n                print(f\"Warning: Invalid abstraction index {abstraction_index} in chapter_order. Skipping.\")\n\n        print(f\"Preparing to write {len(items_to_process)} chapters...\")\n        return items_to_process # Iterable for BatchNode\n\n    def exec(self, item):\n        # This runs for each item prepared above\n        abstraction_name = item[\"abstraction_details\"][\"name\"]\n        chapter_num = item[\"chapter_num\"]\n        project_name = item.get(\"project_name\")  # Get from item\n        print(f\"Writing chapter {chapter_num} for: {abstraction_name} using LLM...\")\n\n        # Prepare file context string from the map\n        file_context_str = \"\\n\\n\".join(\n            f\"--- File: {idx_path.split('# ')[1] if '# ' in idx_path else idx_path} ---\\n{content}\"\n            for idx_path, content in item[\"related_files_content_map\"].items()\n        )\n\n        # Get summary of chapters written *before* this one\n        # Use the temporary instance variable\n        previous_chapters_summary = \"\\n---\\n\".join(self.chapters_written_so_far)\n\n\n        prompt = f\"\"\"\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `{project_name}` about the concept: \"{abstraction_name}\". This is Chapter {chapter_num}.\n\n        Concept Details:\n        - Description:\n        {item[\"abstraction_details\"][\"description\"]}\n\n        Complete Tutorial Structure:\n        {item[\"full_chapter_listing\"]}\n\n        Context from previous chapters (summary):\n        {previous_chapters_summary if previous_chapters_summary else \"This is the first chapter.\"}\n\n        Relevant Code Snippets:\n        {file_context_str if file_context_str else \"No specific code snippets provided for this abstraction.\"}\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter {chapter_num}: {abstraction_name}`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        \"\"\"\n        chapter_content = call_llm(prompt=prompt, use_cache=True, model=item[\"llm_provider\"])\n        # Basic validation/cleanup\n        actual_heading = f\"# Chapter {chapter_num}: {abstraction_name}\"\n        if not chapter_content.strip().startswith(f\"# Chapter {chapter_num}\"):\n             # Add heading if missing or incorrect, trying to preserve content\n             lines = chapter_content.strip().split('\\n')\n             if lines and lines[0].strip().startswith(\"#\"): # If there's some heading, replace it\n                 lines[0] = actual_heading\n                 chapter_content = \"\\n\".join(lines)\n             else: # Otherwise, prepend it\n                 chapter_content = f\"{actual_heading}\\n\\n{chapter_content}\"\n\n        # Add the generated content to our temporary list for the next iteration's context\n        self.chapters_written_so_far.append(chapter_content)\n\n        return chapter_content # Return the Markdown string\n\n    def post(self, shared, prep_res, exec_res_list):\n        # exec_res_list contains the generated Markdown for each chapter, in order\n        shared[\"chapters\"] = exec_res_list\n        # Clean up the temporary instance variable\n        del self.chapters_written_so_far\n        print(f\"Finished writing {len(exec_res_list)} chapters.\")\n\nclass CombineTutorial(Node):\n    def prep(self, shared):\n        project_name = shared[\"project_name\"]\n        output_base_dir = shared.get(\"output_dir\", \"output\") # Default output dir\n        output_path = os.path.join(output_base_dir, project_name)\n        repo_url = shared[\"repo_url\"]  # Get the repository URL\n\n        # Use 'label' from relationships_data['details']\n        relationships_data = shared[\"relationships\"] # {\"summary\": str, \"details\": [{\"from\": int, \"to\": int, \"label\": str}]}\n        chapter_order = shared[\"chapter_order\"] # indices\n        abstractions = shared[\"abstractions\"]   # list of dicts\n        chapters_content = shared[\"chapters\"]   # list of strings\n\n        # --- Generate Mermaid Diagram ---\n        mermaid_lines = [\"flowchart TD\"]\n        # Add nodes for each abstraction\n        for i, abstr in enumerate(abstractions):\n            # Sanitize name for Mermaid ID and label\n            node_id = f\"A{i}\"\n            sanitized_name = abstr['name'].replace('\"', '')\n            node_label = sanitized_name # Using sanitized name only, no index\n            mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n        # Add edges for relationships using 'label'\n        for rel in relationships_data['details']:\n            from_node_id = f\"A{rel['from']}\"\n            to_node_id = f\"A{rel['to']}\"\n            # Sanitize 'label' for edge label\n            edge_label = rel['label'].replace('\"', '').replace('\\n', ' ') # Basic sanitization\n            # Limit edge label length for readability (optional, but good for diagrams)\n            max_label_len = 30 # Make it shorter for labels\n            if len(edge_label) > max_label_len:\n                edge_label = edge_label[:max_label_len-3] + \"...\"\n            mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\n\n        mermaid_diagram = \"\\n\".join(mermaid_lines)\n        # --- End Mermaid ---\n\n\n        # Prepare index.md content\n        index_content = f\"# Tutorial: {project_name}\\n\\n\"\n        index_content += f\"{relationships_data['summary']}\\n\\n\"\n        index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\n\n        # Add Mermaid diagram for relationships\n        index_content += \"```mermaid\\n\"\n        index_content += mermaid_diagram + \"\\n\"\n        index_content += \"```\\n\\n\"\n\n        index_content += \"## Chapters\\n\\n\"\n\n        chapter_files = []\n        # Generate chapter links based on the determined order\n        for i, abstraction_index in enumerate(chapter_order):\n            # Ensure index is valid and we have content for it\n            if 0 <= abstraction_index < len(abstractions) and i < len(chapters_content):\n                abstraction_name = abstractions[abstraction_index][\"name\"]\n                # Sanitize name for filename\n                safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n                # Use chapter number (i+1) for ordering filename\n                filename = f\"{i+1:02d}_{safe_name}.md\"\n                index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n                \n                # Add attribution to chapter content\n                chapter_content = chapters_content[i]\n                if not chapter_content.endswith(\"\\n\\n\"):\n                    chapter_content += \"\\n\\n\"\n                chapter_content += \"---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n                \n                # Store filename and corresponding content\n                chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n            else:\n                 print(f\"Warning: Mismatch between chapter order, abstractions, or content at index {i} (abstraction index {abstraction_index}). Skipping file generation for this entry.\")\n\n        # Add attribution to index content\n        index_content += \"\\n\\n---\\n\\nGenerated by TEG SINGH TIWANA: [Cloud Assignment 2:Github LLM Codebase Knowledge Building Summarizer using Openai/Gemini/Claud](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n\n        return {\n            \"output_path\": output_path,\n            \"index_content\": index_content,\n            \"chapter_files\": chapter_files # List of {\"filename\": str, \"content\": str}\n        }\n\n    def exec(self, prep_res):\n        output_path = prep_res[\"output_path\"]\n        index_content = prep_res[\"index_content\"]\n        chapter_files = prep_res[\"chapter_files\"]\n\n        print(f\"Combining tutorial into directory: {output_path}\")\n        # Rely on Node's built-in retry/fallback\n        os.makedirs(output_path, exist_ok=True)\n\n        # Write index.md\n        index_filepath = os.path.join(output_path, \"index.md\")\n        with open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(index_content)\n        print(f\"  - Wrote {index_filepath}\")\n\n        # Write chapter files\n        for chapter_info in chapter_files:\n            chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n            with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n                f.write(chapter_info[\"content\"])\n            print(f\"  - Wrote {chapter_filepath}\")\n\n        return output_path # Return the final path\n\n\n    def post(self, shared, prep_res, exec_res):\n        shared[\"final_output_dir\"] = exec_res # Store the output path\n        print(f\"\\nTutorial generation complete! Files are in: {exec_res}\")\n\n# New nodes for Streamlit UI flow\nclass SmartSearchRepo(Node):\n    \"\"\"\n    Processes a user query input to determine search mode and extract keywords.\n    Handles direct URLs, local paths, and natural language search.\n    \"\"\"\n    def prep(self, shared):\n        query_input = shared.get(\"query_input\", \"\")\n        llm_provider = shared.get(\"llm_provider\")\n        return {\n            \"query_input\": query_input,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        query_input = prep_res[\"query_input\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Determine the search mode based on input\n        if looks_like_url(query_input):\n            search_mode = \"url\"\n            selected_repo = query_input\n            keywords = []\n        elif os.path.isdir(query_input):\n            search_mode = \"local\"\n            selected_repo = query_input\n            keywords = []\n        else:\n            search_mode = \"nl\"\n            selected_repo = None\n            # Extract keywords from natural language query using the specified LLM provider\n            keywords = extract_keywords(query_input, llm_provider=llm_provider)\n            \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"selected_repo\": selected_repo\n        }\n    \n    def post(self, shared, prep_res, exec_res):\n        shared[\"search_mode\"] = exec_res[\"search_mode\"]\n        shared[\"keywords\"] = exec_res[\"keywords\"]\n        \n        # Set selected_repo if in URL or local mode\n        if exec_res[\"search_mode\"] in [\"url\", \"local\"]:\n            shared[\"selected_repo\"] = exec_res[\"selected_repo\"]\n\nclass FilterRepos(Node):\n    \"\"\"\n    Searches GitHub repositories based on keywords and filter parameters.\n    Summarizes READMEs for found repositories.\n    \"\"\"\n    def prep(self, shared):\n        search_mode = shared.get(\"search_mode\", \"\")\n        keywords = shared.get(\"keywords\", [])\n        filter_params = shared.get(\"filter_params\", {})\n        github_token = shared.get(\"github_token\")\n        selected_repo = shared.get(\"selected_repo\")\n        llm_provider = shared.get(\"llm_provider\")\n        \n        return {\n            \"search_mode\": search_mode,\n            \"keywords\": keywords,\n            \"filter_params\": filter_params,\n            \"github_token\": github_token,\n            \"selected_repo\": selected_repo,\n            \"llm_provider\": llm_provider\n        }\n    \n    def exec(self, prep_res):\n        search_mode = prep_res[\"search_mode\"]\n        keywords = prep_res[\"keywords\"]\n        filter_params = prep_res[\"filter_params\"]\n        github_token = prep_res[\"github_token\"]\n        selected_repo = prep_res[\"selected_repo\"]\n        llm_provider = prep_res[\"llm_provider\"]\n        \n        # Skip API search if in URL or local mode\n        if search_mode in [\"url\", \"local\"]:\n            return []\n        \n        # Search repositories\n        print(f\"Searching GitHub for: {keywords} with filters: {filter_params}\")\n        repos = github_search_repos(keywords, filter_params, github_token)\n        \n        # Get README summaries for each repository\n        for repo in repos:\n            try:\n                readme_content = get_readme_content(repo.full_name, github_token)\n                repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n            except Exception as e:\n                print(f\"Error summarizing README for {repo.full_name}: {e}\")\n                repo.readme_summary = \"No summary available.\"\n        \n        return repos\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set search_results if in NL mode and we have results\n        if prep_res[\"search_mode\"] == \"nl\":\n            shared[\"search_results\"] = exec_res\n\nclass SelectRepository(Node):\n    \"\"\"\n    Handles the selection of a repository from search results,\n    setting up the repo URL or local directory for the tutorial pipeline.\n    \"\"\"\n    def prep(self, shared):\n        selected_repo = shared.get(\"selected_repo\")\n        search_mode = shared.get(\"search_mode\", \"\")\n        search_results = shared.get(\"search_results\", [])\n        \n        return {\n            \"selected_repo\": selected_repo,\n            \"search_mode\": search_mode,\n            \"search_results\": search_results\n        }\n    \n    def exec(self, prep_res):\n        selected_repo = prep_res[\"selected_repo\"]\n        search_mode = prep_res[\"search_mode\"]\n        \n        # If repository already selected (URL or local mode), just propagate\n        if selected_repo:\n            if search_mode == \"url\":\n                return {\"repo_url\": selected_repo, \"local_dir\": None}\n            elif search_mode == \"local\":\n                return {\"repo_url\": None, \"local_dir\": selected_repo}\n        \n        # For search results, the selection will be done through the UI\n        # and this node will be called again with selected_repo set\n        return {\"repo_url\": None, \"local_dir\": None}\n    \n    def post(self, shared, prep_res, exec_res):\n        # Only set if we have a selection\n        if exec_res[\"repo_url\"]:\n            shared[\"repo_url\"] = exec_res[\"repo_url\"]\n            shared[\"local_dir\"] = None\n        elif exec_res[\"local_dir\"]:\n            shared[\"repo_url\"] = None\n            shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\nclass RenderAndDownload(Node):\n    \"\"\"\n    After tutorial generation, renders the output files in the UI\n    and creates a downloadable ZIP archive.\n    \"\"\"\n    def prep(self, shared):\n        final_output_dir = shared.get(\"final_output_dir\")\n        project_name = shared.get(\"project_name\")\n        \n        return {\n            \"final_output_dir\": final_output_dir,\n            \"project_name\": project_name\n        }\n    \n    def exec(self, prep_res):\n        final_output_dir = prep_res[\"final_output_dir\"]\n        project_name = prep_res[\"project_name\"]\n        \n        if not final_output_dir or not os.path.exists(final_output_dir):\n            return {\n                \"success\": False,\n                \"error\": \"Output directory not found\",\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n        \n        try:\n            # Read all Markdown files in the output directory\n            markdown_files = read_all_markdown_files(final_output_dir)\n            \n            # Create a ZIP archive of the output directory\n            zip_path = zip_output_folder(final_output_dir)\n            \n            return {\n                \"success\": True,\n                \"markdown_files\": markdown_files,\n                \"zip_path\": zip_path\n            }\n        except Exception as e:\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"markdown_files\": {},\n                \"zip_path\": None\n            }\n    \n    def post(self, shared, prep_res, exec_res):\n        if exec_res[\"success\"]:\n            shared[\"markdown_files\"] = exec_res[\"markdown_files\"]\n            shared[\"zip_path\"] = exec_res[\"zip_path\"]\n        else:\n            print(f\"Error rendering output: {exec_res.get('error', 'Unknown error')}\")\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py ---\nimport streamlit as st\nimport os\nimport time\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\n\n# Import utilities\nfrom utils.env_loader import load_env_vars, get_api_keys, get_model_names, create_env_example\nfrom utils.call_llm import LLMProvider_enum\nfrom utils.github_api import RepoMetadata\n\n# Import the flow\nfrom flow import create_streamlit_flow\nfrom nodes import SmartSearchRepo, FilterRepos, SelectRepository, RenderAndDownload\n\n# Load environment variables\nload_env_vars()\n\n# 1. Page configuration (with a custom About menu item)\nst.set_page_config(\n    page_title=\"\ud83d\udcd6\ud83e\udd13 LLM Codebase Finder & Documentor\",\n    page_icon=\"\ud83e\uddd0\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'About': (\n            \"### LLM Codebase Documentor\\n\"\n            \"A&nbsp;Streamlit app to auto-generate beginner-friendly tutorials from any GitHub repo.\\n\\n\"\n            \"Built by **TEG SINGH TIWANA** for Cloud Assignment 2:\\n\"\n            \"[GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n        )\n    }\n)\n# Initialize session state if not already done\nif \"initialized\" not in st.session_state:\n    st.session_state.initialized = True\n    st.session_state.search_results = []\n    st.session_state.selected_repo = None\n    st.session_state.filter_params = {\n        \"min_stars\": 0,\n        \"min_forks\": 0,\n        \"language\": \"\",\n        \"sort_by\": \"stars\",\n        \"updated_since\": \"\"\n    }\n    st.session_state.shared_store = {}\n    st.session_state.markdown_files = {}\n    st.session_state.zip_path = None\n    st.session_state.task_completed = False\n    st.session_state.ui_view = \"search\"  # Possible values: search, results, tutorial\n    \n    # Get API keys from environment\n    api_keys = get_api_keys()\n    for key, value in api_keys.items():\n        st.session_state[key] = value or \"\"\n    \n    # Get model names from environment\n    model_names = get_model_names()\n    for key, value in model_names.items():\n        st.session_state[key] = value\n\n# --- Sidebar ---\nwith st.sidebar:\n    st.title(\"LLM Codebase Documentor\")\n    \n    # Create tabs for different sidebar sections\n    tab1, tab2, tab3 = st.tabs([\"Search\", \"API Keys\", \"Model Settings\"])\n    \n    with tab1:\n        # Search input and type\n        st.subheader(\"Repository Search\")\n        query_input = st.text_input(\n            \"Enter GitHub URL, local path, or natural language query:\",\n            placeholder=\"e.g., https://github.com/user/repo OR frameworks for UI prototyping\"\n        )\n        \n        # Filters (only shown for natural language search)\n        st.subheader(\"Filters\")\n        min_stars = st.number_input(\"Minimum Stars\", min_value=0, value=st.session_state.filter_params[\"min_stars\"])\n        min_forks = st.number_input(\"Minimum Forks\", min_value=0, value=st.session_state.filter_params[\"min_forks\"])\n        language = st.selectbox(\n            \"Language\", \n            [\"\", \"Python\", \"JavaScript\", \"TypeScript\", \"Java\", \"Go\", \"Rust\", \"C++\", \"Ruby\", \"PHP\"],\n            index=0\n        )\n        sort_by = st.selectbox(\n            \"Sort By\", \n            [\"stars\", \"forks\", \"updated\", \"help-wanted-issues\"],\n            index=0\n        )\n        updated_since = st.date_input(\n            \"Updated Since\", \n            value=None\n        )\n        \n        # Update filter params in session state\n        if st.button(\"Search\", key=\"search_button\"):\n            # Update filter params\n            st.session_state.filter_params = {\n                \"min_stars\": min_stars,\n                \"min_forks\": min_forks,\n                \"language\": language,\n                \"sort_by\": sort_by,\n                \"updated_since\": updated_since.isoformat() if updated_since else \"\"\n            }\n            \n            # Initialize shared store for the flow\n            st.session_state.shared_store = {\n                \"query_input\": query_input,\n                \"filter_params\": st.session_state.filter_params,\n                \"github_token\": st.session_state.github_token,\n                \"llm_provider\": LLMProvider_enum(st.session_state.provider_selection),\n                \n                # Default parameters from original flow\n                \"include_patterns\": {\n                    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n                    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n                    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n                    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n                    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n                    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \"*.md\", \"*.pdf\"\n                },\n                \"exclude_patterns\": {\n                    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n                    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n                    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n                },\n                \"max_file_size\": 300000,\n                \"output_dir\": \"output\"\n            }\n            \n            # Create the flow\n            flow = create_streamlit_flow()\n            \n            # Run the SmartSearchRepo and FilterRepos nodes\n            with st.spinner(\"Processing query...\"):\n                # Set state to indicate we're waiting for results\n                st.session_state.ui_view = \"results\"\n                \n                # Run the SmartSearch and FilterRepos nodes\n                smart_search = SmartSearchRepo()\n                filter_repos = FilterRepos()\n                \n                # Run nodes sequentially\n                smart_search.run(st.session_state.shared_store)\n                filter_repos.run(st.session_state.shared_store)\n                \n                # Update session state with search results\n                if \"search_results\" in st.session_state.shared_store:\n                    st.session_state.search_results = st.session_state.shared_store[\"search_results\"]\n                \n                # If we have a selected_repo, run the full pipeline\n                if \"selected_repo\" in st.session_state.shared_store:\n                    st.session_state.selected_repo = st.session_state.shared_store[\"selected_repo\"]\n                    with st.spinner(\"Generating tutorial...\"):\n                        st.session_state.ui_view = \"tutorial\"\n                        # Run the full flow\n                        flow.run(st.session_state.shared_store)\n                        \n                        # Get the rendered Markdown files and ZIP path if available\n                        if \"markdown_files\" in st.session_state.shared_store:\n                            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n                        if \"zip_path\" in st.session_state.shared_store:\n                            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n                        \n                        st.session_state.task_completed = True\n    \n    with tab2:\n        st.subheader(\"API Keys\")\n        \n        # GitHub token\n        github_token = st.text_input(\n            \"GitHub Token\",\n            value=st.session_state.github_token,\n            type=\"password\",\n            help=\"Required for private repos or to avoid rate limits\"\n        )\n        st.session_state.github_token = github_token\n        \n        # LLM API keys\n        openai_api_key = st.text_input(\n            \"OpenAI API Key\",\n            value=st.session_state.openai_api_key,\n            type=\"password\"\n        )\n        st.session_state.openai_api_key = openai_api_key\n        \n        anthropic_api_key = st.text_input(\n            \"Anthropic API Key\",\n            value=st.session_state.anthropic_api_key,\n            type=\"password\"\n        )\n        st.session_state.anthropic_api_key = anthropic_api_key\n        \n        gemini_api_key = st.text_input(\n            \"Google Gemini API Key\",\n            value=st.session_state.gemini_api_key,\n            type=\"password\"\n        )\n        st.session_state.gemini_api_key = gemini_api_key\n        \n        # Save API keys to environment\n        if st.button(\"Save API Keys\"):\n            os.environ[\"GITHUB_TOKEN\"] = github_token\n            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n            os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n            os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n            st.success(\"API keys saved to environment!\")\n    \n    with tab3:\n        st.subheader(\"Model Settings\")\n        \n        # LLM Provider selection\n        provider_selection = st.selectbox(\n            \"LLM Provider\",\n            options=[provider.value for provider in LLMProvider_enum],\n            index=0\n        )\n        st.session_state.provider_selection = provider_selection\n        \n        # Model name based on provider\n        if provider_selection == LLMProvider_enum.OPENAI.value:\n            model_name = st.text_input(\n                \"OpenAI Model\",\n                value=st.session_state.openai_model\n            )\n            st.session_state.openai_model = model_name\n            os.environ[\"OPENAI_MODEL\"] = model_name\n        \n        elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n            model_name = st.text_input(\n                \"Anthropic Model\",\n                value=st.session_state.anthropic_model\n            )\n            st.session_state.anthropic_model = model_name\n            os.environ[\"ANTHROPIC_MODEL\"] = model_name\n        \n        else:  # Gemini\n            model_name = st.text_input(\n                \"Google Gemini Model\",\n                value=st.session_state.gemini_model\n            )\n            st.session_state.gemini_model = model_name\n            os.environ[\"GEMINI_MODEL\"] = model_name\n            \n        # Add save button for model settings\n        if st.button(\"Save Model Settings\"):\n            # Save provider selection to session state\n            st.session_state.provider_selection = provider_selection\n            st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(provider_selection)\n            \n            # Save model name based on provider\n            if provider_selection == LLMProvider_enum.OPENAI.value:\n                os.environ[\"OPENAI_MODEL\"] = model_name\n                st.session_state.openai_model = model_name\n            elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n                os.environ[\"ANTHROPIC_MODEL\"] = model_name\n                st.session_state.anthropic_model = model_name\n            else:  # Gemini\n                os.environ[\"GEMINI_MODEL\"] = model_name\n                st.session_state.gemini_model = model_name\n                \n            st.success(f\"Model settings saved! Provider: {provider_selection}, Model: {model_name}\")\n\n# --- Main Content ---\ndef display_repo_card(repo: RepoMetadata, index: int):\n    \"\"\"Display a repository card with details and select button.\"\"\"\n    col1, col2 = st.columns([1, 4])\n    \n    with col1:\n        st.image(\"https://github.githubassets.com/favicons/favicon.png\", width=50)\n        st.button(f\"Select\", key=f\"select_{index}\", on_click=select_repository, args=(repo.url,))\n    \n    with col2:\n        st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n        st.markdown(f\"**Description:** {repo.description}\")\n        st.markdown(f\"**Summary:** {repo.readme_summary}\")\n        st.markdown(f\"**Language:** {repo.language} | **Stars:** {repo.stars} | **Forks:** {repo.forks}\")\n\ndef select_repository(repo_url: str):\n    \"\"\"Handle repository selection from search results.\"\"\"\n    st.session_state.selected_repo = repo_url\n    \n    # Make sure we have the shared store initialized\n    if not st.session_state.shared_store:\n        st.session_state.shared_store = {}\n    \n    # Set the selected repository URL in the shared store\n    st.session_state.shared_store[\"selected_repo\"] = repo_url\n    st.session_state.shared_store[\"repo_url\"] = repo_url\n    st.session_state.shared_store[\"local_dir\"] = None\n    \n    # Make sure we have other necessary parameters in the shared store\n    if \"include_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"include_patterns\"] = {\n            \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n            \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n            \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n            \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n            \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n            \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n        }\n    \n    if \"exclude_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"exclude_patterns\"] = {\n            \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n            \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n            \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n        }\n        \n    if \"max_file_size\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"max_file_size\"] = 300000\n        \n    if \"output_dir\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"output_dir\"] = \"output\"\n        \n    if \"github_token\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"github_token\"] = st.session_state.github_token\n        \n    if \"llm_provider\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(st.session_state.provider_selection)\n    \n    st.session_state.ui_view = \"tutorial\"\n    \n    # Create and run the flow\n    flow = create_streamlit_flow()\n    with st.spinner(\"Generating tutorial...\"):\n        # Run the full flow\n        flow.run(st.session_state.shared_store)\n        \n        # Get the rendered Markdown files and ZIP path if available\n        if \"markdown_files\" in st.session_state.shared_store:\n            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n        if \"zip_path\" in st.session_state.shared_store:\n            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n        \n        st.session_state.task_completed = True\n\n# Conditional display based on current view\nif st.session_state.ui_view == \"search\":\n    # Show welcome message and instructions\n    st.title(\"Welcome to **GITHUB** Finder & Documentor\ud83d\udcd6\ud83e\udd13\")\n    st.markdown(\"\"\"\n    **Built by [TEG SINGH TIWANA](https://github.com/tej172)** for _Cloud Assignment 2_:  \n    [GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n    #### Welcome to **LLM Codebase Documentor**, your friendly assistant for auto-generating beginner-friendly tutorials from any codebase! \n    ###### This tool helps you understand codebases by generating detailed tutorials explaining core abstractions, relationships, and code organization.\n    \n    ### Getting Started\n    1. In the **sidebar**, enter one of:\n       - A **GitHub URL** (e.g. `https://github.com/user/repo`)\n       - A **local folder path** on your machine\n       - A **natural language** query (e.g. \"frameworks for UI prototyping\")\n    2. If you chose natural language, adjust **advanced filters** (stars, forks, language, updated date).\n    3. Click **Search** to fetch repositories or **Generate** to build the tutorial.\n    \n    ### Features\n    - \ud83d\udd0d **Smart Search** powered by GPT / Gemini / Claude for effortless keyword extraction  \n    - \ud83d\udcc2 **Local Path** support\u2014point to any folder you've already cloned  \n    - \u2b50 **Advanced Filters**\u2014stars, forks, issues, language, last\u2010updated  \n    - \ud83d\udcc4 **Markdown Preview**\u2014click through `index.md` and each chapter in-app  \n    - \ud83d\udce5 **Downloadable ZIP**\u2014grab your entire tutorial with one click  \n\n    > _Tip_: Make sure your **API keys** (GITHUB_TOKEN, OPENAI_API_KEY, GEMINI_API_KEY, etc.) are set in the sidebar or in your `.env` file.\n    \"\"\")\n    \nelif st.session_state.ui_view == \"results\":\n    # Show search results\n    st.title(\"Search Results\")\n    \n    if st.session_state.search_results:\n        st.markdown(f\"Found {len(st.session_state.search_results)} repositories matching your query.\")\n        for i, repo in enumerate(st.session_state.search_results):\n            with st.container():\n                display_repo_card(repo, i)\n                st.markdown(\"---\")\n    else:\n        st.info(\"No results found. Try different keywords or filters.\")\n        \nelif st.session_state.ui_view == \"tutorial\":\n    # Show the generated tutorial\n    project_name = st.session_state.shared_store.get(\"project_name\", \"Repository\")\n    st.title(f\"Tutorial for {project_name}\")\n    \n    # Download button for ZIP if available\n    if st.session_state.zip_path:\n        with open(st.session_state.zip_path, \"rb\") as f:\n            st.download_button(\n                label=\"Download Tutorial ZIP\",\n                data=f,\n                file_name=f\"{project_name}_tutorial.zip\",\n                mime=\"application/zip\"\n            )\n    \n    # Display the Markdown files\n    if st.session_state.markdown_files:\n        # First show index.md if it exists\n        if \"index.md\" in st.session_state.markdown_files:\n            st.markdown(st.session_state.markdown_files[\"index.md\"])\n            st.markdown(\"---\")\n        \n        # Then show all other files in order\n        for filename in sorted([f for f in st.session_state.markdown_files if f != \"index.md\"]):\n            with st.expander(f\"{filename}\", expanded=True):\n                st.markdown(st.session_state.markdown_files[filename])\n    else:\n        st.info(\"Generating tutorial content... Please wait.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\n    \"LLM Codebase Documentor NTU \u00a9 2025 | \"\n    \"[View on GitHub](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2) |  \"\n    \"Created by [**TEG SINGH TIWANA (U2122816B)**](https://github.com/tej172) | \"\n    \"[Cloud Assignment 2 \u2013 GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n)\n\n# Create .env.example file if it doesn't exist\nif not os.path.exists(\".env.example\"):\n    create_env_example() \n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\docs\\design2.md ---\n\n## layout: default title: \"System Design 2: Codebase Knowledge Builder + Streamlit Frontend\" nav\\_order: 3\n\n# System Design 2: Codebase Knowledge Builder + Streamlit Frontend \n\n### (System Design 2 is a extention from System Design: Codebase Knowledge Builder(in LLM-CODEBASE-DOCUMENTOR-main\\docs\\design.md) that is alredy built in this code base)\n\n> Builds on the implemented [design.md](docs/design.md) by adding:\n>\n> 1. A Streamlit-based web UI for interactive query input, credential management, filtering, and Markdown rendering.\n> 2. A GitHub API\u2013powered Smart Search that accepts natural\u2011language queries, direct URLs, or local paths.\n> 3. Advanced repository filters (stars, forks, issues, updated date) with clickable results driving the existing tutorial pipeline.\n\n## Requirements\n\n> **Notes for AI:** Mirror the detail of `design.md`, mapping high\u2011level user stories to concrete inputs/outputs.\n\n### User Stories\n\n1. **Tutorial Generation** (existing): As a developer onboarding a new codebase, I want an automated tutorial with core abstractions, relationships, and beginner\u2011friendly descriptions, so I can understand project structure quickly. ([design.md])\n2. **Web Interface**: As a user, I want a browser UI where I can:\n   - Enter a GitHub URL, natural language query, or local folder path.\n   - Configure LLM and GitHub API credentials.\n   - Apply repository filters.\n   - Trigger tutorial generation and view/download results\u2014all without leaving the page.\n3. **Smart GitHub Search**: As a researcher or student, I want to search GitHub by:\n   - **Natural Language**: e.g. \u201cframeworks for UI prototyping\u201d, converting text to search keywords via LLM.\n   - **Direct URL**: paste a repo URL to skip search.\n   - **Local Path**: select an existing clone to use directly.\n4. **Advanced Filtering**: As a power user, I want to sort/filter search results by stars, forks, issues, language, and last-updated to identify the most relevant repositories.\n5. **Seamless Pipeline Integration**: After selecting a repo, the existing tutorial-generation flow runs and displays rendered Markdown files inline, with an option to download a ZIP archive.\n\n### Inputs & Outputs\n\n| Category            | Inputs                                                                                                                                                 | Outputs                                                                                     |\n| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |\n| **Search & Config** | - `query_input`: NL text, GitHub URL, or local path- `filter_params`: stars, forks, issues, language, updated- `.env` API credentials & model settings | - `search_results`: up to 10 `RepoMetadata`- Summary of each README                         |\n| **Tutorial Flow**   | - `repo_url` or `local_dir` set by selection                                                                                                           | - `index.md`, Mermaid diagram, ordered chapter `.md` files- ZIP archive of output directory |\n| **UI Rendering**    | - Generated Markdown files and ZIP path                                                                                                                | - Inline preview via `st.markdown`- `st.download_button` for ZIP                            |\n\n## End\u2011to\u2011End Flow Design\n\n```mermaid\nflowchart TD\n  UI[Streamlit UI] --> S1[SmartSearchRepo]\n  S1 --> F1[FilterRepos]\n  F1 --> Select[SelectRepository]\n  Select --> Fetch[FetchRepo]\n  Fetch --> Identify[IdentifyAbstractions]\n  Identify --> Analyze[AnalyzeRelationships]\n  Analyze --> Order[OrderChapters]\n  Order --> Batch[WriteChapters]\n  Batch --> Combine[CombineTutorial]\n  Combine --> UI_Display[Render & Download]\n```\n\n1. `StreamlitUI` (New Frontend)\n\n   - **Type:** Regular node\n   - **Responsibilities:**\n     - Present sidebar inputs: Query box, filter widgets, credential inputs, model selector, and action buttons.\n     - Display either search results (repo cards) or tutorial previews (Markdown pages).\n     - Handle user interactions: card clicks trigger downstream nodes.\n     - Provide ZIP download.\n\n2. `SmartSearchRepo`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.query_input`, detect `mode`:\n     ```python\n     if looks_like_url(query_input): mode='url'\n     elif os.path.isdir(query_input): mode='local'\n     else: mode='nl'\n     ```\n   - **Exec:**\n     - **NL Mode:** call `extract_keywords(query_input)` \u2192 keywords list, join as search string.\n     - **URL Mode:** validate URL, set `selected_repo`.\n     - **Local Mode:** verify path, set `selected_repo`.\n   - **Post:** Write `shared.search_mode`, `shared.keywords`, and (for URL/local) `shared.selected_repo`.\n\n3. `FilterRepos`\n\n   - **Type:** Regular node\n   - **Prep:** Read `search_mode`, `keywords`, `filter_params`, `github_token`.\n   - **Exec:**\n     - If NL mode: `github_search_repos(keywords, filter_params, token)`\n     - Else (URL/local): skip API search.\n   - **Post:** Store top-10 `shared.search_results` (`RepoMetadata` + README summary via `summarize_readme`).\n\n4. `SelectRepository`\n\n   - **Type:** Regular node\n   - **Prep:** Read `shared.search_results`.\n   - **Exec:** On UI card click, set `shared.selected_repo` (URL or path).\n   - **Post:** Propagate into `shared.repo_url` or `shared.local_dir` for pipeline.\n\n5. **Existing Tutorial Flow** (`FetchRepo` \u2192 `IdentifyAbstractions` \u2192 `AnalyzeRelationships` \u2192 `OrderChapters` \u2192 `WriteChapters` \u2192 `CombineTutorial`)\n\n   - **Type:** Workflow + BatchNode for `WriteChapters`.\n   - **Unchanged:** Implements the tutorial generation as per [design.md].\n\n6. `RenderAndDownload`\n\n   - **Type:** Regular node\n   - **Prep:** After `CombineTutorial`, read `shared.final_output_dir` and loaded Markdown file contents.\n   - **Exec (Streamlit):**\n     - Loop through `.md` files: `st.markdown(file_content)` to render.\n     - ZIP directory with `zip_output_folder(output_dir)`.\n     - `st.download_button(\"Download Tutorial ZIP\", data=open(zip_path, 'rb'), file_name=\"tutorial.zip\")`\n   - **Post:** None (UI-only).\n\n## Utility Functions (New & Extended)\n\n| Function              | Module                 | Input                                                | Output                 | Role                                                                 |\n| --------------------- | ---------------------- | ---------------------------------------------------- | ---------------------- | -------------------------------------------------------------------- |\n| `extract_keywords`    | `utils/search_helpers` | `query_input: str`                                   | `keywords: List[str]`  | LLM\u2011powered extraction of search terms from free text.               |\n| `github_search_repos` | `utils/github_api`     | `keywords or query`, `filter_params`, `token: str`   | `List[RepoMetadata]`   | Calls GitHub search REST API, applies filters, returns top-10 repos. |\n| `summarize_readme`    | `utils/summarizer`     | `readme_text: str`                                   | `summary: str`         | Generates concise README summaries via LLM.                          |\n| `load_env_vars`       | `utils/env_loader`     | `.env` filepath                                      | Populated `os.environ` | Loads API keys and model names from `.env`.                          |\n| `zip_output_folder`   | `utils/io_helpers`     | `folder_path: str`                                   | `zip_path: str`        | Zips output tutorial directory for download.                         |\n| `crawl_github_files`  | as before              | `repo_url, token, patterns, max_size, use_rel_paths` | `files, stats`         | GitHub code crawling (unchanged).                                    |\n| `crawl_local_files`   | as before              | `directory, patterns, max_size, use_rel_paths`       | `files, stats`         | Local code crawling (unchanged).                                     |\n| `call_llm`            | as before              | `prompt: str, model: str`                            | `response: str`        | Central LLM interface for all analysis/generation steps.             |\n\n## Shared Store (Extended)\n\n```python\nshared = {\n  # Existing keys from design.md\n  \"repo_url\": None,\n  \"local_dir\": None,\n  \"project_name\": None,\n  \"github_token\": None,\n  \"output_dir\": \"output\",\n  \"files\": [],\n  \"abstractions\": [],\n  \"relationships\": {\"summary\": None, \"details\": []},\n  \"chapter_order\": [],\n  \"chapters\": [],\n  \"final_output_dir\": None,\n\n  # New keys for UI & search\n  \"query_input\": None,\n  \"search_mode\": None,\n  \"keywords\": [],\n  \"filter_params\": {},\n  \"search_results\": [],\n  \"selected_repo\": None,\n}\n```\n\n## Node Interface Mapping\n\n- \\`\\` \u2192 orchestrates all nodes, binds button callbacks to `SmartSearchRepo`, `FilterRepos`, `SelectRepository`, and downstream pipeline.\n- **New nodes** (`SmartSearchRepo`, `FilterRepos`, `SelectRepository`, `Render & Download`) plug into the existing `Flow` defined in `flow.py`, executed in the order shown in the flowchart.\n\n## Installation & Configuration\n\n1. **Clone & Setup**\n\nvenv\\Scripts\\activate\n\n pip install -r requirements.txt\n\n````\n2. **Environment**  \n   Copy `.env.example` to `.env` and fill:\n   ```env\n   GEMINI_API_KEY=<...>\n   GEMINI_PROJECT_ID=<...>\n   GITHUB_TOKEN=<...>\n   OPENAI_API_KEY=<...>\n   ANTHROPIC_API_KEY=<...>\n   GEMINI_MODEL=gemini-2.5-pro-exp-03-25\n   OPENAI_MODEL=gpt-4o-mini\n   ANTHROPIC_MODEL=claude-3-7-sonnet-latest\n````\n\n3. **Run App**\n   ```bash\n   streamlit run main.py\n   ```\n4. **Access UI**\\\n   Open [http://localhost:8501](http://localhost:8501) in your browser.\n\n---\n\n\n\n\n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 7: Streamlit UI & Search Flow`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 7: Streamlit UI & Search Flow\n\nWelcome back! In [Chapter 6: Shared Data Store](06_shared_data_store.md), we learned how different parts of our documentation pipeline (the \"Nodes\") share information using the central `shared` dictionary, like passing a project binder along an assembly line. We've seen how the tool works from the command line, taking inputs like a GitHub URL and generating files.\n\nBut what if you prefer a more visual way to interact with the tool? What if you want to *search* for interesting repositories directly, filter them, and see the results right in your web browser? That's where this chapter comes in!\n\n**Goal:** Understand how the `LLM-CODEBASE-DOCUMENTOR` provides a user-friendly web interface using Streamlit, allowing users to search for code, select a repository, generate the tutorial, and view/download it, all through their browser.\n\n## The Problem: Command Lines Aren't for Everyone \ud83e\udd14\n\nRunning programs from a terminal (the command line) is powerful, but it can be intimidating for beginners or inconvenient for quick tasks. You need to remember commands, type paths correctly, and look at files generated on your computer.\n\nWouldn't it be nicer to have a web page where you can:\n\n*   Type a search term like \"python data analysis library\".\n*   See a list of matching GitHub projects with summaries.\n*   Click a button to generate the tutorial for one of them.\n*   Read the generated tutorial directly on the page?\n*   Download the whole tutorial as a ZIP file?\n\nThis makes the tool much more accessible and easier to use.\n\n**Use Case:** A student wants to explore popular Python web frameworks. They use the web UI, type \"python web framework\" into a search box, see a list of projects like Flask and Django with summaries, click \"Select\" on Flask, and then read the automatically generated tutorial for Flask directly in their browser before downloading it.\n\n## The Solution: Streamlit - Building Web Apps with Python \u2728\n\nTo create this user-friendly web interface, our project uses **Streamlit**.\n\n*   **What it is:** Streamlit is a fantastic Python library that lets developers build interactive web applications really quickly, often with just Python code. You don't need to be a web development expert!\n*   **How we use it:** The `streamlit_app.py` file contains Python code that uses Streamlit functions (like `st.text_input`, `st.button`, `st.markdown`) to create the entire web page you interact with \u2013 the input boxes, buttons, search results, and tutorial display.\n\n**Analogy:** Think of Streamlit as a set of super easy-to-use building blocks (like Lego) specifically designed for creating data-focused web pages using only Python.\n\nWhen you run `streamlit run streamlit_app.py`, Streamlit starts a local web server and opens the application in your browser.\n\n## A New Workflow for the UI: `create_streamlit_flow`\n\nRemember our PocketFlow assembly line from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)? That was designed for the command-line use case (`FetchRepo` -> `IdentifyAbstractions` -> ...).\n\nFor the Streamlit UI, especially the search functionality, we need a slightly different workflow that includes steps for searching and selecting *before* we get to the main tutorial generation.\n\nThis new workflow is defined in `flow.py` inside the `create_streamlit_flow()` function. It introduces some new Nodes specifically for the UI:\n\n1.  **`SmartSearchRepo`:** The detective that figures out what you're searching for (URL, local path, or plain text) and extracts search keywords if needed.\n2.  **`FilterRepos`:** The librarian that searches GitHub using your keywords and filters, and even reads the README summaries.\n3.  **`SelectRepository`:** The receptionist that notes down which repository you finally choose.\n4.  **`RenderAndDownload`:** The publisher that displays the finished tutorial chapters in the browser and prepares a downloadable ZIP file.\n\nHere\u2019s how these new nodes integrate with the existing ones:\n\n```mermaid\nflowchart TD\n    subgraph Streamlit UI Interaction\n        StartUI[User Input in UI] --> SmartSearch[SmartSearchRepo]\n        SmartSearch --> Filter[FilterRepos]\n        Filter --> DisplayResults[Show Results in UI]\n        DisplayResults --> UserSelect[User Selects Repo]\n        UserSelect --> SelectRepo[SelectRepository]\n    end\n\n    subgraph Core Tutorial Generation (Reused Nodes)\n        Fetch[FetchRepo]\n        Identify[IdentifyAbstractions]\n        Analyze[AnalyzeRelationships]\n        Order[OrderChapters]\n        Write[WriteChapters]\n        Combine[CombineTutorial]\n    end\n\n    subgraph Display Output\n       Render[RenderAndDownload] --> DisplayTutorial[Show Tutorial & Download Button]\n    end\n\n    SelectRepo --> Fetch\n    Fetch --> Identify --> Analyze --> Order --> Write --> Combine\n    Combine --> Render\n```\n\n*Explanation:* The UI flow starts with `SmartSearchRepo`, `FilterRepos`, and `SelectRepository`. Once a repository is selected (via URL, local path, or search results), the `SelectRepository` node sets up the necessary information (`repo_url` or `local_dir`) in the `shared` store. Then, the flow plugs *directly* into the *same* `FetchRepo` node that the command-line version uses! The rest of the tutorial generation pipeline (`IdentifyAbstractions` -> `CombineTutorial`) runs exactly as before. Finally, the `RenderAndDownload` node takes the output and prepares it for display in Streamlit.\n\n**Code Snippet (`flow.py` - `create_streamlit_flow`):**\n\n```python\n# In flow.py\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n\n    # Reuse the original tutorial flow's nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n\n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n\n    # Connect the UI nodes: Search -> Filter -> Select\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n\n    # Connect the selection node TO the start of the original tutorial flow\n    select_repo >> fetch_repo\n\n    # Connect the end of the tutorial flow TO the final rendering node\n    combine_tutorial >> render_download\n\n    # Create the full UI flow, starting with the search\n    ui_flow = Flow(start=smart_search)\n\n    return ui_flow\n```\n\n*Explanation:* This code defines the new workflow. It creates instances of the new UI nodes (`SmartSearchRepo`, `FilterRepos`, etc.) and connects them. Crucially, it links `select_repo` *to* `fetch_repo` (start of the old flow) and `combine_tutorial` (end of the old flow) *to* `render_download`. This cleverly reuses the core logic while adding the UI interaction steps at the beginning and end.\n\n## The Search Process: Finding Your Repository\n\nLet's walk through what happens when you type a natural language query (like \"python web framework\") into the Streamlit UI and click \"Search\".\n\n1.  **User Action:** You type your query in the text box in the sidebar (`st.text_input`) and click the \"Search\" button (`st.button`).\n2.  **Streamlit App (`streamlit_app.py`):**\n    *   The app code takes your query, selected filters (like min stars), and API keys from the UI elements.\n    *   It creates the initial `shared` dictionary, populating it with `shared[\"query_input\"]`, `shared[\"filter_params\"]`, `shared[\"github_token\"]`, `shared[\"llm_provider\"]`, etc.\n    *   It gets the `ui_flow` using `create_streamlit_flow()`.\n    *   It starts the flow by running the first few nodes (`SmartSearchRepo` and `FilterRepos`) maybe inside a `st.spinner(\"Searching...\")`.\n3.  **`SmartSearchRepo` Node:**\n    *   **Prep:** Reads `shared[\"query_input\"]`.\n    *   **Exec:** Detects it's not a URL or local path, so `search_mode = \"nl\"`. It calls the `extract_keywords` utility function, potentially using an LLM ([Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md)), to get keywords like `[\"python\", \"web\", \"framework\"]`.\n    *   **Post:** Writes `shared[\"search_mode\"] = \"nl\"` and `shared[\"keywords\"] = [\"python\", \"web\", \"framework\"]`.\n4.  **`FilterRepos` Node:**\n    *   **Prep:** Reads `shared[\"search_mode\"]`, `shared[\"keywords\"]`, `shared[\"filter_params\"]`, and `shared[\"github_token\"]`.\n    *   **Exec:** Since `search_mode` is \"nl\", it calls the `github_search_repos` utility. This function uses the GitHub API to find repositories matching the keywords and filters. For each result, it might fetch the README content using `get_readme_content` and then call the `summarize_readme` utility (which uses an LLM) to create a short summary. It returns a list of `RepoMetadata` objects (containing URL, name, description, stars, summary, etc.).\n    *   **Post:** Writes the list of found repositories into `shared[\"search_results\"]`.\n5.  **Streamlit App (`streamlit_app.py`):**\n    *   After the initial nodes run, the app checks the `shared` store for `shared[\"search_results\"]`.\n    *   It loops through the `RepoMetadata` objects in the list.\n    *   For each repository, it displays a \"card\" using `st.markdown` and other Streamlit elements, showing the name, description, summary, stars, forks, and crucially, a \"Select\" button (`st.button(..., on_click=select_repository, args=(repo.url,))`).\n\n**Search Flow Sequence Diagram:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant StreamlitApp as streamlit_app.py\n    participant Shared as Shared Data Store\n    participant SmartSearch as SmartSearchRepo Node\n    participant LLM_Keywords as extract_keywords (LLM)\n    participant FilterRepos as FilterRepos Node\n    participant GitHubAPI as GitHub Search API\n    participant LLM_Summary as summarize_readme (LLM)\n\n    User->>StreamlitApp: Enters NL query, clicks Search\n    StreamlitApp->>Shared: Initialize shared store (query, filters, keys)\n    StreamlitApp->>SmartSearch: Run prep(shared)\n    SmartSearch->>Shared: Read query_input\n    StreamlitApp->>SmartSearch: Run exec(...)\n    SmartSearch->>LLM_Keywords: Extract keywords from query\n    LLM_Keywords-->>SmartSearch: Return keywords\n    StreamlitApp->>SmartSearch: Run post(shared, ..., result)\n    SmartSearch->>Shared: Write search_mode=\"nl\", keywords=[...]\n    StreamlitApp->>FilterRepos: Run prep(shared)\n    FilterRepos->>Shared: Read keywords, filters, token\n    StreamlitApp->>FilterRepos: Run exec(...)\n    FilterRepos->>GitHubAPI: Search repos(keywords, filters)\n    GitHubAPI-->>FilterRepos: Return repo list\n    loop For each Repo\n        FilterRepos->>GitHubAPI: Get README content\n        GitHubAPI-->>FilterRepos: Return README text\n        FilterRepos->>LLM_Summary: Summarize README text\n        LLM_Summary-->>FilterRepos: Return summary\n    end\n    FilterRepos-->>StreamlitApp: Return list of RepoMetadata (with summaries)\n    StreamlitApp->>FilterRepos: Run post(shared, ..., repo_list)\n    FilterRepos->>Shared: Write search_results=[...]\n    StreamlitApp->>Shared: Read search_results\n    StreamlitApp->>User: Display repository cards with \"Select\" buttons\n```\n\n**Simplified Code Snippets:**\n\n```python\n# In nodes.py - SmartSearchRepo.exec (Simplified NL path)\ndef exec(self, prep_res):\n    query_input = prep_res[\"query_input\"]\n    llm_provider = prep_res[\"llm_provider\"]\n    # ... logic to detect it's natural language ...\n    search_mode = \"nl\"\n    selected_repo = None\n    # Call utility to get keywords using LLM\n    keywords = extract_keywords(query_input, llm_provider=llm_provider)\n    return {\"search_mode\": search_mode, \"keywords\": keywords, \"selected_repo\": selected_repo}\n\n# In nodes.py - FilterRepos.exec (Simplified NL path)\ndef exec(self, prep_res):\n    # ... get keywords, filters, token ...\n    search_mode = prep_res[\"search_mode\"]\n    llm_provider = prep_res[\"llm_provider\"]\n    if search_mode != \"nl\": return [] # Skip if not NL search\n\n    # Call utility to search GitHub\n    repos = github_search_repos(keywords, filter_params, github_token)\n    # Loop and get summaries using LLM utility\n    for repo in repos:\n        readme_content = get_readme_content(repo.full_name, github_token)\n        repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n    return repos\n\n# In streamlit_app.py - Displaying results (Simplified)\nif st.session_state.ui_view == \"results\":\n    st.title(\"Search Results\")\n    search_results = st.session_state.search_results # Get from state (updated from shared store)\n    if search_results:\n        for i, repo in enumerate(search_results):\n            # Use Streamlit components to display repo info\n            st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n            st.markdown(f\"**Summary:** {repo.readme_summary}\")\n            # Add a button that calls select_repository when clicked\n            st.button(f\"Select\", key=f\"select_{i}\", on_click=select_repository, args=(repo.url,))\n```\n\n## Selecting and Generating: The Main Event\n\nOkay, you see the list of repositories. Now, you click \"Select\" on the one you want (e.g., Flask).\n\n1.  **User Action:** You click the \"Select\" button next to the Flask repository card.\n2.  **Streamlit App (`streamlit_app.py`):**\n    *   The `on_click` function (`select_repository`) associated with that button is called.\n    *   This function updates the application's state (`st.session_state.selected_repo = repo_url`) and ensures the `shared` dictionary also gets this selection: `shared[\"selected_repo\"] = repo_url`.\n    *   Because the state changed, Streamlit re-runs the app script. Crucially, it now knows a repository is selected and will try to run the *full* `ui_flow`.\n    *   It shows a spinner: `st.spinner(\"Generating tutorial...\")`.\n3.  **`SmartSearchRepo` & `FilterRepos` Nodes:** These might run again, but they likely see that `selected_repo` is already set or don't perform heavy work.\n4.  **`SelectRepository` Node:**\n    *   **Prep:** Reads `shared[\"selected_repo\"]`.\n    *   **Exec:** Sees that `selected_repo` (the Flask URL) is present.\n    *   **Post:** Writes the URL into the key needed by the original pipeline: `shared[\"repo_url\"] = \"https://github.com/pallets/flask\"` and `shared[\"local_dir\"] = None`.\n5.  **Core Tutorial Nodes (`FetchRepo` -> `CombineTutorial`):**\n    *   The PocketFlow engine now moves to the next node in the `ui_flow` sequence, which is `FetchRepo`.\n    *   `FetchRepo` reads `shared[\"repo_url\"]` (which was just set by `SelectRepository`).\n    *   The entire tutorial generation pipeline executes exactly as described in previous chapters ([Chapter 1](01_code_fetching___parsing.md), [Chapter 3](03_llm_analysis___generation.md), [Chapter 4](04_tutorial_structure_generation.md), etc.), using the Flask repository URL as input and populating the `shared` store with files, abstractions, relationships, chapters, and finally the `final_output_dir`.\n6.  **`RenderAndDownload` Node:**\n    *   **Prep:** Reads `shared[\"final_output_dir\"]`.\n    *   **Exec:** Reads all the generated `.md` files (index and chapters) from the `final_output_dir`. It also calls a utility (`zip_output_folder`) to create a ZIP archive of that directory.\n    *   **Post:** Writes the content of the Markdown files into `shared[\"markdown_files\"]` (a dictionary like `{\"index.md\": \"...\", \"01_chapter.md\": \"...\"}`) and the path to the ZIP file into `shared[\"zip_path\"]`.\n7.  **Streamlit App (`streamlit_app.py`):**\n    *   After the full flow completes, the app checks the `shared` store for `shared[\"markdown_files\"]` and `shared[\"zip_path\"]`.\n    *   It displays the project title.\n    *   It shows a download button (`st.download_button`) linked to the `zip_path`.\n    *   It loops through the `markdown_files` dictionary, displaying the content of `index.md` first, and then the content of each chapter file using `st.markdown()`.\n\n**Selection & Generation Flow Sequence Diagram:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant StreamlitApp as streamlit_app.py\n    participant Shared as Shared Data Store\n    participant SelectRepo as SelectRepository Node\n    participant CorePipeline as FetchRepo...CombineTutorial Nodes\n    participant RenderDownload as RenderAndDownload Node\n    participant FileSystem as Output Folder & ZIP\n\n    User->>StreamlitApp: Clicks \"Select\" button for a repo URL\n    StreamlitApp->>Shared: Set shared[\"selected_repo\"] = URL\n    Note over StreamlitApp: Trigger full flow run\n    StreamlitApp->>SelectRepo: Run prep/exec/post(shared)\n    SelectRepo->>Shared: Read shared[\"selected_repo\"]\n    SelectRepo->>Shared: Write shared[\"repo_url\"] = URL, shared[\"local_dir\"]=None\n    StreamlitApp->>CorePipeline: Run FetchRepo...CombineTutorial(shared)\n    Note over CorePipeline: Uses shared[\"repo_url\"], generates tutorial files\n    CorePipeline->>FileSystem: Write index.md, chapter files to output dir\n    CorePipeline->>Shared: Write shared[\"final_output_dir\"]\n    StreamlitApp->>RenderDownload: Run prep/exec/post(shared)\n    RenderDownload->>Shared: Read shared[\"final_output_dir\"]\n    RenderDownload->>FileSystem: Read all .md files from output dir\n    RenderDownload->>FileSystem: Create ZIP file from output dir\n    RenderDownload->>Shared: Write shared[\"markdown_files\"] = {...}, shared[\"zip_path\"] = \"...\"\n    StreamlitApp->>Shared: Read shared[\"markdown_files\"], shared[\"zip_path\"]\n    StreamlitApp->>User: Display tutorial content (st.markdown)\n    StreamlitApp->>User: Display download button (st.download_button)\n```\n\n**Simplified Code Snippets:**\n\n```python\n# In nodes.py - SelectRepository.post (Simplified)\ndef post(self, shared, prep_res, exec_res):\n    # exec_res might contain {\"repo_url\": url, \"local_dir\": None}\n    if exec_res.get(\"repo_url\"):\n        shared[\"repo_url\"] = exec_res[\"repo_url\"]\n        shared[\"local_dir\"] = None\n    elif exec_res.get(\"local_dir\"):\n        # Handle local directory selection similarly\n        shared[\"repo_url\"] = None\n        shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\n# In nodes.py - RenderAndDownload.exec (Simplified)\ndef exec(self, prep_res):\n    final_output_dir = prep_res[\"final_output_dir\"]\n    if not final_output_dir: return {\"success\": False, ...}\n\n    try:\n        # Utility to read all .md files into a dict\n        markdown_files = read_all_markdown_files(final_output_dir)\n        # Utility to create a zip file\n        zip_path = zip_output_folder(final_output_dir)\n        return {\"success\": True, \"markdown_files\": markdown_files, \"zip_path\": zip_path}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e), ...}\n\n# In streamlit_app.py - Displaying tutorial (Simplified)\nelif st.session_state.ui_view == \"tutorial\":\n    st.title(f\"Tutorial for {project_name}\")\n    zip_path = st.session_state.zip_path # Get from state\n\n    if zip_path: # If ZIP is ready\n        with open(zip_path, \"rb\") as f:\n            st.download_button(\"Download Tutorial ZIP\", data=f, ...)\n\n    markdown_files = st.session_state.markdown_files # Get from state\n    if markdown_files:\n        if \"index.md\" in markdown_files:\n            st.markdown(markdown_files[\"index.md\"]) # Show index first\n            st.markdown(\"---\")\n        for filename in sorted([f for f in markdown_files if f != \"index.md\"]):\n            st.markdown(markdown_files[filename]) # Show chapters\n```\n\n## Handling URLs and Local Paths Directly\n\nWhat if the user initially enters a direct GitHub URL or a local folder path instead of a natural language query?\n\n*   **`SmartSearchRepo`** detects this!\n    *   If it sees a URL, it sets `search_mode = \"url\"` and directly puts the URL into `selected_repo`.\n    *   If it sees a valid local path, it sets `search_mode = \"local\"` and puts the path into `selected_repo`.\n*   **`FilterRepos`** sees the `search_mode` is \"url\" or \"local\" and *skips* the GitHub API search and README summarization entirely.\n*   **`SelectRepository`** receives the pre-filled `selected_repo` and sets `repo_url` or `local_dir` accordingly.\n*   The flow then proceeds directly to `FetchRepo` and the tutorial generation pipeline, just like after selecting from search results.\n\nThis makes the UI versatile, handling direct inputs just as easily as natural language searches.\n\n## Conclusion\n\nIn this chapter, we explored the user-friendly **Streamlit UI** and the **Search Flow** of the `LLM-CODEBASE-DOCUMENTOR`.\n\n*   We use **Streamlit** (`streamlit_app.py`) to build an interactive web interface.\n*   A new **PocketFlow workflow** (`create_streamlit_flow`) integrates UI-specific nodes with the core tutorial generation pipeline.\n*   New nodes like **`SmartSearchRepo`**, **`FilterRepos`**, **`SelectRepository`**, and **`RenderAndDownload`** handle user interactions:\n    *   Processing search queries (URL, local path, or natural language using LLM for keywords).\n    *   Searching GitHub via API and summarizing READMEs (using LLM).\n    *   Allowing users to select a repository.\n    *   Rendering the final tutorial Markdown in the browser.\n    *   Providing a ZIP download option.\n*   The UI flow cleverly **reuses** the existing tutorial generation nodes (`FetchRepo` to `CombineTutorial`) after a repository source is determined.\n\nThis Streamlit interface makes the power of the codebase documentor accessible to a wider audience through a simple, visual, and interactive web application.\n\n**Next Up:** We've seen how API keys (GitHub, LLMs) and other settings (model names, filters) are crucial. Let's look at how the project manages these configurations in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).", "\n        Write a very beginner-friendly tutorial chapter (in Markdown format) for the project `LLM -CODEBASE-DOCUMENTOR` about the concept: \"Configuration & Environment Management\". This is Chapter 8.\n\n        Concept Details:\n        - Description:\n        Handles user inputs and environment settings. This includes parsing command-line arguments (for the CLI) or web inputs (for Streamlit), managing API keys securely using `.env` files, and setting up default file inclusion/exclusion patterns.\n\n\n        Complete Tutorial Structure:\n        1. [Code Fetching & Parsing](01_code_fetching___parsing.md)\n2. [PocketFlow Workflow](02_pocketflow_workflow.md)\n3. [LLM Analysis & Generation](03_llm_analysis___generation.md)\n4. [Tutorial Structure Generation](04_tutorial_structure_generation.md)\n5. [Node Abstraction](05_node_abstraction.md)\n6. [Shared Data Store](06_shared_data_store.md)\n7. [Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md)\n8. [Configuration & Environment Management](08_configuration___environment_management.md)\n\n        Context from previous chapters (summary):\n        # Chapter 1: Code Fetching & Parsing\n\n```markdown\n# Chapter 1: Code Fetching & Parsing\n\nWelcome to the `LLM-CODEBASE-DOCUMENTOR` tutorial! Imagine you want to write a cookbook about a complex dish. What's the very first thing you need? The ingredients! In our case, the \"dish\" is understanding a software project, and the \"ingredients\" are its source code files.\n\nThis first chapter is all about **getting those ingredients**: fetching the source code and doing some initial preparation (parsing/filtering).\n\n**Goal:** Understand how the tool grabs the necessary code files, either from the internet (GitHub) or from your own computer, and filters them to keep only what's needed.\n\n## Where Does the Code Come From?\n\nOur tool needs the raw source code to analyze. It can get this code from two main places:\n\n### 1. GitHub: The Online Treasure Chest \ud83c\udf10\n\n*   **What it is:** GitHub is a popular website where developers store and share their code publicly (or privately). Think of it like a huge online library for software projects.\n*   **How we fetch:** If the code you want to document lives on GitHub, you can tell our tool its web address (URL). The tool then uses GitHub's official way of talking to computers (the GitHub API) to download the code files. It's like asking the librarian nicely for specific books.\n*   **How to tell the tool:** You use the `--repo` command-line argument followed by the GitHub URL.\n\n    ```bash\n    python main.py --repo https://github.com/someuser/someproject\n    ```\n*   **Code Snippet (from `main.py`):** This code sets up the `--repo` option when you run the script.\n\n    ```python\n    # In main.py\n    parser = argparse.ArgumentParser(...)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    \n    # This line adds the --repo argument\n    source_group.add_argument(\"--repo\", \n        help=\"Specify the URL of a public GitHub repository...\") \n        \n    # Later, the code checks if args.repo was used\n    shared = {\n        \"repo_url\": args.repo,\n        # ... other inputs\n    }\n    ```\n*   **Quick Note on Tokens:** Sometimes, for private GitHub projects or to avoid download limits, you might need a special password called a GitHub Token (`--token` argument or `GITHUB_TOKEN` environment variable). We'll touch more on configuration later!\n\n### 2. Your Local Folder: Your Personal Bookshelf \ud83d\udcbb\n\n*   **What it is:** Maybe you've already downloaded the code onto your computer, or it's a project you're working on locally. It lives in a folder (directory) on your machine.\n*   **How we fetch:** Instead of going online, the tool can simply read the files directly from the folder you specify. It's like grabbing books directly off your own shelf.\n*   **How to tell the tool:** You use the `--dir` command-line argument followed by the path to the folder on your computer.\n\n    ```bash\n    # Example on Linux/macOS\n    python main.py --dir /home/user/my_projects/someproject\n    \n    # Example on Windows\n    python main.py --dir C:\\Users\\User\\Documents\\someproject\n    ```\n*   **Code Snippet (from `main.py`):** Similar to `--repo`, this code sets up the `--dir` option.\n\n    ```python\n    # In main.py (within the same source_group)\n    source_group.add_argument(\"--dir\", \n        help=\"Provide a local directory path to analyze code...\")\n\n    # Later, the code checks if args.dir was used\n    shared = {\n        \"local_dir\": args.dir,\n        # ... other inputs\n    }\n    ```\n\n**Important:** You must choose *one* source \u2013 either `--repo` OR `--dir`. You can't use both at the same time!\n\n## Filtering: Picking the Right Ingredients \ud83e\udd55\n\nNot all files in a codebase are equally important for understanding its core logic. Some are test files, documentation, configuration files, temporary build files, or just *huge* data files. Analyzing everything would be slow and might confuse the AI later.\n\nThink about preparing carrots. You want the orange root, but you'd probably discard the leafy green tops. We do something similar with code:\n\n*   **Include Patterns (`--include`):** Specify which types of files we *want* to keep. Patterns like `*.py` mean \"keep all files ending in `.py`\". If you don't specify any, the tool uses a helpful list of common code file types.\n*   **Exclude Patterns (`--exclude`):** Specify files or folders we want to *ignore*. Patterns like `tests/*` mean \"ignore everything inside any folder named `tests`\". Again, there's a default list to ignore common clutter like test folders, build outputs, and Git files.\n*   **Size Limit (`--max-size`):** Very large files can be difficult for the AI to process effectively. We set a maximum file size (default is about 300KB) and skip anything bigger.\n\n**How to tell the tool:**\n\n```bash\npython main.py --repo <URL> \\\n               --include \"*.py\" \"*.js\" \\\n               --exclude \"tests/*\" \"dist/*\" \\\n               --max-size 500000 # Set max size to 500KB\n```\n\n**Code Snippet (Defaults in `main.py`):** This shows the default patterns used if you don't provide your own `--include` or `--exclude` arguments.\n\n```python\n# In main.py\n\n# Default file types to INCLUDE\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", # Code files\n    \"*.md\", \"*.rst\", # Documentation\n    \"Dockerfile\", \"Makefile\", \"*.yaml\", \"*.yml\", # Config/Build\n    # ... many more common types\n}\n\n# Default file/folder patterns to EXCLUDE\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", # Tests, docs, examples\n    \"dist/*\", \"build/*\", # Build output folders\n    \".git/*\", \".github/*\", \"node_modules/*\", # Version control, dependencies\n    \"*.log\" # Log files\n}\n\n# ... later in main() ...\n\nshared = {\n    # ... other inputs\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size,\n    # ...\n}\n```\nThese filtering steps ensure we feed the AI mostly relevant code, making the documentation process more efficient and accurate.\n\n## How It Works Under the Hood (Simplified) \ud83d\udd75\ufe0f\u200d\u2640\ufe0f\n\nSo, how does the tool actually *do* the fetching and filtering? Inside the project, there's a component responsible for this, which we call a \"Node\". The specific node for this job is named `FetchRepo`.\n\n1.  **Check Arguments:** `FetchRepo` first looks at whether you provided `--repo` or `--dir`.\n2.  **Choose Crawler:**\n    *   If `--repo` was used, it calls a specialized helper function (`crawl_github_files`) designed to talk to the GitHub API, handle URLs, tokens, and download files, applying the include/exclude/size filters along the way.\n    *   If `--dir` was used, it calls a different helper function (`crawl_local_files`) that knows how to walk through local folders, read files, and apply the same filters.\n3.  **Collect Files:** Both helpers return a list of file paths and their content.\n4.  **Store Result:** `FetchRepo` stores this collected list of files (our \"ingredients\") so that other parts of the tool can use them later.\n\n**Code Snippet (Simplified logic in `nodes.py`):** This shows the core decision-making inside the `FetchRepo` node.\n\n```python\n# In nodes.py, inside the FetchRepo class\n\n# This 'exec' method does the main work\ndef exec(self, prep_res): \n    # prep_res contains the prepared inputs like repo_url, local_dir, patterns etc.\n    \n    if prep_res[\"repo_url\"]: # Did the user provide a GitHub URL?\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Use the GitHub crawler utility\n        result = crawl_github_files( \n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    elif prep_res[\"local_dir\"]: # Did the user provide a local directory?\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Use the local directory crawler utility\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            exclude_patterns=prep_res[\"exclude_patterns\"],\n            max_file_size=prep_res[\"max_file_size\"],\n            # ... other options\n        )\n    else:\n        # Neither was provided (shouldn't happen due to argument requirement)\n        print(\"No repository URL or local directory provided.\")\n        result = {\"files\": {}}\n\n    # Convert the result into a list of (path, content) pairs\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n    return files_list # Return the list of files\n```\n\n**Visualizing the Flow:**\n\nHere's a simple diagram showing the sequence:\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant main.py as Main Script\n    participant FetchRepo as FetchRepo Node\n    participant Crawlers as (crawl_github_files / crawl_local_files)\n    participant SharedData as Shared Data Store\n\n    User->>Main Script: Run with --repo URL (or --dir path) + filters\n    Main Script->>FetchRepo: Start processing with inputs\n    FetchRepo->>Crawlers: If URL, call crawl_github_files\n    FetchRepo->>Crawlers: If Path, call crawl_local_files\n    Crawlers-->>FetchRepo: Return filtered file paths & content\n    FetchRepo->>SharedData: Store the list of files\n```\n\n## What Happens Next?\n\nNow that we've successfully fetched and filtered the source code, these \"ingredients\" (the file paths and their content) are stored neatly. They are ready for the next stage where the AI will start analyzing them to understand the project's structure and key concepts.\n\nThis fetching and filtering is the crucial first step in our documentation journey!\n\n## Conclusion\n\nIn this chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` gets the source code it needs to analyze:\n\n*   It can fetch code from a **GitHub repository** using its URL (`--repo`).\n*   Or, it can read code from a **local directory** on your computer (`--dir`).\n*   It intelligently **filters** files using include/exclude patterns and size limits (`--include`, `--exclude`, `--max-size`) to focus on the most relevant code.\n*   The `FetchRepo` node orchestrates this process, using helper functions like `crawl_github_files` and `crawl_local_files`.\n\nWith the code successfully fetched and prepared, we're ready to move on to the next step: understanding how these different processing steps are organized and managed.\n\n**Next Up:** Learn about the overall structure that manages these steps in [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md).\n```\n---\n# Chapter 2: PocketFlow Workflow\n\nWelcome back! In [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md), we learned how our tool gathers the raw ingredients \u2013 the source code files \u2013 needed to understand a project. We fetched them from GitHub or a local folder and filtered them to keep only the essentials.\n\nBut gathering ingredients is just the first step. Think about baking a cake. You have flour, sugar, eggs, etc. Now you need a **recipe** that tells you exactly what order to mix them in, when to preheat the oven, and how long to bake. Without a recipe, you'd just have a messy pile of ingredients!\n\nThis chapter introduces the \"recipe\" for our `LLM-CODEBASE-DOCUMENTOR`: the **PocketFlow Workflow**.\n\n**Goal:** Understand how the tool organizes the different tasks (like fetching code, analyzing it, writing chapters) into a sequence, ensuring they run in the correct order and work together smoothly.\n\n## The Problem: Too Many Steps!\n\nGenerating a helpful tutorial from code involves several distinct steps:\n\n1.  **Get the Code:** Fetch and filter the source files (like we did in Chapter 1).\n2.  **Understand the Big Picture:** Identify the main concepts or \"abstractions\" in the code.\n3.  **See How Things Connect:** Figure out how these main concepts relate to each other.\n4.  **Plan the Tutorial:** Decide the best order to explain these concepts.\n5.  **Write the Chapters:** Generate the actual text for each concept.\n6.  **Put it All Together:** Combine the chapters into a final, organized tutorial.\n\nIf we just wrote code for these steps randomly, how would we make sure step 2 runs *after* step 1? How would the results of step 2 be available for step 3? It would be chaos! We need a manager, an orchestrator, a recipe.\n\n## The Solution: PocketFlow - Our Workflow Engine\n\nThis is where **PocketFlow** comes in. PocketFlow is the core engine that manages the entire tutorial generation process.\n\nThink of it like an **assembly line** for creating documentation:\n\n1.  **The Workflow:** This is the overall plan for the assembly line \u2013 what stations are there and in what order.\n2.  **Nodes:** Each individual step (like \"Fetch Code\" or \"Write Chapters\") is a \"Node\". Think of these as the different stations on the assembly line. Each station performs one specific task.\n3.  **Sequence:** PocketFlow defines the order in which these Nodes run. The output from one Node automatically becomes available for the next Node in the sequence. This is like the conveyor belt moving the product from one station to the next.\n\n**Analogy:** Imagine a simple recipe:\n1.  Crack eggs (Node 1)\n2.  Whisk eggs (Node 2 - uses the cracked eggs from Node 1)\n3.  Add sugar (Node 3 - uses the whisked eggs from Node 2)\n\nPocketFlow ensures we follow these steps *in order*. You can't whisk eggs before you crack them!\n\n## How it Looks in the Code (`flow.py`)\n\nLet's peek into the `flow.py` file to see how this assembly line is defined. Don't worry if the details seem complex; focus on the overall structure.\n\n1.  **Importing the Tools:**\n    First, we need the `Flow` blueprint from the PocketFlow library and the specific \"stations\" (Nodes) we defined elsewhere (`nodes.py`).\n\n    ```python\n    # In flow.py\n    from pocketflow import Flow # The blueprint for our assembly line\n\n    # Import all the different task stations (Nodes)\n    from nodes import (\n        FetchRepo,              # Station 1: Get the code\n        IdentifyAbstractions,   # Station 2: Find main concepts\n        AnalyzeRelationships, # Station 3: See connections\n        OrderChapters,        # Station 4: Plan the tutorial\n        WriteChapters,        # Station 5: Write the content\n        CombineTutorial       # Station 6: Put it all together\n        # ... (other nodes for UI might be here too)\n    )\n    ```\n    *Explanation:* This code brings in the necessary tools: `Flow` to define the assembly line itself, and each specific `Node` class (like `FetchRepo`, `IdentifyAbstractions`, etc.) which represents a task station.\n\n2.  **Setting up the Stations (Nodes):**\n    We create an instance of each Node class, like setting up the actual machines at each station.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create an instance of each Node (station)\n    fetch_repo = FetchRepo()\n    identify_abstractions = IdentifyAbstractions(...) # Some nodes might have settings\n    analyze_relationships = AnalyzeRelationships(...)\n    order_chapters = OrderChapters(...)\n    write_chapters = WriteChapters(...)\n    combine_tutorial = CombineTutorial()\n    ```\n    *Explanation:* Here, we're creating the specific workers or machines for our assembly line based on the blueprints we imported. `fetch_repo` is now our dedicated code-fetching station.\n\n3.  **Connecting the Stations (Sequence):**\n    This is the crucial part where we define the order using the `>>` operator. It literally means \"the output of the left node flows into the right node\".\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Connect the nodes in the desired order\n    fetch_repo >> identify_abstractions    # Step 1 -> Step 2\n    identify_abstractions >> analyze_relationships # Step 2 -> Step 3\n    analyze_relationships >> order_chapters      # Step 3 -> Step 4\n    order_chapters >> write_chapters         # Step 4 -> Step 5\n    write_chapters >> combine_tutorial       # Step 5 -> Step 6\n    ```\n    *Explanation:* This is the recipe! We're explicitly telling PocketFlow: \"Run `fetch_repo` first. When it's done, run `identify_abstractions`. Then run `analyze_relationships`, and so on.\" The `>>` connects them like a conveyor belt.\n\n4.  **Starting the Assembly Line:**\n    Finally, we tell PocketFlow which Node is the very *first* one in our sequence.\n\n    ```python\n    # In flow.py, inside create_tutorial_flow() function\n\n    # Create the actual Flow object, specifying the starting point\n    tutorial_flow = Flow(start=fetch_repo)\n\n    # Return the defined flow so it can be run elsewhere\n    return tutorial_flow, fetch_repo, combine_tutorial\n    ```\n    *Explanation:* We create the `Flow` object, telling it to begin the whole process at the `fetch_repo` station. Now, `tutorial_flow` represents our complete, ready-to-run documentation assembly line.\n\n## How it Works Under the Hood (Simplified)\n\nYou don't need to know the deep magic of PocketFlow, but here's the basic idea of what happens when the `tutorial_flow` is executed (usually triggered from the main script):\n\n1.  **Start:** PocketFlow looks at the `start` node (`fetch_repo`).\n2.  **Execute:** It runs the `fetch_repo` node. This node performs its task (fetching code, as described in Chapter 1) and produces output (the list of files).\n3.  **Follow the Arrow:** PocketFlow sees `fetch_repo >> identify_abstractions`.\n4.  **Pass Data (Implicitly):** PocketFlow makes the output of `fetch_repo` (the files) available to `identify_abstractions`. (We'll see exactly *how* data is shared in a later chapter: [Shared Data Store](06_shared_data_store.md)).\n5.  **Execute Next:** It runs the `identify_abstractions` node, which uses the files to find concepts.\n6.  **Repeat:** It follows the `>>` arrows, executing each node in sequence (`AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, `CombineTutorial`), implicitly passing the necessary data along the chain.\n7.  **Finish:** Once the last node (`CombineTutorial`) finishes, the workflow is complete!\n\nHere's a diagram showing this execution sequence:\n\n```mermaid\nsequenceDiagram\n    participant PF as PocketFlow Engine\n    participant FR as FetchRepo Node\n    participant IA as IdentifyAbstractions Node\n    participant AR as AnalyzeRelationships Node\n    participant OC as OrderChapters Node\n    participant WC as WriteChapters Node\n    participant CT as CombineTutorial Node\n\n    PF->>FR: Execute FetchRepo\n    FR-->>PF: Results (Files)\n    PF->>IA: Execute IdentifyAbstractions (using Files)\n    IA-->>PF: Results (Abstractions)\n    PF->>AR: Execute AnalyzeRelationships (using Abstractions)\n    AR-->>PF: Results (Relationships, Summary)\n    PF->>OC: Execute OrderChapters (using Abstractions, Relationships)\n    OC-->>PF: Results (Chapter Order)\n    PF->>WC: Execute WriteChapters (using Order, Abstractions, Files)\n    WC-->>PF: Results (Chapter Content)\n    PF->>CT: Execute CombineTutorial (using Content, Order, etc.)\n    CT-->>PF: Results (Output Path)\n    PF-->>Caller: Flow Finished\n```\n\n## Connecting the Dots\n\n*   The `FetchRepo` node we saw defined here is the *same* node we discussed in [Chapter 1: Code Fetching & Parsing](01_code_fetching___parsing.md). PocketFlow just tells it *when* to run.\n*   Each of these steps (`IdentifyAbstractions`, `AnalyzeRelationships`, etc.) is its own \"Node\". We'll learn more about what makes a Node tick in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n*   The process of identifying abstractions and relationships involves using Large Language Models (LLMs). We'll dive into that in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n*   The way data is passed between nodes relies on a [Shared Data Store](06_shared_data_store.md), which we'll explore soon.\n\n## Conclusion\n\nIn this chapter, we learned about the **PocketFlow Workflow**, the \"recipe\" or \"assembly line\" that organizes how our `LLM-CODEBASE-DOCUMENTOR` works.\n\n*   It defines a **sequence** of tasks called **Nodes**.\n*   It ensures nodes run in the **correct order** using the `>>` connection in `flow.py`.\n*   It manages the **flow of execution**, making the process structured and reliable.\n\nPocketFlow acts as the conductor of our orchestra, making sure each instrument (Node) plays its part at the right time to create the final symphony (the tutorial).\n\nNow that we understand *how* the steps are organized, let's look at *what* some of the most interesting steps actually *do*.\n\n**Next Up:** Explore how the tool uses AI to understand the code in [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md).\n---\n# Chapter 3: LLM Analysis & Generation\n\n```markdown\n# Chapter 3: LLM Analysis & Generation\n\nWelcome back! In [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md), we saw how our project uses a workflow (like an assembly line) to organize all the steps needed to create documentation. We learned that each step is a \"Node\" and they run in a specific order.\n\nNow, let's dive into the most exciting part: the \"magic\" that happens inside some of those nodes. How does the tool actually *understand* the code and *write* the tutorial chapters? The answer lies in **Large Language Models (LLMs)**.\n\n**Goal:** Understand what LLMs are and how `LLM-CODEBASE-DOCUMENTOR` uses them as the core \"brain\" to analyze code, figure out key concepts, structure the tutorial, and generate the text you read.\n\n## The Challenge: Making Sense of Code Mountain \u26f0\ufe0f\n\nImagine being given a huge, complex software project with hundreds of files. How would you start explaining it to someone new? It's tough! You'd need to:\n\n1.  Read through lots of code.\n2.  Figure out the most important parts (the main ideas or \"abstractions\").\n3.  Understand how these parts connect and work together.\n4.  Decide the best order to explain them.\n5.  Write clear, beginner-friendly explanations for each part.\n\nDoing this manually takes a lot of time and expertise. This is exactly the problem our tool aims to solve using AI.\n\n**Use Case:** We want to automatically generate a helpful, multi-chapter tutorial just by pointing the tool at a codebase.\n\n## The Solution: Large Language Models (LLMs) - Our AI Expert \ud83e\udde0\n\nThink of a Large Language Model (LLM) like a super-smart assistant that has read *massive* amounts of text and code from the internet. Because of this training, it's incredibly good at:\n\n*   **Understanding Language:** It gets what you're asking for in plain English.\n*   **Understanding Code:** It can read code (like Python, JavaScript, etc.) and grasp its structure and purpose.\n*   **Generating Text:** It can write human-like text, summaries, explanations, and even code snippets.\n\nPopular examples of LLMs include Google's **Gemini**, Anthropic's **Claude**, and OpenAI's **GPT** models. Our tool is designed to work with these!\n\n**Analogy:** An LLM is like having a very experienced programmer and a skilled technical writer rolled into one, ready to help you understand and explain code.\n\n## How We Use LLMs in the Workflow\n\nRemember the assembly line (PocketFlow workflow) from Chapter 2? Several stations (Nodes) on that line rely heavily on LLMs:\n\n1.  **`IdentifyAbstractions` Node:**\n    *   **Task:** Figure out the main concepts or building blocks of the code.\n    *   **How LLM helps:** We feed the LLM the source code files we fetched in Chapter 1. We ask it: \"Read this code. What are the 6-10 most important ideas someone needs to understand?\" The LLM analyzes the code and provides a list of these concepts (abstractions) with brief descriptions.\n\n2.  **`AnalyzeRelationships` Node:**\n    *   **Task:** Understand how the identified concepts connect.\n    *   **How LLM helps:** We give the LLM the list of abstractions (from the previous step) and relevant code snippets. We ask: \"How do these concepts interact? Which ones use or depend on others? And can you give me a quick summary of the whole project?\" The LLM analyzes the connections and provides a summary and a list of relationships.\n\n3.  **`OrderChapters` Node:**\n    *   **Task:** Decide the best order to explain these concepts in a tutorial.\n    *   **How LLM helps:** We provide the LLM with the list of abstractions and their relationships. We ask: \"What's the most logical order to teach these concepts to a beginner?\" The LLM suggests an order, often starting with foundational or user-facing ideas and moving towards more detailed ones.\n\n4.  **`WriteChapters` Node:**\n    *   **Task:** Generate the actual text for each chapter.\n    *   **How LLM helps:** For each concept (in the order decided above), we give the LLM the concept's description, relevant code snippets, and the overall tutorial structure. We ask: \"Write a beginner-friendly tutorial chapter about this concept, explaining it clearly with analogies and code examples, and linking to other relevant chapters.\" The LLM generates the Markdown text for the chapter.\n\nEssentially, we're using the LLM as an expert analyst and writer at multiple stages of our documentation assembly line.\n\n## Choosing Your AI Brain (`--model`)\n\nOur tool lets you choose which LLM provider you want to use. You might have access to one provider's API key but not another, or you might prefer the writing style of a specific model.\n\nYou tell the tool which LLM to use with the `--model` command-line argument when you run it:\n\n```bash\n# Example: Use Google Gemini (the default)\npython main.py --repo <URL> --model google-gemini\n\n# Example: Use Anthropic Claude\npython main.py --repo <URL> --model anthropic-claude\n\n# Example: Use OpenAI GPT\npython main.py --repo <URL> --model openai-gpt\n```\n\n**Code Snippet (`main.py`):** This code sets up the `--model` argument.\n\n```python\n# In main.py\n# Import the Enum that defines the choices\nfrom utils.call_llm import LLMProvider_enum\n\nparser = argparse.ArgumentParser(...)\n\n# This adds the --model argument with specific choices\nparser.add_argument(\"--model\", type=str, \n                    choices=[e.value for e in LLMProvider_enum], # Choices are 'google-gemini', 'anthropic-claude', 'openai-gpt'\n                    default=LLMProvider_enum.GOOGLE.value, # Default is Google Gemini\n                    help=\"LLM provider to use...\")\n\n# Later, the chosen model is stored in the 'shared' dictionary\nargs = parser.parse_args()\nshared = {\n    # ... other inputs\n    \"model_used\": args.model, # The string name like 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # The Enum value used internally\n    # ...\n}\n```\n*Explanation:* The code defines the `--model` option, restricts the choices to the supported providers using `LLMProvider_enum`, sets a default (`google-gemini`), and stores the user's choice so the rest of the tool knows which AI to talk to.\n\n**Quick Note on API Keys:** Using these LLMs requires API keys (like passwords) from the respective providers (Google, Anthropic, OpenAI). You'll need to set these up. We'll cover managing keys and other settings in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n## Under the Hood: Talking to the LLM (`utils/call_llm.py`)\n\nHow does a Node like `IdentifyAbstractions` actually *talk* to the LLM? It uses a helper function called `call_llm`.\n\n**Analogy:** Think of `call_llm` as the dedicated messenger service that knows how to contact the chosen AI expert (Google, Anthropic, or OpenAI), deliver our instructions (the prompt), and bring back the answer (the response).\n\n1.  **Prepare the Message (Prompt):** The Node (e.g., `IdentifyAbstractions`) gathers all the necessary information (code, context, previous results) and carefully writes detailed instructions for the LLM. This instruction packet is called the \"prompt\".\n2.  **Send the Message:** The Node calls the `call_llm` function, passing the prompt and specifying which LLM provider to use (based on the `--model` choice).\n3.  **`call_llm` Takes Over:**\n    *   It checks if caching is enabled (`use_cache=True`) and if we've asked this *exact* same question before. If yes, it returns the cached answer to save time and cost!\n    *   If not cached, it figures out which LLM provider we want (Google, Anthropic, OpenAI).\n    *   It uses the correct library and API key to connect to that provider's service over the internet.\n    *   It sends the prompt to the LLM service.\n4.  **LLM Does the Work:** The powerful LLM model processes the prompt and generates a text response.\n5.  **Receive the Answer:** `call_llm` receives the response text from the LLM service.\n6.  **Cache and Return:** If caching is enabled, `call_llm` saves the response for next time. It then returns the response text back to the Node that called it.\n7.  **Node Processes the Answer:** The Node receives the LLM's response (e.g., the list of abstractions in YAML format, or the generated chapter text) and continues its task.\n\n**Sequence Diagram: LLM Call Flow**\n\nThis diagram shows how a Node (like `IdentifyAbstractions`) uses `call_llm` to interact with an LLM:\n\n```mermaid\nsequenceDiagram\n    participant Node as IdentifyAbstractions Node\n    participant CallLLM as call_llm Utility\n    participant Cache as LLM Cache (File)\n    participant LLM as External LLM Service (e.g., Gemini API)\n\n    Node->>CallLLM: call_llm(prompt, use_cache=True, model='google-gemini')\n    CallLLM->>Cache: Check if prompt exists in cache?\n    alt Prompt found in cache\n        Cache-->>CallLLM: Return cached response\n        CallLLM-->>Node: Return response from cache\n    else Prompt not found in cache\n        CallLLM->>LLM: Send prompt via API (using API Key)\n        LLM-->>CallLLM: Receive generated text response\n        CallLLM->>Cache: Store prompt & response in cache\n        CallLLM-->>Node: Return new response\n    end\n\n```\n\n**Code Snippet (`utils/call_llm.py`):** The core function signature.\n\n```python\n# In utils/call_llm.py\n\n# Enum defines the choices for the model parameter\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Simplified function definition\ndef call_llm(prompt: str, use_cache: bool = True, model: str = LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    logger.info(f\"PROMPT: {prompt}\") # Log the request\n\n    # 1. Check cache (if use_cache is True)\n    if use_cache and prompt_in_cache(prompt):\n        response_text = get_from_cache(prompt)\n        logger.info(f\"RESPONSE (cached): {response_text}\")\n        return response_text\n\n    # 2. Call the appropriate LLM API based on 'model' value\n    response_text = \"\"\n    if model == LLMProvider_enum.GOOGLE:\n        # Code to call Google Gemini API...\n        response_text = # ... result from Gemini\n    elif model == LLMProvider_enum.ANTHROPIC:\n        # Code to call Anthropic Claude API...\n        response_text = # ... result from Claude\n    else: # Assume OpenAI\n        # Code to call OpenAI GPT API...\n        response_text = # ... result from OpenAI\n\n    logger.info(f\"RESPONSE (new): {response_text}\") # Log the response\n\n    # 3. Update cache (if use_cache is True)\n    if use_cache:\n        update_cache(prompt, response_text)\n\n    return response_text\n```\n*Explanation:* This function takes the `prompt`, checks the `cache`, calls the selected `model`'s API if needed, logs the interaction, updates the `cache`, and returns the `response_text`.\n\n**Code Snippet (`nodes.py` - `IdentifyAbstractions`):** How a Node prepares a prompt and uses `call_llm`.\n\n```python\n# In nodes.py, inside IdentifyAbstractions.exec method\n\n# ... (code to prepare 'context', 'file_listing_for_prompt', 'project_name') ...\nllm_provider = prep_res[-1] # Get the chosen LLM provider\n\nprint(\"Identifying abstractions using LLM...\")\n# Construct the detailed prompt for the LLM\nprompt = f\"\"\"\nFor the project `{project_name}`:\n\nCodebase Context:\n{context}\n\nAnalyze the codebase context.\nIdentify the top 6-10 core most important abstractions...\n... (rest of the instructions, asking for YAML output) ...\n\nList of file indices and paths present in the context:\n{file_listing_for_prompt}\n\nFormat the output as a YAML list of dictionaries:\n```yaml\n- name: AbstractionName\n  description: Explanation...\n  file_indices: [0, 3]\n# ... other abstractions\n```\"\"\"\n\n# Call the messenger service!\nresponse = call_llm(prompt=prompt, use_cache=True, model=llm_provider)\n\n# Process the response (parse YAML, validate, etc.)\n# ... (code to handle the YAML response from the LLM) ...\n```\n*Explanation:* The `IdentifyAbstractions` node carefully builds a detailed `prompt` telling the LLM exactly what to do and how to format the answer (as YAML). Then, it simply calls `call_llm` to get the AI's response. Other nodes like `AnalyzeRelationships` and `WriteChapters` follow a similar pattern, just with different prompts tailored to their specific tasks.\n\n## Conclusion\n\nIn this chapter, we pulled back the curtain on the AI magic behind `LLM-CODEBASE-DOCUMENTOR`. We learned:\n\n*   **LLMs (Large Language Models)** like Gemini, Claude, and GPT act as the \"brain\" of the tool.\n*   They are used at multiple steps in the workflow (Nodes) to:\n    *   Identify core code **abstractions**.\n    *   Analyze their **relationships**.\n    *   Determine the tutorial chapter **order**.\n    *   **Write** the actual chapter content.\n*   You can choose your preferred LLM using the `--model` argument.\n*   The `utils/call_llm.py` helper function handles the communication with the chosen LLM service, including caching.\n*   Nodes prepare specific **prompts** (instructions) to guide the LLM's analysis and generation.\n\nBy leveraging LLMs, the tool can automate the complex process of understanding a codebase and generating human-friendly documentation.\n\n**Next Up:** Now that we know LLMs help identify concepts and relationships, let's look specifically at how the tool uses that information to plan the tutorial's structure in [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md).\n```\n---\n# Chapter 4: Tutorial Structure Generation\n\n```markdown\n# Chapter 4: Tutorial Structure Generation\n\nWelcome back! In [Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md), we saw how the \"AI brain\" (the LLM) helps us understand the code by identifying key concepts (abstractions), figuring out how they relate, deciding the best order to teach them, and even writing the first draft of each chapter's content.\n\nNow, imagine you've written all the individual recipes for your cookbook. You have the text for the cake recipe, the cookie recipe, the bread recipe, etc., all on separate pieces of paper. You also have a summary of the whole cookbook and maybe a diagram showing which recipes use similar ingredients. What's the next step? You need to **assemble the cookbook**!\n\nThis chapter is all about that final assembly process: **Tutorial Structure Generation**.\n\n**Goal:** Understand how the tool takes all the generated pieces \u2013 the project summary, the concept relationships, the chapter order, and the written chapter text \u2013 and organizes them into a final, polished tutorial structure, complete with an index file, diagrams, and correctly linked chapter files.\n\n## The Problem: From Scattered Notes to a Polished Book \ud83d\udcda\n\nAfter the previous steps, we have several valuable pieces of information floating around:\n\n1.  **Abstractions:** The list of key concepts identified by the LLM (e.g., \"Client Session\", \"Tools\").\n2.  **Relationships:** How these concepts connect, including a project summary (e.g., \"Client Session calls Tools\").\n3.  **Chapter Order:** The best sequence to explain these concepts (e.g., explain \"Client Session\" first, then \"Tools\").\n4.  **Chapter Content:** The actual Markdown text generated by the LLM for each concept.\n\nBut these are just separate bits of data inside the computer's memory. We need to transform them into something a human can actually use: a set of neatly organized Markdown files that form a coherent tutorial.\n\n**Use Case:** We want to take all the intermediate results generated by the LLMs and the workflow and produce the final output directory. This directory should contain:\n*   An `index.md` file acting as the table of contents, with a summary and a relationship diagram.\n*   Individual chapter files (like `01_client_session.md`, `02_tools.md`) containing the generated text.\n*   Proper links connecting the index to the chapters and the chapters to each other.\n\n## The Solution: The `CombineTutorial` Node - Our Assembler \ud83d\udee0\ufe0f\n\nThe final step in our documentation assembly line ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) is handled by a specific node called `CombineTutorial`. Think of this node as the **final assembler** or the **bookbinder**. Its job is to take all the finished components and put them together correctly.\n\nHere's what the `CombineTutorial` node does:\n\n1.  **Gathers Inputs:** It collects all the necessary pieces created by the previous nodes:\n    *   The project summary and relationship details (from `AnalyzeRelationships`).\n    *   The list of abstractions (from `IdentifyAbstractions`).\n    *   The determined chapter order (from `OrderChapters`).\n    *   The generated Markdown content for each chapter (from `WriteChapters`).\n    *   The project name and desired output location.\n\n2.  **Generates the Relationship Diagram:** It uses the relationship data (which abstraction connects to which, and the label for that connection) to automatically create a visual flowchart using Mermaid syntax. This helps users see the connections at a glance.\n\n3.  **Creates `index.md` Content:** It prepares the text for the main `index.md` file. This includes:\n    *   The project title.\n    *   The high-level summary generated earlier.\n    *   A link to the source code repository (if available).\n    *   The Mermaid diagram it just generated.\n    *   An ordered list of chapters, with links to the corresponding chapter files (e.g., \"1. [Client Session](01_client_session.md)\").\n\n4.  **Prepares Chapter Files:** For each chapter's generated Markdown content:\n    *   It determines the correct filename based on the chapter number and a \"safe\" version of the abstraction name (e.g., replacing spaces with underscores).\n    *   It adds a standard attribution line at the end.\n\n5.  **Writes Everything to Disk:**\n    *   It creates the final output directory (usually named after the project, inside an `output` folder).\n    *   It writes the prepared `index.md` content to `index.md` inside that directory.\n    *   It writes each chapter's prepared content to its corresponding filename (e.g., `01_client_session.md`, `02_tools.md`, etc.) inside the same directory.\n\n## How It Works Under the Hood (Simplified Walkthrough) \ud83d\udeb6\u200d\u2640\ufe0f\n\nLet's follow the `CombineTutorial` node step-by-step:\n\n1.  **Get Ready (`prep`):** The node first gathers all the ingredients it needs from the shared data store: the summary, relationship details (`relationships['details']`), chapter order (`chapter_order`), abstraction list (`abstractions`), chapter content (`chapters`), project name, and output directory path.\n2.  **Draw the Map (`prep`):** It looks at the `relationships['details']` (which contains pairs like `from: 0, to: 1, label: \"Calls\"`) and generates the Mermaid code for the diagram.\n3.  **Write the Introduction (`prep`):** It starts building the `index.md` text string, adding the title, summary, and the Mermaid diagram code.\n4.  **Create the Table of Contents (`prep`):** It iterates through the `chapter_order` list. For each abstraction index in the order, it finds the abstraction's name, creates a safe filename (like `01_client_session.md`), and adds a numbered link like `1. [Client Session](01_client_session.md)` to the `index.md` text. It also prepares a list of all chapter filenames and their corresponding content (adding the attribution).\n5.  **Build the Output Folder (`exec`):** The node creates the actual folder on your computer (e.g., `output/my_project/`).\n6.  **Save the Index (`exec`):** It saves the complete `index.md` text string into the `index.md` file within the new folder.\n7.  **Save the Chapters (`exec`):** It goes through the list of prepared chapter filenames and content, saving each one as a separate `.md` file in the folder.\n8.  **Done! (`post`):** It records the path to the final output directory in the shared store and prints a success message.\n\n**Visualizing the Flow:**\n\nHere's a simple sequence diagram for the `CombineTutorial` node:\n\n```mermaid\nsequenceDiagram\n    participant CT_Prep as CombineTutorial (Prep Phase)\n    participant SharedData as Shared Data Store\n    participant CT_Exec as CombineTutorial (Exec Phase)\n    participant FileSystem as Your Computer's Files\n\n    CT_Prep ->> SharedData: Read summary, relationships, order, abstractions, chapters, project name\n    CT_Prep ->> CT_Prep: Generate Mermaid Diagram from relationships\n    CT_Prep ->> CT_Prep: Prepare index.md content (title, summary, diagram, links)\n    CT_Prep ->> CT_Prep: Prepare list of chapter filenames & content\n    CT_Prep -->> CT_Exec: Pass prepared index & chapter data\n\n    CT_Exec ->> FileSystem: Create output directory (e.g., output/project_name)\n    CT_Exec ->> FileSystem: Write index.md content to file\n    loop For each chapter file\n        CT_Exec ->> FileSystem: Write chapter content to its .md file\n    end\n    CT_Exec ->> SharedData: Store final output path\n    CT_Exec -->> PocketFlow: Signal completion\n```\n\n## Code Snippets (`nodes.py` - `CombineTutorial`)\n\nLet's look at simplified snippets from the `CombineTutorial` node to see how it prepares and writes the files.\n\n**1. Preparing the Index and Chapter Data (`prep` method):**\n\nThis part gathers data and builds the structure in memory *before* writing any files.\n\n```python\n# In nodes.py, inside CombineTutorial.prep method\n\n# --- Assume these are loaded from shared data ---\n# project_name = \"My Cool Project\"\n# relationships_data = {\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"Uses\"}, ...]}\n# chapter_order = [0, 1, 2] # Indices of abstractions in order\n# abstractions = [{\"name\": \"Core Logic\", ...}, {\"name\": \"Data Handler\", ...}, {\"name\": \"API Interface\", ...}]\n# chapters_content = [\"Markdown for Core Logic...\", \"Markdown for Data Handler...\", \"...\"]\n# output_base_dir = \"output\"\n# repo_url = \"https://github.com/user/repo\"\n# -------------------------------------------------\n\noutput_path = os.path.join(output_base_dir, project_name) # e.g., \"output/My Cool Project\"\n\n# --- Generate Mermaid Diagram ---\nmermaid_lines = [\"flowchart TD\"]\n# Add nodes (e.g., A0[\"Core Logic\"])\nfor i, abstr in enumerate(abstractions):\n    node_id = f\"A{i}\"\n    node_label = abstr['name'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {node_id}[\"{node_label}\"]')\n# Add edges (e.g., A0 -- \"Uses\" --> A1)\nfor rel in relationships_data['details']:\n    from_node_id = f\"A{rel['from']}\"\n    to_node_id = f\"A{rel['to']}\"\n    edge_label = rel['label'].replace('\"', '') # Sanitize\n    mermaid_lines.append(f'    {from_node_id} -- \"{edge_label}\" --> {to_node_id}')\nmermaid_diagram = \"\\n\".join(mermaid_lines)\n# --- End Mermaid ---\n\n# --- Prepare index.md content ---\nindex_content = f\"# Tutorial: {project_name}\\n\\n\"\nindex_content += f\"{relationships_data['summary']}\\n\\n\"\nif repo_url:\n    index_content += f\"**Source Repository:** [{repo_url}]({repo_url})\\n\\n\"\nindex_content += \"```mermaid\\n\" + mermaid_diagram + \"\\n```\\n\\n\"\nindex_content += \"## Chapters\\n\\n\"\n\nchapter_files = [] # To store {filename, content} pairs\n# Generate chapter links and prepare chapter file data\nfor i, abstraction_index in enumerate(chapter_order):\n    abstraction_name = abstractions[abstraction_index][\"name\"]\n    # Create safe filename (e.g., 01_core_logic.md)\n    safe_name = \"\".join(c if c.isalnum() else '_' for c in abstraction_name).lower()\n    filename = f\"{i+1:02d}_{safe_name}.md\"\n    # Add link to index.md content\n    index_content += f\"{i+1}. [{abstraction_name}]({filename})\\n\"\n\n    # Prepare chapter content (add attribution)\n    chapter_content = chapters_content[i] + \"\\n\\n---\\n\\nGenerated by ...\" # Add attribution\n    # Store filename and content for later writing\n    chapter_files.append({\"filename\": filename, \"content\": chapter_content})\n\n# Add attribution to index.md\nindex_content += \"\\n\\n---\\n\\nGenerated by ...\"\n\n# Return all prepared data for the exec step\nreturn {\n    \"output_path\": output_path,\n    \"index_content\": index_content,\n    \"chapter_files\": chapter_files\n}\n```\n*Explanation:* The `prep` method acts like the planning stage. It uses the data from previous steps (`relationships`, `chapter_order`, etc.) to generate the Mermaid diagram string and the full text content for `index.md`, including formatted links. It also prepares a list (`chapter_files`) containing the final filename and content for each chapter.\n\n**2. Writing the Files (`exec` method):**\n\nThis part takes the prepared data and actually creates the files and folders.\n\n```python\n# In nodes.py, inside CombineTutorial.exec method\n\n# Get the prepared data from the prep step\noutput_path = prep_res[\"output_path\"]       # e.g., \"output/My Cool Project\"\nindex_content = prep_res[\"index_content\"]   # The full text for index.md\nchapter_files = prep_res[\"chapter_files\"]   # List of {\"filename\": \"...\", \"content\": \"...\"}\n\nprint(f\"Combining tutorial into directory: {output_path}\")\n# Create the directory if it doesn't exist\nos.makedirs(output_path, exist_ok=True)\n\n# Write index.md\nindex_filepath = os.path.join(output_path, \"index.md\")\nwith open(index_filepath, \"w\", encoding=\"utf-8\") as f:\n    f.write(index_content)\nprint(f\"  - Wrote {index_filepath}\")\n\n# Write chapter files\nfor chapter_info in chapter_files:\n    chapter_filepath = os.path.join(output_path, chapter_info[\"filename\"])\n    with open(chapter_filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(chapter_info[\"content\"])\n    print(f\"  - Wrote {chapter_filepath}\")\n\n# Return the path where the tutorial was saved\nreturn output_path\n```\n*Explanation:* The `exec` method is the action stage. It takes the `output_path`, `index_content`, and `chapter_files` prepared by `prep`. It creates the directory, then simply writes the `index_content` into `index.md` and loops through `chapter_files`, writing each chapter's content into its respective `.md` file.\n\n## Connecting the Dots\n\n*   This `CombineTutorial` node relies heavily on the outputs produced by previous nodes like `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, and `WriteChapters`. Without their results, it wouldn't have anything to assemble!\n*   The use of indices throughout the process (identifying files, relationships, and order by index) makes it easier for `CombineTutorial` to correctly link everything together at the end.\n\n## Conclusion\n\nIn this chapter, we learned about the final assembly step: **Tutorial Structure Generation**.\n\n*   The `CombineTutorial` node acts as the \"bookbinder\" for our project.\n*   It takes the project summary, relationship data, chapter order, and chapter content generated in earlier steps.\n*   It creates the final output directory.\n*   It generates a **Mermaid diagram** visualizing the relationships between concepts.\n*   It writes the main **`index.md`** file, including the summary, diagram, and an ordered, linked list of chapters.\n*   It writes the individual **chapter `.md` files** with the LLM-generated content and proper filenames.\n\nThis node transforms the intermediate data into a usable, structured set of documentation files, ready for a developer to read!\n\n**Next Up:** We've seen several \"Nodes\" mentioned (`FetchRepo`, `IdentifyAbstractions`, `CombineTutorial`, etc.). What exactly *is* a Node, and how are they defined? Let's explore the building blocks of our workflow in [Chapter 5: Node Abstraction](05_node_abstraction.md).\n```\n---\n# Chapter 5: Node Abstraction\n\n```markdown\n# Chapter 5: Node Abstraction\n\nWelcome back! In [Chapter 4: Tutorial Structure Generation](04_tutorial_structure_generation.md), we saw how the `CombineTutorial` step acts like a bookbinder, taking all the generated pieces \u2013 summaries, chapter text, diagrams \u2013 and assembling them into the final tutorial files.\n\nNow, let's zoom out a bit. We've mentioned several specific steps like `FetchRepo`, `IdentifyAbstractions`, `AnalyzeRelationships`, `OrderChapters`, `WriteChapters`, and `CombineTutorial`. These are the core workers in our documentation \"assembly line\". How are these individual workers defined? What makes them tick?\n\nThis chapter dives into the fundamental building block of our workflow: the **Node Abstraction**.\n\n**Goal:** Understand what a \"Node\" is in the PocketFlow workflow, learn about its three stages (`prep`, `exec`, `post`), and see how this structure helps keep our complex process organized, modular, and easier to manage.\n\n## The Problem: Managing a Complex Recipe \ud83c\udf5d\n\nImagine trying to cook a complex multi-course meal. If you just had one giant page of instructions mixing everything together \u2013 chopping vegetables for the salad while boiling pasta and preheating the oven for dessert \u2013 it would be incredibly confusing and prone to errors.\n\nSimilarly, generating a code tutorial involves many different tasks: getting code, analyzing it with AI, figuring out relationships, deciding the order, writing text, formatting output. Trying to write one massive script to do all of this would be a nightmare!\n\n**Use Case:** How can we break down the complex task of generating a tutorial into smaller, manageable, and reusable pieces? We need a way to define distinct steps, handle their inputs and outputs cleanly, and ensure they work together smoothly.\n\n## The Solution: Nodes - The Specialized Workers \ud83e\uddd1\u200d\ud83d\udd27\n\nIn the `LLM-CODEBASE-DOCUMENTOR`, each specific task is handled by a **Node**. Think of our PocketFlow workflow (from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)) as an assembly line, and each **Node** is a specialized station or worker on that line.\n\n*   `FetchRepo` is the station that **gets the raw materials** (source code).\n*   `IdentifyAbstractions` is the station that **inspects the materials** to find the main parts.\n*   `AnalyzeRelationships` figures out **how the parts connect**.\n*   `OrderChapters` decides the **assembly sequence**.\n*   `WriteChapters` **builds each component** (writes the chapter text).\n*   `CombineTutorial` performs the **final assembly and packaging**.\n\nEach Node has a very specific job description. It knows what inputs it needs, what task it performs, and what output it produces.\n\n## The Three Stages of a Node: Prep, Exec, Post\n\nTo make these workers efficient and organized, every Node follows a standard three-stage process:\n\n1.  **`prep` (Prepare / Get Ready):**\n    *   **What it does:** Gathers all the necessary inputs (ingredients and tools) needed for the main task. It usually reads data from the central `shared` data store (we'll cover this in [Chapter 6: Shared Data Store](06_shared_data_store.md)). It might also do some initial setup or validation.\n    *   **Analogy:** A chef reading the recipe, gathering ingredients from the pantry, and getting out the right knives and bowls.\n    *   **Output:** Passes the prepared inputs to the `exec` stage.\n\n2.  **`exec` (Execute / Do the Work):**\n    *   **What it does:** Performs the Node's main job. This is where the core logic happens \u2013 calling an external service (like the GitHub API or an LLM), running a calculation, processing data, etc.\n    *   **Analogy:** The chef chopping the vegetables, mixing the batter, or putting the dish in the oven.\n    *   **Output:** Returns the result of its work to the `post` stage.\n\n3.  **`post` (Post-process / Clean Up & Store):**\n    *   **What it does:** Takes the raw result from the `exec` stage, cleans it up or formats it if necessary, and most importantly, saves the final output back into the `shared` data store so that subsequent Nodes can use it.\n    *   **Analogy:** The chef plating the finished dish, garnishing it, and putting the plate on the serving counter (the shared store) for the waiter (the next Node). They might also quickly clean their station.\n\nThis `prep -> exec -> post` structure ensures that each Node clearly separates gathering inputs, doing the work, and storing outputs.\n\n## Example Walkthrough: The `FetchRepo` Node\n\nLet's revisit the `FetchRepo` node, which is responsible for getting the source code. Here's how its `prep`, `exec`, and `post` stages work (simplified):\n\n**1. `prep` Stage:**\n\n*   **Goal:** Get the repository URL or local directory path, filtering rules, and GitHub token (if needed) from the shared store. Prepare these for the crawler functions.\n*   **Analogy:** Get the cookbook (shared store), find the recipe source (URL or local path), check for ingredient preferences (filters), and get the special key to the pantry (token).\n\n```python\n# Simplified from nodes.py - FetchRepo.prep\n\ndef prep(self, shared):\n    # 1. Read necessary info from the shared store\n    repo_url = shared.get(\"repo_url\")\n    local_dir = shared.get(\"local_dir\")\n    github_token = shared.get(\"github_token\")\n    include_patterns = shared[\"include_patterns\"]\n    exclude_patterns = shared[\"exclude_patterns\"]\n    max_file_size = shared[\"max_file_size\"]\n\n    # (Determine project_name if not already set - omitted for simplicity)\n\n    # 2. Prepare a dictionary of inputs for the 'exec' stage\n    inputs_for_exec = {\n        \"repo_url\": repo_url,\n        \"local_dir\": local_dir,\n        \"token\": github_token,\n        \"include_patterns\": include_patterns,\n        \"exclude_patterns\": exclude_patterns,\n        \"max_file_size\": max_file_size,\n        \"use_relative_paths\": True # Example setting\n    }\n    return inputs_for_exec # Pass prepared inputs to exec\n```\n\n**2. `exec` Stage:**\n\n*   **Goal:** Based on the inputs from `prep`, call the correct utility function (`crawl_github_files` or `crawl_local_files`) to actually fetch the code.\n*   **Analogy:** If you have a URL, call the \"Online Shopper\" service. If you have a local path, call the \"Pantry Raider\" service. Get the list of ingredients (files).\n\n```python\n# Simplified from nodes.py - FetchRepo.exec\n\n# 'prep_res' contains the dictionary returned by the 'prep' stage\ndef exec(self, prep_res):\n    # 1. Check if we are fetching from GitHub or local directory\n    if prep_res[\"repo_url\"]:\n        print(f\"Crawling repository: {prep_res['repo_url']}...\")\n        # Call the GitHub utility function with prepared inputs\n        result = crawl_github_files(\n            repo_url=prep_res[\"repo_url\"],\n            token=prep_res[\"token\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    elif prep_res[\"local_dir\"]:\n        print(f\"Crawling directory: {prep_res['local_dir']}...\")\n        # Call the local directory utility function\n        result = crawl_local_files(\n            directory=prep_res[\"local_dir\"],\n            include_patterns=prep_res[\"include_patterns\"],\n            # ... other arguments ...\n        )\n    else:\n        # Handle case where neither is provided (should not happen normally)\n        result = {\"files\": {}}\n\n    # 2. Process the result (convert dict to list of (path, content))\n    files_list = list(result.get(\"files\", {}).items())\n    print(f\"Fetched {len(files_list)} files.\")\n\n    return files_list # Return the raw result (list of files) to post\n```\n\n**3. `post` Stage:**\n\n*   **Goal:** Take the list of files returned by `exec` and save it into the shared store under the key `\"files\"`.\n*   **Analogy:** Take the bag of ingredients (list of files) and place it neatly on the shared kitchen counter (shared store) labeled \"Fetched Files\".\n\n```python\n# Simplified from nodes.py - FetchRepo.post\n\n# 'shared' is the shared data store\n# 'prep_res' is the output of prep (not usually needed here)\n# 'exec_res' is the list of files returned by exec\ndef post(self, shared, prep_res, exec_res):\n    # 1. Save the result from 'exec' into the shared store\n    shared[\"files\"] = exec_res\n    print(\"Saved fetched files list to shared store.\")\n    # (Could also save project_name here if prep determined it)\n```\n\n**Internal Node Flow Diagram:**\n\nHere's how the data flows within a typical Node like `FetchRepo`:\n\n```mermaid\nsequenceDiagram\n    participant Prep as FetchRepo.prep\n    participant SharedIn as Shared Store (Input)\n    participant Exec as FetchRepo.exec\n    participant Crawler as (crawl_github_files / crawl_local_files)\n    participant Post as FetchRepo.post\n    participant SharedOut as Shared Store (Output)\n\n    Prep ->> SharedIn: Read repo_url, local_dir, patterns, etc.\n    Prep -->> Exec: Pass prepared parameters (e.g., token, patterns)\n    Exec ->> Crawler: Call appropriate crawler function\n    Crawler -->> Exec: Return raw file data (dict)\n    Exec -->> Post: Pass processed result (list of files)\n    Post ->> SharedOut: Write file list to shared[\"files\"]\n```\n\nThis clear separation of concerns (`prep`, `exec`, `post`) makes each Node easier to understand and manage.\n\n## Benefits of Node Abstraction\n\nUsing Nodes to structure our workflow provides several advantages:\n\n*   **Modularity:** Each Node is a self-contained unit with a single responsibility. This makes the overall system easier to understand, as you can focus on one task at a time. Need to change how code is fetched? Just look at the `FetchRepo` node. Need to improve chapter writing? Focus on `WriteChapters`.\n*   **Reusability:** While not heavily utilized in this specific project, Nodes designed for common tasks could potentially be reused across different workflows.\n*   **Testability:** This is a huge benefit! You can test each Node individually. You can create fake input data (`shared` state before the node runs), run the Node's `prep`, `exec`, and `post` stages, and then check if the output in the `shared` store is what you expected. This makes debugging much easier.\n*   **Clarity:** When you look at the workflow definition in `flow.py` (remember the `>>` connections from Chapter 2?), you see a high-level view of the process connecting these well-defined Nodes. It's like reading a flowchart of the main steps, rather than digging through one giant script.\n\n```python\n# From flow.py (Illustrating clarity)\n\n# Define the stations (Nodes)\nfetch_repo = FetchRepo()\nidentify_abstractions = IdentifyAbstractions(...)\n# ... other nodes\n\n# Connect them clearly\nfetch_repo >> identify_abstractions >> analyze_relationships # ... etc\n\n# Define the starting point\ntutorial_flow = Flow(start=fetch_repo)\n```\n\n## Conclusion\n\nIn this chapter, we learned about the **Node Abstraction**, the core building block of the PocketFlow workflow used in `LLM-CODEBASE-DOCUMENTOR`.\n\n*   A Node is like a **specialized worker** performing one specific task in the documentation generation process (e.g., `FetchRepo`, `WriteChapters`).\n*   Each Node follows a three-stage lifecycle:\n    *   **`prep`**: Gathers inputs from the shared store.\n    *   **`exec`**: Performs the main task.\n    *   **`post`**: Stores the results back into the shared store.\n*   This structure promotes **modularity**, **reusability**, **testability**, and **clarity** in our complex workflow.\n\nNodes handle *what* gets done and *how* it's done internally. But how do they communicate? How does the output of `FetchRepo` actually become the input for `IdentifyAbstractions`? That's where the central communication hub comes in.\n\n**Next Up:** Explore the mechanism that allows Nodes to share data with each other in [Chapter 6: Shared Data Store](06_shared_data_store.md).\n```\n---\n# Chapter 6: Shared Data Store\n\nWelcome back! In [Chapter 5: Node Abstraction](05_node_abstraction.md), we learned about the specialized workers, called \"Nodes,\" that perform specific tasks like fetching code (`FetchRepo`) or writing chapters (`WriteChapters`). We saw how each Node has a `prep -> exec -> post` lifecycle.\n\nBut how do these workers talk to each other? How does the `FetchRepo` node hand off the list of code files it found to the `IdentifyAbstractions` node that needs them? If they couldn't share information, our assembly line would grind to a halt!\n\nThis chapter introduces the central communication hub: the **Shared Data Store**.\n\n**Goal:** Understand the central Python dictionary (`shared`) that acts as a shared workspace, allowing different Nodes in the workflow to read inputs left by previous Nodes and write their results for subsequent Nodes.\n\n## The Problem: Workers Need to Communicate! \ud83d\udce2\n\nImagine our documentation assembly line again ([Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)).\n*   Station 1 (`FetchRepo`) gathers the ingredients (code files).\n*   Station 2 (`IdentifyAbstractions`) needs those ingredients to figure out the main concepts.\n*   Station 3 (`AnalyzeRelationships`) needs the concepts from Station 2 to see how they connect.\n*   ...and so on.\n\nIf each station worked in complete isolation, how would the ingredients get from Station 1 to Station 2? How would the list of concepts get from Station 2 to Station 3? The whole process relies on **passing information** between steps.\n\n**Use Case:** We need a mechanism to make the output of one Node (like the list of files from `FetchRepo`) available as input to the next Node in the sequence (like `IdentifyAbstractions`), enabling the entire workflow defined in `flow.py` to function.\n\n## The Solution: The `shared` Dictionary - Our Shared Whiteboard \u26aa\n\nThe `LLM-CODEBASE-DOCUMENTOR` solves this communication problem with a simple yet effective approach: a central **Python dictionary** called `shared`.\n\nThink of this `shared` dictionary as a **shared whiteboard** or a central **project binder** that gets passed along the assembly line.\n\nHere's how it works:\n\n1.  **Initialization:** When you start the `main.py` script, a dictionary named `shared` is created. It's initially filled with the inputs you provided (like the repo URL, model choice, include/exclude patterns) and some empty placeholders for results.\n2.  **Passing Along:** The PocketFlow engine takes this *single* `shared` dictionary and passes it to *every* Node as it runs through the workflow sequence (`FetchRepo >> IdentifyAbstractions >> ...`).\n3.  **Reading Inputs (`prep`):** When a Node starts its `prep` stage, it looks at the `shared` dictionary to find the information it needs (e.g., `IdentifyAbstractions` reads the `\"files\"` key).\n4.  **Writing Outputs (`post`):** After a Node finishes its `exec` stage, its `post` stage takes the results and writes them *back into the same* `shared` dictionary under specific keys (e.g., `FetchRepo` writes the list of files into `shared[\"files\"]`).\n5.  **Flowing Data:** Because every Node reads from and writes to the *exact same* dictionary object, the data automatically flows. When `IdentifyAbstractions` runs, the `shared` dictionary it receives already contains the `\"files\"` list written by `FetchRepo`.\n\n**Analogy:** Imagine a team building a car.\n*   The `shared` dictionary is the main blueprint and parts checklist that gets passed from team to team.\n*   Team 1 (`FetchRepo`) gets the blueprint, gathers the chassis and engine parts, and checks them off on the list (`shared[\"files\"] = ...`).\n*   They pass the updated blueprint/checklist to Team 2 (`IdentifyAbstractions`).\n*   Team 2 reads the checklist (`files = shared[\"files\"]`), identifies which major components they are (`abstractions = ...`), and adds *that* information to the blueprint (`shared[\"abstractions\"] = ...`).\n*   This continues until the final team (`CombineTutorial`) uses all the collected information on the blueprint to assemble the final car documentation.\n\n## How It Looks in the Code\n\nLet's see how the `shared` dictionary is used in practice.\n\n**1. Initialization in `main.py`:**\n\nThe `shared` dictionary is created right at the beginning, populated with command-line arguments and defaults.\n\n```python\n# Simplified from main.py\n\ndef main():\n    # ... (argparse setup to get args like args.repo, args.model, etc.) ...\n    args = parser.parse_args()\n\n    # Get GitHub token (example of combining arg and env var)\n    github_token = args.token or os.environ.get('GITHUB_TOKEN')\n\n    # Initialize the shared dictionary\n    shared = {\n        # Inputs from user or defaults\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Might be None initially\n        \"github_token\": github_token,\n        \"output_dir\": args.output,\n        \"model_used\": args.model,\n        \"llm_provider\": LLMProvider_enum(args.model),\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Placeholders for outputs from Nodes\n        \"files\": [],             # To be filled by FetchRepo\n        \"abstractions\": [],      # To be filled by IdentifyAbstractions\n        \"relationships\": {},     # To be filled by AnalyzeRelationships\n        \"chapter_order\": [],     # To be filled by OrderChapters\n        \"chapters\": [],          # To be filled by WriteChapters\n        \"final_output_dir\": None # To be filled by CombineTutorial\n    }\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow, passing the shared dictionary\n    # PocketFlow manages passing 'shared' to each Node internally\n    tutorial_flow.run(shared)\n\n# ...\n```\n*Explanation:* We create a standard Python dictionary `shared`. We fill it with initial values derived from user input (`args`) or defaults. Crucially, we also include keys like `\"files\"`, `\"abstractions\"`, etc., initially empty, ready to be populated by the Nodes later in the workflow. This entire dictionary is then passed to `tutorial_flow.run()`.\n\n**2. Reading from `shared` in a Node's `prep` stage:**\n\nNodes access the `shared` dictionary passed to their `prep` method to get the inputs they need.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.prep\n\nclass IdentifyAbstractions(Node):\n    def prep(self, shared): # 'shared' is the dictionary passed by PocketFlow\n        # Read the list of files generated by FetchRepo\n        files_data = shared[\"files\"]\n        # Read the project name (might have been set by FetchRepo)\n        project_name = shared[\"project_name\"]\n        # Read the chosen LLM provider\n        llm_provider = shared[\"llm_provider\"]\n\n        # ... (prepare context using files_data) ...\n\n        # Return necessary items for the 'exec' stage\n        return context, file_listing_for_prompt, len(files_data), project_name, llm_provider\n```\n*Explanation:* The `prep` method receives the `shared` dictionary as its argument. It simply accesses the required keys (like `shared[\"files\"]`, `shared[\"project_name\"]`) to retrieve the data needed for its task.\n\n**3. Writing to `shared` in a Node's `post` stage:**\n\nNodes write their results back into the `shared` dictionary in their `post` method.\n\n```python\n# Simplified from nodes.py - IdentifyAbstractions.post\n\nclass IdentifyAbstractions(Node):\n    # ... prep and exec methods ...\n\n    # 'shared' is the dictionary, 'exec_res' is the result from exec()\n    def post(self, shared, prep_res, exec_res):\n        # 'exec_res' contains the list of identified abstractions\n        # Write this list back into the shared dictionary\n        shared[\"abstractions\"] = exec_res\n        print(f\"Saved {len(exec_res)} abstractions to shared store.\")\n```\n*Explanation:* The `post` method also receives the `shared` dictionary. It takes the result produced by the `exec` stage (`exec_res`) and assigns it to the appropriate key (e.g., `shared[\"abstractions\"] = ...`). This modification happens directly on the `shared` dictionary object that PocketFlow is managing.\n\n## How PocketFlow Orchestrates the Sharing\n\nYou might wonder how the *same* dictionary gets passed around. It's handled automatically by the PocketFlow engine when you call `flow.run(shared)`:\n\n1.  **Start:** PocketFlow receives the initial `shared` dictionary.\n2.  **Node 1 (`FetchRepo`):**\n    *   It calls `FetchRepo.prep(shared)`.\n    *   It calls `FetchRepo.exec(...)` which returns `files_list`.\n    *   It calls `FetchRepo.post(shared, ..., files_list)`. The `post` method modifies the `shared` dictionary by adding the `\"files\"` key.\n3.  **Node 2 (`IdentifyAbstractions`):**\n    *   PocketFlow now takes the *modified* `shared` dictionary (which now contains `\"files\"`) and passes it to the next node in the sequence.\n    *   It calls `IdentifyAbstractions.prep(shared)`. This `prep` method can now successfully read `shared[\"files\"]`.\n    *   It calls `IdentifyAbstractions.exec(...)` which returns `abstractions_list`.\n    *   It calls `IdentifyAbstractions.post(shared, ..., abstractions_list)`. This modifies `shared` again by adding the `\"abstractions\"` key.\n4.  **Continue:** This process repeats for every node in the workflow (`AnalyzeRelationships`, `OrderChapters`, etc.), with each node potentially reading data written by previous nodes and writing its own results back into the *same* `shared` dictionary.\n\n**Sequence Diagram: Data Flow via `shared`**\n\n```mermaid\nsequenceDiagram\n    participant Main as main.py\n    participant PF as PocketFlow Engine\n    participant Shared as Shared Dictionary\n    participant Node1Prep as FetchRepo.prep\n    participant Node1Post as FetchRepo.post\n    participant Node2Prep as IdentifyAbstractions.prep\n    participant Node2Post as IdentifyAbstractions.post\n\n    Main->>Shared: Create shared dict (with inputs)\n    Main->>PF: flow.run(shared)\n    PF->>Node1Prep: prep(shared)\n    Node1Prep->>Shared: Read repo_url, etc.\n    PF->>Node1Post: post(shared, ..., files_list)\n    Node1Post->>Shared: Write shared[\"files\"] = files_list\n    Note over Shared: shared now contains \"files\"\n    PF->>Node2Prep: prep(shared)\n    Node2Prep->>Shared: Read shared[\"files\"]\n    PF->>Node2Post: post(shared, ..., abstractions_list)\n    Node2Post->>Shared: Write shared[\"abstractions\"] = abstractions_list\n    Note over Shared: shared now contains \"files\", \"abstractions\"\n    PF-->>Main: Flow finished (shared contains all results)\n```\n\nThis diagram shows how the single `shared` dictionary object is passed along and modified by each Node's `post` method, making the results available to subsequent Nodes.\n\n## Conclusion\n\nIn this chapter, we explored the **Shared Data Store**, the central communication hub of the `LLM-CODEBASE-DOCUMENTOR` workflow.\n\n*   It's implemented as a simple **Python dictionary** named `shared`.\n*   It's initialized in `main.py` with user inputs and placeholders.\n*   The **PocketFlow engine** passes this *same* dictionary object sequentially to each Node in the workflow.\n*   Nodes **read** their required inputs from `shared` in their `prep` stage.\n*   Nodes **write** their results back into `shared` in their `post` stage.\n*   This mechanism allows data (like fetched files, identified abstractions, chapter content) to **flow smoothly** between the different processing steps.\n\nThe `shared` dictionary acts as the vital link connecting our specialized workers (Nodes), turning a series of isolated tasks into a cohesive, functional documentation generation pipeline.\n\nNow that we understand the core workflow, the node structure, and how data flows between them, let's look at how a user might interact with this system through a graphical interface.\n\n**Next Up:** Explore the user interface and a different workflow focused on searching for code in [Chapter 7: Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md).\n---\n# Chapter 7: Streamlit UI & Search Flow\n\nWelcome back! In [Chapter 6: Shared Data Store](06_shared_data_store.md), we learned how different parts of our documentation pipeline (the \"Nodes\") share information using the central `shared` dictionary, like passing a project binder along an assembly line. We've seen how the tool works from the command line, taking inputs like a GitHub URL and generating files.\n\nBut what if you prefer a more visual way to interact with the tool? What if you want to *search* for interesting repositories directly, filter them, and see the results right in your web browser? That's where this chapter comes in!\n\n**Goal:** Understand how the `LLM-CODEBASE-DOCUMENTOR` provides a user-friendly web interface using Streamlit, allowing users to search for code, select a repository, generate the tutorial, and view/download it, all through their browser.\n\n## The Problem: Command Lines Aren't for Everyone \ud83e\udd14\n\nRunning programs from a terminal (the command line) is powerful, but it can be intimidating for beginners or inconvenient for quick tasks. You need to remember commands, type paths correctly, and look at files generated on your computer.\n\nWouldn't it be nicer to have a web page where you can:\n\n*   Type a search term like \"python data analysis library\".\n*   See a list of matching GitHub projects with summaries.\n*   Click a button to generate the tutorial for one of them.\n*   Read the generated tutorial directly on the page?\n*   Download the whole tutorial as a ZIP file?\n\nThis makes the tool much more accessible and easier to use.\n\n**Use Case:** A student wants to explore popular Python web frameworks. They use the web UI, type \"python web framework\" into a search box, see a list of projects like Flask and Django with summaries, click \"Select\" on Flask, and then read the automatically generated tutorial for Flask directly in their browser before downloading it.\n\n## The Solution: Streamlit - Building Web Apps with Python \u2728\n\nTo create this user-friendly web interface, our project uses **Streamlit**.\n\n*   **What it is:** Streamlit is a fantastic Python library that lets developers build interactive web applications really quickly, often with just Python code. You don't need to be a web development expert!\n*   **How we use it:** The `streamlit_app.py` file contains Python code that uses Streamlit functions (like `st.text_input`, `st.button`, `st.markdown`) to create the entire web page you interact with \u2013 the input boxes, buttons, search results, and tutorial display.\n\n**Analogy:** Think of Streamlit as a set of super easy-to-use building blocks (like Lego) specifically designed for creating data-focused web pages using only Python.\n\nWhen you run `streamlit run streamlit_app.py`, Streamlit starts a local web server and opens the application in your browser.\n\n## A New Workflow for the UI: `create_streamlit_flow`\n\nRemember our PocketFlow assembly line from [Chapter 2: PocketFlow Workflow](02_pocketflow_workflow.md)? That was designed for the command-line use case (`FetchRepo` -> `IdentifyAbstractions` -> ...).\n\nFor the Streamlit UI, especially the search functionality, we need a slightly different workflow that includes steps for searching and selecting *before* we get to the main tutorial generation.\n\nThis new workflow is defined in `flow.py` inside the `create_streamlit_flow()` function. It introduces some new Nodes specifically for the UI:\n\n1.  **`SmartSearchRepo`:** The detective that figures out what you're searching for (URL, local path, or plain text) and extracts search keywords if needed.\n2.  **`FilterRepos`:** The librarian that searches GitHub using your keywords and filters, and even reads the README summaries.\n3.  **`SelectRepository`:** The receptionist that notes down which repository you finally choose.\n4.  **`RenderAndDownload`:** The publisher that displays the finished tutorial chapters in the browser and prepares a downloadable ZIP file.\n\nHere\u2019s how these new nodes integrate with the existing ones:\n\n```mermaid\nflowchart TD\n    subgraph Streamlit UI Interaction\n        StartUI[User Input in UI] --> SmartSearch[SmartSearchRepo]\n        SmartSearch --> Filter[FilterRepos]\n        Filter --> DisplayResults[Show Results in UI]\n        DisplayResults --> UserSelect[User Selects Repo]\n        UserSelect --> SelectRepo[SelectRepository]\n    end\n\n    subgraph Core Tutorial Generation (Reused Nodes)\n        Fetch[FetchRepo]\n        Identify[IdentifyAbstractions]\n        Analyze[AnalyzeRelationships]\n        Order[OrderChapters]\n        Write[WriteChapters]\n        Combine[CombineTutorial]\n    end\n\n    subgraph Display Output\n       Render[RenderAndDownload] --> DisplayTutorial[Show Tutorial & Download Button]\n    end\n\n    SelectRepo --> Fetch\n    Fetch --> Identify --> Analyze --> Order --> Write --> Combine\n    Combine --> Render\n```\n\n*Explanation:* The UI flow starts with `SmartSearchRepo`, `FilterRepos`, and `SelectRepository`. Once a repository is selected (via URL, local path, or search results), the `SelectRepository` node sets up the necessary information (`repo_url` or `local_dir`) in the `shared` store. Then, the flow plugs *directly* into the *same* `FetchRepo` node that the command-line version uses! The rest of the tutorial generation pipeline (`IdentifyAbstractions` -> `CombineTutorial`) runs exactly as before. Finally, the `RenderAndDownload` node takes the output and prepares it for display in Streamlit.\n\n**Code Snippet (`flow.py` - `create_streamlit_flow`):**\n\n```python\n# In flow.py\n\ndef create_streamlit_flow():\n    \"\"\"Creates and returns the Streamlit UI flow with search and filtering.\"\"\"\n\n    # Reuse the original tutorial flow's nodes\n    tutorial_flow, fetch_repo, combine_tutorial = create_tutorial_flow()\n\n    # Instantiate new UI nodes\n    smart_search = SmartSearchRepo()\n    filter_repos = FilterRepos()\n    select_repo = SelectRepository()\n    render_download = RenderAndDownload()\n\n    # Connect the UI nodes: Search -> Filter -> Select\n    smart_search >> filter_repos\n    filter_repos >> select_repo\n\n    # Connect the selection node TO the start of the original tutorial flow\n    select_repo >> fetch_repo\n\n    # Connect the end of the tutorial flow TO the final rendering node\n    combine_tutorial >> render_download\n\n    # Create the full UI flow, starting with the search\n    ui_flow = Flow(start=smart_search)\n\n    return ui_flow\n```\n\n*Explanation:* This code defines the new workflow. It creates instances of the new UI nodes (`SmartSearchRepo`, `FilterRepos`, etc.) and connects them. Crucially, it links `select_repo` *to* `fetch_repo` (start of the old flow) and `combine_tutorial` (end of the old flow) *to* `render_download`. This cleverly reuses the core logic while adding the UI interaction steps at the beginning and end.\n\n## The Search Process: Finding Your Repository\n\nLet's walk through what happens when you type a natural language query (like \"python web framework\") into the Streamlit UI and click \"Search\".\n\n1.  **User Action:** You type your query in the text box in the sidebar (`st.text_input`) and click the \"Search\" button (`st.button`).\n2.  **Streamlit App (`streamlit_app.py`):**\n    *   The app code takes your query, selected filters (like min stars), and API keys from the UI elements.\n    *   It creates the initial `shared` dictionary, populating it with `shared[\"query_input\"]`, `shared[\"filter_params\"]`, `shared[\"github_token\"]`, `shared[\"llm_provider\"]`, etc.\n    *   It gets the `ui_flow` using `create_streamlit_flow()`.\n    *   It starts the flow by running the first few nodes (`SmartSearchRepo` and `FilterRepos`) maybe inside a `st.spinner(\"Searching...\")`.\n3.  **`SmartSearchRepo` Node:**\n    *   **Prep:** Reads `shared[\"query_input\"]`.\n    *   **Exec:** Detects it's not a URL or local path, so `search_mode = \"nl\"`. It calls the `extract_keywords` utility function, potentially using an LLM ([Chapter 3: LLM Analysis & Generation](03_llm_analysis___generation.md)), to get keywords like `[\"python\", \"web\", \"framework\"]`.\n    *   **Post:** Writes `shared[\"search_mode\"] = \"nl\"` and `shared[\"keywords\"] = [\"python\", \"web\", \"framework\"]`.\n4.  **`FilterRepos` Node:**\n    *   **Prep:** Reads `shared[\"search_mode\"]`, `shared[\"keywords\"]`, `shared[\"filter_params\"]`, and `shared[\"github_token\"]`.\n    *   **Exec:** Since `search_mode` is \"nl\", it calls the `github_search_repos` utility. This function uses the GitHub API to find repositories matching the keywords and filters. For each result, it might fetch the README content using `get_readme_content` and then call the `summarize_readme` utility (which uses an LLM) to create a short summary. It returns a list of `RepoMetadata` objects (containing URL, name, description, stars, summary, etc.).\n    *   **Post:** Writes the list of found repositories into `shared[\"search_results\"]`.\n5.  **Streamlit App (`streamlit_app.py`):**\n    *   After the initial nodes run, the app checks the `shared` store for `shared[\"search_results\"]`.\n    *   It loops through the `RepoMetadata` objects in the list.\n    *   For each repository, it displays a \"card\" using `st.markdown` and other Streamlit elements, showing the name, description, summary, stars, forks, and crucially, a \"Select\" button (`st.button(..., on_click=select_repository, args=(repo.url,))`).\n\n**Search Flow Sequence Diagram:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant StreamlitApp as streamlit_app.py\n    participant Shared as Shared Data Store\n    participant SmartSearch as SmartSearchRepo Node\n    participant LLM_Keywords as extract_keywords (LLM)\n    participant FilterRepos as FilterRepos Node\n    participant GitHubAPI as GitHub Search API\n    participant LLM_Summary as summarize_readme (LLM)\n\n    User->>StreamlitApp: Enters NL query, clicks Search\n    StreamlitApp->>Shared: Initialize shared store (query, filters, keys)\n    StreamlitApp->>SmartSearch: Run prep(shared)\n    SmartSearch->>Shared: Read query_input\n    StreamlitApp->>SmartSearch: Run exec(...)\n    SmartSearch->>LLM_Keywords: Extract keywords from query\n    LLM_Keywords-->>SmartSearch: Return keywords\n    StreamlitApp->>SmartSearch: Run post(shared, ..., result)\n    SmartSearch->>Shared: Write search_mode=\"nl\", keywords=[...]\n    StreamlitApp->>FilterRepos: Run prep(shared)\n    FilterRepos->>Shared: Read keywords, filters, token\n    StreamlitApp->>FilterRepos: Run exec(...)\n    FilterRepos->>GitHubAPI: Search repos(keywords, filters)\n    GitHubAPI-->>FilterRepos: Return repo list\n    loop For each Repo\n        FilterRepos->>GitHubAPI: Get README content\n        GitHubAPI-->>FilterRepos: Return README text\n        FilterRepos->>LLM_Summary: Summarize README text\n        LLM_Summary-->>FilterRepos: Return summary\n    end\n    FilterRepos-->>StreamlitApp: Return list of RepoMetadata (with summaries)\n    StreamlitApp->>FilterRepos: Run post(shared, ..., repo_list)\n    FilterRepos->>Shared: Write search_results=[...]\n    StreamlitApp->>Shared: Read search_results\n    StreamlitApp->>User: Display repository cards with \"Select\" buttons\n```\n\n**Simplified Code Snippets:**\n\n```python\n# In nodes.py - SmartSearchRepo.exec (Simplified NL path)\ndef exec(self, prep_res):\n    query_input = prep_res[\"query_input\"]\n    llm_provider = prep_res[\"llm_provider\"]\n    # ... logic to detect it's natural language ...\n    search_mode = \"nl\"\n    selected_repo = None\n    # Call utility to get keywords using LLM\n    keywords = extract_keywords(query_input, llm_provider=llm_provider)\n    return {\"search_mode\": search_mode, \"keywords\": keywords, \"selected_repo\": selected_repo}\n\n# In nodes.py - FilterRepos.exec (Simplified NL path)\ndef exec(self, prep_res):\n    # ... get keywords, filters, token ...\n    search_mode = prep_res[\"search_mode\"]\n    llm_provider = prep_res[\"llm_provider\"]\n    if search_mode != \"nl\": return [] # Skip if not NL search\n\n    # Call utility to search GitHub\n    repos = github_search_repos(keywords, filter_params, github_token)\n    # Loop and get summaries using LLM utility\n    for repo in repos:\n        readme_content = get_readme_content(repo.full_name, github_token)\n        repo.readme_summary = summarize_readme(readme_content, llm_provider=llm_provider)\n    return repos\n\n# In streamlit_app.py - Displaying results (Simplified)\nif st.session_state.ui_view == \"results\":\n    st.title(\"Search Results\")\n    search_results = st.session_state.search_results # Get from state (updated from shared store)\n    if search_results:\n        for i, repo in enumerate(search_results):\n            # Use Streamlit components to display repo info\n            st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n            st.markdown(f\"**Summary:** {repo.readme_summary}\")\n            # Add a button that calls select_repository when clicked\n            st.button(f\"Select\", key=f\"select_{i}\", on_click=select_repository, args=(repo.url,))\n```\n\n## Selecting and Generating: The Main Event\n\nOkay, you see the list of repositories. Now, you click \"Select\" on the one you want (e.g., Flask).\n\n1.  **User Action:** You click the \"Select\" button next to the Flask repository card.\n2.  **Streamlit App (`streamlit_app.py`):**\n    *   The `on_click` function (`select_repository`) associated with that button is called.\n    *   This function updates the application's state (`st.session_state.selected_repo = repo_url`) and ensures the `shared` dictionary also gets this selection: `shared[\"selected_repo\"] = repo_url`.\n    *   Because the state changed, Streamlit re-runs the app script. Crucially, it now knows a repository is selected and will try to run the *full* `ui_flow`.\n    *   It shows a spinner: `st.spinner(\"Generating tutorial...\")`.\n3.  **`SmartSearchRepo` & `FilterRepos` Nodes:** These might run again, but they likely see that `selected_repo` is already set or don't perform heavy work.\n4.  **`SelectRepository` Node:**\n    *   **Prep:** Reads `shared[\"selected_repo\"]`.\n    *   **Exec:** Sees that `selected_repo` (the Flask URL) is present.\n    *   **Post:** Writes the URL into the key needed by the original pipeline: `shared[\"repo_url\"] = \"https://github.com/pallets/flask\"` and `shared[\"local_dir\"] = None`.\n5.  **Core Tutorial Nodes (`FetchRepo` -> `CombineTutorial`):**\n    *   The PocketFlow engine now moves to the next node in the `ui_flow` sequence, which is `FetchRepo`.\n    *   `FetchRepo` reads `shared[\"repo_url\"]` (which was just set by `SelectRepository`).\n    *   The entire tutorial generation pipeline executes exactly as described in previous chapters ([Chapter 1](01_code_fetching___parsing.md), [Chapter 3](03_llm_analysis___generation.md), [Chapter 4](04_tutorial_structure_generation.md), etc.), using the Flask repository URL as input and populating the `shared` store with files, abstractions, relationships, chapters, and finally the `final_output_dir`.\n6.  **`RenderAndDownload` Node:**\n    *   **Prep:** Reads `shared[\"final_output_dir\"]`.\n    *   **Exec:** Reads all the generated `.md` files (index and chapters) from the `final_output_dir`. It also calls a utility (`zip_output_folder`) to create a ZIP archive of that directory.\n    *   **Post:** Writes the content of the Markdown files into `shared[\"markdown_files\"]` (a dictionary like `{\"index.md\": \"...\", \"01_chapter.md\": \"...\"}`) and the path to the ZIP file into `shared[\"zip_path\"]`.\n7.  **Streamlit App (`streamlit_app.py`):**\n    *   After the full flow completes, the app checks the `shared` store for `shared[\"markdown_files\"]` and `shared[\"zip_path\"]`.\n    *   It displays the project title.\n    *   It shows a download button (`st.download_button`) linked to the `zip_path`.\n    *   It loops through the `markdown_files` dictionary, displaying the content of `index.md` first, and then the content of each chapter file using `st.markdown()`.\n\n**Selection & Generation Flow Sequence Diagram:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant StreamlitApp as streamlit_app.py\n    participant Shared as Shared Data Store\n    participant SelectRepo as SelectRepository Node\n    participant CorePipeline as FetchRepo...CombineTutorial Nodes\n    participant RenderDownload as RenderAndDownload Node\n    participant FileSystem as Output Folder & ZIP\n\n    User->>StreamlitApp: Clicks \"Select\" button for a repo URL\n    StreamlitApp->>Shared: Set shared[\"selected_repo\"] = URL\n    Note over StreamlitApp: Trigger full flow run\n    StreamlitApp->>SelectRepo: Run prep/exec/post(shared)\n    SelectRepo->>Shared: Read shared[\"selected_repo\"]\n    SelectRepo->>Shared: Write shared[\"repo_url\"] = URL, shared[\"local_dir\"]=None\n    StreamlitApp->>CorePipeline: Run FetchRepo...CombineTutorial(shared)\n    Note over CorePipeline: Uses shared[\"repo_url\"], generates tutorial files\n    CorePipeline->>FileSystem: Write index.md, chapter files to output dir\n    CorePipeline->>Shared: Write shared[\"final_output_dir\"]\n    StreamlitApp->>RenderDownload: Run prep/exec/post(shared)\n    RenderDownload->>Shared: Read shared[\"final_output_dir\"]\n    RenderDownload->>FileSystem: Read all .md files from output dir\n    RenderDownload->>FileSystem: Create ZIP file from output dir\n    RenderDownload->>Shared: Write shared[\"markdown_files\"] = {...}, shared[\"zip_path\"] = \"...\"\n    StreamlitApp->>Shared: Read shared[\"markdown_files\"], shared[\"zip_path\"]\n    StreamlitApp->>User: Display tutorial content (st.markdown)\n    StreamlitApp->>User: Display download button (st.download_button)\n```\n\n**Simplified Code Snippets:**\n\n```python\n# In nodes.py - SelectRepository.post (Simplified)\ndef post(self, shared, prep_res, exec_res):\n    # exec_res might contain {\"repo_url\": url, \"local_dir\": None}\n    if exec_res.get(\"repo_url\"):\n        shared[\"repo_url\"] = exec_res[\"repo_url\"]\n        shared[\"local_dir\"] = None\n    elif exec_res.get(\"local_dir\"):\n        # Handle local directory selection similarly\n        shared[\"repo_url\"] = None\n        shared[\"local_dir\"] = exec_res[\"local_dir\"]\n\n# In nodes.py - RenderAndDownload.exec (Simplified)\ndef exec(self, prep_res):\n    final_output_dir = prep_res[\"final_output_dir\"]\n    if not final_output_dir: return {\"success\": False, ...}\n\n    try:\n        # Utility to read all .md files into a dict\n        markdown_files = read_all_markdown_files(final_output_dir)\n        # Utility to create a zip file\n        zip_path = zip_output_folder(final_output_dir)\n        return {\"success\": True, \"markdown_files\": markdown_files, \"zip_path\": zip_path}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e), ...}\n\n# In streamlit_app.py - Displaying tutorial (Simplified)\nelif st.session_state.ui_view == \"tutorial\":\n    st.title(f\"Tutorial for {project_name}\")\n    zip_path = st.session_state.zip_path # Get from state\n\n    if zip_path: # If ZIP is ready\n        with open(zip_path, \"rb\") as f:\n            st.download_button(\"Download Tutorial ZIP\", data=f, ...)\n\n    markdown_files = st.session_state.markdown_files # Get from state\n    if markdown_files:\n        if \"index.md\" in markdown_files:\n            st.markdown(markdown_files[\"index.md\"]) # Show index first\n            st.markdown(\"---\")\n        for filename in sorted([f for f in markdown_files if f != \"index.md\"]):\n            st.markdown(markdown_files[filename]) # Show chapters\n```\n\n## Handling URLs and Local Paths Directly\n\nWhat if the user initially enters a direct GitHub URL or a local folder path instead of a natural language query?\n\n*   **`SmartSearchRepo`** detects this!\n    *   If it sees a URL, it sets `search_mode = \"url\"` and directly puts the URL into `selected_repo`.\n    *   If it sees a valid local path, it sets `search_mode = \"local\"` and puts the path into `selected_repo`.\n*   **`FilterRepos`** sees the `search_mode` is \"url\" or \"local\" and *skips* the GitHub API search and README summarization entirely.\n*   **`SelectRepository`** receives the pre-filled `selected_repo` and sets `repo_url` or `local_dir` accordingly.\n*   The flow then proceeds directly to `FetchRepo` and the tutorial generation pipeline, just like after selecting from search results.\n\nThis makes the UI versatile, handling direct inputs just as easily as natural language searches.\n\n## Conclusion\n\nIn this chapter, we explored the user-friendly **Streamlit UI** and the **Search Flow** of the `LLM-CODEBASE-DOCUMENTOR`.\n\n*   We use **Streamlit** (`streamlit_app.py`) to build an interactive web interface.\n*   A new **PocketFlow workflow** (`create_streamlit_flow`) integrates UI-specific nodes with the core tutorial generation pipeline.\n*   New nodes like **`SmartSearchRepo`**, **`FilterRepos`**, **`SelectRepository`**, and **`RenderAndDownload`** handle user interactions:\n    *   Processing search queries (URL, local path, or natural language using LLM for keywords).\n    *   Searching GitHub via API and summarizing READMEs (using LLM).\n    *   Allowing users to select a repository.\n    *   Rendering the final tutorial Markdown in the browser.\n    *   Providing a ZIP download option.\n*   The UI flow cleverly **reuses** the existing tutorial generation nodes (`FetchRepo` to `CombineTutorial`) after a repository source is determined.\n\nThis Streamlit interface makes the power of the codebase documentor accessible to a wider audience through a simple, visual, and interactive web application.\n\n**Next Up:** We've seen how API keys (GitHub, LLMs) and other settings (model names, filters) are crucial. Let's look at how the project manages these configurations in [Chapter 8: Configuration & Environment Management](08_configuration___environment_management.md).\n\n        Relevant Code Snippets:\n        --- File: LLM-CODEBASE-DOCUMENTOR-main\\main.py ---\nimport dotenv\nimport os\nimport argparse\n# Import the function that creates the flow\nfrom flow import create_tutorial_flow\nfrom utils.call_llm import LLMProvider_enum\n\n# Load environment variables from .env file\ndotenv.load_dotenv()\n\n# Default file patterns\nDEFAULT_INCLUDE_PATTERNS = {\n    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n}\n\nDEFAULT_EXCLUDE_PATTERNS = {\n    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n}\n\n# --- Main Fn ---\ndef main():\n    parser = argparse.ArgumentParser(description=\"Use this script to analyze a GitHub repository or local codebase and generate a structured, \" \\\n                                                \"AI-friendly tutorial. Ideal for experimenting with LLMs and building a SaaS code-search or \" \\\n                                                \"doc generation tool.\")\n    \n    # Create mutually exclusive group for source\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"If GITHUB dir is online: Specify the URL of a public GitHub repository \"\n                                            \"(i.e. https://github.com/[user]/[project]). This will fetch code using the GitHub REST API.\")\n    \n    source_group.add_argument(\"--dir\", help=\"If GITHUB dir is pulled locally to local folder dir: Provide a local directory path to analyze \" \\\n                                            \"code on your machine instead of pulling from GitHub.\")\n\n    # Added argument for LLM provider\n    # Note: LLMProvider_enum is an Enum class that contains the available LLM providers\n    # The choices are the values of the enum members: \n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    parser.add_argument(\"--model\", type=str, choices=[e.value for e in LLMProvider_enum], \n                        default=LLMProvider_enum.GOOGLE.value, help=\"LLM provider to use: \" \\\n                        \"'anthropic-claude', 'google-gemini', or 'openai-gpt'. Choose based \" \\\n                        \"on your access/API tokens, model preference and task requirements. Default is 'google-gemini'.\")\n    \n    # Optional arguments    \n    parser.add_argument(\"-n\", \"--name\", help=\"Optional: Define a custom project name for output purposes. \" \\\n                                            \"If omitted, the name is inferred from the repo URL or folder name.\")\n    \n    parser.add_argument(\"-t\", \"--token\", help=\"Optional: Your GitHub Personal Access Token for authenticated API access. \" \\\n                                            \"Required if accessing private repositories or hitting rate limits. \" \\\n                                            \"If not provided, it checks the GITHUB_TOKEN environment variable.\")\n    \n    parser.add_argument(\"-o\", \"--output\", default=\"output\", help=\"Base output directory for all generated documentation \"\n                                                                \"and results (default is './output').\")\n    \n    parser.add_argument(\"-i\", \"--include\", nargs=\"+\", help=\"List of file patterns to include during analysis (e.g., '*.py', '*.js'). \" \\\n                                                            \"Useful if you're building a WebGPT-based documentation generator or \" \\\n                                                            \"doing language-specific LLM tasks on specific file tupes. Defaults to common code files if not specified.\")\n    \n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", help=\"List of file or directory patterns to ignore (e.g., 'tests/*', 'docs/*'). \" \\\n                                                            \"Helps exclude irrelevant or noisy files from analysis. Exclude file patterns \"\n                                                            \"(e.g. 'tests/*' 'docs/*'). Defaults to test/build directories if not specified.\")\n    \n    parser.add_argument(\"-s\", \"--max-size\", type=int, default=300000, help=\"Ignore files larger than this size (in bytes). \" \\\n                                                            \"Default is 300,000 (~300KB). This prevents feeding overly large files into the LLM, \" \\\n                                                            \"which may affect generation quality or exceed context limits. Maximum file size \"\n                                                            \"in bytes (default: 300,000 (300000), about 300KB).\")\n\n    args = parser.parse_args()\n\n    # Get GitHub token from argument or environment variable if using repo\n    github_token = None\n    if args.repo:\n        github_token = args.token or os.environ.get('GITHUB_TOKEN')\n        if not github_token:\n            print(\"Warning: No GitHub token provided. You might hit rate limits for public repositories.\")\n\n    # Initialize the shared dictionary with inputs\n    shared = {\n        \"repo_url\": args.repo,\n        \"local_dir\": args.dir,\n        \"project_name\": args.name, # Can be None, FetchRepo will derive it\n        \"github_token\": github_token,\n        \"output_dir\": args.output, # Base directory for output tut\n        \"model_used\": args.model, # LLM provider to use as string\n        \"llm_provider\": LLMProvider_enum(args.model), # LLM provider enum version, usable in call_llm()\n\n\n        # Add include/exclude patterns and max file size\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size,\n\n        # Outputs will be populated by the nodes\n        \"files\": [],\n        \"abstractions\": [],\n        \"relationships\": {},\n        \"chapter_order\": [],\n        \"chapters\": [],\n        \"final_output_dir\": None\n    }\n\n    print(f\"Starting tutorial generation for: {args.repo or args.dir}\")\n\n    # Create the flow instance\n    tutorial_flow = create_tutorial_flow()\n\n    # Run the flow\n    tutorial_flow.run(shared)\n    \nif __name__ == \"__main__\":\n    main()\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\streamlit_app.py ---\nimport streamlit as st\nimport os\nimport time\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime\n\n# Import utilities\nfrom utils.env_loader import load_env_vars, get_api_keys, get_model_names, create_env_example\nfrom utils.call_llm import LLMProvider_enum\nfrom utils.github_api import RepoMetadata\n\n# Import the flow\nfrom flow import create_streamlit_flow\nfrom nodes import SmartSearchRepo, FilterRepos, SelectRepository, RenderAndDownload\n\n# Load environment variables\nload_env_vars()\n\n# 1. Page configuration (with a custom About menu item)\nst.set_page_config(\n    page_title=\"\ud83d\udcd6\ud83e\udd13 LLM Codebase Finder & Documentor\",\n    page_icon=\"\ud83e\uddd0\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\",\n    menu_items={\n        'About': (\n            \"### LLM Codebase Documentor\\n\"\n            \"A&nbsp;Streamlit app to auto-generate beginner-friendly tutorials from any GitHub repo.\\n\\n\"\n            \"Built by **TEG SINGH TIWANA** for Cloud Assignment 2:\\n\"\n            \"[GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n        )\n    }\n)\n# Initialize session state if not already done\nif \"initialized\" not in st.session_state:\n    st.session_state.initialized = True\n    st.session_state.search_results = []\n    st.session_state.selected_repo = None\n    st.session_state.filter_params = {\n        \"min_stars\": 0,\n        \"min_forks\": 0,\n        \"language\": \"\",\n        \"sort_by\": \"stars\",\n        \"updated_since\": \"\"\n    }\n    st.session_state.shared_store = {}\n    st.session_state.markdown_files = {}\n    st.session_state.zip_path = None\n    st.session_state.task_completed = False\n    st.session_state.ui_view = \"search\"  # Possible values: search, results, tutorial\n    \n    # Get API keys from environment\n    api_keys = get_api_keys()\n    for key, value in api_keys.items():\n        st.session_state[key] = value or \"\"\n    \n    # Get model names from environment\n    model_names = get_model_names()\n    for key, value in model_names.items():\n        st.session_state[key] = value\n\n# --- Sidebar ---\nwith st.sidebar:\n    st.title(\"LLM Codebase Documentor\")\n    \n    # Create tabs for different sidebar sections\n    tab1, tab2, tab3 = st.tabs([\"Search\", \"API Keys\", \"Model Settings\"])\n    \n    with tab1:\n        # Search input and type\n        st.subheader(\"Repository Search\")\n        query_input = st.text_input(\n            \"Enter GitHub URL, local path, or natural language query:\",\n            placeholder=\"e.g., https://github.com/user/repo OR frameworks for UI prototyping\"\n        )\n        \n        # Filters (only shown for natural language search)\n        st.subheader(\"Filters\")\n        min_stars = st.number_input(\"Minimum Stars\", min_value=0, value=st.session_state.filter_params[\"min_stars\"])\n        min_forks = st.number_input(\"Minimum Forks\", min_value=0, value=st.session_state.filter_params[\"min_forks\"])\n        language = st.selectbox(\n            \"Language\", \n            [\"\", \"Python\", \"JavaScript\", \"TypeScript\", \"Java\", \"Go\", \"Rust\", \"C++\", \"Ruby\", \"PHP\"],\n            index=0\n        )\n        sort_by = st.selectbox(\n            \"Sort By\", \n            [\"stars\", \"forks\", \"updated\", \"help-wanted-issues\"],\n            index=0\n        )\n        updated_since = st.date_input(\n            \"Updated Since\", \n            value=None\n        )\n        \n        # Update filter params in session state\n        if st.button(\"Search\", key=\"search_button\"):\n            # Update filter params\n            st.session_state.filter_params = {\n                \"min_stars\": min_stars,\n                \"min_forks\": min_forks,\n                \"language\": language,\n                \"sort_by\": sort_by,\n                \"updated_since\": updated_since.isoformat() if updated_since else \"\"\n            }\n            \n            # Initialize shared store for the flow\n            st.session_state.shared_store = {\n                \"query_input\": query_input,\n                \"filter_params\": st.session_state.filter_params,\n                \"github_token\": st.session_state.github_token,\n                \"llm_provider\": LLMProvider_enum(st.session_state.provider_selection),\n                \n                # Default parameters from original flow\n                \"include_patterns\": {\n                    \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n                    \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n                    \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n                    \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n                    \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n                    \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \"*.md\", \"*.pdf\"\n                },\n                \"exclude_patterns\": {\n                    \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n                    \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n                    \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n                },\n                \"max_file_size\": 300000,\n                \"output_dir\": \"output\"\n            }\n            \n            # Create the flow\n            flow = create_streamlit_flow()\n            \n            # Run the SmartSearchRepo and FilterRepos nodes\n            with st.spinner(\"Processing query...\"):\n                # Set state to indicate we're waiting for results\n                st.session_state.ui_view = \"results\"\n                \n                # Run the SmartSearch and FilterRepos nodes\n                smart_search = SmartSearchRepo()\n                filter_repos = FilterRepos()\n                \n                # Run nodes sequentially\n                smart_search.run(st.session_state.shared_store)\n                filter_repos.run(st.session_state.shared_store)\n                \n                # Update session state with search results\n                if \"search_results\" in st.session_state.shared_store:\n                    st.session_state.search_results = st.session_state.shared_store[\"search_results\"]\n                \n                # If we have a selected_repo, run the full pipeline\n                if \"selected_repo\" in st.session_state.shared_store:\n                    st.session_state.selected_repo = st.session_state.shared_store[\"selected_repo\"]\n                    with st.spinner(\"Generating tutorial...\"):\n                        st.session_state.ui_view = \"tutorial\"\n                        # Run the full flow\n                        flow.run(st.session_state.shared_store)\n                        \n                        # Get the rendered Markdown files and ZIP path if available\n                        if \"markdown_files\" in st.session_state.shared_store:\n                            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n                        if \"zip_path\" in st.session_state.shared_store:\n                            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n                        \n                        st.session_state.task_completed = True\n    \n    with tab2:\n        st.subheader(\"API Keys\")\n        \n        # GitHub token\n        github_token = st.text_input(\n            \"GitHub Token\",\n            value=st.session_state.github_token,\n            type=\"password\",\n            help=\"Required for private repos or to avoid rate limits\"\n        )\n        st.session_state.github_token = github_token\n        \n        # LLM API keys\n        openai_api_key = st.text_input(\n            \"OpenAI API Key\",\n            value=st.session_state.openai_api_key,\n            type=\"password\"\n        )\n        st.session_state.openai_api_key = openai_api_key\n        \n        anthropic_api_key = st.text_input(\n            \"Anthropic API Key\",\n            value=st.session_state.anthropic_api_key,\n            type=\"password\"\n        )\n        st.session_state.anthropic_api_key = anthropic_api_key\n        \n        gemini_api_key = st.text_input(\n            \"Google Gemini API Key\",\n            value=st.session_state.gemini_api_key,\n            type=\"password\"\n        )\n        st.session_state.gemini_api_key = gemini_api_key\n        \n        # Save API keys to environment\n        if st.button(\"Save API Keys\"):\n            os.environ[\"GITHUB_TOKEN\"] = github_token\n            os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n            os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_api_key\n            os.environ[\"GEMINI_API_KEY\"] = gemini_api_key\n            st.success(\"API keys saved to environment!\")\n    \n    with tab3:\n        st.subheader(\"Model Settings\")\n        \n        # LLM Provider selection\n        provider_selection = st.selectbox(\n            \"LLM Provider\",\n            options=[provider.value for provider in LLMProvider_enum],\n            index=0\n        )\n        st.session_state.provider_selection = provider_selection\n        \n        # Model name based on provider\n        if provider_selection == LLMProvider_enum.OPENAI.value:\n            model_name = st.text_input(\n                \"OpenAI Model\",\n                value=st.session_state.openai_model\n            )\n            st.session_state.openai_model = model_name\n            os.environ[\"OPENAI_MODEL\"] = model_name\n        \n        elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n            model_name = st.text_input(\n                \"Anthropic Model\",\n                value=st.session_state.anthropic_model\n            )\n            st.session_state.anthropic_model = model_name\n            os.environ[\"ANTHROPIC_MODEL\"] = model_name\n        \n        else:  # Gemini\n            model_name = st.text_input(\n                \"Google Gemini Model\",\n                value=st.session_state.gemini_model\n            )\n            st.session_state.gemini_model = model_name\n            os.environ[\"GEMINI_MODEL\"] = model_name\n            \n        # Add save button for model settings\n        if st.button(\"Save Model Settings\"):\n            # Save provider selection to session state\n            st.session_state.provider_selection = provider_selection\n            st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(provider_selection)\n            \n            # Save model name based on provider\n            if provider_selection == LLMProvider_enum.OPENAI.value:\n                os.environ[\"OPENAI_MODEL\"] = model_name\n                st.session_state.openai_model = model_name\n            elif provider_selection == LLMProvider_enum.ANTHROPIC.value:\n                os.environ[\"ANTHROPIC_MODEL\"] = model_name\n                st.session_state.anthropic_model = model_name\n            else:  # Gemini\n                os.environ[\"GEMINI_MODEL\"] = model_name\n                st.session_state.gemini_model = model_name\n                \n            st.success(f\"Model settings saved! Provider: {provider_selection}, Model: {model_name}\")\n\n# --- Main Content ---\ndef display_repo_card(repo: RepoMetadata, index: int):\n    \"\"\"Display a repository card with details and select button.\"\"\"\n    col1, col2 = st.columns([1, 4])\n    \n    with col1:\n        st.image(\"https://github.githubassets.com/favicons/favicon.png\", width=50)\n        st.button(f\"Select\", key=f\"select_{index}\", on_click=select_repository, args=(repo.url,))\n    \n    with col2:\n        st.markdown(f\"### [{repo.full_name}]({repo.url})\")\n        st.markdown(f\"**Description:** {repo.description}\")\n        st.markdown(f\"**Summary:** {repo.readme_summary}\")\n        st.markdown(f\"**Language:** {repo.language} | **Stars:** {repo.stars} | **Forks:** {repo.forks}\")\n\ndef select_repository(repo_url: str):\n    \"\"\"Handle repository selection from search results.\"\"\"\n    st.session_state.selected_repo = repo_url\n    \n    # Make sure we have the shared store initialized\n    if not st.session_state.shared_store:\n        st.session_state.shared_store = {}\n    \n    # Set the selected repository URL in the shared store\n    st.session_state.shared_store[\"selected_repo\"] = repo_url\n    st.session_state.shared_store[\"repo_url\"] = repo_url\n    st.session_state.shared_store[\"local_dir\"] = None\n    \n    # Make sure we have other necessary parameters in the shared store\n    if \"include_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"include_patterns\"] = {\n            \"*.py\", \"*.js\", \"*.jsx\", \"*.ts\", \"*.tsx\", \"*.go\", \"*.java\", \"*.pyi\", \"*.pyx\", \n            \"*.c\", \"*.cc\", \"*.cpp\", \"*.h\", \"*.md\", \"*.rst\", \"Dockerfile\", \n            \"Makefile\", \"*.yaml\", \"*.yml\", \"*.ipynb\", \"*.html\", \"*.css\", \"*.scss\",\n            \"*.json\", \"*.txt\", \"*.csv\", \"*.xml\", \"*.proto\", \"*.sql\", \"*.sh\",\n            \"*.bat\", \"*.ps1\", \"*.rb\", \"*.php\", \"*.swift\", \"*.kotlin\", \"*.dart\",\n            \"*.pl\", \"*.asm\", \"*.asmx\", \"*.gohtml\", \"*.vue\", \"*.twig\", \"*.less\", \".md\"\n        }\n    \n    if \"exclude_patterns\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"exclude_patterns\"] = {\n            \"*test*\", \"tests/*\", \"docs/*\", \"examples/*\", \"v1/*\", \n            \"dist/*\", \"build/*\", \"experimental/*\", \"deprecated/*\", \n            \"legacy/*\", \".git/*\", \".github/*\", \".next/*\", \".vscode/*\", \"obj/*\", \"bin/*\", \"node_modules/*\", \"*.log\"\n        }\n        \n    if \"max_file_size\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"max_file_size\"] = 300000\n        \n    if \"output_dir\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"output_dir\"] = \"output\"\n        \n    if \"github_token\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"github_token\"] = st.session_state.github_token\n        \n    if \"llm_provider\" not in st.session_state.shared_store:\n        st.session_state.shared_store[\"llm_provider\"] = LLMProvider_enum(st.session_state.provider_selection)\n    \n    st.session_state.ui_view = \"tutorial\"\n    \n    # Create and run the flow\n    flow = create_streamlit_flow()\n    with st.spinner(\"Generating tutorial...\"):\n        # Run the full flow\n        flow.run(st.session_state.shared_store)\n        \n        # Get the rendered Markdown files and ZIP path if available\n        if \"markdown_files\" in st.session_state.shared_store:\n            st.session_state.markdown_files = st.session_state.shared_store[\"markdown_files\"]\n        if \"zip_path\" in st.session_state.shared_store:\n            st.session_state.zip_path = st.session_state.shared_store[\"zip_path\"]\n        \n        st.session_state.task_completed = True\n\n# Conditional display based on current view\nif st.session_state.ui_view == \"search\":\n    # Show welcome message and instructions\n    st.title(\"Welcome to **GITHUB** Finder & Documentor\ud83d\udcd6\ud83e\udd13\")\n    st.markdown(\"\"\"\n    **Built by [TEG SINGH TIWANA](https://github.com/tej172)** for _Cloud Assignment 2_:  \n    [GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\n\n    #### Welcome to **LLM Codebase Documentor**, your friendly assistant for auto-generating beginner-friendly tutorials from any codebase! \n    ###### This tool helps you understand codebases by generating detailed tutorials explaining core abstractions, relationships, and code organization.\n    \n    ### Getting Started\n    1. In the **sidebar**, enter one of:\n       - A **GitHub URL** (e.g. `https://github.com/user/repo`)\n       - A **local folder path** on your machine\n       - A **natural language** query (e.g. \"frameworks for UI prototyping\")\n    2. If you chose natural language, adjust **advanced filters** (stars, forks, language, updated date).\n    3. Click **Search** to fetch repositories or **Generate** to build the tutorial.\n    \n    ### Features\n    - \ud83d\udd0d **Smart Search** powered by GPT / Gemini / Claude for effortless keyword extraction  \n    - \ud83d\udcc2 **Local Path** support\u2014point to any folder you've already cloned  \n    - \u2b50 **Advanced Filters**\u2014stars, forks, issues, language, last\u2010updated  \n    - \ud83d\udcc4 **Markdown Preview**\u2014click through `index.md` and each chapter in-app  \n    - \ud83d\udce5 **Downloadable ZIP**\u2014grab your entire tutorial with one click  \n\n    > _Tip_: Make sure your **API keys** (GITHUB_TOKEN, OPENAI_API_KEY, GEMINI_API_KEY, etc.) are set in the sidebar or in your `.env` file.\n    \"\"\")\n    \nelif st.session_state.ui_view == \"results\":\n    # Show search results\n    st.title(\"Search Results\")\n    \n    if st.session_state.search_results:\n        st.markdown(f\"Found {len(st.session_state.search_results)} repositories matching your query.\")\n        for i, repo in enumerate(st.session_state.search_results):\n            with st.container():\n                display_repo_card(repo, i)\n                st.markdown(\"---\")\n    else:\n        st.info(\"No results found. Try different keywords or filters.\")\n        \nelif st.session_state.ui_view == \"tutorial\":\n    # Show the generated tutorial\n    project_name = st.session_state.shared_store.get(\"project_name\", \"Repository\")\n    st.title(f\"Tutorial for {project_name}\")\n    \n    # Download button for ZIP if available\n    if st.session_state.zip_path:\n        with open(st.session_state.zip_path, \"rb\") as f:\n            st.download_button(\n                label=\"Download Tutorial ZIP\",\n                data=f,\n                file_name=f\"{project_name}_tutorial.zip\",\n                mime=\"application/zip\"\n            )\n    \n    # Display the Markdown files\n    if st.session_state.markdown_files:\n        # First show index.md if it exists\n        if \"index.md\" in st.session_state.markdown_files:\n            st.markdown(st.session_state.markdown_files[\"index.md\"])\n            st.markdown(\"---\")\n        \n        # Then show all other files in order\n        for filename in sorted([f for f in st.session_state.markdown_files if f != \"index.md\"]):\n            with st.expander(f\"{filename}\", expanded=True):\n                st.markdown(st.session_state.markdown_files[filename])\n    else:\n        st.info(\"Generating tutorial content... Please wait.\")\n\n# Footer\nst.markdown(\"---\")\nst.markdown(\n    \"LLM Codebase Documentor NTU \u00a9 2025 | \"\n    \"[View on GitHub](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2) |  \"\n    \"Created by [**TEG SINGH TIWANA (U2122816B)**](https://github.com/tej172) | \"\n    \"[Cloud Assignment 2 \u2013 GitHub LLM Codebase Knowledge Building Summarizer](https://github.com/tej172/cloud_indv_assignments/tree/main/ass_2)\"\n)\n\n# Create .env.example file if it doesn't exist\nif not os.path.exists(\".env.example\"):\n    create_env_example() \n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\utils\\call_llm.py ---\nfrom google import genai\nfrom anthropic import Anthropic\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport os\nimport logging\nimport json\nfrom datetime import datetime\nfrom enum import Enum\n\n# Load environment variables from .env file\nload_dotenv()  # Loads from .env file\n\nclass LLMProvider_enum(str, Enum):\n    ANTHROPIC = 'anthropic-claude'\n    GOOGLE = 'google-gemini'\n    OPENAI = 'openai-gpt'\n\n# Configure logging\nlog_directory = os.getenv(\"LOG_DIR\", \"logs\")\nos.makedirs(log_directory, exist_ok=True)\nlog_file = os.path.join(log_directory, f\"llm_calls_{datetime.now().strftime('%Y%m%d')}.log\")\n\n# Set up logger\nlogger = logging.getLogger(\"llm_logger\")\nlogger.setLevel(logging.INFO)\nlogger.propagate = False  # Prevent propagation to root logger\nfile_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\nfile_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\nlogger.addHandler(file_handler)\n\n# Simple cache configuration\ncache_file = \"llm_cache.json\"\n\n# By default, we Google Gemini 2.5 pro gievn recent high bench marks\ndef call_llm(prompt: str, use_cache: bool = True, model: str=LLMProvider_enum.GOOGLE) -> str:\n    print(f\"Calling LLM with model: {model}\")\n    \n    # Log the prompt\n    logger.info(f\"PROMPT: {prompt}\")\n    \n    # Check cache if enabled\n    if use_cache:\n        # Load cache from disk\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n                logger.warning(f\"Loaded and Using cache\")\n            except:\n                logger.warning(f\"Failed to load cache, starting with empty cache\")\n        \n        # Return from cache if exists\n        if prompt in cache:\n            logger.info(f\"RESPONSE: {cache[prompt]}\")\n            return cache[prompt]\n        \n\n    # If not in cache, call the LLM\n    # Call the LLM if not in cache or cache disabled\n    if(model==LLMProvider_enum.GOOGLE):\n        # Use Google Gemini\n        \n        # client = genai.Client(\n        #     vertexai=True, \n        #     # TODO: change to your own project id and location\n        #     project=os.getenv(\"GEMINI_PROJECT_ID\", \"llm-code-explainer\"),\n        #     location=os.getenv(\"GEMINI_LOCATION\", \"us-central1\")\n        # )\n        # \n        client = genai.Client(\n            api_key=os.getenv(\"GEMINI_API_KEY\"),\n        )\n        model = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n        response = client.models.generate_content(\n            model=model,\n            contents=[prompt]\n        )\n        response_text = response.text\n    elif(model==LLMProvider_enum.ANTHROPIC):\n        # Use Anthropic Claude\n        client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n        response = client.messages.create(\n            # Use Anthropic Claude 3.7 Sonnet Extended Thinking\n            model=os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-20250219\"), \n            max_tokens=15000, #If have extra api budget, can increase this to 21000\n            thinking={\n                \"type\": \"enabled\", \n                \"budget_tokens\": 10000 # If have extra api budget, can increase this to 20000\n            },\n            messages=[\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        )\n        response_text = response.content[1].text\n    else: # Assume OpenAI\n        # Use the default LLM, which is OpenAI (Use OpenAI o1/4o/gpt-4o-mini) depedning on the model & api budget\n        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n        response = client.chat.completions.create(\n            model=os.environ.get(\"OPENAI_MODEL\", \"o4-mini\"),\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            response_format={\n                \"type\": \"text\"\n            },\n            reasoning_effort=\"medium\",\n            store=False\n        )\n        response_text = response.choices[0].message.content\n\n    # Log the response\n    logger.info(f\"RESPONSE: {response_text}\")\n    \n    # Update cache if enabled\n    if use_cache:\n        # Load cache again to avoid overwrites\n        cache = {}\n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    cache = json.load(f)\n            except:\n                pass\n        \n        # Add to cache and save\n        cache[prompt] = response_text\n        try:\n            with open(cache_file, 'w') as f:\n                json.dump(cache, f)\n        except Exception as e:\n            logger.error(f\"Failed to save cache: {e}\")\n    \n    return response_text\n\nif __name__ == \"__main__\":\n    test_prompt = \"Hello, how are you? What is the exact model are you using?\"\n    test_prompt2 = \"Hello, how bug is your model that are you using?\"\n    \n    # First call - should hit the API and return vals\n    print(\"Making call 1...\")\n    # LLMProvider_enum('anthropic-claude'),  LLMProvider_enum('google-gemini') & 'openai-gpt': LLMProvider_enum('openai-gpt') /or/\n    # LLMProvider_enum.ANTHROPIC , LLMProvider_enum.GOOGLE & openai: LLMProvider_enum.OPENAI\n    response1 = call_llm(test_prompt, use_cache=False, model=LLMProvider_enum('openai-gpt'))\n    print(f\"Response: {response1}\\n======================\")\n\n    # Second call - should hit the cache and return vals\n    print(\"Making call 2...\")\n    response1 = call_llm(test_prompt, use_cache=True, model=LLMProvider_enum.GOOGLE)\n    print(f\"Response: {response1}\")\n    \n\n\n--- File: LLM-CODEBASE-DOCUMENTOR-main\\utils\\env_loader.py ---\nimport os\nimport dotenv\nimport logging\nfrom typing import Dict, Any, Optional\n\ndef load_env_vars(env_file_path: str = \".env\") -> Dict[str, str]:\n    \"\"\"\n    Load environment variables from .env file.\n    \n    Args:\n        env_file_path (str): Path to the .env file\n        \n    Returns:\n        Dict[str, str]: Dictionary of environment variables\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(env_file_path):\n        logging.warning(f\".env file not found at {env_file_path}\")\n        return {}\n    \n    # Load the .env file\n    dotenv.load_dotenv(env_file_path)\n    \n    # Return a dictionary of the loaded variables\n    env_vars = {}\n    try:\n        with open(env_file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                    \n                try:\n                    key, value = line.split('=', 1)\n                    env_vars[key.strip()] = value.strip().strip('\"\\'')\n                except ValueError:\n                    # Skip lines that don't have key=value format\n                    continue\n    except Exception as e:\n        logging.error(f\"Error reading .env file: {e}\")\n    \n    return env_vars\n\ndef get_api_keys() -> Dict[str, Optional[str]]:\n    \"\"\"\n    Get API keys from environment variables.\n    \n    Returns:\n        Dict[str, Optional[str]]: Dictionary of API keys\n    \"\"\"\n    return {\n        \"github_token\": os.environ.get(\"GITHUB_TOKEN\"),\n        \"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n        \"anthropic_api_key\": os.environ.get(\"ANTHROPIC_API_KEY\"),\n        \"gemini_api_key\": os.environ.get(\"GEMINI_API_KEY\")\n    }\n\ndef get_model_names() -> Dict[str, str]:\n    \"\"\"\n    Get model names from environment variables.\n    \n    Returns:\n        Dict[str, str]: Dictionary of model names\n    \"\"\"\n    return {\n        \"openai_model\": os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n        \"anthropic_model\": os.environ.get(\"ANTHROPIC_MODEL\", \"claude-3-7-sonnet-latest\"),\n        \"gemini_model\": os.environ.get(\"GEMINI_MODEL\", \"gemini-2.5-pro-exp-03-25\")\n    }\n\ndef create_env_example() -> None:\n    \"\"\"\n    Create a .env.example file with placeholders.\n    \"\"\"\n    example_content = \"\"\"# API Keys\nGITHUB_TOKEN=your_github_token_here\nOPENAI_API_KEY=your_openai_key_here\nANTHROPIC_API_KEY=your_anthropic_key_here\nGEMINI_API_KEY=your_gemini_key_here\n\n# Model Names\nGEMINI_MODEL=gemini-2.5-pro-exp-03-25\nOPENAI_MODEL=gpt-4o-mini\nANTHROPIC_MODEL=claude-3-7-sonnet-latest\n\"\"\"\n    \n    try:\n        with open(\".env.example\", 'w') as f:\n            f.write(example_content)\n        print(\"Created .env.example file\")\n    except Exception as e:\n        print(f\"Error creating .env.example file: {e}\")\n\nif __name__ == \"__main__\":\n    # Test the environment loading\n    env_vars = load_env_vars()\n    print(\"Loaded environment variables:\", list(env_vars.keys()))\n    \n    api_keys = get_api_keys()\n    print(\"API Keys available:\", [k for k, v in api_keys.items() if v])\n    \n    # Create example .env file\n    create_env_example() \n\n        Instructions for the chapter:\n        - Start with a clear heading (e.g., `# Chapter 8: Configuration & Environment Management`).\n\n        - If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n        - Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n        - If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n        - Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n        - Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n        - Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n        `participant QP as Query Processing`\n\n        - Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n        - IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n        - Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n        - Heavily use analogies and examples throughout to help beginners understand.\n\n        - End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n        - Ensure the tone is welcoming and easy for a newcomer to understand.\n\n        - Output *only* the Markdown content for this chapter.\n\n        Now, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n        ": "# Chapter 8: Configuration & Environment Management\n\nWelcome to the final chapter! In [Chapter 7: Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md), we saw how the project provides a user-friendly web interface with Streamlit, allowing users to search for repositories and view the generated tutorials right in their browser.\n\nThroughout this tutorial, we've mentioned things like command-line arguments (`--repo`, `--model`), API keys, and default file filters. How does the tool keep track of all these settings? How does it know which LLM to use, which files to ignore, or how to access your private GitHub repository?\n\nThis chapter explores **Configuration & Environment Management** \u2013 the way the tool handles all your instructions and necessary secrets.\n\n**Goal:** Understand how the tool receives user inputs (from the command line or the web UI), manages sensitive information like API keys securely, and uses default settings when you don't provide specific instructions.\n\n## The Problem: Telling the Tool What To Do (and How) \u2699\ufe0f\n\nImagine you want our `LLM-CODEBASE-DOCUMENTOR` to do a specific job:\n*   Analyze a **private** GitHub repository (not a public one).\n*   Use the **Anthropic Claude** LLM instead of the default Google Gemini.\n*   Make sure to **exclude** all files inside any `legacy/` folders.\n\nHow do you communicate all these instructions and the necessary access credentials (like your GitHub token) to the tool? If you couldn't configure it, the tool would be very limited!\n\n**Use Case:** A developer needs to generate documentation for their company's private project hosted on GitHub. They prefer Claude's writing style and want to ignore old code in `legacy/` directories. They need a way to provide the repository URL, their GitHub token, specify the 'anthropic-claude' model, and add an exclusion pattern.\n\n## The Solution: Settings, Keys, and Defaults\n\nOur tool manages these instructions through several mechanisms:\n\n1.  **User Inputs:** You provide specific instructions when you run the tool.\n2.  **API Keys & Secrets:** You provide necessary \"passwords\" (API keys) for accessing external services.\n3.  **Default Settings:** The tool has sensible defaults for things you *don't* specify.\n\nLet's look at how these work.\n\n### 1. User Inputs: Giving Instructions\n\nYou can interact with the tool in two main ways, and both allow you to provide settings:\n\n**A. Command-Line Arguments (CLI)**\n\nWhen you run the tool from your terminal using `python main.py`, you can add flags (arguments) to specify options. This is handled using Python's `argparse` library in `main.py`.\n\n*   **Analogy:** It's like ordering food by telling the waiter exactly what you want: \"I'll have the pasta (`--repo URL`), make it spicy (`--model claude`), and no onions (`--exclude 'legacy/*'`).\"\n\n*   **Example Command (for our use case):**\n\n    ```bash\n    python main.py \\\n      --repo https://github.com/your-company/private-project \\\n      --token YOUR_GITHUB_TOKEN_HERE \\\n      --model anthropic-claude \\\n      --exclude \"legacy/*\"\n    ```\n\n*   **Code Snippet (`main.py` - Setting up arguments):** This code defines the command-line options.\n\n    ```python\n    # In main.py\n    import argparse\n    from utils.call_llm import LLMProvider_enum\n\n    parser = argparse.ArgumentParser(...)\n\n    # Source (Repo URL or Local Dir)\n    source_group = parser.add_mutually_exclusive_group(required=True)\n    source_group.add_argument(\"--repo\", help=\"GitHub repository URL...\")\n    source_group.add_argument(\"--dir\", help=\"Local directory path...\")\n\n    # LLM Model Choice\n    parser.add_argument(\"--model\", type=str,\n                        choices=[e.value for e in LLMProvider_enum], # Defines allowed values\n                        default=LLMProvider_enum.GOOGLE.value, # Sets 'google-gemini' as default\n                        help=\"LLM provider to use...\")\n\n    # GitHub Token\n    parser.add_argument(\"-t\", \"--token\", help=\"GitHub Personal Access Token...\")\n\n    # Exclude Pattern(s)\n    parser.add_argument(\"-e\", \"--exclude\", nargs=\"+\", # Allows multiple values\n                        help=\"File/directory patterns to ignore...\")\n    # ... other arguments like --include, --max-size ...\n    ```\n    *Explanation:* This code sets up the command-line interface. `add_argument` defines each flag (like `--repo`, `--model`). `choices` restricts allowed values for `--model`, `default` sets a fallback, and `nargs=\"+` allows multiple values for `--exclude`.\n\n**B. Streamlit Web UI Inputs**\n\nAs we saw in [Chapter 7: Streamlit UI & Search Flow](07_streamlit_ui___search_flow.md), the web interface uses Streamlit widgets (`st.text_input`, `st.selectbox`, etc.) in `streamlit_app.py` to gather the same information visually.\n\n*   **Analogy:** It's like ordering food using a tablet menu with dropdowns and text boxes. You tap on \"Pasta\", select \"Spicy\" from a dropdown, and type \"No onions\" in a special instructions box.\n\n*   **Code Snippet (`streamlit_app.py` - UI elements):**\n\n    ```python\n    # In streamlit_app.py\n    import streamlit as st\n    from utils.call_llm import LLMProvider_enum\n\n    # Input for Repo URL / Query\n    query_input = st.text_input(\"Enter GitHub URL, local path, or query:\")\n\n    # Dropdown for LLM Provider\n    provider_selection = st.selectbox(\n        \"LLM Provider\",\n        options=[provider.value for provider in LLMProvider_enum], # 'google-gemini', etc.\n        index=0 # Default selection\n    )\n    st.session_state.provider_selection = provider_selection # Store choice\n\n    # Password input for GitHub Token\n    github_token = st.text_input(\n        \"GitHub Token\",\n        type=\"password\", # Hides the input\n        value=st.session_state.get(\"github_token\", \"\") # Pre-fill if known\n    )\n    st.session_state.github_token = github_token # Store token\n    # ... similar inputs for filters like exclude patterns ...\n    ```\n    *Explanation:* This code creates the interactive elements in the Streamlit sidebar. `st.text_input` creates a text box, `st.selectbox` creates a dropdown, and `type=\"password\"` hides sensitive input like the token. The user's choices are stored in `st.session_state`.\n\n### 2. Managing Secrets: API Keys & `.env` Files\n\nSome settings are sensitive, like API keys needed for:\n*   **GitHub Token:** Accessing private repositories or avoiding rate limits on public ones.\n*   **LLM API Keys:** Accessing services like Google Gemini, Anthropic Claude, or OpenAI GPT (these usually cost money to use).\n\nIt's **very important** not to write these secrets directly into your code or commit them to version control (like Git). How do we handle them securely?\n\n**Methods:**\n\n1.  **Command Line Arguments:** You can pass the GitHub token via `--token` (as shown above). *Less recommended for sensitive keys as they might appear in your shell history.*\n2.  **Streamlit UI Inputs:** The web UI provides password fields (as shown above). *Better, but still requires manual input.*\n3.  **Environment Variables (Recommended):** This is the standard, secure way. You store the keys in your operating system's environment. Our tool can then read them.\n4.  **`.env` Files (Most Convenient & Secure):** A special file named `.env` in your project directory where you list your secrets. This file should **NEVER** be committed to Git (it's usually listed in `.gitignore`). Our tool uses the `python-dotenv` library to automatically load variables from this file into the environment when it starts.\n\n*   **Analogy:** Think of `.env` as a locked drawer next to your workstation. You put your secret keys (API keys) in the drawer. Only you (and the tools running on your machine) can access them. Your main recipe book (your code) just says \"get the key from the drawer\" instead of writing the key down.\n\n*   **Example `.env` file:**\n\n    ```dotenv\n    # .env - Keep this file secure and DO NOT commit it!\n    GITHUB_TOKEN=ghp_YourSecretGitHubTokenValue...\n    OPENAI_API_KEY=sk-YourSecretOpenAIKeyValue...\n    ANTHROPIC_API_KEY=sk-ant-YourSecretAnthropicKeyValue...\n    GEMINI_API_KEY=AIzaSyYourSecretGeminiKeyValue...\n\n    # You can also store model names here\n    # GEMINI_MODEL=gemini-1.5-flash\n    ```\n\n*   **Code Snippet (`main.py` - Loading `.env`):**\n\n    ```python\n    # In main.py (at the top)\n    import dotenv\n    import os\n\n    # Load environment variables from .env file if it exists\n    dotenv.load_dotenv()\n\n    # Later, when getting the token:\n    # Try the command-line arg first, then the environment variable (loaded from .env)\n    github_token = args.token or os.environ.get('GITHUB_TOKEN')\n    ```\n    *Explanation:* `dotenv.load_dotenv()` automatically finds and loads the `.env` file. `os.environ.get('GITHUB_TOKEN')` then retrieves the value, whether it was set system-wide or loaded from `.env`. The code prioritizes the command-line argument (`args.token`) if provided.\n\n*   **Utility (`utils/env_loader.py`):** This file contains helper functions like `get_api_keys()` which centralize the logic for retrieving keys from the environment (after `dotenv` has potentially loaded them).\n\n    ```python\n    # In utils/env_loader.py\n    import os\n\n    def get_api_keys() -> dict:\n        \"\"\"Get API keys from environment variables.\"\"\"\n        return {\n            \"github_token\": os.environ.get(\"GITHUB_TOKEN\"),\n            \"openai_api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n            \"anthropic_api_key\": os.environ.get(\"ANTHROPIC_API_KEY\"),\n            \"gemini_api_key\": os.environ.get(\"GEMINI_API_KEY\")\n        }\n    ```\n    *Explanation:* This function simply fetches the standard API key names from the environment variables. The Streamlit app uses this to pre-populate API key fields.\n\n### 3. Default Settings: Sensible Fallbacks\n\nWhat if you don't specify any include or exclude patterns? Or don't set a maximum file size? The tool won't just crash; it has built-in **defaults**.\n\n*   **Analogy:** If you order pizza without specifying toppings, you'll likely get a plain cheese pizza (the default), not an empty box.\n\n*   **Code Snippet (`main.py` - Defining Defaults):**\n\n    ```python\n    # In main.py\n\n    # Default file types to INCLUDE if --include is not used\n    DEFAULT_INCLUDE_PATTERNS = { \"*.py\", \"*.js\", \"*.md\", /* ... many more ... */ }\n\n    # Default file/folder patterns to EXCLUDE if --exclude is not used\n    DEFAULT_EXCLUDE_PATTERNS = { \"*test*\", \"tests/*\", \".git/*\", /* ... */ }\n\n    # Default max file size if --max-size is not used\n    DEFAULT_MAX_SIZE = 300000 # ~300KB\n\n    # --- Later, inside main() when creating 'shared' dict ---\n    args = parser.parse_args() # Get command-line arguments\n\n    shared = {\n        # ... other settings ...\n        \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n        \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n        \"max_file_size\": args.max_size if args.max_size is not None else DEFAULT_MAX_SIZE,\n        # ...\n    }\n    ```\n    *Explanation:* Default sets (`DEFAULT_INCLUDE_PATTERNS`, `DEFAULT_EXCLUDE_PATTERNS`) and values (`DEFAULT_MAX_SIZE`) are defined. When populating the `shared` dictionary, the code checks if the user provided the corresponding argument (`args.include`, `args.exclude`, `args.max_size`). If they did, the user's value is used; otherwise, the default is used.\n\n## Putting It All Together: Populating the `shared` Store\n\nRegardless of whether you use the CLI (`main.py`) or the Streamlit UI (`streamlit_app.py`), the first step is to gather all these configuration settings:\n*   Direct user inputs (CLI args or UI widgets)\n*   Secrets (from args, UI, or `.env`/environment variables)\n*   Defaults (for settings the user didn't specify)\n\nAll these pieces are collected and placed into the central **`shared` dictionary** we learned about in [Chapter 6: Shared Data Store](06_shared_data_store.md). This dictionary is then passed to the PocketFlow workflow, ensuring all subsequent Nodes have access to the correct configuration.\n\n**Simplified Flow:**\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant Interface as CLI / Streamlit UI\n    participant Script as main.py / streamlit_app.py\n    participant EnvFile as .env File\n    participant Shared as Shared Dictionary\n    participant Workflow as PocketFlow Engine\n\n    User->>Interface: Provide inputs (URL, model, filters, maybe token)\n    Interface->>Script: Pass user inputs\n    Script->>EnvFile: Load API keys (dotenv.load_dotenv())\n    Script->>Script: Get Env Vars (os.environ.get) & Defaults\n    Script->>Shared: Populate initial shared dict (User inputs + Env vars + Defaults)\n    Script->>Workflow: Start flow.run(shared)\n    Workflow->>Nodes: Pass shared dict to Nodes...\n```\n\n**Code Snippet (`main.py` - Initializing `shared`):**\n\n```python\n# Simplified from main.py\n\n# ... (argparse setup and dotenv.load_dotenv() called earlier) ...\nargs = parser.parse_args()\ngithub_token = args.token or os.environ.get('GITHUB_TOKEN')\n\n# Initialize the shared dictionary\nshared = {\n    # From CLI Args\n    \"repo_url\": args.repo,\n    \"local_dir\": args.dir,\n    \"project_name\": args.name, # Can be None\n    \"model_used\": args.model, # e.g., 'google-gemini'\n    \"llm_provider\": LLMProvider_enum(args.model), # Enum version\n\n    # From Arg or Env Var\n    \"github_token\": github_token,\n\n    # From Args or Defaults\n    \"include_patterns\": set(args.include) if args.include else DEFAULT_INCLUDE_PATTERNS,\n    \"exclude_patterns\": set(args.exclude) if args.exclude else DEFAULT_EXCLUDE_PATTERNS,\n    \"max_file_size\": args.max_size, # Argparse handles default value here\n\n    # From other args/defaults\n    \"output_dir\": args.output,\n\n    # Placeholders for Node outputs\n    \"files\": [], \"abstractions\": [], # ... etc\n}\n\n# ... then call tutorial_flow.run(shared) ...\n```\n*Explanation:* This dictionary brings together values from command-line arguments (`args.repo`), potentially overridden environment variables (`github_token`), and defaults (`include_patterns` if `args.include` is empty). This `shared` dictionary becomes the single source of truth for configuration throughout the workflow run. The Streamlit app (`streamlit_app.py`) performs a similar consolidation based on `st.session_state` values before running its flow.\n\n## Conclusion\n\nIn this final chapter, we learned how `LLM-CODEBASE-DOCUMENTOR` manages **Configuration and Environment Settings**:\n\n*   It accepts user instructions via **command-line arguments** (`main.py`) or a **Streamlit web UI** (`streamlit_app.py`).\n*   It handles sensitive **API keys** securely using environment variables, ideally loaded from a `.env` file (using `python-dotenv`).\n*   It provides sensible **default settings** (like file include/exclude patterns) when users don't specify their own.\n*   All these configuration details are gathered and centralized into the `shared` dictionary ([Chapter 6: Shared Data Store](06_shared_data_store.md)) before the main processing workflow begins.\n\nThis careful management of settings allows you to customize the tool's behavior, provide necessary credentials securely, and ensures the different parts of the system work together seamlessly based on your requirements.\n\nThank you for following along with this tutorial series! We hope you now have a good understanding of how the `LLM-CODEBASE-DOCUMENTOR` fetches code, analyzes it with LLMs using an organized workflow, generates tutorial content, and manages its configuration. Happy documenting!"}